<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING 1 SALSA: Spatial Cue-Augmented Log-Spectrogram Features for Polyphonic Sound Event Localization and Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thi</forename><surname>Ngoc</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tho</forename><surname>Nguyen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Karn</forename><forename type="middle">N</forename><surname>Watcharasupat</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Khanh</forename><surname>Nguyen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Douglas</forename><forename type="middle">L</forename><surname>Jones</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Woon-Seng</forename><surname>Gan</surname></persName>
						</author>
						<title level="a" type="main">IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING 1 SALSA: Spatial Cue-Augmented Log-Spectrogram Features for Polyphonic Sound Event Localization and Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-deep learning</term>
					<term>feature extraction</term>
					<term>microphone array</term>
					<term>spatial cues</term>
					<term>sound event localization and detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sound event localization and detection (SELD) consists of two subtasks, which are sound event detection and direction-of-arrival estimation. While sound event detection mainly relies on time-frequency patterns to distinguish different sound classes, direction-of-arrival estimation uses amplitude and/or phase differences between microphones to estimate source directions. As a result, it is often difficult to jointly optimize these two subtasks. We propose a novel feature called Spatial cue-Augmented Log-SpectrogrAm (SALSA) with exact time-frequency mapping between the signal power and the source directional cues, which is crucial for resolving overlapping sound sources. The SALSA feature consists of multichannel log-spectrograms stacked along with the normalized principal eigenvector of the spatial covariance matrix at each corresponding time-frequency bin. Depending on the microphone array format, the principal eigenvector can be normalized differently to extract amplitude and/or phase differences between the microphones. As a result, SALSA features are applicable for different microphone array formats such as first-order ambisonics (FOA) and multichannel microphone array (MIC). Experimental results on the TAU-NIGENS Spatial Sound Events 2021 dataset with directional interferences showed that SALSA features outperformed other state-of-the-art features. Specifically, the use of SALSA features in the FOA format increased the F1 score and localization recall by 6 % each, compared to the multichannel log-mel spectrograms with intensity vectors. For the MIC format, using SALSA features increased F1 score and localization recall by 16 % and 7 %, respectively, compared to using multichannel logmel spectrograms with generalized cross-correlation spectra. ). estimating the directions of arrival (DOA), the onsets, and the offsets of detected sound events <ref type="bibr" target="#b4">[5]</ref>. Because of a need for source localization, SELD typically requires multichannel audio inputs from a microphone array, which has several formats in current use, such as first-order ambisonics (FOA) and multichannel microphone array (MIC).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S OUND event localization and detection (SELD) has many applications in urban sound sensing <ref type="bibr" target="#b0">[1]</ref>, wildlife monitoring <ref type="bibr" target="#b1">[2]</ref>, surveillance <ref type="bibr" target="#b2">[3]</ref>, autonomous driving, and robotics <ref type="bibr" target="#b3">[4]</ref>. SELD is an emerging research field that unifies the tasks of sound event detection (SED) and direction-of-arrival estimation (DOAE) by jointly recognizing the sound classes, and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Existing methods</head><p>Over the past few years, there have been many major developments for SELD in the areas of data augmentation, feature engineering, model architectures, and output formats. In 2015, an early monophonic SELD work by Hirvonen <ref type="bibr" target="#b5">[6]</ref> formulated SELD as a classification task. In 2018, Adavanne et al. <ref type="bibr" target="#b4">[5]</ref> pioneered a seminal polyphonic SELD work that used an end-to-end convolutional recurrent neural network (CRNN), SELDnet, to jointly detect sound events and estimate the corresponding DOAs. In 2019, SELD task was introduced in the Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE). Cao et al. <ref type="bibr" target="#b6">[7]</ref> proposed a two-stage strategy by training separate SED and DOAE models. Mazzon et al. <ref type="bibr" target="#b7">[8]</ref> proposed a spatial augmentation method by swapping channels of FOA format. Nguyen et al. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> explored a hybrid approach called a Sequence Matching Network (SMN) that matched the SED and DOAE output sequences using a bidirectional gated recurrent unit (BiGRU).</p><p>In 2020, moving sound sources were introduced in the DCASE SELD Challenge. Cao et al. <ref type="bibr" target="#b10">[11]</ref> proposed Event Independent Network (EIN) that used soft parameter sharing between the SED and DOAE encoder branches and output track-wise predictions. An improved version of this network, EINv2, replaced the biGRUs with multi-head self-attention (MHSA) <ref type="bibr" target="#b11">[12]</ref>. Sato et al. <ref type="bibr" target="#b12">[13]</ref> designed a CRNN that is invariant to rotation, scale, and time translation for FOA signals. Phan et al. <ref type="bibr" target="#b13">[14]</ref> formulated SELD as regression problems for both SED and DOAE to improve training convergence. Wang et al. <ref type="bibr" target="#b14">[15]</ref> focused on several data augmentation methods to overcome the data sparsity problem in SELD. Shimada et al. <ref type="bibr" target="#b15">[16]</ref> unified SED and DOAE losses into one regression loss using a representation technique called Activity-Coupled Cartesian Direction of Arrival (ACCDOA). In 2021, unknown interferences were introduced in the DCASE SELD challenge. Lee et al. <ref type="bibr" target="#b16">[17]</ref> enhanced EINv2 by adding crossmodal attention between the SED and DOAE branches.   <ref type="bibr" target="#b6">[7]</ref> FOA/MIC Log-mel spectrograms, GCC-PHAT Two-stage CRNNs class-wise Nguyen et al. <ref type="bibr" target="#b8">[9]</ref> FOA Log-mel spectrograms, directional SS histograms Sequence matching CRNN track-wise Xue et al. <ref type="bibr" target="#b17">[18]</ref> MIC Log-mel spectrograms, IV, pair-wise phase differences Modified two-stage CRNNs class-wise Cao et al. <ref type="bibr" target="#b11">[12]</ref> FOA Log-mel spectrograms, IV EINv2 track-wise Shimada et al. <ref type="bibr" target="#b15">[16]</ref> FOA Linear amplitude spectrograms, IPD CRNN with D3Net class-wise Sato et al. <ref type="bibr" target="#b12">[13]</ref> FOA Complex spectrograms Invariant CRNN class-wise Phan et al. <ref type="bibr" target="#b13">[14]</ref> FOA/MIC Log-mel spectrograms, IV, GCC-PHAT CRNN with self attention class-wise Park et al. <ref type="bibr" target="#b18">[19]</ref> FOA Log-mel spectrograms, IV, harmonic percussive separation CRNN with feature pyramid class-wise Emmanuel et al. <ref type="bibr" target="#b19">[20]</ref> FOA Constant-Q spectrograms, log-mel spectrograms, IV Multi-scale network with MHSA track-wise Lee et al. <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Input features for SELD</head><p>In this paper, we focus on input features for SELD. When SELDnet was first introduced, it was trained on multichannel magnitude and phase spectrograms <ref type="bibr" target="#b4">[5]</ref>. Subsequently, different features, such as multichannel log-spectrograms and intensity vector (IV) for the FOA format, and generalized crosscorrelation with phase transform (GCC-PHAT) for the MIC format in the mel scale were shown to be more effective for SELD <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>.</p><p>Due to the smaller dimension size and stronger emphasis on the lower frequency bands, where signal contents are mostly populated, the mel frequency scale has been used more frequently than the linear frequency scale for SELD. However, combining the IV or GCC-PHAT features with the mel spectrograms is not trivial and the implicit DOA information stored in the former features are often compromised. In practice, the IVs are also passed through the mel filters which merge DOA cues in different narrow bands into one mel band, making it more difficult to resolve different DOAs in multi-source scenarios. Likewise, in order to stack the GCC-PHAT with the mel spectrograms, longer time-lags on the GCC-PHAT have to be truncated. Since the linear scale has the advantage of preserving the directional information at each frequency band, several works have attempted to use spectrogram, inter-channel phase differences (IPD), and IVs in linear scale <ref type="bibr" target="#b15">[16]</ref> or the constant-Q scale <ref type="bibr" target="#b19">[20]</ref>. However, there is lack of experimental results that directly compare these features over different scales.</p><p>Referring to <ref type="table" target="#tab_1">Table I</ref>, more SELD algorithms have been developed for the FOA format compared to the MIC format, even though the MIC format is more common in practice. The baselines for three DCASE SELD challenges so far have indicated that using FOA inputs performs slightly better than that with MIC inputs <ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref>. In addition, it is more straightforward to stack IVs with the spectrograms in the FOA format compared to stacking GCC-PHAT with spectrograms. When IVs are stacked with spectrograms, there is a direct frequency correspondence between the IVs and the spectrograms. This frequency correspondence is crucial for networks to associate the sound classes and the DOAs of multiple sound events, where signals of different sound sources are often distributed differently along the frequency dimension. On the other hand, the time-lag dimension of the GCC-PHAT features does not have a local linear one-to-one mapping with the frequency dimension of the spectrograms. As a result, all of the DOA information is aggregated at the frame level, making it difficult to assign correct DOAs to different sound events. Furthermore, when there are multiple sound sources, GCC-PHAT features are known to be noisy, and the directional cues at overlapping TF bins of IVs are merged. In order to solve SELD more effectively in noisy, reverberant, and multi-source scenarios, a better feature is needed for both audio formats, but especially for the MIC format where feature engineering has largely been lacking compared to the FOA format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Our Contributions</head><p>We propose a novel feature for SELD called Spatial Cue-Augmented Log-Spectrogram (SALSA) with exact spectrotemporal mapping between the signal power and the source DOA for both FOA and MIC formats. The feature consists of multichannel log-magnitude linear-frequency spectrograms stacked with a normalized version of the principal eigenvector of the spatial covariance matrix at each TF bin on the spectrograms. The principal eigenvector is normalized such that it represents the inter-channel intensity difference (IID) for the FOA format, and/or inter-channel phase difference (IPD) for the MIC format.</p><p>To further improve the performance, only eigenvectors from approximately single-source TF bins are included in the features since these directional cues are less noisy. A TF bin is considered a single-source bin when it contains energy mostly from only one source <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. We evaluated the effectiveness of the proposed feature on both the FOA and the MIC formats using the TAU-NIGENS Spatial Sound Events (TNSSE) 2021 dataset used in DCASE 2021 SELD Challenge. Experimental results showed that the SALSA feature outperformed, for the FOA format, both mel-and linear-frequency log-magnitude spectrograms with IV, and for the MIC format, the logmagnitude spectrogram with GCC-PHAT.</p><p>In addition, SALSA features bridged the performance gap between the FOA and the MIC formats, and achieved the state-of-the-art performance for a single (non-ensemble) model on the TNSSE 2021 development dataset for both formats. Similarly, when evaluated on the TNSSE 2020 dataset, SALSA also achieved the top performance for a single model for both formats on both the development and the evaluation datasets. Our ensemble model trained on an early version of SALSA features ranked second in the team category of the DCASE 2021 SELD challenge <ref type="bibr" target="#b28">[29]</ref>.</p><p>Our paper offers several contributions, as follows: 1) a novel and effective feature for SELD that works for both FOA and MIC formats, 2) an improvement to the proposed feature by utilizing signal processing-based methods to select single-source TF bins, 3) a comprehensive analysis of feature importance of each components in SALSA for SELD, and, 4) an extensive ablation study of different data augmentation methods for the newly proposed SALSA feature, as well as for the log-magnitude spectrograms with IV and GCC-PHAT in both linear-and mel-frequency scales. The rest of the paper is organized as follows. Section II presents the proposed SALSA features for both the FOA and the MIC formats. Section III briefly describes common SELD features used as benchmarks. Section IV presents the network architecture employed in all of the experiments. Section V elaborates the experimental settings. Section VI presents the experimental results and discussion with extensive ablation study. Finally, we conclude the paper in Section VII. The source code for reproducing our work can be found at https://github.com/thomeou/SALSA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. SPATIAL CUE-AUGMENTED LOG-SPECTROGRAM</head><p>FEATURES FOR SELD The proposed SALSA features consist of two major components: multichannel log-linear spectrograms and normalized principal eigenvectors. For the rest of this paper, spectrograms refer to multichannel spectrograms unless otherwise stated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Signal Model</head><p>Let M be the number of microphones and L be the number of sound sources. The short-time Fourier transform (STFT) signal observed by an M -channel microphone array of arbitrary geometry in the TF domain is given by</p><formula xml:id="formula_0">X(t, f ) = L i=1 S i (t, f )H(f, ? i , ? i ) + V(t, f ) ? C M ,<label>(1)</label></formula><p>where t and f are time and frequency indices, respectively; S i is the ith source signal; H(f, ? i , ? i ) is the frequency-domain steering vector corresponding to the DOA (? i , ? i ) of the ith source, where ? i and ? i are the azimuth and elevation angles, respectively; and V is the noise vector. For moving sources, ? i = ? i (t) and ? i = ? i (t) are functions of time. For brevity, the time variable is omitted in ? i and ? i for some equations. Note that Eq. (1) is applicable for TF bins that have relatively low reverberation, which can be absorbed into the V term. TF bins with relative high direct-to-reverberant energy ratios would be preferably excluded from the estimation in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multichannel log-linear spectrograms</head><p>The log-linear spectrograms are computed from the complex spectrograms X(t, f ) by</p><formula xml:id="formula_1">LINSPEC(t, f ) = log |X(t, f )| 2 ? R M ?T ?F ,<label>(2)</label></formula><p>where |?|, is the elementwise complex modulus, T is the number of time frames and F is the number of frequency bins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Normalized principal eigenvectors</head><p>Assuming the signal and noise are zero-mean and uncorrelated, the true covariance matrix, R(t, f ) ? C M ?M , is a linear combination of rank-one outer products of the steering vectors weighted by signal powers ? 2 i (t, f ) of the ith source at the TF bin (t, f ), that is,</p><formula xml:id="formula_2">R(t, f ) = E[X(t, f )X H (t, f )] (3) = L i=1 ? 2 i (t, f )H(f, ? i , ? i )H H (f, ? i , ? i ) + R n (t, f ),<label>(4)</label></formula><p>where R n (t, f ) is the noise covariance matrix, and (?) H denotes the Hermitian transpose. Note that although the reverberation is also absorbed into the noise vector, the uncorrelated noise assumption can generally hold if the reverberation level is sufficiently low. In practice, under the assumption that the sources are slow-moving within a small time window, Eq. (3) can be approximated usin?</p><formula xml:id="formula_3">R(t, f ) = 1 2T r + 1 Tr ? =?Tr X(t + ?, f )X H (t + ?, f ) (5)</formula><p>where 2T r +1 is the window size. In this work, we use T r = 3.</p><p>Eq. (4) shows that at single-source TF bins, where only one sound source is dominant over other sources and reverberation, the theoretical steering vector H(f, ?, ?) can be approximated by the principal eigenvector U(t, f ) of the covariance matrix, a technique previously utilized in multichannel speech separation <ref type="bibr" target="#b29">[30]</ref>, as well as in our previous works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. Therefore, we can reliably extract directional cues from these principal eigenvectors at these bins. For TF bins which are not single-source, the values of the directional cues can be set to a predefined default value such as zero. In the next sections, we elaborate on how to normalize the principal eigenvectors to extract directional cues, which are encoded in the IID and IPD for FOA arrays and far-field microphone arrays, respectively. 1) Eigenvector-based intensity vector for FOA arrays: FOA arrays have four channels and the directional cues are encoded in the IID. A typical steering vector for an FOA array can be defined by</p><formula xml:id="formula_4">H FOA (t, ?, ?) = ? ? ? ? H W (t, ?, ?) H X (t, ?, ?) H Y (t, ?, ?) H Z (t, ?, ?) ? ? ? ? = ? ? ? ? 1 cos(?) cos(?) sin(?) cos(?) sin(?) ? ? ? ? ? R 4 ,<label>(6)</label></formula><p>where ? = ?(t) and ? = ?(t) are the time-dependent azimuth and elevation angles of a sound source with respect to the array, respectively.</p><p>We can compute an eigenvector-based intensity vector (EIV) to approximate [H X , H Y , H Z ] T from the principle eigenvector U = U(t, f ) as follows. First, we normalize U by its first element, which corresponds to the omni-directional channel, then discard the first element to obtain?. Afterwards, we take the real part of? and normalize it to obtain unit-norm EIV U = (?)/ (?) . SALSA features for the FOA format are formed by stacking the four-channel spectrograms with the three-channel EIV U. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates SALSA features of a 16-second audio segment in multi-source cases for an FOA array with an EIV cutoff frequency of 9 kHz. The three EIV channels are visually discriminant for different sources originating from different directions. The green areas in the EIV channels correspond to zeroed-out TF bins. Moreover, due to the spectrotemporal alignment properties of SALSA, it can be observed that the TF patterns of the sources in the spectrogram channels, and the patterns of the corresponding directional cues share similar activation patterns, facilitating multichannel feature extraction that is also spectrotemporally meaningful when used as an input to convolutional layers.</p><p>2) Eigenvector-based phase vector for microphone arrays: For a far-field microphone array, the directional cues are encoded in the IPD. The steering vector of an M -channel far-field array of an arbitrary geometry can be modelled by H MIC (t, f, ?, ?) ? C M , whose elements are given by</p><formula xml:id="formula_5">H MIC m (t, f, ?, ?) = exp (??2?f d 1m (?(t), ?(t))/c) ,<label>(7)</label></formula><p>where ? is the imaginary unit, c ? 343 m s ?1 is the speed of sound; d 1m (?(t), ?(t)) is the distance of arrival, in metres, travelled by a sound source, between the mth microphone and the reference (m = 1) microphone. In theory, the distance of arrival is given by</p><formula xml:id="formula_6">d 1m (?(t), ?(t)) = (? 1 ? ? m ) T ? ? cos(?(t)) cos(?(t)) sin(?(t)) cos(?(t)) sin(?(t)) ? ? ? R,<label>(8)</label></formula><p>where ? 1 and ? m are the Cartesian coordinates of the reference and the mth microphones, respectively. ? 1m (?(t), ?(t)) = d 1m (?(t), ?(t))/c is the time difference of arrival (TDOA), travelled by the sound source, between the mth and the reference microphones.</p><p>The directional cues of a far-field microphone array can be presented in several forms such as the relative distance of arrival (RDOA) and TDOA. In this study, we choose to extract the directional cues in the form of RDOA. One advantage of RDOA is that we do not need to know the exact coordinates of the individual microphones, since spatial information of the microphones are already implicitly encoded in the RDOA. We can compute an eigenvector-based phase vector (EPV) to approximate [d 12 , . . . , d 1M ] T from the principle eigenvector U as follows. First we normalize U by its first element, which is chosen arbitrarily as the reference microphone, then discard the first element to obtain?. After that, we take the phase of? and normalize it by ?2?f /c to obtain the EPV U = ?c??/(2?f ). The SALSA features for far-field microphone arrays are formed by stacking the M -channel spectrograms with the (M ? 1)-channel EPV. To avoid spatial aliasing, the values of U are set to zero for all TF bins above aliasing frequency. <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates the SALSA feature of a 16-second audio segment in multi-source cases for a four-channel microphone array with an EPV cutoff frequency of 4 kHz. Similar to the FOA counterpart, the three EPV channels are visually discriminant for different sources originating from different directions. The directional cues in the EPV channels also similarly display patterns corresponding to the sources. The green areas in the EPV channels corresponds to zeroed-out TF bins that are not single-source or above aliasing frequency.</p><p>The proposed method to extract spatial cues can also be extended to near-field and baffled microphone arrays, where directional cues are encoded in both IID and IPD. For those arrays, we can approximate their array response model using the far-field model, or we can compute both EIV and EPV as shown in Section II-C1 and Section II-C2, respectively.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Single-source time-frequency bin selection</head><p>The selection of single-source TF bins have been shown to be effective for DOAE in noisy, reverberant and multi-source cases <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>. There are several methods to select singlesource TF bins <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>. In this paper, we apply two tests to select single-source TF bins, namely, the magnitude and coherence tests. The magnitude test aims to select only TF bins that contain signal from foreground sound sources <ref type="bibr" target="#b26">[27]</ref>. A TF bin passes the magnitude test if its signal-to-noise ratio (SNR) with respect to the adaptive noise floor ?[t, f ] is above a threshold ? SNR <ref type="bibr" target="#b26">[27]</ref>. In practice, the magnitude test indicator is given by The number of channels are calculated based on four-channel inputs.</p><formula xml:id="formula_7">MAGTEST[t, f ] = I X 1 [t, f ] &gt; ? SNR ? ?[t, f ] ? {0, 1}, (9)</formula><p>where I [?] is the Iverson bracket, and X 1 [t, f ] is a running root-mean-square of the magnitude of X 1 [t, f ] over a 3-frame window. We set the threshold ? SNR = 1.5 based on past experiments across several DOAE <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33]</ref>, and SELD <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref> tasks. We used a fast and simple method to estimate the frequency-wise noise floor ?[t, f ] as follows <ref type="bibr" target="#b26">[27]</ref>. The noise floor is initialized using the first few audio frames, which are assumed to contain only noise. After that, the noise floor is slightly increased or decreased if the magnitude of X 1 [t, f ] is above or below the previous noise floor, respectively. The noise floor can also be computed using other estimators such as the one proposed in <ref type="bibr" target="#b35">[36]</ref>. The coherence test aims to find TF bins that contain signal from mostly one source <ref type="bibr" target="#b30">[31]</ref>. Specifically, consider an eigendecomposition, which in practice can be equivalently performed by the more numerically stable SVD,</p><formula xml:id="formula_8">R(t, f ) = U(t, f )?(t, f )U H (t, f )<label>(10)</label></formula><p>where</p><formula xml:id="formula_9">?(t, f ) = diag ([? 1 (t, f ), ? 2 (t, f ), ? ? ? , ? M (t, f )]), and ? 1 (t, f ) ? ? 2 (t, f ) ? ? ? ? ? ? M (t, f ).</formula><p>The direct-toreverberant ratio (DRR) is given by</p><formula xml:id="formula_10">?(t, f ) = ? 1 (t, f ) ? 2 (t, f ) .<label>(11)</label></formula><p>Since the DRR can be interpreted as the relative strength between the paths of signals arriving at the microphone array, the DRR can be also interpreted as a measure of source dominance. When the DRR is low, it is likely that there are either multiple dominant sources at the TF bin, or the reverberation is high even if there is only one source. A TF bin passes the coherence test if its DRR is above a coherence threshold ? DRR <ref type="bibr" target="#b36">[37]</ref>. In this work, we used ? DRR = 5 which obtained the best performance on the validation based on a grid search. <ref type="figure" target="#fig_2">Fig. 3</ref> shows the distribution of TF bins that fail magnitude test, pass magnitude test but fail coherence test, and pass both tests for the FOA and MIC formats from the TNSSE 2021 development dataset <ref type="bibr" target="#b24">[25]</ref>. The lower cutoff frequency for both formats is 50 Hz while the upper cutoff frequency for the FOA and MIC formats are 9 kHz and 4 kHz, respectively. For both formats, around 40 % of TF bins in the passband pass both tests. The two tests significantly reduce the number of EIVs or EPVs to be computed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. COMMON INPUT FEATURES FOR SELD</head><p>We compare the proposed SALSA features with logspectrograms and IV for the FOA format, and logspectrograms and GCC-PHAT for the MIC format in both meland linear-frequency scales, of which the mel-scale features are the more popular for SELD. The log-mel spectrograms are computed from the complex spectrograms X by</p><formula xml:id="formula_11">MELSPEC(t, k) = log |X(t, f )| 2 ? W mel (f, k) ,<label>(12)</label></formula><p>where k is the mel index, and W mel is the mel filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Log-spectrograms and IV for FOA format</head><p>The four channels of the FOA format consist of the omni-, X-, Y-, and Z-directional components. The IV expresses intensity differences of the X, Y, and Z components with respect to the omni-directional component, and thus carries the DOA cues <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>. The active IV is computed in the TF domain by</p><formula xml:id="formula_12">?(t, f ) = ? 1 0 c ? ? X * W (t, f ) ? ? X X (t, f ) X Y (t, f ) X Z (t, f ) ? ? ? ? ,<label>(13)</label></formula><p>where 0 is the sound density <ref type="bibr" target="#b10">[11]</ref>. Physically, the active IV corresponds to the flow of acoustic energy thus the directional cues of the location(s) of sound source(s) can be extracted <ref type="bibr" target="#b39">[40]</ref>. The IV features are then normalized <ref type="bibr" target="#b10">[11]</ref> to have unit norm vi? ?(f, t) = ?(f, t)/ ?(f, t) . In order to combine IVs and the multichannel log-mel spectrograms, the IVs are passed through the same set of mel filters W mel used to compute the log-mel spectrograms; we refer to this feature as MELSPECIV. Linearscale IV can also be stacked with log-linear spectrograms, referred to as LINSPECIV. The dimensions of MELSPECIV and LINSPECIV are 7 ? T ? K and 7 ? T ? F , respectively, where K is the number of mel filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Log-spectrograms and GCC-PHAT for MIC format</head><p>GCC-PHAT is computed for each audio frame for each of the microphone pairs (i, j) by <ref type="bibr" target="#b6">[7]</ref> </p><formula xml:id="formula_13">GCC-PHAT i,j (t, ? ) = F ?1 f ?? X i (t, f )X H j (t, f ) X i (t, f )X H j (t, f ) ,<label>(14)</label></formula><p>where ? is the time lag, F ?1 is the inverse Fourier transform. The maximum time lag of the GCC-PHAT spectrum is f s d max /c, where f s is the sampling rate, and d max is the largest distance between two microphones. When the GCC-PHAT features are stacked with mel-or linear-scale spectrograms, the ranges of time lags to be included in the GCC-PHAT spectrum are (?K/2, K/2] or (?F/2, F/2], respectively. We refer to these two features as MELSPECGCC and LINSPECGCC, respectively. The dimensions of the MELSPECGCC and LIN- IV. NETWORK ARCHITECTURE AND PIPELINE <ref type="figure" target="#fig_3">Figure 4</ref> shows the SELD network architecture that is used for all the experiments in this paper. Since the CRNN structure is arguably the most commonly used architecture in SELD <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b28">29]</ref>, we constructed the network as a CRNN, consisting of a CNN based on the PANN ResNet22 model for audio tagging <ref type="bibr" target="#b40">[41]</ref>, a two-layer BiGRU, and fully connected (FC) output layers. We opted to use a CNN backbone based on the PANNs <ref type="bibr" target="#b40">[41]</ref> given its common usage across many audio-related applications.</p><formula xml:id="formula_14">SPECGCC feature are (M + M (M ? 1)/2) ? T ? K and (M + M (M ? 1)/2) ? T ? F , respectively.</formula><p>The network can be adapted for different input features in <ref type="table" target="#tab_1">Table II</ref> by setting the number of input channels in the first convolutional layer to that of the input features. During inference, sound classes whose probabilities are above the SED threshold are considered active classes. The DOAs corresponding to these classes are selected accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Loss function</head><p>We use the class-wise output format for SELD, in which the SED is formulated as a multilabel multiclass classification and the DOAE as a three-dimensional Cartesian regression. The loss function used is given by</p><formula xml:id="formula_15">L(?, Y) = ?L BCE (? SED , Y SED ) + ?1 active L MSE (? DOA , Y DOA ),<label>(15)</label></formula><p>where T o is the number of output frames; and N is the number of target sound classes;?, Y ? R To?N ?4 are the SELD prediction and target tensors, respectively;? SED , Y SED ? R To?N are the SED prediction and target tensors, respectively; Y DOA , Y DOA ? R To?N ?3 are the DOA prediction and target tensors, respectively. The DOA loss is only computed for the active classes in each frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature normalization</head><p>The four features MELSPECIV, LINSPECIV, MEL-SPECGCC, and LINSPECGCC are globally normalized for zero mean and unit standard deviation vectors per channel <ref type="bibr" target="#b41">[42]</ref>. For the SALSA features, only the spectrogram channels are similarly normalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Data augmentation</head><p>To tackle the problem of small datasets in SELD, we investigate the effectiveness of three data augmentation techniques for all features listed in <ref type="table" target="#tab_1">Table II</ref>: channel swapping (CS) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref>, random cutout (RC) <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>, and frequency shifting (FS). All the three augmentation techniques can be performed in the STFT domain on the fly during training. Only channel swapping changes the ground truth, while random cutout and frequency shifting do not alter the ground truth. Each training sample has an independent 50 % chance to be augmented by each of the three techniques.</p><p>In channel swapping, there are 16 and 8 ways to swap channels for the FOA <ref type="bibr" target="#b7">[8]</ref> and MIC <ref type="bibr" target="#b14">[15]</ref> formats, respectively. The IV, GCC-PHAT, EIV, EPV, and target labels are altered accordingly when channels are swapped. channel swapping augmentation technique greatly increases the variation of DOAs in the dataset.</p><p>In random cutout, we either apply random cutout <ref type="bibr" target="#b42">[43]</ref> or TF masking via SpecAugment <ref type="bibr" target="#b43">[44]</ref> on all the channels of the input features. Random cutout produces a rectangular mask on the spectrograms while SpecAugment produces a cross-shaped mask. For the LINSPEC and MELSPEC channels, the value of the mask is set to a random value within these channels' value range. For the IV, GCC-PHAT, EIV and EPV channels, the value of the mask is set to zero. All the channels share the same mask. The random cutout technique aims to improve network redundancy.</p><p>We also introduce frequency shifting as a new data augmentation for SELD. frequency shifting in the frequency domain is similar to pitch shift in the time domain <ref type="bibr" target="#b0">[1]</ref>. We randomly shift all the channels input features up or down along the frequency dimension by up to 10 bands. For MELSPECGCC and LINSPECGCC features, the GCC-PHAT channels are not shifted. The frequency shifting augmentation technique increases the variation of frequency patterns of sound events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL SETTINGS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>The main dataset used in the majority of our experiments is the TNSSE 2021 dataset <ref type="bibr" target="#b24">[25]</ref>. Since this dataset is relatively new, we also use the TNSSE 2020 dataset <ref type="bibr" target="#b23">[24]</ref> to compare our models with state-of-the-art methods. The development subset of each TNSSE dataset consists of 400, 100, and 100 oneminute audio recordings for the train, validation, and test split, respectively. The evaluation subset of each dataset consists of 200 one-minute audio recording. Unless otherwise stated, the validation set was used for model selection while the test set was used for evaluation. Both TNSSE datasets were recorded using a 32-microphone Eigenmike spherical array with a radius of 4.2 cm. The 32channel signals were converted into FOA format, whose array response is approximately frequency-independent up to around 9 kHz. Therefore, we compute EIV for SALSA features between 50 Hz and 9 kHz. Out of the 32 microphones, four microphones that form a tetrahedron are used for the MIC format. Since the radius of the spherical array corresponds to an aliasing frequency of 4 kHz, we computed EPV for MIC format between 50 Hz and 4 kHz. Even though the microphones are mounted on an acoustically-hard spherical baffle, we found that the far-field array model in Section II-C2 is sufficient to extract the spatial cues for the MIC format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation</head><p>To evaluate SELD performance, we used the official evaluation metrics <ref type="bibr" target="#b44">[45]</ref> that were introduced in the 2021 DCASE Challenge as our default metrics. A sound event is considered a correct detection only if it has correct class prediction and its estimated DOA is less than D?away from the DOA ground truth, where D = 20?is the most commonly used value. The DOAE metrics are class-dependent, that is, the detected sound class will have to be correct in order for the corresponding localization predictions to count. Since some state-of-the-art SELD systems only reported the 2020 version of the DCASE evaluation metrics <ref type="bibr" target="#b45">[46]</ref>, we also used these metrics in some experiments to fairly compare the results.</p><p>Both the 2020 and 2021 SELD evaluation metrics consist of four metrics: location-dependent error rate (ER ?20?) and F1 score (F ?20?) for SED; and class-dependent localization error (LE CD ), and localization recall (LR CD ) for DOAE. We also computed an aggregated SELD error metric that was used as the ranking metric for the 2019 and 2020 DCASE Challenges as follows,</p><formula xml:id="formula_16">E SELD = 1 4 ER ?20?+ (1 ? F ?20?) + LE CD 180?+ (1 ? LR CD ) .<label>(16)</label></formula><p>E SELD was used for model and hyperparameter selection. A good SELD system should have low ER ?20?, high F ?20?, low LE CD , high LR CD , and low aggregated error metric E SELD .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Hyperparameters</head><p>We used a sampling rate of 24 kHz, window length of 512 samples, hop length of 300 samples, Hann window, 512 FFT points, and 128 mel bands. As a result, the input frame rate of all the features was 80 fps. Since the model temporally downsampled the input by a factor of 16, we temporally upsampled the final outputs by a factor of 2 to match the label frame rate of 10 fps. To reduce the feature dimensions to speed up the training time, we linearly compressed frequency bands above 9 kHz, which correspond to frequency bin index 192 and above, by a factor of 8, i.e., 8 consecutive bands will be averaged into a single band. As the results, the frequency dimension is F = 200 for all linear-scale features. Unless stated otherwise, 8-second audio chunks were used for model training. The loss weights for SED and DOAE were set to ? = 0.3 and ? = 0.7, respectively. Adam optimizer was used for all training. The learning rate was initially set to 3 ? 10 ?4 and linearly decreased to 10 ?4 over last 15 epochs of the total 50 training epochs. A threshold of 0.3 was used to binarize active class predictions in the SED outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RESULTS AND DISCUSSION</head><p>We performed a series of experiments to compare the performances of each input feature with and without data augmentation. Afterwards, the effect of data augmentation on each feature was examined in details. We analyzed the effects of the magnitude and coherence tests on the performance of SELD systems running on SALSA features. Next, we studied the feature importance of LINSPEC, EIV and EPV that constitute SALSA features. In addition, effect of different segment lengths on SALSA performance was investigated. For the MIC format, we examined effect of spatial aliasing on the SELD performance with SALSA features. Finally, we compared the performance of models trained on the proposed SALSA features with several state-of-the-art SELD systems on both the 2020 and 2021 TNSSE datasets. <ref type="table" target="#tab_1">Table IV</ref> shows benchmark performances of all considered features without data augmentation. Linear-scale features (LINSPEC-based) appear to perform better than their mel-scale counterparts (MELSPEC-based) for both audio formats. For the 'traditional' features, the performance gap between the FOA and MIC formats is large, with both IV-based features outperforming GCC-based features. Without data augmentation, FOA SALSA performed better than MELSPECIV but slightly worse than LINSPECIV, while MIC SALSA performed much better than both GCC-based features. <ref type="table">Table V</ref> shows the performance of all features with their respective best combination of the three data augmentation techniques investigated. For the FOA format, the experimental results, again, showed that linear-scale features achieved better performance than mel-scale features. For the MIC format, the mel-scale features performed slightly better than linear-scale features. The large performance gap between the FOA and MIC formats still remained with data augmentation applied. IV-based features significantly outperform GCC-based features across all the evaluation metrics. With data augmentation, the proposed SALSA features achieved the best overall performances for both the FOA and MIC formats. SALSA scored the highest in F ?20?a nd LR CD ; and the lowest in ER ?20?a nd E SELD among the setups investigated in <ref type="table">Table V</ref>. It is expected that a high LR CD often leads to a high LE CD . With a higher LR CD , SALSA also has a higher LE CD than LINSPECIV by 2?. SALSA outperformed both GCC-based features by a large margin. Compared to MELSPECGCC, SALSA feature substantially reduced ER ?20?b y 20 %, increased F ?20?b y 16 %, reduced LE CD by 5.3?, and increased LR CD by 7 %. The overall E SELD was impressively reduced by 21 %.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparison between SALSA and other SELD features</head><p>The performance gap between the IV-and GCC-based features, and the similar performances of SALSA for both array formats indicated that the exact TF mapping between the signal power and the directional cues, as per SALSA, MELSPECIV, and LINSPECIV, are much better for SELD than simply stacking spectrograms and GCC-PHAT spectra as per MELSPECGCC and LINSPECGCC. This exact TF mapping also facilitates the learning of CNNs, as the filters can more conveniently learn the multichannel local patterns on the image-like input features. Most importantly, the results showed that the extracted spatial cues for SALSA features are effective for both FOA and MIC formats. Therefore, SALSA can be considered as a unified SELD feature regardless of the array format. The outstanding performance gains in models trained with SALSA features shown in both <ref type="table" target="#tab_1">Table IV</ref> and V indicate that SALSA as a very effective feature for deep learning-based SELD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Effect of data augmentation</head><p>We report the effect of different data augmentation techniques on each feature in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature</head><p>Data Aug.</p><p>FOA format MIC format When frequency shifting was used together with channel swapping, the performance was improved further for all features. Compared to channel swapping alone, the combination of channel swapping and frequency shifting on average reduced ER ?20?b y a further 7.3 %, increased F ?20?b y 5.8 %, reduced LE CD by 1.2?, increased LR CD by 5.3 %, and reduced E SELD by 8.6 %. These results showed that varying the SED and DOA patterns by frequency shifting and channel swapping helped the models learn more effectively.</p><formula xml:id="formula_17">? ER ?20?? F ?20?? LE CD ? LR CD ? E SELD ? ER ?20?? F ?20?? LE CD ? LR CD ? E</formula><p>When random cutout was used together with channel swapping and frequency shifting, the performance was further improved for LINSPECIV and all MIC features; but not for MELSPECIV and FOA SALSA. For subsequent experiments, the best combinations of data augmentation techniques for each feature, as shown in boldface in <ref type="table" target="#tab_1">Table VI</ref>, are used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Effect of magnitude and coherence tests</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Feature importance</head><p>Table VIII reports the feature importance of each component in SALSA feature: multichannel log-linear spectrogram LINSPEC, as well as spatial features EIV and EPV for FOA and MIC formats, respectively. MONO-SALSA is an ablation feature formed by stacking the log-linear spectrogram of only the first microphone with the corresponding spatial features. For both formats, SALSA achieved the best performance, followed by MONO-SALSA.</p><p>For the FOA format, LINSPEC alone could not meaningfully estimate DOAs. One possible reason is that the spatial cues of FOA format are encoded in the signed amplitude differences between microphones, but LINSPEC retains only the unsigned magnitude differences. The sign ambiguity caused the confusion between the input features and the target labels. Therefore, the model trained on LINSPEC feature failed to detect the correct DOAs. On the other hand, the model trained on only the EIV feature performed reasonably well. The EIV feature preserved some coarse spatiotemporal patterns of each sound class (see <ref type="figure" target="#fig_0">Fig. 1</ref>), thus the model was able to distinguish different sound classes. SALSA feature significantly outperformed its constituent features, LINSPEC and EIV. In the absence of the X, Y, and Z channels of the linear spectrograms, MONO-SALSA performed slightly worse than SALSA on the SED metrics but similarly on the DOAE metrics. These results suggest that the main contribution of the X, Y, and Z channels in the linear spectrograms is to distinguish different sound classes.</p><p>For the MIC format, LINSPEC alone performed reasonably well for SELD. Referring to Section V-A, the MIC format of the DCASE SELD dataset is not a true far-field array, but rather a baffled microphone array, where some spatial cues are also encoded in the magnitude differences between  microphones. Therefore, not only is the model trained on LINSPEC feature able to classify sound sources, but it is also able to estimate DOAs. The EPV feature alone returned a lower SELD performance compared to the EIV feature of the FOA format, likely because the EPV feature is computed with an upper cutoff frequency of 4 kHz, which is much lower than that of EIV at 9 kHz. The model trained on only EPV also has the highest ER ?20?a nd lowest F ?20?a mong all ablation models of the MIC format. SALSA feature significantly outperformed its individual feature component across all the metrics. The performance gap between SALSA and MONO-SALSA is larger for the MIC format than the FOA format. The reason is likely that the spatial cues are also encoded in the magnitude of different input channels, and the EPV is all zeroed out above the upper cutoff frequency. Therefore, the multichannel nature of the spectrograms play an important role in both sound class recognition and DOA estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Effect of spatial aliasing on SELD for microphone array</head><p>For narrow band signals, spatial aliasing occurs at high frequency bins, where half of the signal wavelength is less than the distance between two microphones. To investigate the effect of spatial aliasing when SALSA features for MIC format are used, we report the performances of SALSA with different upper cutoff frequencies in <ref type="table" target="#tab_1">Table IX</ref>. The upper cutoff frequencies were computed using the spatial aliasing formula for narrow band signals, f alias = c/(2d max ), where d max is the maximum distance between any two microphones in the array. The investigated values of d max are the arc length between any two microphones (8.0 cm) and the radius of the Eigenmike array (4.2 cm), which correspond to aliasing frequencies of 2 kHz, and 4 kHz, respectively. In addition, we also tested a cutoff frequency of 9 kHz to investigate the case where spatial aliasing is ignored. <ref type="table" target="#tab_1">Table IX</ref> shows that cutoff frequencies at 2 kHz and 4 kHz result in similar performances. One possible reason is that the spatial aliasing might not significantly occur in all of the microphone pairs beyond 2 kHz for some DOAs. On the other hand, with the 9 kHz cutoff frequency, spatial aliasing has occurred in too many highfrequency bins, resulting in a slightly lower performance than a loose cutoff frequency at 4 kHz. However, the impact of spatial aliasing appears to be mild, with the model trained on a loose aliasing frequency at 4 kHz achieving the best E SELD . This result is agreeable with the finding in <ref type="bibr" target="#b46">[47]</ref>, where broadband signals were shown to not experience spatial aliasing unless they contain strong harmonic components. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Effect of segment length for training</head><p>Different sound events often have different duration. Thus the segment length that is used during training may affect the model performance. The sound event lengths from the TNSSE 2021 dataset are between 0.2 s and 40.0 s, with a median of 3.2 s, and a mean of 8.3 s. We present the SELD performances on models trained with different input segment lengths, as per <ref type="table" target="#tab_14">Table X</ref>. Models trained with a segment length of 8 s significantly outperformed models trained with a segment length of 4 s for both the FOA and MIC formats. However, increasing the segment length to 12 s did not further improve the overall performance. Thus, it appears that the model requires a certain minimum sequence length to sufficiently learn the temporal dependency, although this temporal dependency does not need to be very long, since the model would likely rely more on recent frames than older frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Comparisons with state-of-the-art methods for SELD</head><p>We compared models trained with the proposed SALSA features with state-of-the-art (SOTA) methods on three datasets: the test and evaluation splits of the TNSSE 2020 dataset <ref type="bibr" target="#b23">[24]</ref> and the test split of the TNSSE 2021 dataset <ref type="bibr" target="#b24">[25]</ref>. Some of the SOTA methods used single models while others used ensemble models. We used the same single-model SELD network shown in Section IV to train all of the models reported, i.e., no ensembling was used. To further improve the performance of our models, we applied test-time augmentation (TTA) during inference <ref type="bibr" target="#b15">[16]</ref>. TTA swaps the channels of the SALSA features in a manner similar to the channel swapping augmentation technique that was employed during training. The estimated DOA outputs were rotated back to the original axes, then averaged to produce the final results. During inference, the whole 60-second features were passed into the models without being split into smaller chunks. Since the SOTA results on the TNSSE 2020 dataset were evaluated using the 2020 SELD evaluation metrics, we evaluated our models using both the 2020 and 2021 metrics, the former for fair comparison with past works, and the latter for ease of comparison with future works.</p><p>1) Performance on the test split of the TNSSE 2020 dataset: <ref type="table" target="#tab_1">Table XI</ref> shows the performances on the test split of the TNSSE 2020 dataset of SOTA systems, and our SALSA models for both the FOA and MIC formats. FOA SALSA models performed slightly better than the MIC counterparts.  The TTA significantly improved location dependent SED metrics ER ?20?a nd F ?20?. The model by Wang et al. <ref type="bibr" target="#b21">[22]</ref> used both the FOA and MIC data as input features and achieved the best performance for ER ?20?, F ?20?, and LR CD for single models. However, it is considerably more expensive to have both FOA and MIC data available in real-life applications due to the more specialized recording setup required. Our FOA SALSA model outperformed the DCASE baseline <ref type="bibr" target="#b23">[24]</ref> by a large margin, and performed better than <ref type="bibr" target="#b15">[16]</ref> in term of ER ?20?, F ?20?, and LE CD . Our FOA SALSA model with TTA also performed on-par with the TTA version of <ref type="bibr" target="#b15">[16]</ref>. On average, the 2021 evaluation metrics return similar ER ?20?, F ?20?a nd LE CD compared to the 2020 metrics, but stricter LR CD than the 2020 metrics.</p><p>2) Performance on the evaluation split of the TNSSE 2020 dataset: <ref type="table" target="#tab_1">Table XII</ref> shows the performances on the evaluation split of the TNSSE 2020 dataset of SOTA systems, and our SALSA models for both the FOA and MIC formats. Our models were trained using all 600 audio clips from the development split of the TNSSE 2020 dataset. Interestingly,  <ref type="bibr" target="#b11">[12]</ref> while the MIC SALSA model performed slightly better. The MIC SALSA model with TTA achieved comparable performance as the top ensemble model <ref type="bibr" target="#b21">[22]</ref> from the 2020 DCASE Challenge, with similar ER ?20?, LE CD , LR CD and higher F ?20?. The 2021 metrics again returned similar ER ?20?, F ?20?, LE CD results and stricter LR CD than the 2020 metrics.</p><p>3) Performance on the test split of the TNSSE 2021 dataset: <ref type="table" target="#tab_1">Table XIII</ref> shows the performances on the test split of the TNSSE 2021 dataset of SOTA systems, and our SALSA models for both the FOA and MIC formats. The FOA SALSA models performed similarly in ER ?20?, LE CD , LR CD as and higher F ?20?c ompared to the MIC SALSA models. The TTA significantly improved their ER ?20?, F ?20?, and LE CD but not LR CD . The models trained on SALSA features of both formats outperformed the DCASE baseline by the large margin, and performed better than the highest-ranked system from the 2021 DCASE Challenge <ref type="bibr" target="#b22">[23]</ref> in terms of ER ?20?a nd F ?20?. With TTA, the models trained on SALSA features achieved much better ER ?20?a nd F ?20?, similar LE CD , and slightly lower LR CD compared to <ref type="bibr" target="#b22">[23]</ref>. An ensemble model trained on a variant of our proposed SALSA features <ref type="bibr" target="#b28">[29]</ref> officially ranked second in the team category of the SELD taks in the 2021 DCASE Challenge. The SALSA variant in <ref type="bibr" target="#b28">[29]</ref> included an additional channel for the estimated DRR at each TF bin.</p><p>Compared to the TNSSE 2020 dataset, the TNSSE 2021 dataset is more challenging since it has more overlapping sound events and unknown directional interferences. Overall, the performances of models listed in <ref type="table" target="#tab_1">Table XIII</ref> are lower than those of the models listed in <ref type="table" target="#tab_1">Table XI across all metrics.</ref> The results in Tables XI to XIII consistently show that the proposed SALSA features for both the FOA and MIC formats are very effective for SELD. Simple CRNN models trained on SALSA features surpassed or performed comparably to many SOTA systems, both single models and ensembles, on different datasets across all evaluation metrics.  Overall, the trajectories of predicted events were smooth and followed the ground truths closely. The model was able to correctly detect the sound classes and estimate DOAs across different numbers of overlapping sound sources (up to three overlapping sources). An unknown interference was misclassified as a PIANO event (purple) and an ALARM event (pink) between the 4th and the 12th seconds. Since we used the class-wise output format to train the model, when there were two overlapping CRASH events between the 22nd and the 24th seconds, the model only predicted one CRASH event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Qualitative evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In conclusion, we proposed a novel and effective feature for polyphonic SELD named Spatial cue-Augmented Log-SpectrogrAm (SALSA), which consists of multichannel logspectrograms and normalized principal eigenvector of the spatial covariance matrix at each TF bin of the spectrograms. There are two key characteristics that contribute to the effectiveness of the proposed feature. Firstly, SALSA spectrotemporally aligns the signal power and the source directional cues, which aids in resolving overlapping sound sources. This locally linear alignment works well with CNNs, where the filters learn the multichannel local pattern of the image-like input features. Secondly, SALSA includes helpful directional cues extracted from the principal eigenvectors of the spatial covariance matrices. Depending on the array type, where the directional cues might be encoded as interchannel amplitude and/or phase differences, the principal eigenvectors can be easily normalized to extract these cues. Therefore, SALSA features are versatile to use with different microphone array formats, such as FOA and MIC.</p><p>The proposed SALSA features can be further enhanced by incorporating signal processing-based methods such as magnitude and coherence tests to select more reliable directional cues and improve SELD performance. In addition, for multichannel arrays, spatial aliasing has little effect on the performance of models trained on SALSA. More importantly, the training segment length must be sufficient long for the model to capture the temporal dependency in the data.</p><p>In addition, data augmentation techniques such as channel swapping, frequency shifting, and random cutout can be readily applied to SALSA on the fly during training. These data augmentation techniques mitigated the problem of small datasets and significantly improved the performance of models trained on SALSA features. Simple CRNN models trained on the SALSA features achieved similar or even better SELD performance than many state-of-the-art systems on the TNSSE 2020 and 2021 datasets.</p><p>Thi Ngoc Tho Nguyen (S'19) is a Ph.D. student at the Nanyang Technological University (NTU) in Singapore. Prior to joining NTU, she worked at University of Illinois Research Center in Singapore for five years as a research engineer. Her research interests are audio signal processing, deep learning, microphone array signal processing, and real-time processing. She has published several papers on the topics of direction-of-arrival estimation of multiple sound sources, and sound event localization and detection. From 2018 to 2020, she was with the NTU EEE Media Technology Laboratory. In Spring 2020, she was a visiting research student at Music Informatics Group, Center for Music Technology (GTCMT), Georgia Institute of Technology, Atlanta, GA, USA, before returning again remotely since Spring 2021. Concurrently since 2021, she has been with the Digital Signal Processing Laboratory, the Smart Nation Translational Laboratory, and the Alibaba-NTU Singapore Joint Research Institute, NTU.</p><p>Her research interests are in signal processing, machine learning, and artificial intelligence for music and audio applications. Since 2021, she has published more than 10 papers in international conferences and journals on music information retrieval, soundscapes, spatial audio, speech enhancement, and blind source separation. Woon-Seng Gan (S'90--M'93-SM'00) received his BEng (1st Class Hons) and PhD degrees, both in Electrical and Electronic Engineering from the University of Strathclyde, UK in 1989 and 1993 respectively. He is currently a Professor of Audio Engineering and the Director of the Smart Nation Lab in the School of Electrical and Electronic Engineering in Nanyang Technological University. He also served as the Head of the Information Engineering Division in the School of Electrical and Electronic Engineering in Nanyang Technological University (2011-2014), and the Director of the Centre for Infocomm Technology (2016-2019). His research has been concerned with the connections between the physical world, signal processing and sound control, which resulted in the practical demonstration and licensing of spatial audio algorithms, directional sound beam, and active noise control for headphones and open windows.</p><p>He has published more than 400 international refereed journals and conferences, and has translated his research into 6 granted patents. He had co-authored three books on Subband Adaptive Filtering: Theory and Implementation <ref type="figure">(</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>SALSA features of a 16-second audio segment of FOA format in a multi-source scenario. The vertical axis represents frequency in kHz. In the spectrogram channels, the colormap represents the signal log-magnitude in each TF bin. In the EIV channels, the colormap represents the values of the computed EIV features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>SALSA features of a 16-second audio segment of a four-channel microphone array (MIC format) in a multi-source scenario. The vertical axis represents frequency in kHz. In the spectrogram channels, the colormap represents the signal log-magnitude in each TF bin. In the EPV channels, the colormap represents the values of the computed EPV features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Distributions of TF bins that (a) fail magnitude test, (b) pass magnitude test but fail coherence test, and (c) pass both tests, for the FOA and MIC formats. The distributions shown are independent of the true number of sources.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Block diagram of the SELD network, which is a CRNN. This network can be adapted for different input features such as SALSA, MELSPECIV, MELSPECGCC, etc. by changing the number of input channels in the first convolutional layer of the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5</head><label>5</label><figDesc>shows the plots of ground truth and predicted azimuth angles for a audio clip from the test set of the TNSSE 2021</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Visualization of ground truth and predicted azimuth for test clip fold6_room2_mix041 of the TNSSE 2021 dataset. Legend lists the ground truth events in chronological order. Sound classes are color-coded. PIANO event (purple) and an ALARM event (pink) were misclassified between the 4 th and the 12 th seconds dataset. The angles were predicted by a CRNN model trained on FOA SALSA features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Karn N. Watcharasupat (S'19) was born in Bangkok, Thailand, in 1999. She received her B.Eng. (Hons) in Electrical and Electronic Engineering under the CN Yang Scholars Programme, from Nanyang Technological University (NTU), Singapore, in 2022. She is currently a research engineer at the School of Electrical and Electronic Engineering (EEE), NTU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Ngoc</head><label></label><figDesc>Khanh Nguyen received his B.Eng. (Hons) in Electronics and Computer System from Swinburne University, Australia in 2017. He is currently a software engineer. He is keen on topics in computer sciences such as algorithms, database, machine learning and deep learning. He has also participated in several Kaggle competitions. Douglas L. Jones (S'82-M'83--S'84-M'87--SM'97-F'02) received the BSEE, MSEE, and Ph.D. degrees from Rice University in 1983, 1986, and 1987, respectively. During the 1987-1988 academic year, he was at the University of Erlangen-Nuremberg in Germany on a Fulbright postdoctoral fellowship. Since 1988, he has been with the University of Illinois at Urbana-Champaign, where he is currently a Professor in Electrical and Computer Engineering, Neuroscience, the Coordinated Science Laboratory, and the Beckman Institute. He was on sabbatical leave at the University of Washington in Spring 1995 and at the University of California at Berkeley in Spring 2002. In the Spring semester of 1999 he served as the Texas Instruments Visiting Professor at Rice University. He is an author of two DSP laboratory textbooks, and was selected as the 2003 Connexions Author of the Year. He is a Fellow of the IEEE. He served on the Board of Governors of the IEEE Signal Processing Society from 2002-2004. His research interests are in digital signal processing, including nonstationary signal analysis, adaptive processing, multisensor data processing, OFDM, and various applications such as low-power implementations, biology and neuroengineering, and advanced hearing aids and other audio systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>John Wiley, 2009); Embedded Signal Processing with the Micro Signal Architecture, (Wiley-IEEE, 2007); and Digital Signal Processors: Architectures, Implementations, and Applications (Prentice Hall, 2005). In 2017, he won the APSIPA Sadaoki Furui Prize Paper Award. He is a Fellow of the Audio Engineering Society (AES), a Fellow of the Institute of Engineering and Technology (IET), and a Senior Member of the IEEE. He served as an Associate Editor of the IEEE/ACM Transaction on Audio, Speech, and Language Processing (TASLP; 2012-15) and was presented with an Outstanding TASLP Editorial Board Service Award in 2016. He also served as the Associate Editor for the IEEE Signal Processing Letters (2015-19). He is currently serving as a Senior Area Editor of the IEEE Signal Processing Letters (2019-); Associate Technical Editor of the Journal of Audio Engineering Society (JAES; 2013-); Editorial member of the Asia Pacific Signal and Information Processing Association (APSIPA; 2011-) Transaction on Signal and Information Processing; Associate Editor of the EURASIP Journal on Audio, Speech and Music Processing (2007-).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Table I summarizes some notable and state-of-the-art deep learning methods for SELD. arXiv:2110.00275v3 [eess.AS] 6 Jun 2022</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF THE PROPOSED METHOD WITH SOME EXISTING DEEP LEARNING-BASED METHODS FOR POLYPHONIC SELD. FOA/MIC Magnitude &amp; phase spectrograms End-to-end CRNN class-wise Cao et al.</figDesc><table><row><cell>Approach</cell><cell>Format</cell><cell>Input Features</cell><cell>Network Architecture</cell><cell>Output</cell></row><row><cell>Adavanne et al.</cell><cell>[5]</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>PHAT features follow the frequency scale (linear, mel, constant-Q) of the spectrograms. TDNN stands for time delay neural networks. IPD stands for interchannel phase differences. Top'YY denotes the top ranked systems for the respective DCASE SELD Challenges.</figDesc><table><row><cell></cell><cell>17] FOA</cell><cell>Log-mel spectrograms, IV</cell><cell>EINv2 with cross-model attention</cell><cell>track-wise</cell></row><row><cell>(Top'19) Kapka et al.</cell><cell>[21] FOA</cell><cell>Log-mel spectrograms, IV</cell><cell>Ensemble of CRNNs</cell><cell>class-wise</cell></row><row><cell>(Top'20) Wang et al.</cell><cell cols="2">[22] FOA+MIC Log-mel spectrograms, IV, GCC-PHAT</cell><cell cols="2">Ensemble of CRNNs &amp; CNN-TDNNs class-wise</cell></row><row><cell cols="2">(Top'21) Shimada et al. [23] FOA</cell><cell>Linear amplitude spectrograms, IPD, cosIPD, sinIPD</cell><cell>Ensemble of CRNNs &amp; EINv2</cell><cell>class-wise</cell></row><row><cell>Proposed method</cell><cell cols="2">FOA/MIC SALSA: Log-linear spectrograms &amp; normalized eigenvec-</cell><cell>End-to-end CRNN</cell><cell>class-wise</cell></row><row><cell></cell><cell></cell><cell>tors</cell><cell></cell><cell></cell></row><row><cell>IV and GCC-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II FEATURE</head><label>II</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>NAMES AND DESCRIPTIONS</cell><cell></cell></row><row><cell>Name</cell><cell cols="2">Format Components</cell><cell># channels</cell></row><row><cell>MELSPECIV</cell><cell>FOA</cell><cell>MELSPEC + IV</cell><cell>7</cell></row><row><cell>LINSPECIV</cell><cell>FOA</cell><cell>LINSPEC + IV</cell><cell>7</cell></row><row><cell>MELSPECGCC</cell><cell>MIC</cell><cell>MELSPEC + GCC-PHAT</cell><cell>10</cell></row><row><cell>LINSPECGCC</cell><cell>MIC</cell><cell>LINSPEC + GCC-PHAT</cell><cell>10</cell></row><row><cell>SALSA</cell><cell>FOA</cell><cell>LINSPEC + EIV</cell><cell>7</cell></row><row><cell>SALSA</cell><cell>MIC</cell><cell>LINSPEC + EPV</cell><cell>7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III CHARACTERISTICS</head><label>III</label><figDesc>OF TNSSE 2020 AND 2021 DATASETS</figDesc><table><row><cell>Characteristics</cell><cell>2020</cell><cell>2021</cell></row><row><cell>Channel format</cell><cell>FOA, MIC</cell><cell>FOA, MIC</cell></row><row><cell>Moving sources</cell><cell></cell><cell></cell></row><row><cell>Ambiance noise</cell><cell></cell><cell></cell></row><row><cell>Reverberation</cell><cell></cell><cell></cell></row><row><cell>Unknown interferences</cell><cell>?</cell><cell></cell></row><row><cell>Maximum degree of polyphony</cell><cell>2</cell><cell>3</cell></row><row><cell>Number of target sound classes</cell><cell>14</cell><cell>12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table</head><label></label><figDesc>III summarizes some key characteristics of the two datasets. The azimuth and elevation ranges of both datasets are [?180?, 180?) and [?45?, 45?], respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table VI .</head><label>VI</label><figDesc>The experimental results clearly demonstrated that channel swapping significantly improved the performance for all features across all metrics. On average, ER ?20?d ecreased by 14.8 %, F ?20?i ncreased by 13.8 %, LE CD decreased by 2.3?, and LR CD increased by 7.8 %. Channel swapping reduced the aggregated error metric E</figDesc><table /><note>SELD by between 13 % and 16 %, where the larger reductions are observed for MIC features such as MELSPECGCC, LIN- SPECGCC, and MIC SALSA.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IV BASELINE</head><label>IV</label><figDesc>SELD PERFORMANCES OF DIFFERENT FEATURES WITHOUT DATA AUGMENTATION. ER ?20?? F ?20?? LE CD ? LR CD ? E SELD ? ER ?20?? F ?20?? LE CD</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>FOA format</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MIC format</cell><cell></cell></row><row><cell>Feature</cell><cell>Data Aug.</cell><cell cols="9">? ? LR CD</cell><cell>? E SELD</cell></row><row><cell>MELSPECIV</cell><cell>None</cell><cell>0.555</cell><cell>0.584</cell><cell cols="2">15.9?0.625</cell><cell>0.358</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LINSPECIV</cell><cell>None</cell><cell>0.527</cell><cell>0.609</cell><cell cols="2">15.6?0.642</cell><cell>0.341</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MELSPECGCC</cell><cell>None</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.660</cell><cell>0.455</cell><cell cols="2">21.1?0.521</cell><cell>0.450</cell></row><row><cell>LINSPECGCC</cell><cell>None</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.622</cell><cell>0.506</cell><cell cols="2">19.6?0.583</cell><cell>0.410</cell></row><row><cell>FOA SALSA</cell><cell>None</cell><cell>0.543</cell><cell>0.592</cell><cell cols="2">15.4?0.627</cell><cell>0.352</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MIC SALSA</cell><cell>None</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.528</cell><cell>0.601</cell><cell cols="2">15.9?0.644</cell><cell>0.343</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE V</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="9">SELD PERFORMANCES OF DIFFERENT FEATURES WITH BEST COMBINATION OF DATA AUGMENTATION TECHNIQUES.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Table VII shows the effect of magnitude and coherence tests on the performance of models trained on SALSA features. Fig. 3 indicates that around 33 % of all TF bins are removed after the magnitude test, and an additional 20 % of bins are removed after the coherence test. These tests aim to only include approximately single-source TF bins with reliable directional cues. The magnitude test improved the performance of the MIC format but not the FOA format. On the other hand, using both the magnitude and coherence tests significantly improved the performance of the FOA format. Overall, when both tests are applied to compute SALSA features, the performances were improved compared to when no test was applied. For subsequent experiments, both tests were applied to compute SALSA features.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE VI PERFORMANCE</head><label>VI</label><figDesc>OF MELSPECIV, LINSPECIV, MELSPECGCC, LINSPECGCC, AND SALSA WITH DIFFERENT DATA AUGMENTATION. Data Aug.? ER ?20?? F ?20?? LE CD ? LR CD ? E SELD</figDesc><table><row><cell>MELSPECIV</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>None</cell><cell>0.555</cell><cell>0.584</cell><cell>15.9?0.625</cell><cell>0.358</cell></row><row><cell>CS</cell><cell>0.472</cell><cell>0.655</cell><cell>12.0?0.653</cell><cell>0.308</cell></row><row><cell>CS+FS</cell><cell>0.444</cell><cell>0.686</cell><cell>11.8?0.686</cell><cell>0.284</cell></row><row><cell>CS+FS+RC</cell><cell>0.440</cell><cell>0.683</cell><cell>10.2?0.668</cell><cell>0.286</cell></row><row><cell>LINSPECIV</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>None</cell><cell>0.527</cell><cell>0.609</cell><cell>15.6?0.642</cell><cell>0.341</cell></row><row><cell>CS</cell><cell>0.459</cell><cell>0.669</cell><cell>12.3?0.678</cell><cell>0.295</cell></row><row><cell>CS+FS</cell><cell>0.423</cell><cell>0.700</cell><cell>10.8?0.692</cell><cell>0.273</cell></row><row><cell>CS+FS+RC</cell><cell>0.410</cell><cell>0.710</cell><cell>10.5?0.702</cell><cell>0.264</cell></row><row><cell>FOA SALSA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>None</cell><cell>0.543</cell><cell>0.592</cell><cell>15.4?0.627</cell><cell>0.352</cell></row><row><cell>SC</cell><cell>0.462</cell><cell>0.655</cell><cell>14.9?0.666</cell><cell>0.306</cell></row><row><cell>CS+FS</cell><cell>0.404</cell><cell>0.724</cell><cell>12.5?0.727</cell><cell>0.255</cell></row><row><cell>CS+FS+RC</cell><cell>0.413</cell><cell>0.713</cell><cell>11.5?0.713</cell><cell>0.263</cell></row><row><cell>MELSPECGCC</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>None</cell><cell>0.660</cell><cell>0.455</cell><cell>21.1?0.521</cell><cell>0.450</cell></row><row><cell>CS</cell><cell>0.552</cell><cell>0.556</cell><cell>18.1?0.583</cell><cell>0.378</cell></row><row><cell>CS+FS</cell><cell>0.507</cell><cell>0.609</cell><cell>17.0?0.646</cell><cell>0.337</cell></row><row><cell>CS+FS+RC</cell><cell>0.507</cell><cell>0.614</cell><cell>17.9?0.679</cell><cell>0.328</cell></row><row><cell>LINSPECGCC</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>None</cell><cell>0.622</cell><cell>0.506</cell><cell>19.6?0.583</cell><cell>0.410</cell></row><row><cell>CS</cell><cell>0.532</cell><cell>0.589</cell><cell>18.6?0.658</cell><cell>0.347</cell></row><row><cell>CS+FS</cell><cell>0.514</cell><cell>0.604</cell><cell>17.7?0.666</cell><cell>0.336</cell></row><row><cell>CS+FS+RC</cell><cell>0.514</cell><cell>0.606</cell><cell>17.8?0.676</cell><cell>0.333</cell></row><row><cell>MIC SALSA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>None</cell><cell>0.528</cell><cell>0.601</cell><cell>15.9?0.644</cell><cell>0.343</cell></row><row><cell>CS</cell><cell>0.447</cell><cell>0.675</cell><cell>13.7?0.683</cell><cell>0.291</cell></row><row><cell>CS+FS</cell><cell>0.431</cell><cell>0.696</cell><cell>12.3?0.709</cell><cell>0.274</cell></row><row><cell>CS+FS+RC</cell><cell>0.408</cell><cell>0.715</cell><cell>12.6?0.728</cell><cell>0.259</cell></row><row><cell cols="4">CS: channel swapping; FS: frequency shifting; RC: random cutout.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE VII EFFECT</head><label>VII</label><figDesc>OF MAGNITUDE AND COHERENCE TESTS ON SALSA FEATURES.Test? ER ?20?? F ?20?? LE CD ? LR CD ? E SELD</figDesc><table><row><cell>FOA SALSA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>None</cell><cell>0.418</cell><cell>0.706</cell><cell>12.0?0.710</cell><cell>0.267</cell></row><row><cell>Magnitude</cell><cell>0.434</cell><cell>0.698</cell><cell>11.9?0.701</cell><cell>0.275</cell></row><row><cell>Magnitude + Coherence</cell><cell>0.404</cell><cell>0.724</cell><cell>12.5?0.727</cell><cell>0.255</cell></row><row><cell>MIC SALSA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>None</cell><cell>0.414</cell><cell>0.701</cell><cell>12.1?0.700</cell><cell>0.270</cell></row><row><cell>Magnitude</cell><cell>0.407</cell><cell>0.716</cell><cell>12.3?0.721</cell><cell>0.260</cell></row><row><cell>Magnitude + Coherence</cell><cell>0.408</cell><cell>0.715</cell><cell>12.6?0.728</cell><cell>0.259</cell></row><row><cell cols="4">CS: channel swapping; FS: frequency shifting; RC: random cutout.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE VIII FEATURE</head><label>VIII</label><figDesc>IMPORTANCE OF FOA AND MIC SALSA. Components ? ER ?20?? F ?20?? LE CD ? LR CD ? E SELD</figDesc><table><row><cell>FOA SALSA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LINSPEC</cell><cell>0.835</cell><cell>0.123</cell><cell>87.2?0.608</cell><cell>0.647</cell></row><row><cell>EIV</cell><cell>0.577</cell><cell>0.557</cell><cell>14.1?0.571</cell><cell>0.382</cell></row><row><cell>MONO-SALSA</cell><cell>0.421</cell><cell>0.705</cell><cell>12.8?0.723</cell><cell>0.266</cell></row><row><cell>SALSA</cell><cell>0.404</cell><cell>0.724</cell><cell>12.5?0.727</cell><cell>0.255</cell></row><row><cell>MIC SALSA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LINSPEC</cell><cell>0.506</cell><cell>0.616</cell><cell>18.1?0.698</cell><cell>0.323</cell></row><row><cell>EPV</cell><cell>0.629</cell><cell>0.502</cell><cell>17.4?0.547</cell><cell>0.419</cell></row><row><cell>MONO-SALSA</cell><cell>0.443</cell><cell>0.680</cell><cell>14.7?0.710</cell><cell>0.284</cell></row><row><cell>SALSA</cell><cell>0.408</cell><cell>0.715</cell><cell>12.6?0.728</cell><cell>0.259</cell></row><row><cell></cell><cell cols="2">TABLE IX</cell><cell></cell><cell></cell></row><row><cell cols="5">EFFECT OF SPATIAL ALIASING ON SALSA FEATURE OF MIC FORMAT.</cell></row><row><cell>Cutoff frequency</cell><cell cols="4">? ER ?20?? F ?20?? LE CD ? LR CD ? E SELD</cell></row><row><cell>2.0 kHz</cell><cell>0.403</cell><cell>0.714</cell><cell>12.5?0.707</cell><cell>0.261</cell></row><row><cell>4.0 kHz</cell><cell>0.408</cell><cell>0.715</cell><cell>12.6?0.728</cell><cell>0.259</cell></row><row><cell>9.0 kHz</cell><cell>0.425</cell><cell>0.698</cell><cell>12.8?0.720</cell><cell>0.270</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE X EFFECT</head><label>X</label><figDesc>OF SEGMENT LENGTH DURING TRAINING ON SELD PERFORMANCE USING SALSA. Length ? ER ?20?? F ?20?? LE CD ? LR CD ? E SELD</figDesc><table><row><cell>FOA SALSA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4 s</cell><cell>0.468</cell><cell>0.658</cell><cell>13.4?0.646</cell><cell>0.310</cell></row><row><cell>8 s</cell><cell>0.404</cell><cell>0.724</cell><cell>12.5?0.727</cell><cell>0.255</cell></row><row><cell>12 s</cell><cell>0.414</cell><cell>0.717</cell><cell>11.8?0.720</cell><cell>0.261</cell></row><row><cell>MIC SALSA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4 s</cell><cell>0.449</cell><cell>0.664</cell><cell>14.1?0.678</cell><cell>0.297</cell></row><row><cell>8 s</cell><cell>0.408</cell><cell>0.715</cell><cell>12.6?0.728</cell><cell>0.259</cell></row><row><cell>12 s</cell><cell>0.413</cell><cell>0.714</cell><cell>12.7?0.730</cell><cell>0.260</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE XI SELD</head><label>XI</label><figDesc>PERFORMANCES OF SOTA SYSTEMS AND SALSA-BASED MODELS ON TEST SPLIT OF THE TNSSE 2020 DATASET.</figDesc><table><row><cell>System</cell><cell>Format</cell><cell>ER ?20?F?20?L E CD LR CD</cell></row><row><cell>2020 Metrics</cell><cell></cell><cell></cell></row><row><cell>DCASE baseline [24]</cell><cell>FOA</cell><cell>0.72 0.374 22.8?0.607</cell></row><row><cell cols="2">Shimada et al. [16] w/o TTA FOA</cell><cell>0.36 0.730 10.2?0.791</cell></row><row><cell>Shimada et al. [16] w/ TTA</cell><cell>FOA</cell><cell>0.32 0.768 7.9?0.805</cell></row><row><cell>Wang et al. [22]</cell><cell>FOA+MIC</cell><cell>0.29 0.764 9.4?0.828</cell></row><row><cell>('20 #1) Wang et al. [22]</cell><cell>FOA+MIC</cell><cell>0.26 0.800 7.4?0.847</cell></row><row><cell>FOA SALSA w/o TTA</cell><cell>FOA</cell><cell>0.338 0.748 7.9?0.784</cell></row><row><cell>MIC SALSA w/o TTA</cell><cell>MIC</cell><cell>0.379 0.717 8.2?0.762</cell></row><row><cell>FOA SALSA w/ TTA</cell><cell>FOA</cell><cell>0.318 0.761 7.4?0.797</cell></row><row><cell>MIC SALSA w/ TTA</cell><cell>MIC</cell><cell>0.341 0.741 7.8?0.783</cell></row><row><cell>2021 Metrics</cell><cell></cell><cell></cell></row><row><cell>FOA SALSA w/o TTA</cell><cell>FOA</cell><cell>0.344 0.755 8.1?0.755</cell></row><row><cell>MIC SALSA w/o TTA</cell><cell>MIC</cell><cell>0.383 0.727 8.3?0.738</cell></row><row><cell>FOA SALSA w/ TTA</cell><cell>FOA</cell><cell>0.323 0.768 7.5?0.763</cell></row><row><cell>MIC SALSA w/ TTA</cell><cell>MIC</cell><cell>0.342 0.749 7.9?0.744</cell></row><row><cell>denotes an ensemble model.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE XII SELD</head><label>XII</label><figDesc>PERFORMANCES OF SOTA SYSTEMS AND SALSA-BASED MODELS ON EVALUATION SPLIT OF TNSSE 2020 DATASET.</figDesc><table><row><cell>System</cell><cell>Format</cell><cell>ER ?20?F?20?L E CD LR CD</cell></row><row><cell>2020 Metrics</cell><cell></cell><cell></cell></row><row><cell>DCASE'21 baseline [24]</cell><cell>MIC</cell><cell>0.69 0.413 23.1?0.624</cell></row><row><cell>Cao et al. [12]</cell><cell>FOA</cell><cell>0.233 0.832 6.8?0.861</cell></row><row><cell>('20 #2) Nguyen et al. [34]</cell><cell>FOA</cell><cell>0.23 0.820 9.3?0.900</cell></row><row><cell>('20 #1) Wang et al. [22]</cell><cell>FOA+MIC</cell><cell>0.20 0.849 6.0?0.885</cell></row><row><cell>FOA SALSA w/o TTA</cell><cell>FOA</cell><cell>0.237 0.823 6.9?0.858</cell></row><row><cell>MIC SALSA w/o TTA</cell><cell>MIC</cell><cell>0.227 0.836 6.7?0.869</cell></row><row><cell>FOA SALSA w/ TTA</cell><cell>FOA</cell><cell>0.219 0.840 6.5?0.869</cell></row><row><cell>MIC SALSA w/ TTA</cell><cell>MIC</cell><cell>0.202 0.854 6.0?0.884</cell></row><row><cell>2021 Metrics</cell><cell></cell><cell></cell></row><row><cell>FOA SALSA w/o TTA</cell><cell>FOA</cell><cell>0.244 0.830 7.0?0.831</cell></row><row><cell>MIC SALSA w/o TTA</cell><cell>MIC</cell><cell>0.234 0.842 6.7?0.849</cell></row><row><cell>FOA SALSA w/ TTA</cell><cell>FOA</cell><cell>0.225 0.844 6.6?0.838</cell></row><row><cell>MIC SALSA w/ TTA</cell><cell>MIC</cell><cell>0.208 0.858 6.0?0.856</cell></row><row><cell>denotes an ensemble model.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE XIII SELD</head><label>XIII</label><figDesc>PERFORMANCES OF SOTA SYSTEMS AND SALSA-BASED MODELS ON TEST SPLIT OF TNSSE 2021 DATASET.</figDesc><table><row><cell>System</cell><cell cols="2">Format ER ?20?F?20?L E CD LR CD</cell></row><row><cell>2021 Metrics</cell><cell></cell><cell></cell></row><row><cell>DCASE baseline [25]</cell><cell>FOA</cell><cell>0.73 0.307 24.5?0.448</cell></row><row><cell>('21 #1) Shimada et al. [23]</cell><cell>FOA</cell><cell>0.43 0.699 11.1?0.732</cell></row><row><cell>('21 #4) Lee et al. [17]</cell><cell>FOA</cell><cell>0.46 0.609 14.4?0.733</cell></row><row><cell>FOA SALSA w/o TTA</cell><cell>FOA</cell><cell>0.404 0.724 12.5?0.727</cell></row><row><cell>MIC SALSA w/o TTA</cell><cell>MIC</cell><cell>0.408 0.715 12.6?0.728</cell></row><row><cell>FOA SALSA w/ TTA</cell><cell>FOA</cell><cell>0.376 0.744 11.1?0.722</cell></row><row><cell>MIC SALSA w/ TTA</cell><cell>MIC</cell><cell>0.376 0.735 11.2?0.722</cell></row><row><cell>('21 #2) Nguyen et al. [29]</cell><cell>FOA</cell><cell>0.37 0.737 11.2?0.741</cell></row><row><cell>denotes an ensemble model.</cell><cell></cell><cell></cell></row><row><cell cols="3">when more data are available for training, models trained on</cell></row><row><cell cols="3">MIC SALSA features performed better than models trained on</cell></row><row><cell cols="3">FOA SALSA features across all metrics. The FOA SALSA</cell></row><row><cell cols="3">model has competitive performance compared to</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="283" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bird detection in audio: A survey and a challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Workshop Mach. Learn. Signal Process</title>
		<meeting>IEEE Int. Workshop Mach. Learn. Signal ess</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Audio Surveillance of Roads: A System for Detecting Anomalous Sounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Foggia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Petkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saggese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Strisciuglio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intelligent Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="279" to="288" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Localization of simultaneous moving sound sources for mobile robot using a frequency-domain steered beamformer approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Valin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hadjou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rouat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robotics Automation</title>
		<meeting>IEEE Int. Conf. Robotics Automation</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1033" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sound Event Localization and Detection of Overlapping Sources Using Convolutional Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adavanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Politis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nikunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Signal Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="48" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Classification of spatial audio location and content using Convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirvonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 138th</title>
		<meeting>138th</meeting>
		<imprint>
			<publisher>Audio Eng. Soc. Conv</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="622" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Polyphonic Sound Event Detection and Localization using a Two-Stage Strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Workshop Detect</title>
		<meeting>4th Workshop Detect</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">First Order Ambisonics Domain Spatial Augmentation for DNN-based Direction of Arrival Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mazzon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yasuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Workshop Detect</title>
		<meeting>4th Workshop Detect</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="154" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Sequence Matching Network for Polyphonic Sound Event Localization and Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust. Speech Signal ess</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="71" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A General Network Architecture for Sound Event Localization and Detection Using Transfer Learning and Recurrent Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust. Speech Signal ess</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="935" to="939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Event-independent Network for Polyphonic Sound Event Localization and Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Workshop Detect</title>
		<meeting>5th Workshop Detect</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An Improved Event-Independent Network for Polyphonic Sound Event Localization and Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust. Speech Signal ess</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="885" to="889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ambisonic Signal Processing DNNs Guaranteeing Rotation, Scale and Time Translation Equivariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Niwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1449" to="1462" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On Multitask Loss Function for Audio Event Detection and Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Q K</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mcloughlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mertins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Workshop Detect</title>
		<meeting>5th Workshop Detect</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A Four-Stage Data Augmentation Approach to ResNet-Conformer Based Acoustic Modeling for Sound Event Localization and Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ACCDOA: Activity-Coupled Cartesian Direction of Arrival Representation for Sound Event Localization And Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust. Speech Signal ess</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="915" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sound Event Localization and Detection Using Cross-Modal Attention and Parameter Sharing for DCASE2021 Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-B</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sound event localization and detection based on multiple DOA beamforming and multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Conf. Int</title>
		<meeting>Annu. Conf. Int</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5091" to="5095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sound Event Localization and Detection with Various Loss Functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-scale Network for Sound Event Localization and Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Emmanuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Horton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sound Source Detection, Localization And Classification Using Consecutive Ensemble Of CRNN Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kapka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewandowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The USTC-iFlytek System for Sound Event Localization and Detection of DCASE2020 Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ensemble of ACCDOA-and EINV2-based Systems with D3Nets and Impulse Response Simulation for Sound Event Localization and Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tsunoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Dataset of Reverberant Spatial Sound Scenes with Moving Sources for Sound Event Localization and Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Politis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adavanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Workshop Detect</title>
		<meeting>5th Workshop Detect</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="165" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Dataset of Dynamic Reverberant Sound Scenes with Directional Interferers for Sound Event Localization and Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Politis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adavanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deleforge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Workshop Detect</title>
		<meeting>6th Workshop Detect</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="125" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Multiroom Reverberant Dataset for Sound Event Localization and Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adavanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Politis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Workshop Detect</title>
		<meeting>4th Workshop Detect</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust DOA estimation of multiple speech sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust. Speech Signal ess</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2287" to="2291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust Source Counting and DOA Estimation Using Spatial Pseudo-Spectrum and Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2626" to="2637" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DCASE 2021 Task 3: Spectrotemporally-aligned Features for Polyphonic Sound Event Localization and Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watcharasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Detection and separation of speech events in meeting recordings using a microphone array</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ogata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Audio, Speech, Music. Process</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Localization of multiple acoustic sources with small arrays using a coherence test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Lockwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2136" to="2147" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Real-Time Multiple Sound Source Localization and Counting Using a Circular Microphone Array</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavlidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Puigt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mouchtaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2193" to="2206" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robust speech recognition using beamforming with adaptive microphone gains and multichannel noise reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop Automatic Speech Recognit. Underst</title>
		<meeting>IEEE Workshop Automatic Speech Recognit. Underst</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="460" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ensemble of sequence matching networks for dynamic sound event localization, detection, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Workshop Detect</title>
		<meeting>5th Workshop Detect</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="120" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">DCASE 2019 Task 3: A two-step system for sound event localization and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayabalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unbiased MMSEbased noise power estimation with low complexity and low tracking delay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gerkmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1383" to="1393" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Speaker localization in reverberant rooms based on direct path dominance test statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rafaely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kolossa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust. Speech Signal ess</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6120" to="6124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Underdetermined direction of arrival estimation using acoustic vector sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saluev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Process</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="160" to="168" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Two-Stage Sound Event Localization and Detection using Intensity Vector and Generalized Cross-Correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Galindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">DOA estimation with histogram analysis of spatially constrained active intensity vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Delikaris-Manias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavlidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mouchtaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pulkki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust. Speech Signal ess</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="526" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2880" to="2894" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">DCASE 2021: Sound Event Localization and Detection with Directional Interference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adavanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Politis</surname></persName>
		</author>
		<ptr target="https://github.com/sharathadavanne/seld-dcase2021" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 34th AAAI Conf</title>
		<meeting>34th AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">SpecAugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Conf. Int</title>
		<meeting>Annu. Conf. Int</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Overview and Evaluation of Sound Event Localization and Detection in DCASE 2019</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Politis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adavanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="684" to="698" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Joint Measurement of Localization and Detection of Sound Events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adavanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Politis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop Appl. Signal Process. Audio Acoust</title>
		<meeting>IEEE Workshop Appl. Signal ess. Audio Acoust</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">On spatial aliasing in microphone arrays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dmochowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benesty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aff?s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1383" to="1395" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
