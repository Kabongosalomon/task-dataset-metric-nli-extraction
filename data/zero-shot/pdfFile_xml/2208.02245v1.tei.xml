<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MinVIS: A Minimal Video Instance Segmentation Framework without Video-based Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Huang</surname></persName>
							<email>deahuang@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
							<email>zhidingy@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>NVIDIA</roleName><forename type="first">Anima</forename><forename type="middle">Anandkumar</forename><surname>Caltech</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MinVIS: A Minimal Video Instance Segmentation Framework without Video-based Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code is available at: https://github.com/NVlabs/MinVIS</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose MinVIS, a minimal video instance segmentation (VIS) framework that achieves state-of-the-art VIS performance with neither video-based architectures nor training procedures. By only training a query-based image instance segmentation model, MinVIS outperforms the previous best result on the challenging Occluded VIS dataset by over 10% AP. Since MinVIS treats frames in training videos as independent images, we can drastically sub-sample the annotated frames in training videos without any modifications. With only 1% of labeled frames, Min-VIS outperforms or is comparable to fully-supervised state-of-the-art approaches on YouTube-VIS 2019/2021. Our key observation is that queries trained to be discriminative between intra-frame object instances are temporally consistent and can be used to track instances without any manually designed heuristics. MinVIS thus has the following inference pipeline: we first apply the trained query-based image instance segmentation to video frames independently. The segmented instances are then tracked by bipartite matching of the corresponding queries. This inference is done in an online fashion and does not need to process the whole video at once. MinVIS thus has the practical advantages of reducing both the labeling costs and the memory requirements, while not sacrificing the VIS performance. Code is available at: https://github.com/NVlabs/MinVIS Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video instance segmentation (VIS) aims to simultaneously detect, segment, and track object instances in videos <ref type="bibr" target="#b0">[1]</ref>. The requirement to accurately track object instances through an entire video makes VIS much more challenging than image instance segmentation. Most of the early approaches for VIS build on image instance segmentation models, and process videos on a per-frame basis <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. The segmented object instances for each frame are then matched temporally with a post-processing step. This post-processing step often involves manually designed heuristics that do not generalize well to challenging scenarios like occlusions and large appearance deformations.</p><p>Recent VIS works address this issue by taking a per-clip approach, where the spatial-temporal volume of a video is processed as a whole to directly predict the spatial-temporal mask for each object instance <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. Many of these end-to-end VIS approaches are built upon the recent advances of Transformers for end-to-end object detection <ref type="bibr" target="#b5">[6]</ref>. Given learned embeddings called queries, Transformers process the queries jointly with the input video using cross-attention, so that each of the processed queries can be used to predict the spatial-temporal mask for an object instance in the video.</p><p>While these per-clip methods have led to considerable improvements for VIS, using attention to process the whole video, especially longer ones, requires large memory and computation. It is also not straightforward to adapt per-clip methods from offline to online processing to reduce the computational requirements. This limits their practical application, and maintaining the effectiveness of these per-clip methods while improving their efficiency remains an active research direction <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>.  Another limitation for existing VIS methods is the requirement on annotation. Annotating object instance masks for each video frame is prohibitively expensive at scale. While there have been works that alleviate this annotation requirement through weak supervision or image-based annotation, there is still a significant performance gap compared to state-of-the-art fully-supervised methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Our Approach. We simultaneously address both of the aforementioned challenges of computational and labeling costs by showing that we can achieve state-of-the-art VIS performance by only training a query-based image instance segmentation model. During inference, MinVIS first applies the query-based image instance segmentation to video frames independently. The segmented instances are then associated by bipartite matching of the corresponding queries. MinVIS processes each frame independently in an online fashion and does not need to process the whole video at once. MinVIS does not use any video-based training procedure and thus does not need annotations for all the frames in a video. Our contributions are summarized below:</p><p>1. We show that video-based architecture and training are not required for competitive VIS performances. MinVIS outperforms previous state-of-the-art on YouTube-VIS 2019 and 2021 datasets by 1% and 3% AP while only training an image instance segmentation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>We show that image instance segmentation models capable of segmenting occluded instances are also well suited to track occluded instances in videos in our framework. MinVIS outperforms its per-clip counterpart by over 13% AP on the challenging Occluded VIS (OVIS) dataset, which is over 10% improvement compared to the previous best performance on the dataset.</p><p>3. Our image-based approach allows us to significantly sub-sample the required segmentation annotations in training without any change to the model. With only 1% of labeled frames, MinVIS outperforms or is comparable to fully-supervised state-of-the-art approaches on all three datasets.</p><p>Our key observation is that queries trained to be discriminative between intra-frame object instances are temporally consistent and can be used to track instances without being trained with video-based loss functions. MinVIS achieves this by requiring its image instance segmentation model to generate masks by convolving query embeddings with features of the whole input image, including regions of other object instances. A query is thus trained to only have high responses on features of its corresponding instance. Other query embeddings should instead have low responses on these features because instance masks are non-overlapping. This design encourages the query embeddings for different instances in a frame to be well-separated. On the other hand, the query embeddings that segment the same instance from two consecutive frames still need to be similar enough since the instance's image features to be convoluted do not change drastically between frames. This leads to temporally consistent query embeddings for tracking without the need of video-based training.</p><p>MinVIS thus has the following design for inference: We first apply a query-based image instance segmentation model on video frames independently. The segmented instances are then associated between frames by bipartite matching of the corresponding query embeddings. This post-processing step does not need any additional heuristics based on mask overlaps or classification scores as in previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11]</ref>. This is because query embeddings already contain these information to track the instances. An overview of MinVIS's training and inference is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Since video frames are treated as independent images to train MinVIS, there is no requirement to annotate all the frames in a video for training. This allows us to significantly sub-sample and reduce the annotation without any change to our model. We find that on YouTube-VIS 2019/2021 datasets <ref type="bibr" target="#b0">[1]</ref>, where there are less variations between video frames, using only 1% of labeled frames leads to less than 3% drop in AP for MinVIS.</p><p>We further evaluate MinVIS on the Occluded VIS (OVIS) dataset <ref type="bibr" target="#b11">[12]</ref>. One common critique of per-frame approaches is that their tracking heuristics based on mask overlaps would not work when there are heavy occlusions. This is not the case for MinVIS, as we do not use any manually designed heuristics. We show that our query-matching approach generalizes to occluded scenarios. MinVIS with Swin Transformers backbone <ref type="bibr" target="#b12">[13]</ref> achieves 39.4% AP on OVIS, which is over 10% improvement from the previous best result on the dataset <ref type="bibr" target="#b13">[14]</ref>. We further show that our image-based strategy leads to easier and better learning on OVIS. MinVIS outperforms its per-clip counterpart by over 13% AP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Video Instance Segmentation. Per-frame approaches for VIS process each frame independently and later track instances by post-processing. MaskTrack R-CNN <ref type="bibr" target="#b0">[1]</ref> adds a tracking head to Mask R-CNN <ref type="bibr" target="#b14">[15]</ref> for VIS. MaskProp <ref type="bibr" target="#b15">[16]</ref> instead adds a mask propagation head to propagate object instance masks. CrossVIS <ref type="bibr" target="#b1">[2]</ref> uses crossover learning to improve instance representation across video frames. QueryTrack <ref type="bibr" target="#b10">[11]</ref> adds a contrastive tracking head to QueryInst <ref type="bibr" target="#b16">[17]</ref> for instance association. Concurrent work IDOL <ref type="bibr" target="#b17">[18]</ref> shows that per-frame models can still outperform per-clip models. Our approach also builds on image instance segmentation models, but unlike previous approaches, we need neither additional parameters nor additional losses to apply to VIS. Our query embeddings from image instance segmentation can directly be used for tracking without video-based training.</p><p>Recent per-clip approaches build on the success of Detection Transformer (DETR) <ref type="bibr" target="#b5">[6]</ref>. VisTR <ref type="bibr" target="#b3">[4]</ref> adopts the query-based approach of DETR to VIS, and there has been several follow-up works, such as Mask2Former-VIS <ref type="bibr" target="#b2">[3]</ref> and SeqFormer <ref type="bibr" target="#b4">[5]</ref>. One limitation of these approaches is the need to process the whole video at once. IFC <ref type="bibr" target="#b6">[7]</ref> reduces the overhead of temporal message passing by using memory tokens. TeViT <ref type="bibr" target="#b7">[8]</ref> uses a parameter-shared self attention to efficiently model temporal contexts. We also use a query-based approach, but instead of using cross-attention to process the whole video, we process each frame independently while not losing VIS performance. Our use of queries to associate instances is also related to other works that build on DETR for tracking in related fields. For example, MOTR <ref type="bibr" target="#b18">[19]</ref> and TrackFormer <ref type="bibr" target="#b19">[20]</ref> use identity preserving track queries for multi-object tracking (MOT).</p><p>Reducing Supervision for Video Instance Segmentation. Annotating instance masks for each video frame can be prohibitively expensive. Compared to video object segmentation <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref> and image instance segmentation <ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>, there have been less works on reducing supervision for VIS <ref type="bibr" target="#b27">[28]</ref>. FlowIRN <ref type="bibr" target="#b9">[10]</ref> extends IRN <ref type="bibr" target="#b23">[24]</ref> with motion and temporal consistency cues to have a weakly-supervised VIS framework that only requires classification labels. SOLO-Track <ref type="bibr" target="#b8">[9]</ref> learns to track instances without video annotations. It uses instance contrastive learning on SOLO <ref type="bibr" target="#b28">[29]</ref> to learn grid cell embeddings for instance tracking. We make the same observation that disciminating between instances within frames is beneficial or even sufficient for instance tracking. However, unlike our query-based association, the grid cell embeddings still need threshold-based post-processing and additional loss functions to better handle birth and death of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>MinVIS is a minimal VIS framework that does not require video-based training and thus can be easily applied to real-world applications that only have sparse image instance segmentation annotations. MinVIS is a two stage approach: (1) image instance segmentation on each frame independently, (2) associating instances between frames by matching queries. We will first discuss the image instance segmentation architecture in MinVIS. We will then discuss the temporal association of object instances. Finally, we will discuss training and reducing supervision for MinVIS.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image Instance Segmentation Architecture for VIS</head><p>MinVIS builds on the query-based transformer architectures for detection and segmentation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>, which has the following main components: (1) Image Encoder that learn to extract features from input images. (2) Transformer Decoder that processes the outputs of Image Encoder to iteratively update the query embeddings. (3) Prediction Heads that use the final query embeddings to predict desired outputs (e.g., segmentation masks, class labels). The queries play an important role for the success of such end-to-end pipeline for set prediction with unknown number of outputs. The number of queries are selected as the maximum number of output instanes of the model. During inference, a subset of queries predict ? outputs to dynamically adjust the number of valid outputs.</p><p>An overview of MinVIS's image instance segmentation is shown in <ref type="figure" target="#fig_2">Figure 2</ref> The constraint to have Q convolve with the whole feature map F ?1 is important for MinVIS. Consider two queries Q i and Q j that corresponds to two distinct object instances and thus non-overlapping masks. This formulation ensures that Q i should only have high inner products with features in F ?1 that are covered by the mask of instance i. Since the instance masks are non-overlapping, Q j should instead have low inner products with these features. This implicitly constrains the query embeddings to be discriminative between each other. On the other hand, if we apply this pipeline to two consecutive frames X t and X t+1 . Then Q t i should still have higher inner product with Q t+1 i compared to Q t+1 j . This is because Q t+1 i and Q t+1 j are also discriminative between each other, while Q t i and Q t+1 i both need to have high inner products with features of the same instance, which do not change drastically between consecutive frames. We visualize our learned query embeddings in <ref type="figure" target="#fig_4">Figure 3</ref>. Each plot is for a video. Query embeddings belonging to the same instance (from different frames) have the same color. These embeddings are already grouped by instances without any video-based training. Further details are in Section 4.2.</p><p>While not all image instance segmentation models satisfy our architectural constraints (e.g., ROIbased architectures), we believe these are rather flexible designs that are compatible with various query-based instance segmentation models. We use Mask2Former <ref type="bibr" target="#b29">[30]</ref> in this work. The Image Encoder E includes both the backbone and the pixel decoder of Mask2Former. We also find that having fully-connected layers to further process Q before convolution is beneficial to the performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tracking by Query Matching</head><p>MinVIS is a per-frame two-stage approach, which requires a post-processing step to temporally associate instances. This post-processing often involves heuristics like mask overlaps, which does not generalize well to scenarios with heavy occlusions. Unlike previous two-stage approaches, MinVIS associate instances solely based on the query embeddings Q. Given two consecutive frames X t and X t+1 . We have Q t = T (F t ,Q), where F t = E(X t ), and similarly for Q t+1 . Q t i is the query embedding for instance i. Tracking in MinVIS is done by the assignment of applying the Hungarian algorithm on a score matrix S, where S ij = cos(Q t i , Q t+1 j ). cos(?, ?) is the cosine similarity. This approach is less affected by occlusions because each instance is represented by a query that does not have a spatial extent. In addition, we do not need heuristics to handle the birth and death of object instances in this framework. Since the number of queries is larger than the number of instances, there are queries that produce empty masks. The death of an object instance happens when its embedding is matched to such a null query. On the other hand, the birth of an instance is correctly handled if the matched query embeddings have been null before the actual birth of the object instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training with Less Supervision for VIS</head><p>Since the matching process does not need training, only the image instance segmentation model needs to be trained. There are two outputs of the model: classification scores O ? R N,K and segmentation masks M ? R N,H,W for N queries, K object classes, and image size H, W . We can process the groundtruth video instances to groundtruth image instances O * ? R L,K and M * ? R L,H,W , where L is the number of groundtruth instances (N &gt;&gt; L) and O * j is a one-hot vector of groundtruth class. Given a loss function L(O i , M i , O * j , M * j ) between predicted instance i and groundtruth instance j, we first use bipartite matching to find the assignments between predicted and groundtruth instances that minimize the overall loss function, and train on those matched predictions with the loss function.</p><p>More specifically, there are two terms in the loss function: L cls and L mask . We use cross entropy loss for L cls and binary cross entropy plus dice loss <ref type="bibr" target="#b31">[32]</ref> for L mask as in previous works <ref type="bibr" target="#b29">[30]</ref>. Both terms are purely image-based. The groudtruth video instances are first processed to instances for each frame independently. Therefore, even if there are only sparse frames labeled with instance, we can still train our model with the annotated frames. This provide a straightforward way to reduce the supervision for VIS. <ref type="figure" target="#fig_2">Figure 2</ref>(b) shows the annotation sub-sampling to reduce supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets. We evaluate MinVIS on three datasets: YouTube-VIS 2019/2021 <ref type="bibr" target="#b0">[1]</ref> and Occluded VIS (OVIS) <ref type="bibr" target="#b11">[12]</ref>. The YouTube-VIS datasets contain 40 object classes. YouTube-VIS 2019 contains 2238/302/343 videos for training/validation/testing, while YouTube-VIS 2021 expands the dataset to 2985/421/453 videos for training/validation/testing, and includes higher quality annotations. OVIS has 25 object classes and contains 607/140/154 for training/validation/testing. While the number of videos is smaller, OVIS has more objects per frame, and the videos are also longer. This leads to more annotated instance masks compared to the YouTube-VIS datasets. In addition, OVIS also has much higher Bounding-box Occlusion Rate (0.22 v.s. 0.06/0.07) compared to the YouTube-VIS datasets, which indicates heavier occlusions between object instances.</p><p>Metrics. We follow previous works and use Average Precision (AP) and Average Recall (AR) as evaluation metrics <ref type="bibr" target="#b0">[1]</ref>. AP is computed based on 10 intersection-over-union (IoU) thresholds from 50% to 95% with 5% increment. The reported AP and AR are first computed for each object class and then averaged over all classes. All three datasets have public evaluation servers.</p><p>Baselines. We focus on results using ResNet50 and Swin-L backbones. ResNet50 is still the most widely used backbone for VIS, while Swin-L gives the best performances. Not all methods report both backbones on all three datasets. We include results that are available. For YouTube-VIS datasets, we include recent state-of-the-art results from SeqFormer <ref type="bibr" target="#b4">[5]</ref>, TeViT <ref type="bibr" target="#b7">[8]</ref>, and Mask2Former-VIS <ref type="bibr" target="#b2">[3]</ref>. These are all Transformer-based per-clip approaches as this paradigm has been recently dominating the field. On the other, out of of these methods, only TeViT is applied to OVIS. Therefore, we further compare to CMaskTrack R-CNN <ref type="bibr" target="#b11">[12]</ref>, CrossVIS <ref type="bibr" target="#b1">[2]</ref>, and STC <ref type="bibr" target="#b32">[33]</ref>. These are all methods that allow online processing. Even TeViT uses a near online inference for OVIS <ref type="bibr" target="#b33">[34]</ref>. This is because OVIS has longer videos that would lead to out-of-memory for most of the per-clip approaches.</p><p>Out of all the baselines, Mask2Former-VIS <ref type="bibr" target="#b2">[3]</ref> is the most related to MinVIS, as MinVIS is built on Mask2Former in this work. Mask2Former-VIS thus can be seen as the per-clip version of ours and is an important baseline for comparison. Therefore, we further apply Mask2Former-VIS on the OVIS dataset. Due to memory constraints, the videos in OVIS are first split into clips of length 30. We use the same post-processing as MinVIS to merge the outputs from these clips. Implementation Details. Unless otherwise noted, our hyper-parameters follow Mask2Former-VIS <ref type="bibr" target="#b2">[3]</ref>. All models are pre-trained with COCO instance segmentation <ref type="bibr" target="#b34">[35]</ref>. For OVIS, we use the same hyper-parameters as YouTube-VIS 2019 except training for 10k iterations instead of 6k. For training losses, the weights are 5.0 for L mask and 2.0 for L cls . All results of MinVIS are averaged over 3 random seeds. We sub-sample training to X% by uniformly sampling frames in the video. We set a minimum of 1 frame per video. Since YouTube-VIS datasets often have videos less than a hundred frames. Our 1% results are better seen as one frame per video results for YouTube-VIS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main Results</head><p>YouTube-VIS 2019. The results for YouTube-VIS 2019 are shown in <ref type="table" target="#tab_1">Table 1</ref>. MinVIS achieves highest AP and most other metrics for both ResNet-50 and Swin-L backbones. SeqFormer shows that it is beneficial to jointly train with images from COCO <ref type="bibr" target="#b34">[35]</ref> that contain YouTubeVIS categories (+C80k in table). TeViT proposes messenger shift transformer (MsgShifT) that are as efficient as ResNet backbones, while improving the VIS performances. Our ResNet-50 results match or outperform their results without further modifications. Compared to the state-of-the-art Mask2Former-VIS, which can be seen as the per-clip approach to apply Mask2Former to VIS, MinVIS consistently outperforms by around 1% for both backbones. MinVIS with X% means sub-sampling the annotated frames for each video in training. Since there are less temporal variations for videos in YouTube-VIS 2019, MinVIS with 1% of training frames only reduces AP by 2.6%. This significantly reduces the annotation effort while not sacrificing much performance.</p><p>YouTube-VIS 2021. The results for YouTube-VIS 2021 are shown in <ref type="table" target="#tab_2">Table 2</ref>. On this more challenging dataset, the performance improvements for MinVIS increase compared to YouTube-VIS 2019. Without better backbone like TeViT and additional training data like SeqFormer, our ResNet-50 results outperform by a large margin for all metrics. This is the also case for Swin-L. MinVIS outperforms previous state-of-the-art Mask2Former-VIS by 2.7%. By using only 1% of training frames, MinVIS's AP decrease by only 2.4%, which means that our 1% result still outperforms  previous state-of-the art. We also see that on YouTube-VIS datasets, reducing the annotations by 10x does not significantly affect the performances (-0.6% AP for 2019 and -0.4% AP for 2021).</p><p>Occluded VIS (OVIS). The results for OVIS are shown in <ref type="table" target="#tab_3">Table 3</ref>. Mask2Former-VIS* denotes our application of Mask2Former-VIS to OVIS. Since videos in OVIS can have up to hundreds of frames, we apply Mask2Former-VIS to non-overlapping sliding windows of length 30. The outputs from these clips are then merged by our post-processing. MinVIS is an online method and does not need modification to apply to OVIS. MinVIS shows significant improvement compared to existing works on OVIS. With ResNet-50 backbone, MinVIS outperforms previous state-of-the-art TeViT with MsgShifT backbone by 7.6% AP. With Swin-L backbone, MinVIS outperforms previous best result MaskTrack R-CNN*+SWA by 10.5% AP, which is the winner of the 1st OVIS Challenge. Their key observation is that sampling frames that are far apart in OVIS leads to drastically different features and makes it hard to train MaskTrack R-CNN. This is in contrast to YouTube-VIS datasets, in which the videos are shorter and there are less temporal variations within the video. We observe the same phenomenon when training Mask2Former-VIS*. However, the limited sampling reference frame strategy of MaskTrack R-CNN*+SWA still does not work in this case. Mask2Former-VIS* uses a fully end-to-end loss instead of an explicit tracking loss to learn temporal association, which makes the learning even harder in OVIS. On the other hand, MinVIS is image-based and does not need to worry about the temporal sampling strategy to train the model. This is contrary to common belief that per-frame approaches are worse for scenarios with heavy occlusions. Instead, our image-based approach leads to easier and better learning on OVIS. We show that an image instance segmentation model that can segment occluded instances in each frame is also good at associating such instances  across frames. <ref type="figure" target="#fig_5">Figure 4</ref> shows qualitative results. MinVIS stably tracks all the sheep in the video. Using mask overlap based heuristics instead of query matching leads to multiple identity switches in tracking. Mask2Former-VIS* does not have as good segmentation masks because its training is interfered by heavy occlusions and large appearance deformations between frames in OVIS. <ref type="figure">Figure 5</ref> shows additional qualitative results on failure cases of MinVIS on the OVIS dataset. As discussed in Section 3.2, MinVIS does not use heuristics to handle the birth and death of object instances. The death of an object instance is correctly handled if its query is matched to a query in the next frame that produces an empty mask. Despite its simplicity and effectiveness, the drawback of this approach is that there is nothing stopping the model from matching the disappearing query to a query with a non-empty mask. From t 3 to t 4 in the top row of <ref type="figure">Figure 5</ref>, as the fish in the lower left leaves the frame, MinVIS associates it to a mask covering the tail of a nearby fish. From t 4 to t 5 , when the fish in the upper left leaves the frame, MinVIS again associates it to a mask covering the head of a nearby fish. Since the associated masks are non-empty, MinVIS fails to correctly handle the death of these instances. On the other hand, when the two dogs in the bottom row of  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analyzing Query Matching</head><p>The success of MinVIS depends on whether query matching is good for tracking instances. We conduct ablation studies by comparing it to manually designed heuristics. We use the bipartite matching heuristics in Section 3.3 for tracking by treating instances in the last frame as groundtruth. The results are in <ref type="table" target="#tab_4">Table 4</ref>. Using heuristics lead to around 3% AP drop on both YouTube-VIS 2019 and 2021. It leads to more significant drop on OVIS (7.7%) due to heavier occlusions. We also combine query matching and heuristics with equal weights, which has mixed results. Our query only approach is simpler and more generalizable without loss of performance.</p><p>We visualize the learned query embeddings by t-SNE <ref type="bibr" target="#b35">[36]</ref> in <ref type="figure" target="#fig_4">Figure 3</ref>. Each plot is for a video in the training set. We visualize the training set to see the effect of an image only objective (to segment instances in an image) on query embeddings across different frames. Query embeddings of the same instance have the same color. We obtain the instance IDs for queries by bipartite matching its outputs to groundtruth instances, which have consistent IDs across frames. Without any video-based tracking objective, query embeddings of the same instances are already grouped into distinct clusters, even for the OVIS dataset. This supports our design of only using image-based objectives. In Appendix E, we further visualize query embeddings on videos not used in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effect of Video-based Training</head><p>While we have shown that MinVIS achieves state-of-the-art VIS performance without video-based training, it is interesting to see how we can leverage video annotation when it is available. We use the video annotation to supervise our matching as in previous per-frame works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b10">11]</ref>. Given two sampled frames, we use a hinge loss to ensure that the correct associations of queries have the highest inner products compared to that of other queries between the two frames <ref type="bibr" target="#b10">[11]</ref>. The results are in <ref type="table" target="#tab_5">Table 5</ref>. The "Supervised Matching" rows mean directly applying the matching supervision to the original frame sampling process. In our case, this means that the two sampled frames might be separated up to 20 frames. As pointed out in previous works, frames that are far separated increase the training difficulty and can hurt model performances especially with occlusions <ref type="bibr" target="#b13">[14]</ref>. We thus also consider the "Limited Range" training to only sample consecutive frames for supervised matching, as we only need to match consecutive frames. From the results, directly applying "Supervised Matching" hurt performances on all three datasets. Adding "Limited Range" recovers most of the performances for YouTube-VIS 2021 and OVIS. On OVIS, it even marginally outperforms the original MinVIS. However, this improvement does rely on the dataset dependent sampling range. We believe it is possible and important to use video-based training to further improve MinVIS, although this would take away MinVIS's practical advantages of only needing sparse annotations and having a simple training pipeline. Appendix A discusses further limitations of not using video information in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We show that a purely image-based training procedure can lead to competitive performances for VIS. Our key finding is that instance tracking naturally emerges in query-based image instance segmentation models with proper architectural constraints. In addition to improving state-of-the-art approaches on YouTube-VIS 2019/2021, we show that this is particularly beneficial for OVIS. The image-based objective reduces the learning difficulty and leads to better performances. MinVIS only requires sparse frame annotations, which makes it much more applicable to real-world scenarios. We believe a promising direction to extend MinVIS is to explore ways to better leverage the video frames that are not annotated to further improve our performances with sub-sampled annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Limitations and Potential Negative Social Impacts</head><p>Limitation. We have discussed in the main paper on the possibility of improving MinVIS with video-based training. While we believe there are practical advantages of using our image-based VIS training pipeline, videos provide lots of extra information that we are not currently leveraging. In particular, temporal supervision from video should make our query embeddings even more suitable for tracking instances. In <ref type="figure">Figure 5</ref>, we visualize failure cases of using our current query embeddings for tracking. We conduct further analysis of Supervised Matching in Appendix F and believe further investigation along this direction should improve our approach. In addition to improving fullysupervised performance, we believe a promising direction is to explore semi-supervised learning at the frame level. In this case, one can temporally propagate the sub-sampled annotations in training to further improve the performance with reduced supervision.</p><p>Potential Negative Social Impacts. Video instance segmentation is a challenging video task, and thus provides fine-grained understanding of videos. The tracking and segmentation of objects of interest might be use for surveillance applications with negative social impact. While "person" is a category in the datasets used in this paper, no further protected attributes are annotated. Therefore, our trained models' performance on human subjects might not be fair with respect to protected attributes.   <ref type="bibr" target="#b36">[37]</ref>, whose videos are in turn from YouTube-8M <ref type="bibr" target="#b37">[38]</ref>. YouTube-8M uses public videos on YouTube but does not discuss the process to filter personally identifiable information or offensive content in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Tables with Standard Deviation</head><p>Tables with standard deviations are shown in <ref type="table" target="#tab_6">Table 6</ref>, <ref type="table" target="#tab_7">Table 7</ref>, and <ref type="table" target="#tab_9">Table 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Reducing Supervision for Mask2Former-VIS</head><p>The results for sub-sampling annotated frames for Mask2Former-VIS <ref type="bibr" target="#b2">[3]</ref> are shown in <ref type="table" target="#tab_10">Table 9</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Visualizing Query Embeddings in Evaluation</head><p>In the main paper, we visualize the learned query embeddings by t-SNE <ref type="bibr" target="#b35">[36]</ref> in <ref type="figure" target="#fig_4">Figure 3</ref>. The videos in <ref type="figure" target="#fig_4">Figure 3</ref> are in the training set and the figure is meant to understand how query embeddings in training cluster by instances without video-based loss function. We can similarly apply the same visualization to videos that are not used in training. One complication here is that this visualization uses groundtruth instance annotations to determine the corresponding instance ID for each query. However, the groundtruth annotation is not publicly available for the three datasets considered in  this work. Our reported results are obtained by submitting our predictions to the datasets' evaluation servers. We therefore perform this analysis by training a new model that only uses 90% of the training videos in YouTube-VIS 2019, and visualize the learned model's query embeddings during evaluation on the 10% videos that are not used to train the model. While the 10% videos are not used in training the model, we still have their groundtruth instances for visualization purposes. This provides a realistic approximation of how our query embeddings would look like for videos not used in training. The visualization is in <ref type="figure">Figure 6</ref>. Despite being noisier than training videos, the query embeddings are still grouped into clusters by object instances without any video-based training. This is also quantitatively supported by our state-of-the-art VIS performance on the three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Further Analysis of Supervised Matching</head><p>We conduct further analysis on the results in Section 4.3. We visualize the query embeddings on the same training videos with and without using supervised matching. In particular, we perform the <ref type="figure">Figure 6</ref>: Visualizing our query embeddings during evaluation on videos not used in training. Each plot is for a video, and query embeddings of the same instance (from different frames) have the same color. Despite being noisier than training videos, the query embeddings are still grouped into clusters by instance without any video-based training.  <ref type="figure">Figure 7</ref>: Visualizing learned query embeddings on the same videos with and without Supervised Matching. Plots in the same column visualize the same video V i . Supervised matching makes the embeddings more evenly distributed and smooths out the outliers in the embedding space. However, it is unclear whether this is overall beneficial to our tracking by query matching.  <ref type="table" target="#tab_5">Table 5</ref>. The visualizations are in <ref type="figure">Figure 7</ref>. While the plots look similar for most videos, one consistent trend we observe is that adding supervised matching makes the embeddings more evenly distributed and smooths out the outliers in the embedding space. This is a reasonable consequence as the objective encourages the embeddings from the same object instance to be closer to each other. However, it is unclear whether this is overall beneficial to our tracking by query matching. For example, in V 3 , the outliers are removed at the cost of mixing embeddings from different instances. We believe it is an important future work to further understand how to better leverage video information to improve MinVIS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Total loss Mask losses Classification loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Baseline Training Curves on OVIS</head><p>As discussed in the main paper, it is difficult to optimize our per-clip baseline on the challenging OVIS dataset. We have included the training curves in <ref type="figure" target="#fig_8">Figure 8</ref> for further illustration. Blue curves are MinVIS and orange curves are Mask2Former-VIS. While the classification loss still optimizes well on OVIS, the per-clip baseline has difficulty optimizing mask related losses.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) MinVIS trains a query-based image instance segmentation model (Image Encoder + Transformer Decoder) using each frame independently. (b) During inference, the trained image instance segmentation model is used for video instance segmentation by bipartite matching of query embeddings across frames. MinVIS does not require further manually designed heuristics for tracking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Architectural Constraints for Image Instance Segmentation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>(a) MinVIS's main architectural constraint is to require the segmentation masks M be generated by convolving the query embeddings Q with the final feature map F ?1 . This makes the query embeddings discriminative between each instances. (b) MinVIS's image-based approach allows direct annotation subsampling of training videos without any modification to the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a). Given an image X ? R H,W , the Image Encoder E extracts a set of features F = E(X) from the image. F = {F 0 . . . F ?1 } is a sequence of multi-scale feature maps F i ? R Hi,Wi,Ci . F ?1 denotes the final output of E. The N initial query embeddingsQ ? R N,C are learnable parameters, where N is a large enough number of outputs. The Transformer Decoder T then take both F andQ to iteratively obtain Q = T (F ,Q), Q ? R N,C . While most recent works focus on the design of T to better process F for Q, MinVIS's architectural constraints are on the Prediction Heads. There are two outputs for each instance: classification and segmentation mask. The classification scores O = C(Q), O ? R N,K for K classes are the output of Classification Head C, and Q should contain the class information for each instance. For segmentation masks M ? R N,H,W , MinVIS requires that M be generated by convolving the query embeddings Q with the final feature map F ?1 . The shape for F ?1 is thus H, W, C. We have M = ?(Q * F ?1 ), where ?(?) is the sigmoid function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Occluded VIS YouTube-VIS 2021 YouTube-VIS 2019 Visualizing our learned query embeddings with only image-based training. Each plot is for a video, and query embeddings of the same instance (from different frames) have the same color. Query embeddings are already grouped into clusters by instance without any video-based training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results on OVIS. MinVIS stably tracks all the sheep in the video. Using mask overlap based heuristics instead leads to multiple identity switches in tracking. Mask2Former-VIS* uses per-clip training that is difficult to optimize on the challenging OVIS dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 are covered in t 2 ,Figure 5 :</head><label>25</label><figDesc>MinVIS correctly associates their queries to empty masks. MinVIS further correctly handles the object births in t 3 . However, MinVIS is limited by the segmentation of the image segmentation model, which fails to segment the close-up person. Failure cases of MinVIS on OVIS. When an object instance disappear from a video, MinVIS can fail by associating its query embedding to a wrong mask without overlap (top). This is because we do not use mask overlap heuristics in our work. On the other hand, we are also limited by the image instance segmentation model, which might not work well on close-up objects (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Comparing the training curves of MinVIS and Mask2Former-VIS on OVIS. Blue curves are MinVIS and orange curves are Mask2Former-VIS. analysis on YouTube-VIS 2019 and compare MinVIS v.s. MinVIS + Supervised Matching + Limited Range, which hurts performance the most in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>YouTube-VIS 2019 results. C80k indicates joint training with COCO images that have YouTube-VIS categories. MinVIS with X% means sub-sampling the annotated frames in training.MethodBackbone Training AP AP 50 AP 75 AR 1 AR 10</figDesc><table><row><cell>TeViT [8]</cell><cell>R50</cell><cell>Full</cell><cell>42.1 67.8</cell><cell cols="3">44.8 41.3 49.4</cell></row><row><cell>TeViT [8]</cell><cell cols="2">MsgShifT Full</cell><cell>46.6 71.3</cell><cell cols="3">51.6 44.9 54.3</cell></row><row><cell>SeqFormer [5]</cell><cell>R50</cell><cell>Full</cell><cell>45.1 66.9</cell><cell cols="3">50.5 45.6 54.6</cell></row><row><cell>SeqFormer [5]</cell><cell>R50</cell><cell cols="2">Full+C80k 47.4 69.8</cell><cell cols="3">51.8 45.5 54.8</cell></row><row><cell cols="2">Mask2Former-VIS [3] R50</cell><cell>Full</cell><cell>46.4 68.0</cell><cell>50.0</cell><cell>-</cell><cell>-</cell></row><row><cell>MinVIS</cell><cell>R50</cell><cell>Full</cell><cell>47.4 69.0</cell><cell cols="3">52.1 45.7 55.7</cell></row><row><cell>TeViT [8]</cell><cell>Swin-L</cell><cell>Full</cell><cell>56.8 80.6</cell><cell cols="3">63.1 52.0 63.3</cell></row><row><cell>SeqFormer [5]</cell><cell>Swin-L</cell><cell cols="2">Full+C80k 59.3 82.1</cell><cell cols="3">66.4 51.7 64.4</cell></row><row><cell cols="2">Mask2Former-VIS [3] Swin-L</cell><cell>Full</cell><cell>60.4 84.4</cell><cell>67.0</cell><cell>-</cell><cell>-</cell></row><row><cell>MinVIS</cell><cell>Swin-L</cell><cell>Full</cell><cell>61.6 83.3</cell><cell cols="3">68.6 54.8 66.6</cell></row><row><cell>MinVIS</cell><cell>Swin-L</cell><cell>1%</cell><cell>59.0 81.6</cell><cell cols="3">64.7 54.0 64.0</cell></row><row><cell>MinVIS</cell><cell>Swin-L</cell><cell>5%</cell><cell>59.3 81.4</cell><cell cols="3">65.8 53.8 64.1</cell></row><row><cell>MinVIS</cell><cell>Swin-L</cell><cell>10%</cell><cell>61.0 83.0</cell><cell cols="3">67.7 54.6 66.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>TeViT [8]</cell><cell cols="2">MsgShifT Full</cell><cell>37.9 61.2</cell><cell cols="3">42.1 35.1 44.6</cell></row><row><cell>SeqFormer [5]</cell><cell>R50</cell><cell cols="2">Full+C80k 40.5 62.4</cell><cell cols="3">43.7 36.1 48.1</cell></row><row><cell cols="2">Mask2Former-VIS [3] R50</cell><cell>Full</cell><cell>40.6 60.9</cell><cell>41.8</cell><cell>-</cell><cell>-</cell></row><row><cell>MinVIS</cell><cell>R50</cell><cell>Full</cell><cell>44.2 66.0</cell><cell cols="3">48.1 39.2 51.7</cell></row><row><cell>SeqFormer [5]</cell><cell>Swin-L</cell><cell cols="2">Full+C80k 51.8 74.6</cell><cell cols="3">58.2 42.8 58.1</cell></row><row><cell cols="2">Mask2Former-VIS [3] Swin-L</cell><cell>Full</cell><cell>52.6 76.4</cell><cell>57.2</cell><cell>-</cell><cell>-</cell></row><row><cell>MinVIS</cell><cell>Swin-L</cell><cell>Full</cell><cell>55.3 76.6</cell><cell cols="3">62.0 45.9 60.8</cell></row><row><cell>MinVIS</cell><cell>Swin-L</cell><cell>1%</cell><cell>52.9 74.9</cell><cell cols="3">58.9 44.7 58.3</cell></row><row><cell>MinVIS</cell><cell>Swin-L</cell><cell>5%</cell><cell>54.3 76.3</cell><cell cols="3">60.1 45.4 59.5</cell></row><row><cell>MinVIS</cell><cell>Swin-L</cell><cell>10%</cell><cell>54.9 76.3</cell><cell cols="3">61.9 45.3 60.1</cell></row></table><note>YouTube-VIS 2021 Results. MinVIS's performance improvement increases on the more challenging YouTube-VIS 2021. Our 1% results already outperform previous state-of-the-art.Method Backbone Training AP AP 50 AP 75 AR 1 AR 10</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>OVIS Results. MinVIS significantly outperform existing approaches on OVIS. Our imagebased framework leads to easier and better learning on this dataset with heavy occlusions.MethodBackbone Training AP AP 50 AP 75 AR 1 AR 10</figDesc><table><row><cell>TeViT [8]</cell><cell></cell><cell cols="2">MsgShifT Full</cell><cell>17.4 34.9</cell><cell cols="3">15.0 11.2 21.8</cell></row><row><cell cols="2">CrossVIS [2]</cell><cell>R50</cell><cell>Full</cell><cell>14.9 32.7</cell><cell cols="3">12.1 10.3 19.8</cell></row><row><cell cols="2">CMaskTrack R-CNN [12]</cell><cell>R50</cell><cell>Full</cell><cell>15.4 33.9</cell><cell>13.1</cell><cell>9.3</cell><cell>20.0</cell></row><row><cell>STC [33]</cell><cell></cell><cell>R50</cell><cell>Full</cell><cell>15.5 33.5</cell><cell cols="3">13.4 11.0 20.8</cell></row><row><cell cols="2">Mask2Former-VIS*</cell><cell>R50</cell><cell>Full</cell><cell>17.3 37.3</cell><cell cols="3">15.1 10.5 23.5</cell></row><row><cell>MinVIS</cell><cell></cell><cell>R50</cell><cell>Full</cell><cell>25.0 45.5</cell><cell cols="3">24.0 13.9 29.7</cell></row><row><cell cols="3">MaskTrack R-CNN*+SWA [14] Swin-L</cell><cell>Full</cell><cell>28.9 56.3</cell><cell cols="3">26.8 13.5 34.0</cell></row><row><cell cols="2">Mask2Former-VIS*</cell><cell>Swin-L</cell><cell>Full</cell><cell>25.8 46.5</cell><cell cols="3">24.4 13.7 32.2</cell></row><row><cell>MinVIS</cell><cell></cell><cell>Swin-L</cell><cell>Full</cell><cell>39.4 61.5</cell><cell cols="3">41.3 18.1 43.3</cell></row><row><cell>MinVIS</cell><cell></cell><cell>Swin-L</cell><cell>1%</cell><cell>31.7 54.9</cell><cell cols="3">31.3 16.3 36.1</cell></row><row><cell>MinVIS</cell><cell></cell><cell>Swin-L</cell><cell>5%</cell><cell>35.7 60.1</cell><cell cols="3">35.8 17.3 39.9</cell></row><row><cell>MinVIS</cell><cell></cell><cell>Swin-L</cell><cell>10%</cell><cell>37.2 60.7</cell><cell cols="3">38.0 17.3 41.1</cell></row><row><cell>Mask2Former-VIS*</cell><cell>"</cell><cell>#</cell><cell>$</cell><cell>%</cell><cell></cell><cell>&amp;</cell></row><row><cell>Heuristics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MinVIS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of post-processing. Heuristics based on mask over laps lead to significant AP drop on OVIS. Our query matching approach has simpler design without loss of performance.</figDesc><table><row><cell>Method</cell><cell>Dataset</cell><cell cols="2">AP AP 50 AP 75 AR 1 AR 10</cell></row><row><cell>heuristics only</cell><cell cols="2">YouTube-VIS 2019 58.2 79.2</cell><cell>64.1 51.3 63.6</cell></row><row><cell cols="3">heuristics + query YouTube-VIS 2019 61.3 82.8</cell><cell>68.7 54.3 66.3</cell></row><row><cell>query only</cell><cell cols="2">YouTube-VIS 2019 61.6 83.3</cell><cell>68.6 54.8 66.6</cell></row><row><cell>heuristics only</cell><cell cols="2">YouTube-VIS 2021 52.7 75.3</cell><cell>57.3 44.4 58.3</cell></row><row><cell cols="3">heuristics + query YouTube-VIS 2021 55.1 76.2</cell><cell>61.9 46.0 60.7</cell></row><row><cell>query only</cell><cell cols="2">YouTube-VIS 2021 55.3 76.6</cell><cell>62.0 45.9 60.8</cell></row><row><cell>heuristics only</cell><cell>Occluded VIS</cell><cell>31.7 56.0</cell><cell>31.3 15.8 35.8</cell></row><row><cell cols="2">heuristics + query Occluded VIS</cell><cell>39.1 62.5</cell><cell>40.8 17.7 43.4</cell></row><row><cell>query only</cell><cell>Occluded VIS</cell><cell>39.4 61.5</cell><cell>41.3 18.1 43.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results for adding supervision to query matching. The supervision can provide dataset dependent benefit if the temporal hyper-parameters are selected properly.MethodDataset AP AP 50 AP 75 AR 1 AR 10</figDesc><table><row><cell>MinVIS</cell><cell cols="2">YouTube-VIS 2019 61.6 83.3</cell><cell>68.6 54.8 66.6</cell></row><row><cell cols="3">+ Supervised Matching YouTube-VIS 2019 61.0 82.1</cell><cell>67.6 54.3 66.1</cell></row><row><cell>+ Limited Range</cell><cell cols="2">YouTube-VIS 2019 60.7 82.5</cell><cell>67.0 54.1 65.5</cell></row><row><cell>MinVIS</cell><cell cols="2">YouTube-VIS 2021 55.3 76.6</cell><cell>62.0 45.9 60.8</cell></row><row><cell cols="3">+ Supervised Matching YouTube-VIS 2021 54.4 75.7</cell><cell>60.6 45.5 59.5</cell></row><row><cell>+ Limited Range</cell><cell cols="2">YouTube-VIS 2021 55.2 77.0</cell><cell>61.5 45.4 60.1</cell></row><row><cell>MinVIS</cell><cell>Occluded VIS</cell><cell>39.4 61.5</cell><cell>41.3 18.1 43.3</cell></row><row><cell cols="2">+ Supervised Matching Occluded VIS</cell><cell>38.7 61.2</cell><cell>39.6 17.9 42.4</cell></row><row><cell>+ Limited Range</cell><cell>Occluded VIS</cell><cell>39.6 63.2</cell><cell>41.0 18.2 43.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>YouTube-VIS 2019 results. C80k indicates joint training with COCO images that have YouTube-VIS categories. MinVIS with X% means sub-sampling the annotated frames in training.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Training</cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell><cell>AR1</cell><cell>AR10</cell></row><row><cell>TeViT [8]</cell><cell>R50</cell><cell>Full</cell><cell>42.1</cell><cell>67.8</cell><cell>44.8</cell><cell>41.3</cell><cell>49.4</cell></row><row><cell>TeViT [8]</cell><cell>MsgShifT</cell><cell>Full</cell><cell>46.6</cell><cell>71.3</cell><cell>51.6</cell><cell>44.9</cell><cell>54.3</cell></row><row><cell>SeqFormer [5]</cell><cell>R50</cell><cell>Full</cell><cell>45.1</cell><cell>66.9</cell><cell>50.5</cell><cell>45.6</cell><cell>54.6</cell></row><row><cell>SeqFormer [5]</cell><cell>R50</cell><cell>+C80k</cell><cell>47.4</cell><cell>69.8</cell><cell>51.8</cell><cell>45.5</cell><cell>54.8</cell></row><row><cell>Mask2Former [3]</cell><cell>R50</cell><cell>Full</cell><cell>46.4</cell><cell>68.0</cell><cell>50.0</cell><cell>-</cell><cell>-</cell></row><row><cell>MinVIS</cell><cell>R50</cell><cell>Full</cell><cell>47.4 ?0.2</cell><cell>69.0 ?2.1</cell><cell>52.1 ?0.2</cell><cell>45.7?0.2</cell><cell>55.7 ?0.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>YouTube-VIS 2021 Results. MinVIS's performance improvement increases on the more challenging YouTube-VIS 2021. Our 1% results already outperform previous state-of-the-art. The YouTube-VIS 2019/2021 datasets are under CC BY 4.0 License, and Occluded VIS is under CC BY-NC-SA 4.0 License. The videos in YouTube-VIS are from YouTube-VOS</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Training</cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell><cell>AR1</cell><cell>AR10</cell></row><row><cell>TeViT [8]</cell><cell>MsgShifT</cell><cell>Full</cell><cell>37.9</cell><cell>61.2</cell><cell>42.1</cell><cell>35.1</cell><cell>44.6</cell></row><row><cell>SeqFormer [5]</cell><cell>R50</cell><cell>+C80k</cell><cell>40.5</cell><cell>62.4</cell><cell>43.7</cell><cell>36.1</cell><cell>48.1</cell></row><row><cell>Mask2Former [3]</cell><cell>R50</cell><cell>Full</cell><cell>40.6</cell><cell>60.9</cell><cell>41.8</cell><cell>-</cell><cell>-</cell></row><row><cell>MinVIS</cell><cell>R50</cell><cell>Full</cell><cell>44.2?0.3</cell><cell>66.0?0.1</cell><cell>48.1?0.7</cell><cell>39.2 ?0.3</cell><cell>51.7 ?0.7</cell></row><row><cell>SeqFormer [5]</cell><cell>Swin-L</cell><cell>+C80k</cell><cell>51.8</cell><cell>74.6</cell><cell>58.2</cell><cell>42.8</cell><cell>58.1</cell></row><row><cell>Mask2Former [3]</cell><cell>Swin-L</cell><cell>Full</cell><cell>52.6</cell><cell>76.4</cell><cell>57.2</cell><cell>-</cell><cell>-</cell></row><row><cell>MinVIS</cell><cell>Swin-L</cell><cell>Full</cell><cell>55.3?0.2</cell><cell>76.6?0.3</cell><cell>62.0?0.8</cell><cell>45.9?0.2</cell><cell>60.8 ?0.3</cell></row><row><cell>MinVIS</cell><cell>Swin-L</cell><cell>1%</cell><cell>52.9 ?0.4</cell><cell>74.9?0.5</cell><cell>58.9 ?0.7</cell><cell>44.7 ?0.3</cell><cell>58.3 ?0.7</cell></row><row><cell>MinVIS</cell><cell>Swin-L</cell><cell>5%</cell><cell>54.3 ?0.3</cell><cell>76.3 ?0.5</cell><cell>60.1?0.3</cell><cell>45.4 ?0.4</cell><cell>59.5 ?0.2</cell></row><row><cell>MinVIS</cell><cell>Swin-L</cell><cell>10%</cell><cell>54.9?0.3</cell><cell>76.3?0.6</cell><cell>61.9?0.2</cell><cell>45.3 ?0.2</cell><cell>60.1 ?0.4</cell></row><row><cell cols="3">B Further Details for Datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>MinVIS consistently outperforms Mask2Former-VIS in all settings. The improvement increases for all three datasets when we sub-sample the annotation: +1.2% for full supervision v.s. +1.7% for 1% supervision on YouTube-VIS 2019. +2.7% for full supervision v.s. +5.8% for 1% supervision on YouTube-VIS 2021. +13.6% for full supervision v.s. +17.2% for 1% supervision on OVIS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>OVIS Results. MinVIS significantly outperform existing approaches on OVIS. Our imagebased framework leads to easier and better learning on this dataset with heavy occlusions.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Training</cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell><cell>AR1</cell><cell>AR10</cell></row><row><cell>TeViT [8]</cell><cell>MsgShifT</cell><cell>Full</cell><cell>17.4</cell><cell>34.9</cell><cell>15.0</cell><cell>11.2</cell><cell>21.8</cell></row><row><cell>CrossVIS [2]</cell><cell>R50</cell><cell>Full</cell><cell>14.9</cell><cell>32.7</cell><cell>12.1</cell><cell>10.3</cell><cell>19.8</cell></row><row><cell>CMaskTrack R-CNN [12]</cell><cell>R50</cell><cell>Full</cell><cell>15.4</cell><cell>33.9</cell><cell>13.1</cell><cell>9.3</cell><cell>20.0</cell></row><row><cell>STC [33]</cell><cell>R50</cell><cell>Full</cell><cell>15.5</cell><cell>33.5</cell><cell>13.4</cell><cell>11.0</cell><cell>20.8</cell></row><row><cell>Mask2Former-VIS*</cell><cell>R50</cell><cell>Full</cell><cell>17.3</cell><cell>37.3</cell><cell>15.1</cell><cell>10.5</cell><cell>23.5</cell></row><row><cell>MinVIS</cell><cell>R50</cell><cell>Full</cell><cell>25.0?0.3</cell><cell>45.5?0.6</cell><cell>24.0?0.7</cell><cell>13.9?0.3</cell><cell>29.7?0.3</cell></row><row><cell>MaskTrack R-CNN*+SWA [14]</cell><cell>Swin-L</cell><cell>Full</cell><cell>28.9</cell><cell>56.3</cell><cell>26.8</cell><cell>13.5</cell><cell>34.0</cell></row><row><cell>Mask2Former-VIS*</cell><cell>Swin-L</cell><cell>Full</cell><cell>25.8</cell><cell>46.5</cell><cell>24.4</cell><cell>13.7</cell><cell>32.2</cell></row><row><cell>MinVIS</cell><cell>Swin-L</cell><cell>Full</cell><cell>39.4?0.5</cell><cell>61.5?0.1</cell><cell>41.3?0.6</cell><cell>18.1?0.1</cell><cell>43.3?0.5</cell></row><row><cell>MinVIS</cell><cell>Swin-L</cell><cell>1%</cell><cell>31.7?0.5</cell><cell>54.9 ?1.0</cell><cell>31.3?0.5</cell><cell>16.3?0.3</cell><cell>36.1?0.3</cell></row><row><cell>MinVIS</cell><cell>Swin-L</cell><cell>5%</cell><cell>35.7?0.4</cell><cell>60.1 ?1.2</cell><cell>35.8?0.7</cell><cell>17.3?0.1</cell><cell>39.9?0.3</cell></row><row><cell>MinVIS</cell><cell>Swin-L</cell><cell>10%</cell><cell>37.2?0.5</cell><cell>60.7 ?1.1</cell><cell>38.0?1.0</cell><cell>17.3?0.2</cell><cell>41.1?0.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Sub-sampling the annotated training frames for MinVIS and Mask2Former-VIS. MinVIS outperforms Mask2Former-VIS for all of our settings. The improvement of MinVIS increases as we further sub-sample the annotated frames.</figDesc><table><row><cell>Method</cell><cell>Dataset</cell><cell cols="2">Backbone Full 10% 5%</cell><cell>1%</cell></row><row><cell cols="3">Mask2Former-VIS YTVIS-19 Swin-L</cell><cell cols="2">60.4 59.0 57.8 57.3</cell></row><row><cell>MinVIS</cell><cell cols="2">YTVIS-19 Swin-L</cell><cell cols="2">61.6 61.0 59.3 59.0</cell></row><row><cell cols="3">Mask2Former-VIS YTVIS-21 Swin-L</cell><cell cols="2">52.6 51.2 50.0 47.1</cell></row><row><cell>MinVIS</cell><cell cols="2">YTVIS-21 Swin-L</cell><cell cols="2">55.3 54.9 54.3 52.9</cell></row><row><cell cols="2">Mask2Former-VIS OVIS</cell><cell>Swin-L</cell><cell cols="2">25.8 24.1 22.3 14.5</cell></row><row><cell>MinVIS</cell><cell>OVIS</cell><cell>Swin-L</cell><cell cols="2">39.4 37.2 35.7 31.7</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Crossover learning for fast online video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mask2former for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anwesa</forename><surname>Choudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10764</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Seqformer: a frustratingly simple model for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.08275</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video instance segmentation using inter-frame communication transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sukjun</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miran</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Temporally efficient vision transformer for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiemin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to track instances without video annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shalini De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weakly supervised instance segmentation for videos with temporal mask consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11963</idno>
		<title level="m">Bin Feng, and Wenyu Liu. Tracking instances as queries</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Occluded video instance segmentation: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01558</idno>
		<editor>Xiang Bai, Serge Belongie, Alan Yuille, Philip Torr, and Song Bai</editor>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Limited sampling reference frame for masktrack r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leilei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICVV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Classifying, segmenting, and tracking object instances in video with mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<title level="m">Bin Feng, and Wenyu Liu. Instances as queries</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">In defense of online models for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Motr: End-to-end multiple-object tracking with transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Trackformer: Multi-object tracking with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from unlabeled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reducing the annotation effort for video object segmentation datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lishu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dystab: Unsupervised object segmentation via dynamic-static bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of instance segmentation with inter-pixel relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Discobox: Weakly supervised instance segmentation and semantic correspondence from box supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashree</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Boxinst: High-performance instance segmentation with box annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Freesolo: Learning to segment objects without annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Shalini De Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose M</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A survey on deep learning technique for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.01153</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Solo: Segmenting objects by locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Masked-attention mask transformer for universal image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2022</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno>ICLR, 2021. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Stc: Spatio-temporal contrastive learning for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangxuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqing</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03747</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stem-seg: Spatio-temporal embeddings for instance segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Athar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabarinath</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Youtube-vos: A large-scale video object segmentation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03327</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nisarg</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonseok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakrishnan</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
		<title level="m">Youtube-8m: A large-scale video classification benchmark</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
