<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RegNet: Self-Regulated Network for Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinglin</forename><surname>Pan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Zhang</forename><surname>Yi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenglin</forename><surname>Xu</surname></persName>
						</author>
						<title level="a" type="main">RegNet: Self-Regulated Network for Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Residue Networks</term>
					<term>Convolutional Recurrent Neural Networks</term>
					<term>Convolutional Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ResNet and its variants have achieved remarkable successes in various computer vision tasks. Despite its success in making gradient flow through building blocks, the simple shortcut connection mechanism limits the ability of reexploring new potentially complementary features due to the additive function. To address this issue, in this paper, we propose to introduce a regulator module as a memory mechanism to extract complementary features, which are further fed to the ResNet. In particular, the regulator module is composed of convolutional RNNs (e.g., Convolutional LSTMs or Convolutional GRUs), which are shown to be good at extracting spatio-temporal information. We named the new regulated networks as RegNet. The regulator module can be easily implemented and appended to any ResNet architectures. We also apply the regulator module for improving the Squeeze-and-Excitation ResNet to show the generalization ability of our method. Experimental results on three image classification datasets have demonstrated the promising performance of the proposed architecture compared with the standard ResNet, SE-ResNet, and other state-of-the-art architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>communication is somehow ignored and some reusable information learned from previous blocks tends to be forgotten in later blocks. To illustrate this point, we visualize the output(residual) feature maps learned by consecutive blocks in ResNet in <ref type="figure">Fig. 1(a)</ref>. It can be see that due to the summation operation among blocks, the adjacent outputs , +1 and +2 look very similar to each other, which indicates that less new information has been learned through consecutive blocks.</p><p>A potential solution to address the above problems is to capture the spatio-temporal dependency between building blocks while constraining the speed of parameter increasing. To this end, we introduce a new regulator mechanism in parallel to the shortcuts in ResNets for controlling the necessary memory information passing to the next building block. In detail, we adopt the Convolutional RNNs ("ConvRNNs") <ref type="bibr" target="#b11">[12]</ref> as the regulator to encode the spatio-temporal memory. We name the new architecture as RNN-Regulated Residual Networks, or "RegNet" for short. As shown in <ref type="figure">Fig. 1(a)</ref>, at the ? building block, a recurrent unit in the convolutional RNN takes the feature from the current building block as the input (denoted by ), and then encodes both the input and the serial information to generate the hidden state (denoted by ); the hidden state will be concatenated with the input for reuse in the next convolution operation (leading to the output feature ), and will also be transported to the next recurrent unit. To better understand the role of the regulator, we visualize the feature maps, as shown in <ref type="figure">Fig. 1(a)</ref>. We can see that the generated by ConvRNN can complement with the input features . After conducting convolution on the concatenated features of and , the proposed model gets more meaningful features with rich edge information than ResNet does. For quantitatively evaluating the information contained in the feature maps, we test their classification ability on test data (by adding average pooling layer and the last fully connected layer to the of the last three blocks). As shown in <ref type="figure">Fig. 1(b)</ref>, we can find that the new architecture can get higher prediction accuracy, which indicates the effectiveness of the regulator from ConvRNNs.</p><p>Thanks to the kind of parallel structure of the regulator module, the RNN-based regulator is easy to implement and can be applicable to other ResNet-based structures, such as the SE-ResNet <ref type="bibr" target="#b10">[11]</ref>, Wide ResNet <ref type="bibr" target="#b7">[8]</ref>, Inception-ResNet <ref type="bibr" target="#b8">[9]</ref>, ResNetXt <ref type="bibr" target="#b9">[10]</ref>, Dual Path Network(DPN) <ref type="bibr" target="#b12">[13]</ref>, and so on. Without loss of generality, as another instance to demonstrate the effectiveness of the proposed regulator, we also apply the ConvRNN module for improving the Squeeze-and-Excitation ResNet (shorted as "SE-RegNet").</p><p>For evaluation, we apply our model to the task of image classification on three highly competitive benchmark datasets, including CIFAR-10, CIFAR-100, and ImageNet. In comparison with the ResNet and SE-ResNet, our experimental results have demonstrated that the proposed architecture can significantly improve the classification accuracy on all the datasets. We further show that the regulator can reduce the required depth of ResNets while reaching the same level of accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Deep neural networks have been achieved empirical breakthroughs in machine learning. However, training networks with sufficient depths is a very tricky problem. Shortcut connection has been proposed to address the difficulty in optimization to some extent <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Via the shortcut, information can flow across layers without attenuation. A pioneering work is the Highway Network <ref type="bibr" target="#b13">[14]</ref>, which implements the shortcut connections by using a gating mechanism. In addition, the ResNet <ref type="bibr" target="#b4">[5]</ref> explicitly requests building blocks fitting a residual mapping, which is assumed to be easier for optimization.</p><p>Due to the powerful capabilities in dealing with vision tasks of ResNets, a number of variants have been proposed, including WRN <ref type="bibr" target="#b7">[8]</ref>, Inception-ResNet <ref type="bibr" target="#b8">[9]</ref>, ResNetXt <ref type="bibr" target="#b9">[10]</ref>, , WResNet <ref type="bibr" target="#b14">[15]</ref>, and so on. ResNet and ResNet-based models have achieved impressive, record-breaking performance in many challenging tasks. In object detection, 50-and 101layered ResNets are usually used as basic feature extractors in many models: Faster R-CNN <ref type="bibr" target="#b15">[16]</ref>, RetinaNet <ref type="bibr" target="#b16">[17]</ref>, Mask R-CNN <ref type="bibr" target="#b17">[18]</ref> and so on. The most recent models aiming at image super-resolution tasks, such as SRResNet <ref type="bibr" target="#b18">[19]</ref>, EDSR and MDSR <ref type="bibr" target="#b19">[20]</ref>, are all based on ResNets, with a little modification. Meanwhile, in <ref type="bibr" target="#b20">[21]</ref>, the ResNet is introduced to remove rain streaks and obtains the state-of-the-art performance.</p><p>Despite the success in many applications, ResNets still suffer from the depth issue <ref type="bibr" target="#b21">[22]</ref>. DenseNet proposed by <ref type="bibr" target="#b5">[6]</ref> concatenates the input features with the output features using a densely connected path in order to encourage the network to reuse all of the feature maps of previous layers. Obviously, not all feature maps need to be reused in the future layers, and consequently the densely connected network also leads to some redundancy with extra computational costs. Recently, Dual Path Network <ref type="bibr" target="#b12">[13]</ref> and Mixed link Network <ref type="bibr" target="#b22">[23]</ref> are the trade-offs between ResNets and DenseNets. In addition, some module-based architectures are proposed to improve the performance of the original ResNet. SENet <ref type="bibr" target="#b10">[11]</ref> proposes a lightweight module to get the channel-wise attention of intermediate feature maps. CBAM <ref type="bibr" target="#b23">[24]</ref> and BAM <ref type="bibr" target="#b24">[25]</ref> design modules to infer attention maps along both channel and spatial dimensions. Despite their success, those modules try to regulate the intermediate feature maps based on the attention information learned by the intermediate feature themselves, so the full utilization of historical spatio-temporal information of previous features still remains an open problem.</p><p>On the other hand, convolutional RNNs (shorted as Con-vRNN), such as ConvLSTM <ref type="bibr" target="#b11">[12]</ref> and ConvGRU <ref type="bibr" target="#b25">[26]</ref>, have been used to capture spatio-temporal information in a number of applications, such as rain removal <ref type="bibr" target="#b26">[27]</ref>, video superresolution <ref type="bibr" target="#b27">[28]</ref>, video compression <ref type="bibr" target="#b28">[29]</ref>, video object detection and segemetation <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. Most of those works embed Con-vRNNs into models to capture the dependency information in a sequence of images. In order to regulate the information flow of ResNet, we propose to leverage ConvRNNs as a separate module aiming to extracting spatio-temporal information as complementary to the original feature maps of ResNets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUR MODEL</head><p>In the section, we first revisit the background of ResNets and two advanced ConvRNNs: ConvLSTM and ConvGRU. Then we present the proposed RegNet architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. ResNet</head><p>The degradation problem which makes the traditional network hard to converge, is exposed when the architecture goes deeper. The problem can be mitigated by ResNet <ref type="bibr" target="#b4">[5]</ref> to some extent. Building blocks are the basic architecture of ResNet, as shown in <ref type="figure" target="#fig_0">Fig. 2(b)</ref>, instead of directly fitting a original underlying mapping, shown in <ref type="figure" target="#fig_0">Fig. 2(a)</ref>. The deep residual network obtained by stacking building blocks has achieved excellent performance in image classification, which proves the competence of the residual mapping. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. ConvRNN and its Variants</head><p>RNN and its classical variants LSTM and GRU have achieved great success in the field of sequence processing. To tackle the spatio-temporal problems, we adopt the basic ConvRNN and its variants ConvLSTM and ConvGRU, which are transformed from the vanilla RNNs by replacing their fully-connected operators with convolutional operators. Furthermore, for reducing the computational overhead, we delicately design the convolutional operation in ConvRNNs. In our implementation, the ConvRNN can be formulated as</p><formula xml:id="formula_0">H = ?( 2 W ? * [X , H ?1 ] + b ? ),<label>(1)</label></formula><p>where is the input 3D feature map, ?1 is the hidden state obtained from the earlier output of ConvRNN and is the output 3D feature map at this state. Both the number of input and output channels in the ConvRNN are N. Additionally, 2 W * X denotes a convolution operation between weights W and input X with the input channel 2N and the output channel N. To make the ConvRNN more efficient, inspired by <ref type="bibr" target="#b29">[30]</ref>, [32], given input X with 2N channels, we conduct the convolution operation in 2 steps:</p><p>(1) Divide the input X with 2N channels into N groups, and use grouped convolutions <ref type="bibr" target="#b32">[33]</ref> with 1 ? 1 kernel to process each group separately for fusing input channels. (2) Divide the feature map obtained by <ref type="bibr" target="#b0">(1)</ref> into N groups, and use grouped convolutions with 3 ? 3 kernel to process each group separately for capturing the spatial information per input channel. Directly applying the original convolutions with 3?3 kernels suffers from high computational complexity. As detailed in <ref type="table" target="#tab_1">Table I,</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. RNN-Regulated ResNet</head><p>To deal with the CIFAR-10/100 datasets and the Imagenet dataset, <ref type="bibr" target="#b4">[5]</ref> proposed two kinds of ResNet building blocks: the non-bottleneck building block and the bottleneck building   <ref type="figure" target="#fig_2">Fig. 3(a)</ref>. Here, we choose ConvLSTM for expounding. ?1 denotes the earlier output from ConvLSTM, and is output of the ConvLSTM at -th module .</p><formula xml:id="formula_1">3 3 ConvRNN H t X 1 t X 2 t X 3 t X 4 t X 5 t X 1 t+1 Concat BN + RELU 1 1 H t 1 T (b)</formula><p>denotes the -th feature map at the -th module.</p><p>The -th RegNet(ConvLSTM) module can be expressed as</p><formula xml:id="formula_2">X 2 = ( (W 12 * X 1 + b 12 ), [H , C ] = ( ( (X 2 , [H ?1 , C ?1 ]))), X 3 = ( (W 23 * [X 2 , H ])), X 4 = (W 34 * X 3 + b 34 ), X +1 1 = (X 1 + X 4 ),<label>(2)</label></formula><p>where W denotes the convolutional kernel which mapping feature map X to X and b denotes the correlative bias. Both W 12 and W 34 are 3 ? 3 convolutional kernels. The W 23 is 1?1 kernel. BN(?) indicates batch normalization.</p><p>[?] refers to the concatenate operation.</p><p>Notice that in Eq (2) the input feature X 2 and the previous output of ConvLSTM H are the inputs of ConvLSTM in -th module. According to the inputs, the ConvLSTM automatically decides whether the information in memory cell will be propagated to the output hidden feature map H .</p><p>2) Bottleneck RNN-Regulated ResNet Module (bottleneck RegNet module): The bottleneck RegNet module based on the bottleneck ResNet building block is shown in <ref type="figure" target="#fig_2">Fig. 3(b)</ref>. The bottleneck building block introduced in <ref type="bibr" target="#b4">[5]</ref> for dealing with the pictures with large size. Based on that, the -th bottleneck RegNet module can be expressed as <ref type="bibr" target="#b22">23</ref> ),</p><formula xml:id="formula_3">X 2 = ( (W 12 * X 1 + b 12 ), [H , C ] = ( ( (X 2 , [H ?1 , C ?1 ]))), X 3 = ( (W 23 * X 2 + b</formula><formula xml:id="formula_4">X 4 = ( (W 34 * [X 3</formula><p>, H ])),</p><formula xml:id="formula_5">X 5 = (W 45 * X 4 + b 45 ), X +1 1 = (X 1 + X 5 ),<label>(3)</label></formula><p>where W <ref type="bibr" target="#b11">12</ref> and W 45 are the two 1 ? 1 kernels, and W 23 is the 3 ? 3 bottleneck kernel. The W 34 is a 1 ? 1 kernel for fusing feature in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we evaluate the effectiveness of the proposed convRNN regulator on three benchmark datasets, including CIFAR-10, CIFAR-100, and ImageNet. We run the algorithms on pytorch. The small-scaled models for CIFAR are trained on a single NVIDIA 1080 Ti GPU, and the large-scaled models for ImageNet are trained on 4 NVIDIA 1080 Ti GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiments on CIFAR</head><p>The CIFAR datasets <ref type="bibr" target="#b33">[34]</ref> consist of RGB image with 32 ? 32 pixels. Each dataset contains 50k training images and 10k testing images. The images in CIFAR-10 and CIFAR-100 are drawn from 10 and 100 classes respectively. We train on the training dataset and evaluate on the test dataset.</p><p>By applying ConvRNNs to ResNet and SE-ResNet, we get the RegNet, and SE-RegNet models separately. Here, we use 20-layered RegNet and SE-RegNet to prove the wide applicability of our method. The SE-RegNet building module in <ref type="figure" target="#fig_2">Fig. 3(a)</ref> is used to analysis CIFAR datasets. The structural details of SE-RegNet are shown in <ref type="table" target="#tab_1">Table II</ref>. The inputs of the network are 32?32 images. In each conv_ , ? {1, 2, 3} layer, there are n RegNet building modules stacked sequentially, and connected together by a ConvRNN. In summary, there are 3 ConvRNNs in our architecture, and each ConvRNN impacts on the n RegNet building modules. The reduction ratio r in SE block is 8.</p><p>In this experiment, we use SGD with a momentum of 0.9 and a weight decay of 1e-4. We train with a batch size of 64 for 150 epoch. The initial learning rate is 0.1 and divided by 10 at 80 epochs. Data augmentation in <ref type="bibr" target="#b34">[35]</ref> is used in training. The results of SE-ResNet on CIFAR are based on our implementation, since the results were not reported in <ref type="bibr" target="#b10">[11]</ref>.</p><p>1) Results on CIFAR: The classification errors on the CIFAR-10/100 test sets are shown in <ref type="table" target="#tab_1">Table III</ref>. We can see from the results, with the same layer, both RegNet and SE-RegNet outperform the original models by a significant margin. Compared with ResNet-20, our RegNet-20 with Con-vLSTM decreases the error rate by 1.51% on CIFAR-10 and 2.04% on CIFAR-100. At the same time, compared with SE-ResNet-20, our SE-RegNet-20 with ConvLSTM decreases the error rate by 1.04% on CIFAR-10 and 2.12% on CIFAR-100. Using ConvGRU as the regulator can reach the same level of accuracy as ConvLSTM. Due to the vanilla ConvRNN lacks gating mechanism, it performs slightly worse but still makes great progress compared with the baseline model.</p><p>2) Parameters Analysis: For a fair comparison, we evaluate our model's ability by regarding the number of models parameters as the contrast reference. As shown in <ref type="table" target="#tab_1">Table IV</ref>, we list the test accuracy of 20, 32, 56-layered ResNets and their respective RegNet counterparts on CIFAR-10/100. After adding minimal additional parameters, both our RegNet with ConvGRU and ConvLSTM surpass the ResNet by a large margin. Our 20-layered RegNet with extra 0.04M parameters even outperforms the 32-layered ResNet on both CIFAR-10/100: our 20-layered RegNet(ConvLSTM) having 0.32M parameters reaches 7.28% error rate on CIFAR-10 surpass the 32-layered ResNet with 7.54% error rate which having 0.47M parameters. <ref type="figure">Fig. 4</ref> demonstrates the parameter efficiency comparisons between RegNet and ResNet. We show our RegNet are more parameter-efficient than simply stacking layers in vanilla ResNet. On both CIFAR-10/100, our RegNets(GRU) get comparable performance with ResNet-56 with nearly 1/2 parameters.</p><p>3) Positions of Feature Reuse: In this subsection, we perform ablation experiment to further analyze the effect of the position of feature reuse. We conduct an experiment to analysis that with ConvRNN which layer has the maximum promotion to the final outcome. Some previous studies <ref type="bibr" target="#b35">[36]</ref> show that the features in an earlier layer are more general while the features in later layers exhibit more specific. As shown in <ref type="table" target="#tab_1">Table II</ref>, the conv_1, conv_2, conv_3 layers are separated by the down sampling operation, which makes the features in conv_1 are more low-level and in conv_3 are more specific for classification. The classification results are shown in <ref type="table" target="#tab_4">Table V</ref>.   <ref type="figure">Fig. 4</ref>. Comparison of parameter efficiency on CIFAR-10 between RegNet and ResNet <ref type="bibr" target="#b4">[5]</ref>. In both 4(a) and 4(b), the curves of our RegNet is always below ResNet <ref type="bibr" target="#b4">[5]</ref> which show that with the same parameters, our models have stronger ability of expression.  <ref type="bibr" target="#b12">[13]</ref>, which is more important for general features in lower layers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments on ImageNet</head><p>We evaluate our model on ImageNet 2012 dataset <ref type="bibr" target="#b2">[3]</ref> which consists of 1.28 million training images and 50k validation images from 1000 classes. Following the previous papers, We report top-1 and top-5 classification errors on the validation dataset. Due to the limited resources of our GPUs and without of loss of generality, we run the experiments of ResNets and RegNets only.</p><p>The bottleneck RegNet building modules are applied to Im-ageNet. We use 4 ConvRNNs in RegNet-50. The ConvRNN , ? {1, 2, 3, 4}, controls {3, 4, 6, 3} bottleneck RegNet modules respectively. In this experiment, we use SGD with a momentum of 0.9 and a weight decay of 1e-4. We train with batch size 128 for 90 epoch. The initial learning rate is 0.06 and divided by 10 at 50 and 70 epochs. The input of the network is 224?224 images, which randomly cropped from the resized original images or their horizontal flips. Data augmentation in <ref type="bibr" target="#b26">[27]</ref> is used in training. We evaluate our model by applying a center-crop with 224 ? 224.</p><p>We evaluate the efficiency of baseline models ResNet-50 and its respectively RegNet counterpart. The comparison is based on the computational overhead. As shown in <ref type="table" target="#tab_1">Table VI</ref> with additional 4.7M parameters, RegNet outperforms the baseline model by 1.38% on top-1 accuracy and 0.85% on top-5 accuracy. <ref type="table" target="#tab_1">Table VII</ref> shows the error rates of some state-of-the-art models on the ImageNet validation set. Compared with the baseline ResNet, our RegNet-50 with 31.3M parameters and 5.12G FLOPs not only surpasses the ResNet-50 but also outperforms ResNet-101 with 44.6M parameters and 7.9G FLOPs. Since the proposed regulator module is essentially a beneficial makeup to the short cut mechanism in ResNets, one can easily apply the regulator module to other ResNet-based models, such as SE-ResNet, WRN-18 <ref type="bibr" target="#b7">[8]</ref>, ResNetXt <ref type="bibr" target="#b9">[10]</ref>, Dual Path Network (DPN) <ref type="bibr" target="#b12">[13]</ref>, etc. Due to computation resource limitation, we leave the implementation of the regulator module in these ResNet extensions as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, we proposed to employ a regulator module with Convolutional RNNs to extract complementary features for improving the representation power of the ResNets. Experimental results on three image-classification datasets have demonstrated the promising performance of the proposed architecture in comparison with standard ResNets and Squeezeand-Excitation ResNets as well as other state-of-the-art architectures.</p><p>In the future, we intend to further improve the efficiency of the proposed architecture and to apply the regulator module to other ResNet-based architectures <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref> to increase their capacity. Besides, we will further explore RegNets for other challenging tasks, such as object detection <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, image super-resolution <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, and so on.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>2(a) shows the original underlying mapping while 2(b) shows the residual mapping in ResNet<ref type="bibr" target="#b4">[5]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>the new modification reduces the required computation by 18N/11 times with comparable result. Similarly, all the convolutions in ConvGRU and ConvLSTM are replaced with the light-weight modification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The RegNet module is shown in 3(a). The bottleneck RegNet block is shown in 3(b). The denotes the number of building blocks as well as the total time steps of ConvRNN. block. Based on those, by applying ConvRNNs as regulators, we get RNN-Regulated ResNet building module and bottleneck RNN-Regulated ResNet building module correspondingly. 1) RNN-Regulated ResNet Module (RegNet module): The illustration of RegNet module is shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The prediction on test data based on the output feature maps of consecutive building blocks. During the test time, we add an average pooling layer and the last fully connected layer to the outputs of the last three building blocks( ? {7, 8, 9}) in ResNet-20 and RegNet-20 to get the classification results. It can be seen that the output of each block aided with the memory information results in higher classification accuracy.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">ResNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">RegNet</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>H t 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Input</cell><cell></cell><cell>Building Block Building Block Building Block O t O t+1 O t+2</cell><cell cols="3">t th (t + 1) th (t + 2) th</cell><cell>O i</cell><cell>O i</cell><cell>H i</cell><cell>I i</cell><cell cols="2">t th (t +1) (t + 2) th th ConvRNN H t+2 ConvRNN ConvRNN H t H t+1</cell><cell cols="2">I t I t+2 I t+1</cell><cell>Building Block Building Block Building Block O t O t+1 O t+2</cell><cell>test accuracy(%)</cell><cell>50 60 70 80 90</cell><cell>55.6</cell><cell>60.2 ResNet RegNet</cell><cell>67.9</cell><cell>71.7</cell><cell>91.6</cell><cell>92.7</cell></row><row><cell>O i</cell><cell cols="3">: The output of</cell><cell>i th</cell><cell cols="2">building block</cell><cell>I i H i</cell><cell cols="3">: The input ouput of ConvRNN at</cell><cell cols="2">i th</cell><cell cols="2">building block</cell><cell></cell><cell>40</cell><cell cols="2">7</cell><cell>8 the output of i th block</cell><cell>9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell></row><row><cell cols="12">Fig. 1. (a):Visualization of feature maps in the ResNet [5] and RegNet. We visualize the outputs</cell><cell></cell><cell></cell><cell cols="5">feature maps of the ? building blocks, ? { , +1, +2}.</cell></row><row><cell>In RegNets,</cell><cell cols="6">denotes the input feature maps.</cell><cell cols="12">denotes the hidden states generated by the ConvRNN at step . By applying convolution operations over</cell></row><row><cell cols="2">the concatenation</cell><cell>with</cell><cell></cell><cell cols="6">, we can get the regulated outputs( denoted by</cell><cell cols="5">) of the ? building block. (b):</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I PERFORMANCE</head><label>I</label><figDesc>OF REGNET-20 WITH CONVGRU AS REGULATORS ON CIFAR-10. WE COMPARE THE TEST ERROR RATES BETWEEN TRADITIONAL 3?3 KERNELS AND OUR NEW MODIFICATION.</figDesc><table><row><cell cols="2">kernel type</cell><cell>err.</cell><cell>Params</cell><cell>FLOPs</cell></row><row><cell></cell><cell>3?3</cell><cell>7.35</cell><cell>+330K</cell><cell>+346M</cell></row><row><cell></cell><cell>Ours</cell><cell>7.42</cell><cell>+44K</cell><cell>+15M</cell></row><row><cell>H t 1</cell><cell>3 3 X 1 t</cell><cell></cell><cell></cell><cell>1 1</cell></row><row><cell>ConvRNN</cell><cell>BN + RELU X 2 t</cell><cell></cell><cell></cell><cell>BN + RELU</cell></row><row><cell>BN + RELU</cell><cell></cell><cell></cell><cell></cell><cell>BN + RELU</cell></row><row><cell>H t</cell><cell></cell><cell></cell><cell></cell><cell>BN + RELU</cell></row><row><cell>T</cell><cell>Concat</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1 1</cell><cell></cell><cell></cell></row><row><cell></cell><cell>BN + RELU</cell><cell></cell><cell></cell></row><row><cell></cell><cell>X 3 t</cell><cell></cell><cell></cell></row><row><cell></cell><cell>3 3 X 4 t BN</cell><cell></cell><cell></cell><cell>BN 1 1</cell></row><row><cell></cell><cell>RELU</cell><cell></cell><cell></cell><cell>RELU</cell></row><row><cell></cell><cell>X 1 t+1</cell><cell></cell><cell></cell></row><row><cell>(a)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II ARCHITECTURES</head><label>II</label><figDesc>FOR CIFAR-10/100 DATASETS. BY SETTING N ? {3, 5, 7}, WE CAN GET THE {20, 32, 56}-LAYERED REGNET.</figDesc><table><row><cell>name</cell><cell>output size</cell><cell cols="4">(6n+2)-layered RegNet</cell></row><row><cell>conv_0</cell><cell>32 ? 32</cell><cell></cell><cell cols="2">3 ? 3, 16</cell></row><row><cell>conv_1</cell><cell>32 ? 32</cell><cell cols="2">ConvRNN 1 +</cell><cell>3 ? 3, 16 3 ? 3, 16</cell><cell>?</cell></row><row><cell>conv_2</cell><cell>16 ? 16</cell><cell cols="2">ConvRNN 2 +</cell><cell>3 ? 3, 32 3 ? 3, 32</cell><cell>?</cell></row><row><cell>conv_3</cell><cell>8 ? 8</cell><cell cols="2">ConvRNN 3 +</cell><cell>3 ? 3, 64 3 ? 3, 64</cell><cell>?</cell></row><row><cell></cell><cell>1 ? 1</cell><cell cols="3">AP, FC, softmax</cell></row><row><cell></cell><cell></cell><cell>TABLE III</cell><cell></cell><cell></cell></row><row><cell cols="6">CLASSIFICATION ERROR RATES ON THE CIFAR-10/100. BEST RESULTS</cell></row><row><cell></cell><cell cols="3">ARE MARKED IN BOLD.</cell><cell></cell></row><row><cell cols="2">model</cell><cell></cell><cell>C10</cell><cell>C100</cell></row><row><cell cols="2">ResNet-20 [5]</cell><cell></cell><cell>8.38</cell><cell>31.72</cell></row><row><cell cols="3">RegNet-20(ConvRNN)</cell><cell>7.60</cell><cell>30.03</cell></row><row><cell cols="3">RegNet-20(ConvGRU)</cell><cell>7.42</cell><cell>29.69</cell></row><row><cell cols="3">RegNet-20(ConvLSTM)</cell><cell>7.28</cell><cell>29.81</cell></row><row><cell cols="2">SE-ResNet-20</cell><cell></cell><cell>8.02</cell><cell>31.14</cell></row><row><cell cols="3">SE-RegNet-20(ConvRNN)</cell><cell>7.55</cell><cell>29.63</cell></row><row><cell cols="3">SE-RegNet-20(ConvGRU)</cell><cell>7.25</cell><cell>29.08</cell></row><row><cell cols="3">SE-RegNet-20(ConvLSTM)</cell><cell>6.98</cell><cell>29.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV TEST</head><label>IV</label><figDesc>ERROR RATES ON CIFAR-10/100. WE USE CONVGRU AND CONVLSTM AS REGULATORS OF RESNET. WE LIST THE INCREASE OF PARAMETER THE ARCHITECTURES AT THE RIGHT CORNER OF THE ERROR RATES. +0.04 ) 7.28 (+0.04 ) 31.72 29.69 (+0.04 ) 29.81 (+0.04 ) 32 7.54 6.60 (+0.06 ) 6.88 (+0.07 ) 29.86 27.42 (+0.07 ) 28.11 (+0.07 ) 56 6.78 6.39 (+0.11 ) 6.45 (+0.12 ) 28.14 27.02 (+0.11 ) 27.26 (+0.12 )</figDesc><table><row><cell></cell><cell></cell><cell>C-10</cell><cell>C-100</cell></row><row><cell cols="3">layer ResNet +ConvGRU +ConvLSTM ResNet</cell><cell>+ConvGRU</cell><cell>+ConvLSTM</cell></row><row><cell>20</cell><cell>8.38</cell><cell>7.42 (</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V TEST</head><label>V</label><figDesc>ERROR RATES ON CIFAR-10/100. WE USE CONVGRU AND CONVLSTM AS REGULATORS OF RESNET. WE LIST THE INCREASE OF PARAMETER THE ARCHITECTURES. IN EACH OF OUR REGNET ( ) MODELS, THERE IS ONLY ONE CONVRNN APPLIED IN LAYER CONV_ , ? {1, 2, 3}.</figDesc><table><row><cell></cell><cell></cell><cell>C-10</cell><cell></cell><cell>C-100</cell></row><row><cell>model</cell><cell>err.</cell><cell>Params</cell><cell>err.</cell><cell>Params</cell></row><row><cell>ResNet [5]</cell><cell>8.38</cell><cell>0.273M</cell><cell>31.72</cell><cell>0.278M</cell></row><row><cell>RegNet (1) (GRU)</cell><cell>7.52</cell><cell>0.279M</cell><cell>30.40</cell><cell>0.285M</cell></row><row><cell>RegNet (2) (GRU)</cell><cell>7.48</cell><cell>0.285M</cell><cell>30.34</cell><cell>0.291M</cell></row><row><cell>RegNet (3) (GRU)</cell><cell>7.49</cell><cell>0.306M</cell><cell>30.30</cell><cell>0.312M</cell></row><row><cell>RegNet (1) (LSTM)</cell><cell>7.56</cell><cell>0.281M</cell><cell>30.23</cell><cell>0.286M</cell></row><row><cell>RegNet (2) (LSTM)</cell><cell>7.49</cell><cell>0.290M</cell><cell>30.28</cell><cell>0.296M</cell></row><row><cell>RegNet (3) (LSTM)</cell><cell>7.52</cell><cell>0.325M</cell><cell>29.92</cell><cell>0.331M</cell></row><row><cell cols="5">In each model, only one ConvRNN is applied. We name</cell></row><row><cell cols="5">the models RegNet ( ) , ? {1, 2, 3} which denotes that only</cell></row><row><cell cols="5">applying a ConvRNN in layer conv_ and maintaining the</cell></row><row><cell cols="5">original ResNet structure in the other layers. For a fair compar-</cell></row><row><cell cols="5">ison, we evaluate the models ability by regarding the number</cell></row><row><cell cols="5">of models parameters as the contrast reference. We can see</cell></row><row><cell cols="5">from the results, using ConvRNNs in a lower layer(conv_1) is</cell></row><row><cell cols="5">more parameter-efficient than higher layer(conv_3). With less</cell></row><row><cell cols="5">parameter increasing in lower layers, they can bring about</cell></row><row><cell cols="5">nearly same improvement in accuracy compared with higher</cell></row><row><cell cols="5">layers. Compared with ResNet, our RegNet (1) (GRU) decrease</cell></row><row><cell cols="5">the test error from 8.38% to 7.52%(-0.86%) on CIFAR-10 with</cell></row><row><cell cols="5">additional 0.006M parameters and from 31.72% to 30.40%(-</cell></row><row><cell cols="5">1.32%) on CIFAR-100 with additional 0.007M parameters.</cell></row><row><cell cols="5">This significant improvement with minimal additional param-</cell></row><row><cell cols="5">eters further proves the effectiveness of the proposed method.</cell></row></table><note>The concatenate operation in our model can fuse features together to explore new features</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI SINGLE</head><label>VI</label><figDesc>-CROP VALIDATION ERROR RATES ON IMAGENET AND COMPLEXITY COMPARISONS. BOTH RESNET AND REGNET ARE 50-LAYER. RESNET * MEANS WE REPRODUCE THE RESULT BY OURSELF.</figDesc><table><row><cell>model</cell><cell cols="2">top-1 err.</cell><cell>top-5 err.</cell><cell>Params</cell><cell>FLOPs</cell></row><row><cell>ResNet [5] ResNet  *</cell><cell>24.7 24.81</cell><cell></cell><cell>7.8 7.78</cell><cell>26.6M</cell><cell>4.14G</cell></row><row><cell>RegNet</cell><cell cols="2">23.43 (?1.38)</cell><cell>6.93 (?0.85)</cell><cell>31.3M</cell><cell>5.12G</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE VII</cell><cell></cell></row><row><cell cols="6">SINGLE-CROP ERROR RATES ON THE IMAGENET VALIDATION SET FOR STATE-OF-THE-ART MODELS. THE RESNET-50  *  MEANS THAT THE</cell></row><row><cell cols="6">RE-IMPLEMENTION RESULT BY OUR EXPERIMENTS.</cell></row><row><cell>model</cell><cell></cell><cell>top-1</cell><cell>top-5</cell><cell>Params(M)</cell><cell>FLOPs(G)</cell></row><row><cell cols="2">WRN-18(widen=2.0) [8]</cell><cell>25.58</cell><cell>8.06</cell><cell>45.6</cell><cell>6.70</cell></row><row><cell>DenseNet-169 [6]</cell><cell></cell><cell>23.80</cell><cell>6.85</cell><cell>28.9</cell><cell>7.7</cell></row><row><cell cols="2">SE-ResNet-50 [11]</cell><cell>23.29</cell><cell>6.62</cell><cell>26.7</cell><cell>4.14</cell></row><row><cell>ResNet-50 [5] ResNet-50  *</cell><cell></cell><cell>24.7 24.81</cell><cell>7.8 7.78</cell><cell>-26.6</cell><cell>-4.14</cell></row><row><cell>ResNet-101 [5]</cell><cell></cell><cell>23.6</cell><cell>7.1</cell><cell>44.5</cell><cell>7.51</cell></row><row><cell>RegNet-50</cell><cell></cell><cell>23.43</cell><cell>6.93</cell><cell>31.3</cell><cell>5.12</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Convolutional networks for images, speech, and time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The handbook of brain theory and neural networks</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">3361</biblScope>
			<biblScope unit="page">1995</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<ptr target="http://arxiv.org/abs/1409.1556" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>abs/1409.4842</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>abs/1608.06993</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>abs/1707.07012</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>abs/1605.07146</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Inception-v4, inceptionresnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<idno>abs/1602.07261</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno>abs/1611.05431</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1709.01507</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Woo</surname></persName>
		</author>
		<idno>abs/1506.04214</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dual path networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno>abs/1707.01629</idno>
		<ptr target="http://arxiv.org/abs/1707.01629" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Highway networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>abs/1505.00387</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Weighted residuals for very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="936" to="941" />
		</imprint>
	</monogr>
	<note>international conference on systems</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1506.01497</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<idno>abs/1708.02002</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno>abs/1703.06870</idno>
	</analytic>
	<monogr>
		<title level="j">Mask R-CNN</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>CoRR</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<idno>abs/1609.04802</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, Workshops</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Removing rain from single images via a deep detail network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="1715" to="1723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>abs/1603.09382</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mixed link networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<idno>abs/1802.01808</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">CBAM: convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<idno>abs/1807.06521</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BAM: bottleneck attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<idno>abs/1807.06514</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Delving deeper into convolutional networks for learning video representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<idno>abs/1511.06432</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recurrent squeeze-andexcitation context aggregation net for single image deraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<idno>abs/1807.05698</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multimemory convolutional neural network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2530" to="2544" />
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Non-local convlstm for video compression artifact reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Looking fast and slow: Memory-guided mobile video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional gated recurrent networks for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Valipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>J?gersand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ray</surname></persName>
		</author>
		<idno>abs/1611.05435</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Interleaved group convolutions for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1707.02725</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<ptr target="http://jmlr.org/proceedings/papers/v38/" />
		<title level="m">Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2015</title>
		<editor>G. Lebanon and S. V. N. Vishwanathan</editor>
		<meeting>the Eighteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2015<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
	<note>ser. JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<idno>abs/1411.1792</idno>
		<title level="m">How transferable are features in deep neural networks?&quot; CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
