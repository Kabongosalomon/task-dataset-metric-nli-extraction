<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MRCNet: Crowd Counting and Density Map Estimation in Aerial and Ground Imagery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Bahmanyar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Remote Sensing Technology Institute</orgName>
								<orgName type="institution" key="instit2">German Aerospace Center (DLR)</orgName>
								<address>
									<settlement>Wessling</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><forename type="middle">Bahmanyar@dlr</forename><surname>De</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Remote Sensing Technology Institute</orgName>
								<orgName type="institution" key="instit2">German Aerospace Center (DLR)</orgName>
								<address>
									<settlement>Wessling</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleonora</forename><surname>Vig</surname></persName>
							<email>eleonora.vig@dlr.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Remote Sensing Technology Institute</orgName>
								<orgName type="institution" key="instit2">German Aerospace Center (DLR)</orgName>
								<address>
									<settlement>Wessling</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Reinartz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Remote Sensing Technology Institute</orgName>
								<orgName type="institution" key="instit2">German Aerospace Center (DLR)</orgName>
								<address>
									<settlement>Wessling</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Reinartz@dlr</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Remote Sensing Technology Institute</orgName>
								<orgName type="institution" key="instit2">German Aerospace Center (DLR)</orgName>
								<address>
									<settlement>Wessling</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Remote Sensing Technology Institute</orgName>
								<orgName type="institution" key="instit2">German Aerospace Center (DLR)</orgName>
								<address>
									<settlement>Wessling</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MRCNet: Crowd Counting and Density Map Estimation in Aerial and Ground Imagery</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>BAHMANYAR ET AL.: MRCNET -CROWD COUNTING IN AERIAL IMAGERY 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In spite of the many advantages of aerial imagery for crowd monitoring and management at mass events, datasets of aerial images of crowds are still lacking in the field. As a remedy, in this work we introduce a novel crowd dataset, the DLR Aerial Crowd Dataset (DLR-ACD), which is composed of 33 large aerial images acquired from 16 flight campaigns over mass events with 226,291 persons annotated. To the best of our knowledge, DLR-ACD is the first aerial crowd dataset and will be released publicly. To tackle the problem of accurate crowd counting and density map estimation in aerial images of crowds, this work also proposes a new encoder-decoder convolutional neural network, the so-called Multi-Resolution Crowd Network (MRCNet). The encoder is based on the VGG-16 network and the decoder is composed of a set of bilinear upsampling and convolutional layers. Using two losses, one at an earlier level and another at the last level of the decoder, MRCNet estimates crowd counts and high-resolution crowd density maps as two different but interrelated tasks. In addition, MRCNet utilizes contextual and detailed local information by combining high-and low-level features through a number of lateral connections inspired by the Feature Pyramid Network (FPN) technique. We evaluated MRCNet on the proposed DLR-ACD dataset as well as on the ShanghaiTech dataset, a CCTV-based crowd counting benchmark. The results demonstrate that MR-CNet outperforms the state-of-the-art crowd counting methods in estimating the crowd counts and density maps for both aerial and CCTV-based images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Crowd counting and crowd density estimation play essential roles in safety monitoring and behavior analysis especially in the case of mass events. They can lead to early detection of congestion or security-related abnormalities informing and helping organizers and decision makers to avoid crowd disasters <ref type="bibr" target="#b7">[8]</ref>. Closed-Circuit Television (CCTV) surveillance cameras have been conventionally used for crowd monitoring and they have become ubiquitous in recent years providing large number of images with various perspectives, scales, and illumination conditions. However, for mass events spread over wide open areas with thousands of people attending, monitoring the crowd from above using aerial imagery (e.g., using airborne platforms) was shown to be advantageous due to the wider field of view and smaller c 2019. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.   <ref type="table" target="#tab_1">Table 1</ref> using the given IDs. occlusion effects as compared to CCTV images <ref type="bibr" target="#b4">[5]</ref>. Nevertheless, in spite of the increasing volume of available aerial images due to the advances in airborne and UAV platforms, crowd counting and density estimation datasets and methods for aerial imagery are still lacking in the domain. Therefore, as one of the two main contributions of this work, we introduce a novel crowd dataset, the DLR Aerial Crowd Dataset (DLR-ACD), which is composed of 33 large aerial images (the average image size is 3619?5226 pixels) acquired by standard DSLR cameras installed on an airborne platform on a helicopter. The images come from 16 flight campaigns, i.e. different mass events, and the dataset contains 226,291 person annotations. <ref type="figure" target="#fig_1">Figure 1</ref> shows example images from DLR-ACD. To the best of our knowledge, DLR-ACD is the first aerial image crowd dataset and, with it, we hope to promote research on aerial crowd analysis. The dataset will be released at https://www.dlr.de/eoc/en/ desktopdefault.aspx/tabid-12760/22294_read-58354/.</p><p>Despite the many benefits, crowd counting and density estimation is still a complex task in practice. For example, detecting and counting people in low resolution surveillance images, in which each person may only cover a few pixels and occlusion is frequent, is very difficult even for human experts <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>. Therefore, developing automatic methods for precise crowd counting and density estimation is of high interest. In recent years, methods based on Convolutional Neural Networks (CNNs) have achieved promising results <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25]</ref>. In contrast to using preset features, CNN-based methods learn features during the training which allows them to better cope with images with arbitrary perspectives, scales, and crowd densities <ref type="bibr" target="#b19">[20]</ref>.</p><p>Considering the advantages of the existing crowd counting and density estimation methods and the new challenges presented by our aerial dataset such as extremely small objects and complex backgrounds, in this work, we propose Multi-Resolution Crowd Network (MR-CNet) which relies on a pre-trained VGG-16 model as an encoder and a combination of bi-linear upsampling and convolution layers as a decoder. In addition, in order to preserve as much high-resolution signal as possible while extracting multi-scale features, the encoder and decoder are connected through lateral connections with element-wise addition similar to the Feature Pyramid Network (FPN) architecture <ref type="bibr" target="#b10">[11]</ref>. While high-level features provide contextual information, the low-level features extract detailed information. In order to collect information from people with various sizes and in different crowd density conditions, MRCNet propagates information in all levels from the bottom to the top. Taking advantage of this structure, MRCNet shows high robustness and transferability by achieving superior crowd counting and density map estimation results on both aerial and CCTV-based datasets. It outperforms the state-of-the-art methods on the ShanghaiTech dataset <ref type="bibr" target="#b24">[25]</ref>, a challenging CCTV-based benchmark, by achieving the smallest Mean Absolute count Error (MAE). In addition, on our proposed DLR-ACD dataset, MRCNet achieves the smallest counting MAE and the highest F1-score in person detection.</p><p>In summary, our main contributions are:</p><p>? We introduce a novel aerial crowd dataset, the so-called DLR-ACD.</p><p>? We propose MRCNet which is able to deal with the existing challenges in both aerial and CCTV-based crowd images. ? MRCNet achieves state-of-the-art results on the DLR-ACD and ShanghaiTech datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>There is a vast literature on crowd counting and crowd density estimation in computer vision <ref type="bibr" target="#b20">[21]</ref>. In order to deal with scale variation, a number of previous works proposed employing networks with multi-column architectures to obtain receptive fields with various sizes and extract features at different scales. A three-column architecture developed by Zhang et al. <ref type="bibr" target="#b24">[25]</ref>, the so-called MCNN, and a combination of shallow and deep networks proposed by Boominathan et al. <ref type="bibr" target="#b1">[2]</ref> are examples of multi-column CNNs. As an approach toward improving the network performance in the presence of significant crowd density variations, Sam et al. <ref type="bibr" target="#b15">[16]</ref> proposed a switching multi-column CNN in which each column is trained separately on image patches of specific crowd densities. Ranjan et al. <ref type="bibr" target="#b13">[14]</ref> proposed ic-CNN, a twobranch network with a feed-forward structure, which incorporates low-resolution prediction maps for generating high-resolution crowd density maps.</p><p>Despite the improvement achieved by the multi-column approaches, their scale-invariance highly depends on the number of columns and their receptive field sizes <ref type="bibr" target="#b20">[21]</ref>. Furthermore, their depths are usually limited as they present heavy computational overhead. Therefore, single-column architectures have been favoured in most of the recent crowd counting networks. Zeng et al. <ref type="bibr" target="#b22">[23]</ref> proposed a single column CNN, the so-called MSCNN, composed of scale aggregation blocks to tackle scale variations. Later Cao et al. <ref type="bibr" target="#b2">[3]</ref> used more sophisticated scale aggregation blocks in a deeper CNN together with a composition loss (Euclidean and local pattern consistency loss), the so-called SANet, and improved the count accuracy significantly. Wang et al. <ref type="bibr" target="#b21">[22]</ref> also relied on scale aggregation blocks in designing scale aware residual modules for SCNet, a single column crowd counting CNN. In order to tackle the variations in crowd density and appearance, Sam et al. <ref type="bibr" target="#b16">[17]</ref> proposed a growing CNN, the so-called IG-CNN, in which a base CNN is recursively split into two child-CNNs each becoming an expert on certain crowd types during training.</p><p>The advantages of multi-task CNNs have inspired Sindagi et al. <ref type="bibr" target="#b19">[20]</ref> to develop a cascaded multi-task CNN, the so-called CMTL, which simultaneously classifies crowds into different density levels and estimates density maps. Idrees et al. <ref type="bibr" target="#b7">[8]</ref> developed a CNN that solves crowd counting, density estimation, and localization simultaneously. The network consists of a series of DenseNet <ref type="bibr" target="#b5">[6]</ref> blocks with a composition of multiple intermediate losses, each optimizing the network for a ground-truth crowd density map smoothed with a different Gaussian kernel, and a final count loss.</p><p>A number of previous works took advantage of pre-trained networks as back-bones for crowd counting networks. In CSRNet, Li et al. <ref type="bibr" target="#b9">[10]</ref> coupled VGG-16 with a dilated CNN as the back-end to obtain larger receptive fields and thus, better count accuracy. Later Liu et al. <ref type="bibr" target="#b11">[12]</ref> combined the high and low level features by employing the VGG-16 network and FPN. This allows preserving and propagating the fine-grained details of small targets and also incorporating a large degree of context information. Dealing with the over-fitting problem of crowd counting networks, Shi et al. <ref type="bibr" target="#b17">[18]</ref> proposed a learning strategy based on deep negative correlation learning applied to a modified VGG-16 network <ref type="bibr" target="#b18">[19]</ref> which results in generalizable features through learning a pool of regressors to estimate the crowd density.</p><p>Our proposed MRCNet is based on a single-column encoder-decoder architecture similar to SegNet <ref type="bibr" target="#b0">[1]</ref>, U-Net <ref type="bibr" target="#b14">[15]</ref>, and Li et al. <ref type="bibr" target="#b11">[12]</ref>. It uses a pre-trained VGG-16 as encoder. MRCNet is different from SegNet and U-Net in the decoder structure and the lateral connections. While the lateral connections of SegNet and U-Net are based on max-pooling indices and concatenations of earlier convolutional layers, respectively, MRCNet takes advantage of FPN-based lateral connections; however, it is different from <ref type="bibr" target="#b11">[12]</ref> in the number and wiring of the lateral connections and the decoder structure (e.g., the technique and the level of upsampling). It is different from <ref type="bibr" target="#b9">[10]</ref> in the number of the used convolutional blocks and the decoder structure. MRCNet considers crowd counting and density map estimation as two interrelated tasks and performs a multi-resolution prediction. It is different from the multi-task networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20]</ref> in the network structure and the task formulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DLR's Aerial Crowd Dataset</head><p>DLR's Aerial Crowd Dataset (DLR-ACD) is a collection of 33 large RGB aerial images with average size of 3619?5226 pixels acquired through 16 different flight campaigns performed between 2011 and 2017. The aerial images were captured at various mass events and over urban scenes involving crowds, such as sport events, city centers, open-air fairs and festivals. The images were recorded using a camera system composed of three standard DSLR cameras (a nadir-looking and two side-looking cameras) mounted on an airborne platform installed on a helicopter flying at an altitude between 500 m to 1600 m. The different flight altitudes resulted in a range of spatial resolutions (or ground sampling distances -GSD) from 4.5 cm/pixel to 15 cm/pixel and we also consider different viewing angles. Furthermore, the images were selected so that they represent different crowd densities and crowd behavior from the sparse moving crowds in city centers to the very dense (mostly) stationary ones at concerts. The dataset was labeled manually with point-annotations on individual people taking about 80 hours, and resulted in 226,291 person annotations, ranging from 285 to 24,368 annotations per image. Crowd annotation in aerial images is a challenging task due to the large image sizes as well as the large number and the small size of the people in the images. While in dense crowd areas, discriminating each person from adjacent people is difficult, in sparse crowd areas localizing and discriminating each person from similar-looking objects is also challenging and time consuming. <ref type="table" target="#tab_1">Table 1</ref> shows detailed information about the images and the annotations. Our images come from four types of events: sports events, fairs (e.g. trade fairs, Oktorberfest, etc.), and (music) festivals. To ensure that all scenes are covered in our train/test splits and that images from the same campaign are either in the training or in the test set, the dataset was manually split into 19 training and 14 test images, and the splits were not randomized. The counts in the training and test sets are 138,151 and 88,140 persons. <ref type="table" target="#tab_2">Table 2</ref> and <ref type="figure" target="#fig_2">Figure 2</ref> show the statistics of existing crowd datasets as well as our dataset. Among them, DLR-ACD is the first dataset that provides aerial views of crowds and therefore presents different challenges than existing datasets. Its images are much larger in size which might lead to higher computational costs and memory requirements. For example, when converted to match the image size of the widely-used ShanghaiTech-A dataset, DLR-ACD is 2.5? larger.</p><p>In addition, as <ref type="figure" target="#fig_2">Figure 2</ref> shows, most of the images in DLR-ACD contain a large number of people (&gt; 2K) which is very different from the other crowd datasets. Furthermore, crowd densities vary significantly within and between images due to their large fields of view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MRCNet</head><p>The Multi-Resolution Crowd Network (MRCNet) utilizes an encoder-decoder structure to extract image features and generate crowd density maps. It takes a single image of arbitrary size and, in a fully-convolutional manner, predicts two density maps, one with 1/4 of the input image size for the people counting task and the other one with the input image size for the density map estimation task. For the encoder, MRCNet relies on a pre-trained VGG-16 network <ref type="bibr" target="#b18">[19]</ref> (without batch normalization) composed of five CNN blocks, where the spatial size is reduced by half after each block using a max-pooling layer. The decoder is composed of five CNN blocks, each preceded by an up-sampling layer based on bilinear interpolation which increases the spatial size by a factor of two. <ref type="figure">Figure 3</ref> illustrates the network structure. The number of feature maps and the used convolution kernel sizes are given below each box. After each convolution, ReLU nonlinearity is applied except for the layers with 1?1 kernels. Dealing with the diverse backgrounds of the crowd images (and of the aerial images in particular), using a deep CNN with multiple pooling layers helps reducing the influence of high frequency and small irrelevant background objects by increasing the receptive field size and extracting more contextual information. However, this could also remove the relevant details of the target objects (people) which is critical due to their small sizes. Therefore, MRCNet employs an FPN-based mechanism to combine the contextual information of the   L L L H <ref type="figure">Figure 3</ref>: Structure of the proposed MRCNet.</p><p>higher-level features and the detailed information provided by the lower-level features by element-wise adding the feature maps from the earlier stages to the ones in the later stages. This also helps avoiding the vanishing gradient problem. Using convolution layers with 1?1 kernels on each lateral connection allows linear transformation and dimension reduction in the filter space.</p><p>While most of the proposed methods focus on the counting task, MRCNet considers crowd counting and density map estimation as two interrelated tasks. To this end, being inspired by the effective composition of multiple losses in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20]</ref>, MRCNet takes advantage of two losses at different resolutions for the counting and density map estimation tasks. It has been shown by a number of previous works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25]</ref> that crowd counting could be performed without upsampling the prediction maps to the size of input images. This reduces the prediction complexity as the fine details do not have to be predicted. Taking this into account, MRCNet generates a low-resolution prediction (1/4 times smaller than the input image) at an earlier stage of the decoder and compares it to a downsampled ground-truth, optimizing the L L pixel-wise Mean Squared Error (MSE) loss. The network is supposed to predict the image count in this stage and should use the rest of the decoder for predicting the full-resolution crowd density maps, with a high localization precision, while keeping the count close to the ground truth by optimizing the L H loss, another pixel-wise MSE loss. MSE is widely used by crowd counting networks. The total loss is then computed as: L total = L L + ? L H , where ? is empirically set to 0.0001. The number of network parameters is 20.3 M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>In this section, MRCNet is evaluated on the DLR-ACD and ShanghaiTech datasets. In the training, the Adam optimization algorithm with a learning rate of 3e ?6 was employed and the batch sizes were empirically set to 60 and 40 for the DLR-ACD and ShanghaiTech datasets, respectively. In addition, apart from the VGG's parameters, all network parameters were randomly initialized by a Gaussian distribution with a zero mean and a standard deviation of 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation Metrics</head><p>Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) are the metrics that have been widely used for evaluating crowd counting performance. However, these metrics treat all images equally without considering the differences between their true counts. This could be problematic for the datasets with large variations of the image counts. For example, in the DLR-ACD dataset, I 10 and I 19 contain 285 and 24,368 people, respectively. If the prediction for each image has a count error of 1K, MAE and RMSE consider them as equal errors for both images. Nevertheless, 1K for I 10 is about 350% off its true count whereas for I <ref type="bibr" target="#b18">19</ref> it is only about 4%. Thus, in order to represent model performances on each image in single-image crowd counting scenarios, evaluation metrics should also consider the image differences such as in their counts. In this work, for analyzing model counting performance on the DLR-ACD dataset, in addition to the conventional metrics, we use Mean Normalized Absolute Error (MNAE), that evaluates the predictions for each image differently by considering the image's true count. Assuming C i and? i as the ground-truth and predicted counts for image i (i ? {1, 2, ..., N}), respectively, the metrics are computed as:</p><formula xml:id="formula_0">MAE = 1 N N ? i=1 |C i ?? i |, MNAE = 1 N N ? i=1 |C i ?? i | C i , RMSE = 1 N N ? i=1 (C i ?? i ) 2 . (1)</formula><p>For person detection, people locations are extracted by detecting local maxima in the predicted high-resolution density maps. For MRCNet, the number of people (or maxima) to be extracted is given by the person count estimated from the low-resolution prediction (i.e. the output of the earlier stage of the decoder). To evaluate the detections, we use the standard precision, recall, and F1-score. A detection is counted as true positive, if it lies within half a meter of a ground-truth person location, which is quite a strict requirement. Here, we assume that the GSD (in pixel/m) of the aerial image is known at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiments on the DLR-ACD dataset</head><p>In order to generate the ground truth density maps, we adopt the standard approach of smoothing the person locations with a small 2D Gaussian <ref type="bibr" target="#b24">[25]</ref>, a procedure which is aimed at avoiding the imbalance between the number of positive and negative samples (a pixel on each person versus all other image pixels). As the spatial resolution (GSD) of our aerial images are known and the distortions caused by the homography between the ground and image planes are negligible, the Gaussian smoothing is adapted according to the GSDs of the images. To this end, assuming that the area covered by each person (looking from above) is roughly a square of 0.5?0.5 m, the standard deviation (in pixels) of the Gaussian is computed as: ? = 1 3 0.5 GSD , where the GSD is in meters and each person will lie within 3? of the Gaussian distribution. Since the Gaussian kernels being used are normalized ( G ? (x) = 1), a sum over the ground-truth density map gives the total person count in the image.</p><p>For training, the images were tiled evenly into patches of 320?320 pixels with 50% overlap, which resulted in 11,908 patches. Then, from each patch, two samples of size 256?256 were randomly cropped, where one sample was used as it was and the other was randomly augmented. For the augmentation, three rotations (90 ? , 180 ? , and 270 ? ), two flips (left-right and up-down), and two scaling (up-and downsample) were considered on a random basis.</p><p>As the qualitative results in <ref type="figure" target="#fig_3">Figure 4</ref> show, MRCNet performs well in estimating highresolution crowd maps in both dense and sparse crowd scenarios. However, it misses some   <ref type="table">Table 3</ref>: Crowd counting and detection results on the DLR-ACD dataset.</p><p>people due to background clutter. Furthermore, the quantitative results of <ref type="table">Table 3</ref> demonstrate that MRCNet outperforms other methods by a better count estimation (lowest MNAE) and a higher quality of the estimated density maps for detection tasks (highest F1-score). In addition, considering the MRCNet's number of parameters (20.3 M), its average inference time is 0.03 ms per image patch of 256?256 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiments on the ShanghaiTech dataset</head><p>We also trained and validated MRCNet on the ShanghaiTech crowd dataset <ref type="bibr" target="#b24">[25]</ref>, which is one of the most widely used crowd benchmarks. This dataset is composed of two parts: Part-A contains 482 images (300 training and 182 test images) and Part-B contains 716 images (400 training and 316 test images). Statistics about this dataset can be seen in <ref type="table" target="#tab_2">Table 2</ref> and <ref type="figure" target="#fig_2">Figure 2</ref>. In order to generate ground truth density maps, we followed the approach proposed in <ref type="bibr" target="#b24">[25]</ref>. In order to avoid over-fitting, we randomly cropped 20 patches of size 224?224 from each training image. Then, as data augmentation, we applied left-right flipping to 30% of the patches on a random basis. In addition, the images were converted into gray scale.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This work proposed Multi-Resolution Crowd Network (MRCNet), a convolutional neural network for accurate crowd counting and density map estimation in aerial and ground imagery. MRCNet considers crowd counting and density map estimation as two interrelated tasks addressed at different resolutions. In addition, a novel aerial crowd dataset, the socalled DLR-ACD, was introduced which promotes crowd monitoring and management from aerial imagery. The superior performance of MRCNet on the DLR-ACD and ShanghaiTech (a ground imagery benchmark) datasets were shown through quantitative and qualitative results. Furthermore, results demonstrated that the estimated crowd maps can be used also for person detection thanks to their high-resolution and accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>arXiv:1909.12743v1 [cs.CV] 27 Sep 2019</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Example images from the DLR-ACD dataset captured at a sport event, city center, a trade fair, and a concert. The image details can be found in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>(a) Distribution of per-image person counts of five datasets including our aerial crowd dataset (DLR-ACD). Graph inspired by<ref type="bibr" target="#b7">[8]</ref>. (b) Histogram of the DLR-ACD's crowd counts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>MRCNet's results on the DLR-ACD dataset. Each row (from left to right) shows a sample cropped image, its ground truth, the estimated density map, and detection results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>STH Southside music festival, OAC Open Air Concert (Germany), and WIT Church day (Wittenberg, Germany). The dataset is split into 19 training and 14 test images.</figDesc><table><row><cell>Dataset</cell><cell>Data</cell><cell>Average</cell><cell cols="5">Number of Total Person Average Maximum Minimum</cell></row><row><cell></cell><cell>Collection</cell><cell>Image Size (px)</cell><cell>Images</cell><cell>Count</cell><cell>Count</cell><cell>Count</cell><cell>Count</cell></row><row><cell>UCSD [4]</cell><cell>CCTV cameras</cell><cell>158?238</cell><cell>2,000</cell><cell>49,885</cell><cell>25</cell><cell>46</cell><cell>11</cell></row><row><cell>UCF_CC_50 [7]</cell><cell>Web search</cell><cell>2101?2888</cell><cell>50</cell><cell>63,974</cell><cell>1,279</cell><cell>4,633</cell><cell>94</cell></row><row><cell>WorldExpo'10 [24]</cell><cell>CCTV cameras</cell><cell>576?720</cell><cell>3,980</cell><cell>225,216</cell><cell>56</cell><cell>334</cell><cell>1</cell></row><row><cell>ShanghaiTech-A [25]</cell><cell>Web search</cell><cell>589?868</cell><cell>482</cell><cell>241,677</cell><cell>501</cell><cell>3,139</cell><cell>33</cell></row><row><cell cols="2">ShanghaiTech-B [25] CCTV cameras</cell><cell>768?1024</cell><cell>716</cell><cell>88,488</cell><cell>123</cell><cell>578</cell><cell>9</cell></row><row><cell>UCF-QNRF [8]</cell><cell>Web search</cell><cell>2013?2902</cell><cell>1,535</cell><cell>1,251,642</cell><cell>815</cell><cell>12,865</cell><cell>49</cell></row><row><cell>DLR-ACD</cell><cell>Aerial imagery</cell><cell>3619?5226</cell><cell>33</cell><cell>226,291</cell><cell>6,857</cell><cell>24,368</cell><cell>285</cell></row></table><note>DLR's Aerial Crowd Dataset consists of 33 aerial images from various scenes ac- quired through 16 flight campaigns, where different scene types and campaigns are denoted by different colors including ALZ Allianz Arena (Munich, Germany), BOR Signal Iduna Park (Dortmund, Germany), MUC Munich city center (Germany), BRA Braunschweig city center (Germany), BAU Bauma construction trade fair (Munich, Germany), WIE Oktober- fest (Munich, Germany), RAR Rock am Ring concert (Nuremberg, Germany),</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="3">Conv. Max pooling + Conv. Deconv.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">VGG_LIU_US_VI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Add</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>128</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(1x1)</cell><cell></cell><cell></cell><cell>(1x1) 256</cell><cell></cell><cell></cell><cell>(1x1) 256</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>256</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(1x1)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>128 (3x3)</cell><cell>128 (3x3)</cell><cell>256 (3x3)</cell><cell>256 (3x3)</cell><cell>256 (3x3)</cell><cell>512 (3x3)</cell><cell>512 (3x3)</cell><cell>512 (3x3)</cell><cell>512 (3x3)</cell><cell>512 (3x3)</cell><cell>256 (3x3) Max pooling + Conv. 512 (3x3) 256 (3x3) Conv.</cell><cell>256 (3x3)</cell><cell>256 (3x3)</cell><cell>256 (3x3)</cell><cell>256 (3x3)</cell><cell>256 (3x3)</cell><cell>128 (3x3) (1x1) 1</cell><cell>128 (3x3)</cell><cell>128 (3x3)</cell></row><row><cell>1 (1x1)</cell><cell>64 (3x3)</cell><cell>64 (3x3)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Up-sampling + Conv. Add</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>128 (3x3)</cell><cell>1 (1x1) (3x3) 128</cell></row></table><note>Statistics of existing crowd datasets and the proposed DLR-ACD dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>shows crowd counting performance of MRCNet compared to the state of the art on the ShanghaiTech dataset. MRCNet outperforms all other methods on Part-A by achieving the lowest MAE and RMSE values, and achieves competitive results on Part-B.</figDesc><table><row><cell>Method</cell><cell>Output</cell><cell cols="2">Part-A</cell><cell cols="2">Part-B</cell></row><row><cell></cell><cell>size</cell><cell cols="4">MAE RMSE MAE RMSE</cell></row><row><cell>MCNN [25]</cell><cell>1:4</cell><cell cols="2">110.2 173.2</cell><cell>26.4</cell><cell>41.3</cell></row><row><cell>CMTL [20]</cell><cell>1:1</cell><cell cols="2">101.3 152.4</cell><cell>20.0</cell><cell>31.1</cell></row><row><cell>Switching CNN [16]</cell><cell>1:4</cell><cell>90.4</cell><cell>135.0</cell><cell>21.6</cell><cell>33.4</cell></row><row><cell>MSCNN [23]</cell><cell>1:4</cell><cell>83.8</cell><cell>127.4</cell><cell>17.7</cell><cell>30.2</cell></row><row><cell>D-ConvNet-v1 [18]</cell><cell>1:8</cell><cell>73.5</cell><cell>112.3</cell><cell>18.7</cell><cell>26.0</cell></row><row><cell>IG-CNN [17]</cell><cell>1:4</cell><cell>72.5</cell><cell>118.2</cell><cell>13.6</cell><cell>21.1</cell></row><row><cell>SCNet [22]</cell><cell>1:1</cell><cell>71.9</cell><cell>117.9</cell><cell>9.3</cell><cell>14.4</cell></row><row><cell>ic-CNN [14]</cell><cell>1:1</cell><cell>69.8</cell><cell>117.3</cell><cell>10.4</cell><cell>16.7</cell></row><row><cell>CSRNet [10]</cell><cell>1:8</cell><cell>68.2</cell><cell>115.0</cell><cell>10.6</cell><cell>16.0</cell></row><row><cell>Liu et al. [12]</cell><cell>1:4</cell><cell>67.6</cell><cell>110.6</cell><cell>10.1</cell><cell>18.8</cell></row><row><cell>SANet [3]</cell><cell>1:1</cell><cell>67.0</cell><cell>104.5</cell><cell>8.4</cell><cell>13.6</cell></row><row><cell>MRCNet (ours)</cell><cell>1:1</cell><cell>66.2</cell><cell>102.0</cell><cell>10.3</cell><cell>18.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Crowd counting results on the ShanghaiTech dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SegNet: A deep convolutional encoderdecoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">CrowdNet: A Deep Convolutional Network for Dense Crowd Counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Boominathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kruthiventi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia (MM)</title>
		<meeting>the ACM International Conference on Multimedia (MM)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="640" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scale aggregation network for accurate and efficient crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Privacy preserving crowd monitoring: Counting people without people models or tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bayesian linear regression for crowd density estimation in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Meynberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Reinartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Joint Urban Remote Sensing Event (JURSE)</title>
		<meeting>Joint Urban Remote Sensing Event (JURSE)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-source multi-scale counting in extremely dense crowd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2547" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Composition loss for counting, density map estimation and localization in dense crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tayyab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Athrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Al-M?adeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Rajpoot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Beyond Counting: Comparisons of Density Maps for Crowd Analysis Tasks -Counting, Detection, and Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019-05" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1408" to="1422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">CSRNet: Dilated convolutional neural networks for understanding the highly congested scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1091" to="1100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Crowd counting with fully convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Image Processing (ICIP)</title>
		<meeting>IEEE International Conference on Image Processing (ICIP)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="953" to="957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Leveraging unlabeled data for crowd counting by learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7661" to="7669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Iterative crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="278" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<meeting>Medical Image Computing and Computer-Assisted Intervention (MICCAI)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Switching convolutional neural network for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4031" to="4039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Divide and grow: Capturing huge diversity in crowd images with incrementally growing CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Sajjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3618" to="3626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Crowd counting with deep negative correlation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5382" to="5390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">CNN-based cascaded multi-task learning of high-level prior and density estimation for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<meeting>IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A survey of recent advances in CNN-based single image crowd counting and density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="3" to="16" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">In defense of single-column networks for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of British Machine Vision Conference (BMVC)</title>
		<meeting>British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-scale convolutional neural networks for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Image Processing (ICIP)</title>
		<meeting>IEEE International Conference on Image Processing (ICIP)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="465" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cross-scene crowd counting via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="833" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Single-image crowd counting via multi-column convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="589" to="597" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
