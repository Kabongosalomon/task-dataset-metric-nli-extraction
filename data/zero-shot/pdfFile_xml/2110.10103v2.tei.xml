<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CONTINUAL SELF-TRAINING WITH BOOTSTRAPPED REMIXING FOR SPEECH ENHANCEMENT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efthymios</forename><surname>Tzinis</surname></persName>
							<email>etzinis2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Adi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Meta AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vamsi</forename><forename type="middle">K</forename><surname>Ithapu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buye</forename><surname>Xu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Kumar</surname></persName>
							<email>anuragkr@fb.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CONTINUAL SELF-TRAINING WITH BOOTSTRAPPED REMIXING FOR SPEECH ENHANCEMENT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Self-supervised learning</term>
					<term>speech enhancement</term>
					<term>unsupervised denoising</term>
					<term>zero-shot learning</term>
					<term>domain adaptation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose RemixIT, a simple and novel self-supervised training method for speech enhancement. The proposed method is based on a continuously self-training scheme that overcomes limitations from previous studies including assumptions for the in-domain noise distribution and having access to clean target signals. Specifically, a separation teacher model is pre-trained on an out-of-domain dataset and is used to infer estimated target signals for a batch of in-domain mixtures. Next, we bootstrap the mixing process by generating artificial mixtures using permuted estimated clean and noise signals. Finally, the student model is trained using the permuted estimated sources as targets while we periodically update teacher's weights using the latest student model. Our experiments show that RemixIT outperforms several previous state-of-the-art self-supervised methods under multiple speech enhancement tasks. Additionally, RemixIT provides a seamless alternative for semisupervised and unsupervised domain adaptation for speech enhancement tasks, while being general enough to be applied to any separation task and paired with any separation model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Neural networks have been found to be highly effective and widely applicable to a large number of audio and speech problems, including speech enhancement, where the goal is to improve the quality and intelligibility of degraded speech signals. In recent years, several neural architectures have reported state-of-the-art results for supervised <ref type="bibr" target="#b0">[1]</ref>, real-time <ref type="bibr" target="#b1">[2]</ref> and semi-supervised <ref type="bibr" target="#b2">[3]</ref> speech enhancement tasks. Mostly driven by supervised learning, training such models requires large amounts of audio data which are expected to closely match the distribution of the test time input noisy recordings. While limited supervised data might often be available, supervised speech enhancement systems trained on them provides severely inferior performance due to the mismatch with the actual test distribution.</p><p>To address these problems and to reduce reliance on purely supervised data, several speech enhancement and audio source separation studies have shifted their focus towards self-supervised methods <ref type="bibr" target="#b3">[4]</ref>. In <ref type="bibr" target="#b4">[5]</ref>, a model was trained to estimate the signal-to-noise ratio (SNR) of noisy mixture recordings and assign a confidence value on each noisy segment. Next, a separation model was trained on the noisy speech mixtures using a weighted reconstruction loss function to filter out the contribution of noisy ground-truth speech utterances. Lately, mixture invariant training (MixIT) has been proposed in <ref type="bibr" target="#b5">[6]</ref> * Work done during an internship at Meta Reality Labs Research. which enables unsupervised training of separation models by generating artificial mixtures of mixtures (MoMs) and letting the separation model estimate and reassign the sources back to the ground-truth mixtures. MixIT has been experimentally shown to provide a robust unsupervised solution for speech enhancement under a variety of setups <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. However, MixIT assumes access to in-domain noise samples and slightly alters the input SNR distribution by training on artificial MoMs with more than one noise sources <ref type="bibr" target="#b8">[9]</ref>.</p><p>On the other hand, teacher-student setups have shown significant improvements in several audio processing tasks <ref type="bibr" target="#b9">[10]</ref>. In <ref type="bibr" target="#b10">[11]</ref>, a student model was trained at the outputs of a pre-trained MixIT model for solving the problem of the artificially created input SNR mismatch between the train and test mixture distributions. An energy threshold was used to reduce the number of sources appearing in the noisy mixtures. Moreover, a student model can be adapted to a given test set using regression over the pre-trained teacher's estimates <ref type="bibr" target="#b11">[12]</ref>. Closest to our work is the self-training framework, proposed in <ref type="bibr" target="#b12">[13]</ref>, for semi-supervised singing voice separation where the teacher was pre-trained on an out-of-domain (OOD) supervised data and used for predicting estimated sources on the larger in-domain noisy dataset. The new self-labeled version of the dataset was filtered for lowquality separated sources and stored for offline training of a new student model using artificially generated mixtures from the estimated self-labeled estimated sources. Although all the aforementioned works assumed a frozen teacher, other works in automatic speech recognition have shown significant benefits when updating the teacher model using a moving mean teacher <ref type="bibr" target="#b13">[14]</ref>.</p><p>In this paper, we propose a self-training method capable of performing self-supervised learning using large in-domain noisy datasets, while requiring only an OOD pre-trained teacher model (e.g. MixIT on an OOD dataset). In contrast to self-training methods in the literature which use ad-hoc filtering procedures to enhance the quality of the teacher model estimates, our method trains the student model by performing online remixing of the teacher's estimated sources. Moreover, instead of freezing the teacher model, RemixIT treats the self-training process as a lifelong learning procedure by using sequential and moving averaging update schemes which enables faster convergence. Our experiments showcase the general applicability of our method towards self-supervised speech enhancement, semi-supervised OOD generalization, and zero-shot domain adaptation. We also provide an intuitive explanation of why our bootstrapped remixing process works under minimal assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">REMIXIT METHOD</head><p>We present RemixIT for the general case of speech enhancement where the goal is to reconstruct the clean speech from a noisy speech signal. Formally, we train a separation model fS which outputs M </p><formula xml:id="formula_0">? (0) T ? PRETRAINTEACHER(fT , D ) ?S ? INITIALIZESTUDENT(fS ) for k = 0; k++; while k &lt;= K do for SAMPLEBATCH m ? Dm, m ? R B?T do s, n ? fT (m; ? (k) T ) // Noisy estimates m = s + ? n // Bootstrapped remixing s, n ? fS ( m; ? (k) S ) // Student estimates LRemixIT = B b=1 L( s b , s b ) + L( n b , [? n] b ) ?S ? UPDATESTUDENT(?S , ? ? S LRemixIT) end ? (k+1) T ? UPDATETEACHER(? (k) T , ?S ) end</formula><p>sources for each noisy recording from an input batch x ? R B?T containing B waveforms, each with T samples in the time-domain:</p><formula xml:id="formula_1">s, n = fS (x; ?S ), x = s + M ?1 i=1 ni = s + M ?1 i=1 ni, (1) where s, s ? R B?T , n, n ? R (M ?1)</formula><p>?B?T , ?S are: the estimated speech signals, the clean speech targets, the estimated noise signals, the noise targets and the parameters of the model, respectively. In this work, we force the estimated sources s, n to add up to the initial input mixtures x using a mixture consistency layer <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Mixture invariant training</head><p>MixIT <ref type="bibr" target="#b5">[6]</ref> has proven its effectiveness under various self-supervised speech enhancement settings <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. Specifically, MixIT assumes that the training dataset consists of two portions (Dm, Dn), where Dm is the part of the dataset which carries mixtures of speech and one noise source while Dn contains isolated noise recordings. During training, a new batch of artificial mixtures of mixtures (MoMs) is generated, x = s + n1 + n2, by sampling a batch of noisy speech recordings m ? Dm and a batch of clean noise samples n2 ? Dn, where m = s + n1. The separation model always estimates M = 3 sources ( s, n1, n2 = fS (x; ?S )) and the following permutation invariant loss is minimized for the b-th input MoM in the batch:</p><formula xml:id="formula_2">L (b) MixIT = min ? [L( s b + n ? 1 ,b , m b ) + L( n ? 2 ,b , n 2,b )], ?b, (2)</formula><p>where ? ? {(2, 3), (3, 2)} is the set of permutations of the output noise slots. However, MixIT depends on in-domain isolated noise recordings which makes it impractical for real-world settings. In most of the cases, matching the real noise distribution with the available noise set Dn is a strenuous task. The data augmentation proposed in <ref type="bibr" target="#b8">[9]</ref> shows some improvements when an extra noise source from an OOD noise distribution is injected to the input MoM. Nevertheless, the performance of that method depends on the level of distribution shift between the actual noise distribution and Dn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Continual self-training with bootstrapped remixing</head><p>RemixIT does not assume access to in-domain information. Thus, we can only draw mixtures from the in-domain noisy dataset m = s + n (m ? Dm) where the noisy speech recordings contain a single noise source each and thus, m, s, n ? R B?T . RemixIT leverages a student-teacher framework to bootstrap the remixing process by permuting the previous noisy estimates, remixing them and using them as targets for training. We summarize RemixIT in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">RemixIT's continual self-training framework</head><p>We assume that we can pre-train in a supervised or a self-supervised way a teacher model fT on an OOD dataset D which meets the specifications of Equation 1. Now, the first step is to use the teacher model to estimate some new noisy targets for a given mixture batch m = s + n ? R B?T , m ? Dm as follows:</p><formula xml:id="formula_3">s, n = fT (m; ? (k) T ), m = s + n = s + M ?1 i=1 ni,<label>(3)</label></formula><p>where k denotes the optimization step. If the teacher network was obtained by supervised (unsupervised via MixIT) OOD pre-training, we would have M = 2 (M = 3) output slots. Next, we use these estimated sources to generate new noisy mixtures as shown below:</p><formula xml:id="formula_4">m = s + n (?) ? R B?T , n (?) = ? n, ? ? PB?B,<label>(4)</label></formula><p>where ? is drawn uniformly from the set of all B ? B permutation matrices. Now, we simply use the permuted target pairs to train the student model fS on the bootstrapped mixtures m as follows:</p><formula xml:id="formula_5">s, n = fS ( m; ? (k) S ), s, n ? R B?T L (b) RemixIT = L( s b , s b ) + L( n b , n (?) b ), b ? {1, . . . , B},<label>(5)</label></formula><p>where the proposed loss function resembles a regular supervised setup with the specified signal-level loss function L. Our method does not artificially alter the input SNR distributions similar to MixIT-like <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref> training recipes. Instead, the student model is trained on mixtures with the same number of sources for the bootstrapped mixtures where the teacher model had performed adequately. Unlike previous teacher-student methods which use the same teacher-estimated source-pairs as the targets for the student network <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, the proposed bootstrapped mixtures increase the input mixture diversity and allow faster model training. This is especially useful in settings with a large distribution shift between teacher's and student's training data. Moreover, in contrast to the self-training approach in <ref type="bibr" target="#b12">[13]</ref>, where the teacher model is frozen and the inference on the in-domain dataset Dm is performed offline and the new self-labeled dataset is stored, RemixIT employs a lifelong learning process. Our method is general enough that could be paired with any online co-training method which continuously updates the teacher's weights in addition to the main student training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Error analysis under the Euclidean norm</head><p>We can express the errors produced by the student RS and the teacher RT w.r.t. the initial clean targets as random variables:</p><formula xml:id="formula_6">RS = S ? S, RT = S ? S, (S, N) ? D RS ? P ( RS | S, N, ?), RT ? P ( RT |S, N).<label>(6)</label></formula><p>Now, we focus on the part of the objective function which is minimized at every student optimization step w.r.t. the speech component. Assuming unit-norm vector signals and using a signal-level loss L that minimizes the squared error between the estimated and target signals, the student's RemixIT loss function is equivalent to:</p><formula xml:id="formula_7">LRemixIT ? E || S ? S|| 2 2 = E ||( S ? S) ? ( S ? S)|| 2 2 ? E || RS || 2 2 Supervised Loss + E || RT || 2 2 Constant w.r.t. ? S ?2 E RS , RT Errors correlation .<label>(7)</label></formula><p>Ideally, this loss could lead to the same optimization objective with a supervised setup if the last inner-product term was zero.</p><p>RS , RT = 0 could be achieved if the teacher produced outputs indistinguishable from the clean target signals or the conditional error distributions in Equation <ref type="bibr" target="#b5">6</ref> were independent. Intuitively, as we continually update the teacher model and refine its estimates, we minimize the norm of the teacher error. Additionally, the bootstrapped remixing process forces the errors to be more uncorrelated since the student tries to reconstruct the same clean speech signals s, similar to its teacher, but under a different mixture distribution. Formally, the student tries to reconstruct s when observing the bootstrapped mixtures m = s + n (?) while the teacher tries to reconstruct s from the initial input mixtures m = s + n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTAL FRAMEWORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DNS-Challenge (DNS):</head><p>We use the DNSChallenge 2020 benchmark dataset <ref type="bibr" target="#b15">[16]</ref> which covers a wide variety of noisy speech conditions. This dataset consists of 64,649 and 150 pairs of clean speech and noise recordings for training and testing, respectively. DNS is mainly used for showing the effectiveness of RemixIT at leveraging vast amounts of noisy mixture recordings. LibriFSD50K (LFSD): This dataset consists of a diverse set of speakers drawn from the LibriSpeech <ref type="bibr" target="#b16">[17]</ref> corpus and a wide variety of background noises from FSD50K <ref type="bibr" target="#b17">[18]</ref>. Specifically, 45,602 and 3,081 for training and validation, correspondingly. We follow the same data generation procedure as indicated in <ref type="bibr" target="#b7">[8]</ref>. In this study, this dataset is mainly used for the OOD unsupervised or semi-supervised pre-training of speech enhancement models. WHAM!: We follow the same procedure as in <ref type="bibr" target="#b7">[8]</ref> in order to generate noisy mixtures using speakers and noise sources from the WHAM! <ref type="bibr" target="#b18">[19]</ref> dataset. We use this dataset as a medium-sized dataset with 20,000 training noisy-speech pairs and 3,000 test mixtures. VCTK: We use the test partition of the VCTK dataset proposed in <ref type="bibr" target="#b19">[20]</ref> which includes 586 randomly generated noisy speech samples by mixing recordings from the VCTK speech corpus <ref type="bibr" target="#b20">[21]</ref> and the DEMAND <ref type="bibr" target="#b21">[22]</ref> noisy data collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Separation model</head><p>We want to underline that the proposed method can be applied alongside any separation model. In this work, we chose the Sudo rmrf <ref type="bibr" target="#b22">[23]</ref> architecture since it achieves a good trade-off between speech enhancement quality and time-memory computational requirements. Specifically, we use the Sudo rm -rf variation with group communication <ref type="bibr" target="#b23">[24]</ref> and the default parameters in <ref type="bibr" target="#b25">[25]</ref>, which has shown promising results in speech enhancement tasks <ref type="bibr" target="#b7">[8]</ref>. All models have U = 8 U-ConvBlocks except for the experiments where we increase the depth of the new student networks to 16 and 32. We fix the number of output slots to M = 3 for MixIT models or M = 2 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">RemixIT configurations</head><p>For the unsupervised RemixIT, we assume that the initial teacher model was pre-trained using MixIT at a specified OOD dataset. For semi-supervised RemixIT, we pre-train the teacher using conventional supervised training. We also experiment with various online teacher updating protocols such as:</p><formula xml:id="formula_8">? (K) T := ? (K) S ,? (k+1) T := ?? (k) S + (1 ? ?)? (k) T ,<label>(8)</label></formula><p>where k denotes the training epoch index. For the sequentially updated teacher, we replace the old teacher with the latest student every K = 20 epochs. For the zero-shot domain adaptation experiments, we first set the student to be the same as the teacher ? (0) S :=? (0) T and then use the moving average teacher update with ? = 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training and evaluation details</head><p>Although we could use any valid signal-level loss function (see <ref type="bibr">Equations 2,</ref><ref type="bibr" target="#b4">5)</ref>, we choose the negative scale-invariant signal to distortion ratio (SI-SDR) <ref type="bibr" target="#b26">[26]</ref>:</p><p>L( y, y) = ?SI-SDR( y, y) = ?10 log 10 ?y 2 / ?y? y 2 , <ref type="formula">(9)</ref> where ? = y y/ y 2 makes the loss invariant to the scale of the estimated source y and the target signal y. We train all models using the Adam optimizer <ref type="bibr" target="#b27">[27]</ref> with a batch size of B = 2 and an initial learning rate of 10 ?3 which is divided by 2 every 6 epochs. We evaluate the robustness of our speech enhancement models on the each test set after 100 epochs for the supervised trained and pre-trained teachers as well as 60 epochs for all the other configurations. Specifically, we report the SI-SDR <ref type="bibr" target="#b26">[26]</ref>, the Short-Time Objective Intelligibility (STOI) <ref type="bibr" target="#b28">[28]</ref> and the Perceptual Evaluation of Speech Quality (PESQ) <ref type="bibr" target="#b29">[29]</ref> for 16kHz target signals.  <ref type="figure">Fig. 1</ref>: Speech enhancement performance on the DNS test set when using RemixIT with different teacher update protocols. All approaches use the same teacher architecture with U = 8 which was pre-trained in a supervised way using the training split of WHAM!. Notice that by sequentially updating the teacher (orange solid line) every 20 epochs and replacing it with the previous student, we are able to obtain significant improvements over the methods which use the same static teacher (gray and black dashed lines). In this semisupervised RemixIT setup, considering the large mismatch between WHAM! and DNS datasets, the student model significantly outperforms the initial OOD pre-trained teacher by showing an improvement of more than 1 dB in terms of SI-SDR performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Continuous refinement of teacher's estimates</head><p>Our experimental results validate our hypothesis that a speech enhancement model could be trained faster and more effectively under a lifelong process where the teacher would be continuously updated in parallel to the student. The speech enhancement performance obtained by the sequentially updated and the frozen teacher protocols are shown in <ref type="figure">Figure 1</ref>. We notice that all protocols perform similarly until the 20th epoch where the teacher is still static for all strategies. However, after the 20th epoch, the teacher model is replaced with the latest student while the depth of the next student is increased 8 ? 16. As a result, the student trained with the  <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref> and supervised in-domain training with the same Sudo rm -rf model (U = 8) and the state-of-the-art FullSubNet model in the literature <ref type="bibr" target="#b1">[2]</ref>. sequential method scales better for the same number of epochs (e.g. 40th epoch) compared to the same size student (U = 16) with a frozen teacher. We have experimentally seen that the sequentially updated teacher scales better than other protocols and this is the default strategy which we use across all other experiments except for the zero-shot adaptation where we also show that the running mean teacher updating scheme is also an effective option.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Self-supervised and Self-supervised speech enhancement</head><p>The speech enhancement results of the proposed method alongside supervised and unsupervised baselines are summarized in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>The percentage of the available data denotes the portion of each disjoint splits from the DNS or the LFSD paired data collections. For instance, the unsupervised RemixIT teacher pre-training requires unsupervised MixIT using 80% of the LFSD data pairs to simulate the noisy recordings D m and the other 20% for the clean OOD noise recordings D n , whilst the regular student training leverages the whole noisy DNS dataset. Notice that unsupervised and semi-supervised RemixIT does not depend on clean in-domain noise samples. Despite that, the unsupervised student model significantly outperforms all MixIT-like approaches including the in-domain training and the recently proposed extra noise augmentation where MoMs contain 3 noise sources <ref type="bibr" target="#b8">[9]</ref> (14.5dB ? 16.0dB in terms of SI-SDR). Moreover, the largest unsupervised student (U = 32) outperforms its OOD MixIT unsupervised teacher by a large margin across all evaluation metrics which shows the efficacy of RemixIT for self-supervised settings. The proposed method also yields noticeable gains for the semi-supervised case where the student model performs comparably with in-domain supervised training using the default Sudo rm -rf model with U = 8 and a recent state-of-the-art model. We want to underline that our method could be used with more complex models as teachers, rather than the efficient Sudo rm -rf architecture, and provide even higher quality speech enhancement performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Zero-shot domain adaptation</head><p>We show that RemixIT can also be used with low-resource datasets, where training data are limited but one has access to a test dataset for adapting a pre-trained model. In <ref type="figure">Figure 2</ref>, the performance improvement for the zero-shot speech enhancement task is depicted with a variety of supervised and unsupervised pre-trained networks on larger OOD datasets. Notably, RemixIT yields improvements of up to 0.8dB in terms of SI-SDR compared to the uncalibrated pretrained models while using a limited amount of in-domain mixtures. The performance of our model is correlated with the amount of available noisy mixtures and this is the reason we see the largest (smallest) gains for the WHAM! (DNS) test partition which has 3,000 (only 150) mixtures. Moreover, we also notice a large improvement in cases where there is a large distribution shift between the training data and the mixtures in the adaptation set (e.g. supervised training on WHAM! and adapting on the relatively small DNS test set). Absolute gain in terms of SI-SDR when using RemixIT with an OOD pre-trained model and a given test set to adapt (e.g. DNS, LFSD and WHAM!, from left to right). We show our results on three different pre-trained Sudo rm -rf models with U = 8 and following MixIT unsupervised pre-training on LFSD (blue/leftmost), supervised training on LFSD (yellow) and supervised pre-training on WHAM! (green/rightmost).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>We have proposed a novel continual self-training method for denoising and have experimentally showed its benefits on several realistic speech enhancement tasks. Our method depends only on the existence of in-domain noisy data and a pre-trained model using purely out-of-domain data which might not necessarily capture the in-domain distribution. The coupling of the bootstrap remixing process with the continuously bi-directional teacher-student selftraining framework leads to significant improvements for zero-shot and self-supervised speech enhancement as well as semi-supervised domain adaptation. In the future, we aim to apply our method to other domains and denoising tasks as well as provide stronger theoretical guarantees for the convergence of our algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 2: Absolute gain in terms of SI-SDR when using RemixIT with an OOD pre-trained model and a given test set to adapt (e.g. DNS, LFSD and WHAM!, from left to right). We show our results on three different pre-trained Sudo rm -rf models with U = 8 and following MixIT unsupervised pre-training on LFSD (blue/leftmost), supervised training on LFSD (yellow) and supervised pre-training on WHAM! (green/rightmost).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>REMIXIT for the noisy dataset Dm.</figDesc><table><row><cell>Algorithm 1:</cell><cell></cell></row><row><cell>arXiv:2110.10103v2 [cs.SD] 30 Jan 2022</cell><cell></cell></row><row><cell>To appear in Proc. ICASSP 2022, May 22-27, 2022, Singapore</cell><cell>? IEEE 2022</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Speech enhancement performance on the DNS test set using the proposed RemixIT methods, unsupervised MixIT approaches</figDesc><table><row><cell cols="2">Training Method and Model Details</cell><cell>#Model Params (10 6 )</cell><cell cols="5">Available Training Data (%) Clean Speech Ds Clean Noise Dn Mixture Dm DNS LFSD DNS LFSD DNS LFSD</cell><cell cols="2">Evaluation Metrics SISDR PESQ STOI (dB)</cell></row><row><cell cols="2">Input Noisy Mixture</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>9.2</cell><cell>1.58</cell><cell>0.915</cell></row><row><cell>Unsupervised MixIT with Student (U = 8)</cell><cell>In-domain OOD noise Extra noise [9]</cell><cell>0.79 0.79 0.79</cell><cell></cell><cell>20%</cell><cell>20% 50%</cell><cell>80% 100% 100%</cell><cell></cell><cell>14.4 14.3 14.5</cell><cell>2.13 2.02 2.03</cell><cell>0.933 0.933 0.930</cell></row><row><cell>Unsupervised RemixIT</cell><cell>Teacher (U = 8) Student (U = 32)</cell><cell>0.79 0.97</cell><cell></cell><cell></cell><cell>20%</cell><cell>100%</cell><cell>80%</cell><cell>14.8 16.0</cell><cell>2.15 2.34</cell><cell>0.940 0.952</cell></row><row><cell>Semi-supervised RemixIT</cell><cell>Teacher (U = 8) Student (U = 32)</cell><cell>0.56 0.73</cell><cell>100%</cell><cell></cell><cell>100%</cell><cell>100%</cell><cell></cell><cell>17.6 18.0</cell><cell>2.61 2.60</cell><cell>0.958 0.959</cell></row><row><cell>Supervised In-domain</cell><cell>Student (U = 8) FullSubNet [2]</cell><cell>0.56 5.6</cell><cell>100% 100%</cell><cell>100% 100%</cell><cell></cell><cell></cell><cell></cell><cell>18.6 17.3</cell><cell>2.69 2.78</cell><cell>0.962 0.961</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dense cnn with selfattention for time-domain speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1270" to="1279" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fullsubnet: A full-band and sub-band fusion model for realtime single-channel speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangdong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Horaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP, 2021</title>
		<meeting>ICASSP, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="6633" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Poconet: Better speech enhancement with frequency-positional embeddings, semi-supervised conversational data, and biased loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umut</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritwik</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neerad</forename><surname>Phansalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Valin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Helwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvindh</forename><surname>Krishnaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2487" to="2491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Incorporating real-world noisy speech in neural-network-based speech enhancement systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.05172</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Personalized speech enhancement through self-supervised data augmentation and purification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aswin</forename><surname>Sivaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
		<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="2676" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised sound separation using mixture invariant training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efthymios</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS, 2020</title>
		<meeting>NeurIPS, 2020</meeting>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3846" to="3857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Noisy-target training: A training strategy for dnn-based speech enhancement without clean speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Fujimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuma</forename><surname>Koizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Yatabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryoichi</forename><surname>Miyazaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.08625</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Separate but together: Unsupervised federated learning for speech enhancement from non-iid data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efthymios</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Casebeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhepei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WASPAA, 2021</title>
		<meeting>WASPAA, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="46" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Training speech enhancement systems with noisy speech datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koichi</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Fabbro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.12315</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Teacher-student deep clustering for low-delay single channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Aihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiyuki</forename><surname>Hanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohei</forename><surname>Okato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="690" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Teacher-Student MixIT for Unsupervised and Semi-Supervised Speech Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?t?lin</forename><surname>Zoril?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Doddipatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Barker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
		<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="3495" to="3499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Test-time adaptation toward personalized speech enhancement: Zero-shot learning with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WASPAA, 2021</title>
		<meeting>WASPAA, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="176" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised singing voice separation with noisy self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhepei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritwik</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umut</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Valin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvindh</forename><surname>Krishnaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP, 2021</title>
		<meeting>ICASSP, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Momentum pseudo-labeling for semi-supervised speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosuke</forename><surname>Higuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niko</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
		<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="726" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Differentiable consistency constraints for improved deep speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Chinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rif A</forename><surname>Patton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saurous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="900" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The interspeech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Chandan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2020</title>
		<meeting>Interspeech, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="2492" to="2496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Fsd50k: an open dataset of human-labeled sound events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Favory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Font</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00475</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">WHAM!: Extending Speech Separation to Noisy Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Antognini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><forename type="middle">Richard</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmett</forename><surname>Mcquinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dwight</forename><surname>Crow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Manilow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1368" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention wave-u-net for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritwik</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umut</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvindh</forename><surname>Krishnaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WASPAA</title>
		<meeting>WASPAA</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="249" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">CSTR VCTK Corpus: English multi-speaker corpus for CSTR voice cloning toolkit (version 0.92)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Veaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirsten</forename><surname>Macdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The diverse environments multi-channel acoustic noise database (demand): A database of multichannel environmental noise recordings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Thiemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobutaka</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICA</title>
		<meeting>ICA</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sudo rm-rf: Efficient networks for universal audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efthymios</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhepei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MLSP, 2020</title>
		<meeting>MLSP, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ultra-lightweight speech separation via group communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="16" to="20" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Compute and memory efficient universal sound source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efthymios</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhepei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Signal Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sdr-half-baked or well done?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An algorithm for intelligibility prediction of timefrequency weighted noisy speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2125" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Antony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">G</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andries P</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="749" to="752" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
