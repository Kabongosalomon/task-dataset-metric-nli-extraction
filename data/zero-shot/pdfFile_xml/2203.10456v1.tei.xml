<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">simCrossTrans: A Simple Cross-Modality Transfer Learning for Object Detection with ConvNets or Vision Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoke</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Hunter College</orgName>
								<orgName type="institution" key="instit2">CUNY New York City</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Ioannis Stamos Hunter College &amp; The Graduate Center</orgName>
								<orgName type="institution">CUNY</orgName>
								<address>
									<settlement>New York City</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">simCrossTrans: A Simple Cross-Modality Transfer Learning for Object Detection with ConvNets or Vision Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transfer learning is widely used in computer vision (CV), natural language processing (NLP) and achieves great success. Most transfer learning systems are based on the same modality (e.g. RGB image in CV and text in NLP). However, the cross-modality transfer learning (CMTL) systems are scarce. In this work, we study CMTL from 2D to 3D sensor to explore the upper bound performance of 3D sensor only systems, which play critical roles in robotic navigation and perform well in low light scenarios. While most CMTL pipelines from 2D to 3D vision are complicated and based on Convolutional Neural Networks (ConvNets), ours is easy to implement, expand and based on both Con-vNets and Vision transformers(ViTs): 1) By converting point clouds to pseudo-images, we can use an almost identical network from pre-trained models based on 2D images. This makes our system easy to implement and expand. 2) Recently ViTs have been showing good performance and robustness to occlusions, one of the key reasons for poor performance of 3D vision systems. We explored both ViT and ConvNet with similar model sizes to investigate the performance difference. We name our approach simCrossTrans: simple cross-modality transfer learning with ConvNets or ViTs. Experiments on SUN RGB-D dataset show: with sim-CrossTrans we achieve 13.2% and 16.1% absolute performance gain based on ConvNets and ViTs separately. We also observed the ViTs based performs 9.7% better than the ConvNets one, showing the power of simCrossTrans with ViT. simCrossTrans with ViTs surpasses the previous state-of-the-art (SOTA) by a large margin of +15.4% mAP50. Compared with the previous 2D detection SOTA based RGB images, our depth image only system only has a 1% gap. The code, training/inference logs and models are publicly available at https://github.com/ liketheflower/simCrossTrans. Figure 1. A cloud point from the SUN RGB-D dataset [43].</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human beings (also some animals) have excellent vision systems by observing the colorful word using two eyes. Meanwhile, a stereoscopic vision system from two eyes makes human have an implicit depth sensor to infer the depth information to objects. We can build robots to have similar vision systems by applying 2D camera sensor and 3D Lidar or depth sensors. However, not all the scenarios, 2D cameras are available, such as night time or in some cases, people's privacy are being protected and hence that 2D cameras are not allowed to be deployed on robotics. This makes the 3D sensor only vision systems desirable for robotics. What's more, the 3D sensor only based on system can work well in the low light or no light scenarios, hence, it is environment friendly and cleaner. Thinking about how much electric bills can be saved if we have a warehouse robotic based on 3D sensors only and no need using lights. Motivated by this, we work on exploring the upper bound performance of the 3D sensor only vision systems. <ref type="bibr">Figure 2</ref>. Cross Sensor/Modal Transfer Learning approach: The top left one is a regular image classification, detection and segmentation pipeline with backbone network either ConvNets or Vision Transformers and head for each subtask. The pre-train is based on RGB color images collected by cameras. On the bottom left, the pre-trained model, including the backbone and heads, are fine tuned based the pseudoimages converted from point clouds, which can be collected by depth or Lidar sensors. The right shows clear performance boosting of with simCrossTrans V.S. without simCrossTrans based on the ConvNets (ResNet-50 <ref type="bibr" target="#b15">[16]</ref>) or Transformers (Swin-T <ref type="bibr" target="#b26">[27]</ref>) as backbone network. Both ConvNets and Transformer surpass the previous state-of-the-art depth image only 2D detection system, Frustum Voxnet <ref type="bibr" target="#b40">[40]</ref>, by a large margin. Details of the result see table <ref type="bibr">1.</ref> When thinking about the human being's vision system, we observing the colorful 3D world since we first open our eyes. Unsurprisingly, when the first time a baby see a photo or picture, which is essentially a projection of the 3D world to 2D, it is not hard for the baby to recognize the interesting objects on the photo. Also, in <ref type="figure">Figure 1</ref> before, it is a point cloud image from the SUN RGB-D dataset. Although this may be the first time that a person sees this kind image, it is not hard to recognize the bed, pillows and lamps in this image. Meanwhile, based on experience studying from real life, we can also infer some objects should have low possibility to be observed such as cars, trains, etc. Why it works? One possible reason is although the the madalities used to percept the world is different, if we observe the same world, the context or prior knowledge are sharing, hence can be transferred. The difference between human and robotics are: human is transferring knowledge learning from 3D world to 2D. For robotics, it will be easier if we do the reverse way due to there are significantly more 2D data and models trained based on those data available. How can we transfer the knowledge from 2D sensor to 3D sensor? The answer is quite simple: using transfer learning. Transfer learning has been widely used in CV and NLP and achieve great success. Originally, the transfer learning in CV is following a supervised pretraining approaches, such as pretraining on ImageNet and fine-tune on other datasets for image classifications or object detection and instance segmentation <ref type="bibr" target="#b15">[16]</ref>. At the same time, NLP also achieves a tremendous success by applying self-supervised pretraining approaches to further improve the following subtasks' performance. Classical works are <ref type="bibr" target="#b5">[6]</ref>. Inspired by the success of self-supervised pretraining approaches, CV researchers are also trying to use self-supervised pretraining approaches <ref type="bibr" target="#b13">[14]</ref> to build better transfer learning systems. However, all those systems mentioned here are based on the same modality. Whether we can achieve similar success when applying cross-madality transfer learning, which is pretraining a model based on one madality and fine-tune the model based on another modality? The cross-modality transfer learning (CMTL) work are rare compared with the transfer learning between the same modality. There are more general CMTL system, which pretrains a model from one modality and fine tune a model from a totally different model, such as from text to vision shown in work <ref type="bibr" target="#b30">[31]</ref> and from vision to sound shown in <ref type="bibr" target="#b45">[45]</ref>. This is not the scope of this work due to: our goal is investigating the capability of cross modality transfer learning from 2D to 3D sensors and we need the different modality sharing the same observing target to make sure the knowledge relatively easier to be transferred. Inspired by the ViT <ref type="bibr" target="#b6">[7]</ref> work, where the authors built a image classification system with as less modification to the original transformer <ref type="bibr" target="#b48">[48]</ref> framework as possible to build a unified model which can be used to process vision and languages, we propose a framework with almost zero modification to any 2D object detection based system by adjusting the raw point cloud data to 3-channel pseudoimages (See the bottom left two images as example, the first one is the same point cloud image as <ref type="figure">Figure 1</ref> and the second is the converted 3 channel pseudo image). By doing Each block has 9 images: top left is the original RGB image, it is only used for better observe the objects, the RGB images are not used during the training and testing process. bottom left is the pseudo 2D image converted from point cloud. The top middle is the ResNet-50 backbone based 2D detection result without using simCrossTrans and the top right is with using simCrossTrans. The bottom middle is the Swin-T backbone based 2D detection result without using simCrossTrans and the bottom right is with using simCrossTrans. More results can be found in https://youtu.be/qQ0w-GpPzjI this, we have at least two benefits: 1) simple: almost no need to modify the model architecture, we can train the model based on new modality. 2) easy to be expanded: any improvement in regular 2D vision system can be expanded to further improve our 3D system by following our approach.</p><p>Due to those two advantages mentioned above, we can easily explore the latest progress in vision which is Vision Transformer based models, specifically, it is the Swin-Transformer <ref type="bibr" target="#b26">[27]</ref>. Self-attention-based architectures, in particular Transformers <ref type="bibr" target="#b48">[48]</ref>, have become the model of choice in NLP and achieved great success with transfer learning. The ViT work cuts the image into patches and project each patch's raw pixels to a vector, and the vector can be treated as the word embedding in NLP. Hence the image can be fed into a standard transformer to performance image classification task. With enough pretraining data, the ViT suppass the SOTA on image classification based on ImageNet <ref type="bibr" target="#b4">[5]</ref> dataset. The swin-transformer <ref type="bibr" target="#b26">[27]</ref> further extend the ViT to solve the object detection and instance segmentation tasks in CV by introducing the hierarchical transformer to extract different scale features and shifted window approach approaches instead of sliding window to reduce the computation complexity. The Swin-Transfomer achieved great success and provides the CV an alternative approach besides of using ConvNets. Although the Swin-Transformer performances well based 2D RGB images, why it works better than ConvNets are not very clear. Besides that, whether it can also achieve a better performance with CMTL is not verified. However, from <ref type="bibr" target="#b29">[30]</ref> it shows that ViT includes impressive robustness to severe occlusions, which is one of the critical causes for the poor performance of many 3D vision systems. This observation encourages us to explore the ViT's performance. Based on above, in this work, we explored both ConvNets <ref type="bibr" target="#b19">[20]</ref> and ViTs, and provides detailed comparison and analysis.</p><p>The workflow for our system is pretty straight forward: pretrain based on RGB images and then fine-tune based on the 3 channel pseudo images converted from point cloud. An illusttration can be found in <ref type="figure">Figure 2</ref>. We name our system as simCrossTrans: simple cross-modality transfer learning with ConvNets or ViTs with respect to both ConvNets and Transformers.</p><p>In summary, in this article, we want to find answers for the following questions:</p><p>? Whether we can achieve performance boosting for simCrossTrans based on vision transformers (We did not mention ConvNets, as the performance boosting was observed in Frustum-VoxNet <ref type="bibr" target="#b40">[40]</ref>).</p><p>? If simCrossTrans works, why?</p><p>? For ConvNets and Vision Transformers, which can achieve a better performance when applying sim-CrossTrans? Why one is better than the other?</p><p>Through experiments, we indeed have interesting observations and achieve SOTA results on 2D object detection based on point cloud only images. In <ref type="figure" target="#fig_0">Figure 3</ref> we show some visualization of the 2D object detection by comparing of with and without the simCrossTrans based on ResNet-50 and Swin-T as backbone network. From the result, we can see by using the simCrossTrans, not only we achieve significantly bounding box detection performace boosting but also the system with simCrossTrans can produce reasonable segmentation outputs even without fine tuning on that branch. We will explore details in the following session. Here are our contributions:</p><p>? We highlight the performance boosting of using sim-CrossTrans based on ConvNets.</p><p>? We experimentally show the performance boosting of using simCrossTrans based on vision Transformers.</p><p>? We comprehensively compare the performance difference of applying simCrossTrans between ConvNets and Vision Transformers to provide insights about the difference of those two backbone networks.</p><p>? When applying simCrossTrans on a vision transformer based detection framework, we achieve SOTA on 2D object detection based on point clouds only.</p><p>? We open source our code, training/testing logs and model checkpoints to benefit the vision community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Transfer learning with same modality. Transfer learning is widely used in computer vision (CV), natural language processing (NLP) and biochemistry. Most transfer learning systems are based on the same modality (e.g. RGB image in CV and text in NLP). For the CV, common transfer learning is based on supervised way such as works in R-CNN <ref type="bibr" target="#b9">[10]</ref>, Fast RCNN <ref type="bibr" target="#b8">[9]</ref>, Faster RCNN <ref type="bibr" target="#b36">[37]</ref>, FPN <ref type="bibr" target="#b23">[24]</ref>, mask R-CNN <ref type="bibr" target="#b14">[15]</ref>, YOLO <ref type="bibr" target="#b34">[35]</ref>, YOLO9000 <ref type="bibr" target="#b35">[36]</ref>, Reti-naNet <ref type="bibr" target="#b21">[22]</ref> use a pretrained backbone network model based on ImageNet classification task and the model is further trained based on the following task datasets such as COCO to achieve object detection or/and instance segmentation tasks. In the NLP, the transfer learning such as BERT <ref type="bibr" target="#b5">[6]</ref>, GPT <ref type="bibr" target="#b32">[33]</ref>, GPT-2 <ref type="bibr" target="#b33">[34]</ref>, GPT-3 <ref type="bibr" target="#b1">[2]</ref> are mainly based on self-supervised way and achieve great success. Inspired by the success of the self-supervised way tansfer learning, the CV community is also exploring the self-supervised way to explore new possibilities, one recent work which is similar to the BERT in NLP is MAE <ref type="bibr" target="#b13">[14]</ref>. The MolGNN <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">39]</ref> in bioinformatics use a self-supervised way based on Graph Neural network (GNN) in the pretraining stage and achieve good performance in a few shot learning framework for the following subtasks. For this work, we explore the cross modalitiy transfer learning from a pretrained model under the supervised learning approach.</p><p>Cross sensor/modality transfer learning. In this work, we focus on the vision related cross modality transfer learning. The cross modality between vision and other modalities, such as from text to vision shown in work <ref type="bibr" target="#b30">[31]</ref> and from vision to sound shown in <ref type="bibr" target="#b45">[45]</ref> is beyond the scope of this work and will not be discussed. <ref type="bibr" target="#b17">[18]</ref> explored the cross modality transfer learning from RGB to X-ray images, however, this work was mainly focusing on the image classification and it is based on ConvNets only. <ref type="bibr" target="#b20">[21]</ref> use pre-train ConvNets (VGG16 and ResNet 16) on an unrelated set of medical images (mammograms) first and then fine-tune on brain magnetic resonance (MR) images to address segmentation task. This work is mainly focusing on segmentation and transformer is not explored. <ref type="bibr" target="#b10">[11]</ref> change the one channel depth image into 3 channels and use two separate VGG16 based ConvNets for converted 3 channel depth images and RGB images to extract features. The extracted features are combined later to solve the scene classification task. This framework is complicated due to two backbone networks and post combining networks have to be used. Also, this system does not support the uni-modality inference scenario. Frustum-Voxnet <ref type="bibr" target="#b40">[40]</ref> used pretrained weights from the RGB images to fine tune the point cloud converted pseudo image, which is most close to this work. However, in <ref type="bibr" target="#b40">[40]</ref>, the performance difference of using CMTL and without using the CMTL are not compared. Plus, the work in <ref type="bibr" target="#b40">[40]</ref> is based on ConvNets, we further explored the performance difference of using ViTs and also have a deep analysis about the reasons.</p><p>Projecting 3D sensor data to 2D Pseudo Images. There are different ways to project 3D data to 2D features. HHA was proposed in <ref type="bibr" target="#b11">[12]</ref> where the depth image is encoded with three channels: Horizontal disparity, Height above ground, and the Angle of each pixel's local surface normal with gravity direction. The signed angle feature described in <ref type="bibr" target="#b46">[46]</ref> measures the elevation of the vector formed by two consecutive points and indicates the convexity or concavity of three consecutive points. Input features converted from depth images of normalized depth(D), normalized relative height(H), angle with up-axis(A), signed angle(S), and missing mask(M) were used in <ref type="bibr" target="#b49">[49]</ref>. DHS images are used in <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b41">41]</ref>.</p><p>Object Detection Based on RGB images by ConvNets RGB-based approaches can be summarized as two-stage frameworks (proposal and detection stages) and one-stage frameworks (proposal and detection in parallel). Generally speaking, two-stage methods such as R-CNN <ref type="bibr" target="#b9">[10]</ref>, Fast RCNN <ref type="bibr" target="#b8">[9]</ref>, Faster RCNN <ref type="bibr" target="#b36">[37]</ref>, FPN <ref type="bibr" target="#b23">[24]</ref> and mask R-CNN <ref type="bibr" target="#b14">[15]</ref> can achieve a better detection performance while one-stage systems such as YOLO <ref type="bibr" target="#b34">[35]</ref>, YOLO9000 <ref type="bibr" target="#b35">[36]</ref>, RetinaNet <ref type="bibr" target="#b21">[22]</ref> and FFESSD <ref type="bibr" target="#b42">[42]</ref> are faster at the cost of reduced accuracy. For deep learning based systems, as the size of network is increased, larger datasets are required. Labeled datasets such as PASCAL VOC dataset <ref type="bibr" target="#b7">[8]</ref> and COCO (Common Objects in Context) <ref type="bibr" target="#b22">[23]</ref> have played important roles in the continuous improvement of 2D detection systems. Most systems introduced here are based on ConvNets. Nice reviews of 2D detection systems can be found in <ref type="bibr" target="#b38">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object Detection Based on RGB images by Vision Transformers</head><p>When replacing the backbone network from ConvNets to Vision Transformers, the systems will be adopted to Vision Transformers backbone based object detection systems. The most successfully systems are Swin-transformer <ref type="bibr" target="#b26">[27]</ref> and Swin-transformer v2 <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Convert point clouds to pseudo 2D image</head><p>In order to use pretrained models based on RGB images, we convert point clouds to pseudo 2D images with 3 channels. The point clouds can be converted to HHA or any three channels from DHASM introduced in <ref type="bibr" target="#b50">[50]</ref>.</p><p>For this work, we follow the same approaches in Frustum VoxNet <ref type="bibr" target="#b40">[40]</ref> by using DHS to project 3D depth image to 2D due to: 1) <ref type="bibr" target="#b50">[50]</ref> shows DHS can provide a solid result; 2) have a fair comparison with the Frustum VoxNet <ref type="bibr" target="#b40">[40]</ref> work.</p><p>Here is a summary of the DHS encoding method: Similar to <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b50">50]</ref>, we adopt Depth from the sensor and Height along the sensor-up (vertical) direction as two reliable measures. Signed angle was introduced in <ref type="bibr" target="#b47">[47]</ref>: denote X i,k = [x ik , y ik , z ik ] the vector of 3D coordinates of the k-th point in the i-th scanline. Knowledge of the vertical direction (axis z) is provided by many laser scanners, or even can be computed from the data in indoor or outdoor scenarios (based on line/plane detection or segmentation results from machine learning models) and is thus assumed known. Define D i,k = X i,k+1 ? X i,k (difference of two successive measurements in a given scanline i), and A ik : the angle of the vector D i,k with the pre-determined z axis (0 to 180 degrees). The Signed angle S ik = sgn(D i,k ? D i,k?1 ) * A ik : the sign of the dot product between the vectors D i,k and D i,k?1 , multiplied by V ik . This sign is positive when the two vectors have the same orientation and negative otherwise. Those 3 channel pseudo images are normalized to 0 to 1 for each channel. Some samples DHS images can be seen in <ref type="figure">Figure 2</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Simple Cross-Modality transfer learning</head><p>The simple cross-modality transfer learning approach contains 3 key steps: 1) Pre-train 2D vision system based on 2D RGB images in either supervised way or self-supervised way. In this work, we explore the supervised approach; 2) Convert the point clouds to pseudo images. 3) Fine-tune the pretrain model from the RGB image based on the pseudo images. An illustration of this pipeline is shown in <ref type="figure">Figure  2</ref>. Both ConvetNets and Vision Transformers can be used as backbone networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SUN RGB-D dataset Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">SUN RGB-D dataset</head><p>SUN RGB-D <ref type="bibr" target="#b43">[43]</ref> dataset is an indoor dataset which provides both the point cloud and RGB images. In this work, since we are building a 3D only object detection system, we only use the point clouds for fine tuning. The RGB images are not used during the fine tuning process. For the point clouds, they are collected based on 4 types of sensors: Intel RealSense, Asus Xtion, Kinect v1 and Kinect v2. The first three sensors are using IR light pattern. The Kinect v2 is based on time-of-flight. The longest distance captured by the sensors are around 3.5 to 4.5 meters.</p><p>SUN RGB-D dataset splits the data into a training set which contains 5285 images and a testing set which contains 5050 images. For the training set, it further splits into a training only, which contains 2666 images and a validation set, which contains 2619 images. Similar to <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b44">44]</ref> , we are fine-tuning our model based on the training only set and evaluate our system based on the validation set. We call the only training dataset as train2666 in the future description. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">2D detection framework</head><p>For the 2D detection, we use the classical object detection framework: Mask R-CNN <ref type="bibr" target="#b14">[15]</ref> in mmdetection <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">2D detection backbone networks</head><p>For the backbone network, we tried the 50 layer ConvNets: ResNet-50 <ref type="bibr" target="#b15">[16]</ref> and ViT based Swin-T <ref type="bibr" target="#b26">[27]</ref>. Similar to <ref type="bibr" target="#b26">[27]</ref>, we compare the ConvNets with ViT by using ResNet-50 and Swin-T due to they have similar number of parameters (see <ref type="table" target="#tab_2">Table 3</ref>) to have a relatively fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Pre-train</head><p>Both the Swin-T and ResNet-50 based networks 1 are firstly pre-trained on ImageNet <ref type="bibr" target="#b4">[5]</ref> and then pre-trained on the COCO dataset <ref type="bibr" target="#b22">[23]</ref>.</p><p>Data augmentation When pre-training on COCO dataset, the images augmentation are applied during the training stage by: randomly horizontally flip the image with probability of 0.5; randomly resize the image with width of 1333 and height of several values from 480 to 800 (details see the configure file from the github repository); randomly crop the original image with size of 384 (height) by 600 (width) and resize the cropped image to width of 1333 and height of several values from 480 to 800.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Fine-tuning</head><p>Data augmentation: We follow the same augmentation with pre-train stage. The raw input images has the width of 730 and height of 530. Those raw images are randomly resized and cropped during the training. During testing, the images are resized to width of 1120 and height of 800 which can be divided by 32.</p><p>Hardware: For the fine-tuning, we use a standard single NVIDIA Titan-X GPU, which has 12 GB memory. We <ref type="bibr" target="#b0">1</ref> The pretrained weights are loaded from mmdetection <ref type="bibr" target="#b2">[3]</ref>. fine-tune the Resnet-50 and Swin-T backbone network for 100 epochs 2 . It took 16 hours to train the ResNet-50 based network with batch size of 2 (for 133 iterations) and 29 hours for Swin-T based network with batch size of 2 (for 133K iterations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-tuning subtasks:</head><p>We focus on the 2D object detection performance, so we fine-tune the model based on the 2D detection related labels. We kept the mask branch without training to further verify whether reasonable mask detection can be created by naively apply a model learned from the RGB images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Results</head><p>Evaluation metrics: Following the previous works mentioned in 1, we firstly use the AP50: Average Precision at IoU = 0.5 as evaluation metric. We also use the COCO object detection metric which is AP75: Average Precision at IoU = 0.75 and a more strict one: AP at IoU = .50:.05:.95 <ref type="table">Method   Backbone Network  SUNRGBD10  SUNRGBD16  SUNRGBD66  SUNRGBD79  AP  AP50  AP75  AP  AP50  AP75  AP  AP50  AP75  AP  AP50  AP75  APS  APM</ref>   <ref type="table">Table 2</ref>. More results comparison based on AP@IoU = .75, AP and AP of different scales.  to evaluate the 2D detection performance.</p><p>Evaluation subgroups. SUN RGB-D has about 800 2D objects. Not all the categories are detected in the previous works. In order to compare with different previous works, we group the subgroup to SUNRGBD10, SUNRGBD16 which have 10 categories and 16 categories. We also introduce SUNRGBD66 and SUNRGBD79 which contains 66 and 79 categories. The 66 categories are relatively easier to be recognized from the raw cloud points and the 79 categories contains all the 800 2D objects by putting the rare ones into the category of others.</p><p>Detail list of those sub groups can be found in the appendix. simCrossTrans performance. As shown in table 1, with simCrossTrans we achieve 13.2% and 16.1% absolute performance gain (by evaluating AP50 based the SUN-RGBD16 subgroup) compared with without simCrossTrana based on ConvNets and ViTs separately. This improvement is significant and beyond our expectations showing the effectiveness of using simCrossTrans.</p><p>Compare with previous depth image only based 2D detection. When comparing with the previous SOTA 3D image only system Frustum VoxNet <ref type="bibr" target="#b40">[40]</ref>, our Swin-T based one achieve a significantly improvement by a large margin of +15.4% mAP50. This is promising and we give detailed analysis about the possible reason in the Discussion session.</p><p>Compare with previous RGB or RGB-D image based 2D detection. We further compare with our 3D point clouds based 2D detection system with SOTA RGB image based 2D detection system, there is only 1% gap. It shows that power of using simCrossTrans. This is meaningful as we further reduce the gap between the 3D only image based detection system to the 2D RGB image based system.</p><p>More results. Besides the AP50, which was mainly used in previous works, we also use AP75 and AP to compare the results based on with simCrossTrans and without simCrossTrans. Meanwhile, we also report AP Across Scales of small, medium and large by following the same standard of COCO dataset. Those results can be found in <ref type="table">Table 2</ref>. From the results, we see that the withCrossTrans by either ConvNets based or ViT based, we can achieve significant performance improvements. At the same time ViT, specifically Swin-T based, can achieve a even better results with simCrossTrans.</p><p>Model size and inference speed. We compare the model size by number of parameters and the inference speed in <ref type="table" target="#tab_2">Table 3</ref>. The Swin-T based Mask-RCNN network has slightly more parameters compared to ResNet-50 based one. All the inference speed are tested based on a single Titan GPU. Although Swin-T and ResNet-50 has the similar number of FLOPs, the speed of ResNet-50 is much faster than Swin-T based. We believe this is due to the NVIDIA CUDA Deep Neural Network library (cuDNN)'s optimization on convolution operations. About 10 FPS should be enough for slow moving robotic to make real-time decision. If faster inference speed is needed, we can use more advanced GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Discussion</head><p>Why simCrossTrans works? From the results shown in the <ref type="table">Table 1</ref> and 2 we can see the big influence for the simCrossTrans on both ConvNets and ViT based networks. In <ref type="figure" target="#fig_2">Figure 4</ref>, we plot the mAP curve for the SUNRGBD79 categories to compare the performance of simCrossTrans. It shows clear difference of with and without simCrossTrans on the performance. In order to better understand why the simCrossTrans works so well, we plot the loss for different components related to the 2D object detection as shown in <ref type="figure" target="#fig_3">Figure 5</ref>. From the plots, we can see with the sim-CrossTrans, all component's loss are significantly reduced. We believe this is due to the pre-training based on the RGB images has well learned the context information need to better detect objects. For example, if we can detect the bed, then we will expect there is night stand or lamps near the bed. Representations learned from contextual information was shown to be useful in NLP tasks as shown in BERT <ref type="bibr" target="#b5">[6]</ref>. In <ref type="bibr" target="#b12">[13]</ref> it claims: 1) pre-training leads to wider optima on the loss landscape, and eases optimization compared with training from scratch. 2) The pre-training-then-fine-tuning paradigm is robust to overfitting. From our observation shown in <ref type="figure" target="#fig_2">Figure 4</ref>, by comparing of with simCrossTrans and without simCroassTrasn, we can see the time used to achieve the best performance is almost the same for both the ConvNets based and ViT based. We believe the pretraining can put the fine-tuning stage into an optimization landscape with much lower initial loss values in general. Meanwhile, in the fine-tune stage, the data used to train the model will add more details to the optimization landscape, which makes training from scratch and with pre-training take similar time to reach the (sub)optimal ending point. We did not observe the pre-training's improvement on overfitting. The deep understanding of why simCrossTrans works will be one of the future works.  Why ViT is better than ConvNets? From the results, we can see that the Swin Transformer <ref type="bibr" target="#b26">[27]</ref> based achieved a significant better result than ConvNet based on both with simCrossTrans and without simCrossTrans, although they have a similar computation complexity. In <ref type="bibr" target="#b3">[4]</ref>, the paper claims that a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer. In the ViT <ref type="bibr" target="#b6">[7]</ref> paper, it claims that "Transformers lack some of the inductive biases inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data. But for larger datasets, learning the relevant patterns directly from data is sufficient, even beneficial." From our observation, we believe the following are the critical reasons that Swin-T performs better than ResNet-50:</p><formula xml:id="formula_0">F LOP s = 2 * H * W (C in * K 2 + 1) * C out<label>(1)</label></formula><p>? A better global view: Based on transformer <ref type="bibr" target="#b48">[48]</ref>, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a ConvNet layer requires O(log n k ) operations, where k is the kernel size and n is the sequence length (in vision, it is related to image width/height). Swin-T <ref type="bibr" target="#b26">[27]</ref> applies transformer in each small window, which has M XM image patches. The M is typical set as 7. It means that the receptive field for Swin-T is in a 7X7 scale. Meanwhile, at the first input layer, the image are organized by each patch, each patch contain 4X4 raw pixels images, so the effective field of the first layer is 28x28, which is pretty large compared with ConvNet kernel which has a typical size of 3x3. What's more, the shifted windowing introduced in Swin Transformer <ref type="bibr" target="#b26">[27]</ref> further increase the receptive field. The shifted windowing was proved effective in object detection in the Swin Transformer work. Increasing the kernel size of ConvNet can solve the problem, however, the increased kernel size will greatly increase the FLOPs as it is increasing quadratically with the kernel size (see equation 1 from <ref type="bibr" target="#b28">[29]</ref>). A detailed comparison of the receptive field of ResNet-50 and Swin-T based on the SUN RGB-D dataset we used is shown in <ref type="table" target="#tab_4">Table 4</ref>.</p><p>? Swin-T has more powerful feature extraction unit: As shown in <ref type="table" target="#tab_4">Table 4</ref>, when comparing the number of channel/dimensions used for ResNet-50 and Swin-T, we firstly see that ResNet-50 has more number of channels. However, when taking the number of multihead attention head number into consideration, the Swin-T has more basic computation units.</p><p>Another two important differences between Swin-Transformer to ResNet are: 1) Replacing the pooling operations to Patch merging. 2) Using layer normalization (LN) <ref type="bibr" target="#b0">[1]</ref> instead of batch normalization (BN) <ref type="bibr" target="#b16">[17]</ref>. In <ref type="bibr" target="#b27">[28]</ref>, the auther replacing the BN in ConvNets with LN and did not observe significant performance difference. We are not clear about the influence of those two to the performance. It can be further explored. We plot the histogram of last layer's feature map and also visualize the last layer's feature map output from both the ResNet-50 and Swin-T from our experiments based on using simCrossTrans ones. See <ref type="figure" target="#fig_4">Figure 6</ref> and 7. From <ref type="figure" target="#fig_4">Figure 6</ref>, we can see the clear difference of the last layer's feature map value distribution. For the Swin-T one, the distribution follow a bell shape due to the layer normalization <ref type="bibr" target="#b0">[1]</ref>. In <ref type="figure">Figure 7</ref>, we visualize the last layer backbone's feature map. From the visualization we can clear see that ResNet has a more sparse outputs due to the pooling layers and not using layer normalization. The Swin-T's last layer is more dense. Also, it is interesting to see that for the Swin-T, there is a clear feature map which is similar to the original input image. We thought this might be due to the residual connection in both the ResNet and transformer, however, we did not find similar ones in the ResNet outputs. <ref type="figure">Figure ??</ref> shows more details about 2 channel feature maps from ResNet and Swin-T. The performance of simCrossTrans on different categories In <ref type="figure" target="#fig_6">Figure 9</ref>, we show the performance improvement ratio of with simCrossTrans over without simCrossTrans. From the result, we can see that for easy categories such as bed, sofa chair and bathtub, with simCrossTrans helps. Since the signals of those easy categories are strong enough, the system without simCrossTrans also have a comparable performance. However, for the hard categories, such as laptop, keyboard and stool, with simCrossTrans, the performance is greatly improved. Which shows that the contextual knowledge learned from other modality data are <ref type="bibr">Figure 7</ref>. Combined feature maps of last layer backbone output. For ResNet, the last layer's original shape is <ref type="bibr" target="#b24">(25,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr">2048</ref>) and for Swin-T is <ref type="bibr" target="#b24">(25,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr">768)</ref>. We choose 64 by 32 grids for resnet and 32 by 24 grids for Swin-T and put each channel's feature map into one grid. helpful for detecting those hard objects with depth sensor only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and future directions</head><p>We introduce a simple but efficient cross sensor/modal transfer learning. Instead of using different encoder to extract the information from different modal, we unified the data representation between different modal to build an easy to implement approach, which is simCrossTrans. This simple approach works surprisingly well: by using simCrossTrans, without significantly modifying the model architecture, we can fine-tune the model based on learned weights from other modal and achieve much better performance than train the model from scratch, especially when combing with vision transformers.</p><p>We are excited about the good performance of our approach. This approach can greatly reduce the amount of data needed from expensive or not used by the general public sensors when training a model. Meanwhile, we are think about the future directions of extending our current approach: First, the easy implementation of our approach can help our model's performance improving together with the upgrading of the pre-training models. For example, can we try the larger models, such as Swin Transformer v2 <ref type="bibr" target="#b25">[26]</ref> which has up to 3 billion parameters and push the COCO's detection performance from 50%, to 63.1% update the result (unfortunately, the Swin Transformer v2 needs Nvidia A100-40G GPUs, which are not available in our lab yet)? Second, in this article, we fine tune a new dataset from the pre-trained model only based depth image. One possible feature direction can be fine tuning the pre-trained model on both RGB and depth image based on a same model to make it work for both RGB and pseudo image generated from point cloud. By doing this, changing the model architecture and updating the model weights are not needed when changing the input data modal. This kind of system can work in such scenario: a robotic can run this unified model by taking the RGB data at the day time and using the depth sensor data at the nighttime without any adjustment to the model. Third, the transfer learning used here is based on supervised pre-training. Pre-train a model by using the Self supervised approach such as <ref type="bibr" target="#b13">[14]</ref> based on one modal and then fine tune the model in another modal will be another interesting direction to investigate. Limited to our knowledge, we can not list all the possibilities and we hope our proposed cross sensor transfer learning approach and our finding of combining simCrossTrans with vision transformers will inspire more future works. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>2D detection results based on the SUN RGB-D validation data split. It has 2D detection based on 4 images as shown in the top left, top right, bottom left and bottom right blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>The updating the mAP based on SUNRGBD79 with fine tuning going on.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Train loss for based on different components. As the Mask-RCNN is a two stage detection network, it has the proposal stage and detection stage. For the proposal stage, the Region Proposal Network (RPN)'s classification loss and bounding box loss are shown in top. The detection stage, the classification and bounding box loss based on SUNRGBD79 categories are shown in bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Histogram of the last layer feature map for ResNet-50 and Swin-T. The top one is the histogram of normalize all the values to 0 and 1 without removing the extreme values. The bottom one is the histogram of normalize all the values based on the top one by using the values in range [0, 0.1] for ResNet and [0.57, 0.7] for Swin-T.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Feature maps of channel 36 and 111 from the backbone last layer output. Left two are from the ResNet and the rigth two are from the Swin-T.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Performance of simCrossTrans on different categories. The categories are sorted by the performance improvement ratio of with simCrossTrans vs without. For each one, the top dashed block categories' improvement ratio: [5, ?]; The bottom dashed block: [1, 1.5]. Between two blocks: [1, 1.5]; Below the bottom one: [0, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Table 1. 2D detection results based on SUN-RGBD validation set. All systems are evaluated based on Depth Images. Evaluation metric is average precision with 2D IoU threshold of 0.5.</figDesc><table><row><cell>Image Source</cell><cell>Methods</cell><cell>Backbone</cell><cell>bed toilet</cell><cell cols="7">night stand bathtub chair dresser sofa table desk bookshelf</cell><cell>sofa chair</cell><cell>kitchen counter</cell><cell>kitchen cabinet</cell><cell>garbage bin</cell><cell cols="2">microwave sink</cell><cell>SUNRGBD10 mAP50</cell><cell>SUNRGBD16 mAP50</cell></row><row><cell>RGB &amp;Depth</cell><cell>RGB-D RCNN [12]</cell><cell>VGG</cell><cell>76.0 69.8</cell><cell>37.1</cell><cell>49.6</cell><cell>41.2</cell><cell>31.3</cell><cell cols="2">42.2 43.0 16.6</cell><cell>34.9</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>46.8</cell><cell>N/A</cell><cell>41.9</cell><cell>44.2</cell><cell>N/A</cell></row><row><cell></cell><cell>2D-driven [19]</cell><cell>VGG-16</cell><cell>74.5 86.2</cell><cell>49.5</cell><cell>45.5</cell><cell>53.0</cell><cell>29.4</cell><cell cols="2">49.0 42.3 22.3</cell><cell>45.7</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>49.7</cell><cell>N/A</cell></row><row><cell>RGB</cell><cell>Frustum PointNets [32]</cell><cell cols="2">adjusted VGG from SSD 56.7 43.5</cell><cell>37.2</cell><cell>81.3</cell><cell>64.1</cell><cell>33.3</cell><cell cols="2">57.4 49.9 77.8</cell><cell>67.2</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>56.8</cell><cell>N/A</cell></row><row><cell></cell><cell>F-VoxNet [40]</cell><cell>ResNet 101</cell><cell>81.0 89.5</cell><cell>35.1</cell><cell>50.0</cell><cell>52.4</cell><cell>21.9</cell><cell cols="2">53.1 37.7 18.3</cell><cell>40.4</cell><cell>47.8</cell><cell>22.0</cell><cell>29.8</cell><cell>52.8</cell><cell>39.7</cell><cell>31.0</cell><cell>47.9</cell><cell>N/A</cell></row><row><cell></cell><cell>F-VoxNet [40]</cell><cell>ResNet 101</cell><cell>78.7 77.6</cell><cell>34.2</cell><cell>51.9</cell><cell>51.8</cell><cell>16.5</cell><cell cols="2">48.5 34.9 14.2</cell><cell>19.2</cell><cell>48.7</cell><cell>19.1</cell><cell>18.5</cell><cell>30.3</cell><cell>22.2</cell><cell>30.1</cell><cell>42.8</cell><cell>37.3</cell></row><row><cell></cell><cell>Ours w/o simCrossTrans</cell><cell>ResNet 50</cell><cell>72.2 51.7</cell><cell>22.0</cell><cell>44.4</cell><cell>49.5</cell><cell>9.6</cell><cell>33.6 33.5</cell><cell>9.7</cell><cell>12.2</cell><cell>41.8</cell><cell>15.7</cell><cell>25.3</cell><cell>22.8</cell><cell>12.9</cell><cell>19.5</cell><cell>33.8</cell><cell>29.8</cell></row><row><cell>Point Cloud only</cell><cell cols="2">Ours with simCrossTrans ResNet 50</cell><cell>82.1 78.8</cell><cell>43.5</cell><cell>49.9</cell><cell>60.0</cell><cell>14.8</cell><cell cols="2">50.3 41.1 13.0</cell><cell>31.6</cell><cell>58.9</cell><cell>26.7</cell><cell>41.4</cell><cell>25.6</cell><cell>30.6</cell><cell>39.3</cell><cell>46.5</cell><cell>43.0</cell></row><row><cell></cell><cell>Ours w/o simCrossTrans</cell><cell>Swin-T</cell><cell>76.7 58.4</cell><cell>23.1</cell><cell>49.7</cell><cell>57.2</cell><cell>12.1</cell><cell cols="2">40.5 41.5 13.5</cell><cell>22.8</cell><cell>54.7</cell><cell>19.7</cell><cell>37.8</cell><cell>29.2</cell><cell>23.7</cell><cell>25.2</cell><cell>39.6</cell><cell>36.6</cell></row><row><cell></cell><cell cols="2">Ours with simCrossTrans Swin-T</cell><cell>87.2 87.7</cell><cell>51.6</cell><cell>69.5</cell><cell>69.0</cell><cell>27.0</cell><cell cols="2">60.5 48.1 19.3</cell><cell>38.3</cell><cell>68.1</cell><cell>30.7</cell><cell>61.2</cell><cell>35.5</cell><cell>41.9</cell><cell>47.7</cell><cell>55.8</cell><cell>52.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Number of parameters and inference time comparison between Frustum VoxNet and ours based on ResNet 50 and Swin-T backbone networks with Mask R-CNN framework. All speed</figDesc><table><row><cell>Method</cell><cell>Backbone Network</cell><cell># Parameters (M)</cell><cell>GFLOPs</cell><cell>Inference Time (ms)</cell><cell>FPS</cell></row><row><cell>F-VoxNet [40]</cell><cell>ResNet-101</cell><cell>64</cell><cell>-</cell><cell>110</cell><cell>9.1</cell></row><row><cell>simCrossTrans(ours)</cell><cell>ResNet-50</cell><cell>44</cell><cell>472.1</cell><cell>70</cell><cell>14.3</cell></row><row><cell>simCrossTrans(ours)</cell><cell>Swin-T</cell><cell>48</cell><cell>476.5</cell><cell>105</cell><cell>9.5</cell></row></table><note>testing are based on a standard single NVIDIA Titan-X GPU.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Details of image/tensor shape updates, receptive fields and the number of channel/dimension for ResNet-50 backbone and Swin-T backbone based on our experiment on SUN RGB-D dataset. For the Swin-T backbone, dim means the dimension of the related block's output. The head means the number of heads for the multi-head attention.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For ResNet-50, as the dataset is repeated 3 times for each epoch, so in total it has 399 iterations for 100 epochs. For Swin-T, the dataset is not repeated, in total it has 133K iterations for 100 epochs.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We thank Zhujun Li to prepare the Titan GPU server used for this work. We also would like to thank Katharine Shen for proof reading, helpful comments and advice. We thank Izzy for showing up in our <ref type="figure">Figure 2</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Layer normalization. ArXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ilya Sutskever</title>
		<imprint/>
	</monogr>
	<note>and Dario Amodei. Language models are few-shot learners. CoRR, abs/2005.14165, 2020. 4</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the relationship between self-attention and convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics. 2, 4, 8</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html.5" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fast R-Cnn</surname></persName>
		</author>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation. CoRR, abs/1311.2524</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mohammed Zakariah, and Yousef Ajami Alotaibi. Convolution-based encoding of depth images for transfer learning in rgb-d scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhakrishnan</forename><surname>Gopalapillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepa</forename><surname>Gupta</surname></persName>
		</author>
		<idno>2021. 4</idno>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">Andr?s</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014 -13th European Conference</title>
		<editor>David J. Fleet, Tom?s Pajdla, Bernt Schiele, and Tinne Tuytelaars</editor>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8695</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, Part VII</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visualizing and understanding the effectiveness of bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaru</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>Masked autoencoders are scalable vision learners. CoRR, abs/2111.06377, 2021. 2, 4</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR. 9</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>Francis Bach and David Blei</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="7" to="09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modalitybridge transfer learning for medical image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeoreum</forename><surname>Hak Gu Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Man</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Congress on Image and Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">2d-driven 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Lahoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cross-organ, crossmodality transfer learning: feasibility study for segmentation and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="210194" to="210205" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno>abs/1708.02002</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>abs/1405.0312</idno>
		<title level="m">Microsoft COCO: common objects in context. CoRR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Covid-19 multi-targeted drug repurposing using few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoke</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="18" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Swin transformer V2: scaling up capacity and resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno>abs/2111.09883</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>A convnet for the 2020s. CoRR, abs/2201.03545, 2022. 9</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Pruning convolutional neural networks for resource efficient transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno>abs/1611.06440</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Intriguing properties of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanchana</forename><surname>Ranasinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cross-modality transfer learning for image-text information management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuteng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houbing</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Manage. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Kumar Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>abs/1506.02640</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">YOLO9000: better, faster, stronger. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>abs/1612.08242</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>Corinna Cortes, Neil D</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A survey of object classification and detection based on 2d/3d data. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoke</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Molgnn: Self-supervised motif learning graph neural network for drug discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoke</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Molecules Workshop at NeurIPS 2020</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Frustum voxnet for 3d object detection from rgb-d or depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoke</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Stamos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Xiaoke Shen and Ioannis Stamos. 3d object detection and instance segmentation from 3d range and 2d color images</title>
		<idno>2021. 5</idno>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Ffessd: An accurate and efficient single-shot detector for target detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengli</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dailun</forename><surname>Tan</surname></persName>
		</author>
		<idno>2019. 5</idno>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Deep sliding shapes for amodal 3d object detection in RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<idno>abs/1511.02300</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Crossmodal transfer learning for image and sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Artem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Soroka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trofimov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Computation, Machine Learning, and Cognitive Research V</title>
		<editor>Boris Kryzhanovsky, Witali Dunin-Barkowski, Vladimir Redko, Yury Tiumentsev, and Valentin V. Klimov</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2022" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Online algorithms for classification of urban objects in 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Stamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olympia</forename><surname>Hadjiliadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 Second International Conference on 3D Imaging, Modeling, Processing</title>
		<imprint>
			<publisher>Visualization Transmission</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="332" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Online algorithms for classification of urban objects in 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hadjiliadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 Second International Conference on 3D Imaging, Modeling, Processing, Visualization Transmission</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="332" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cnn-based object segmentation in urban lidar with missing points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zelener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stamos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="417" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Cnn-based object segmentation in urban lidar with missing points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zelener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stamos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
