<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Objects are Different: Flexible Monocular 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Zhang</surname></persName>
							<email>zhang-yp19@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
							<email>lujiwen@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<email>jzhou@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Objects are Different: Flexible Monocular 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The precise localization of 3D objects from a single image without depth information is a highly challenging problem. Most existing methods adopt the same approach for all objects regardless of their diverse distributions, leading to limited performance for truncated objects. In this paper, we propose a flexible framework for monocular 3D object detection which explicitly decouples the truncated objects and adaptively combines multiple approaches for object depth estimation. Specifically, we decouple the edge of the feature map for predicting long-tail truncated objects so that the optimization of normal objects is not influenced. Furthermore, we formulate the object depth estimation as an uncertainty-guided ensemble of directly regressed object depth and solved depths from different groups of keypoints. Experiments demonstrate that our method outperforms the state-of-the-art method by relatively 27% for the moderate level and 30% for the hard level in the test set of KITTI benchmark while maintaining real-time efficiency. Code will be available at https://github. com/zhangyp15/MonoFlex.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D object detection is an indispensable premise for machines to perceive the physical environment and has been widely used in autonomous driving and robot navigation. In this work, we focus on solving the problem with only information from monocular images. Most existing methods for 3D object detection require the LiDAR sensors <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b48">49]</ref> for precise depth measurements or stereo cameras <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45]</ref> for stereo depth estimation, which greatly increases the implementation costs of practical systems. Therefore, monocular 3D object detection has been a promising solution and received much attention in the community <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>For the challenging localization of 3D objects, most ex-* Corresponding author (a) M3D-RPN <ref type="bibr" target="#b2">[3]</ref> (b) D4LCN <ref type="bibr" target="#b12">[13]</ref> (c) Baseline (d) Ours <ref type="figure">Figure 1</ref>: Qualitative comparison among prior arts <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13]</ref>, our baseline, and the proposed method. The cyan and pink bounding boxes represent detected cars and pedestrians. Our approach can effectively detect the heavily truncated object highlighted by the red arrow.</p><p>isting methods handle different objects with a unified approach. For example, <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b51">52]</ref> utilize fully convolutional nets to predict objects of diverse distributions with shared kernels. However, we observe that the equal and joint processing of all objects can lead to unsatisfied performance: (1) As shown in <ref type="figure">Figure 1</ref>, the heavily truncated objects can be hardly detected by state-of-the-art methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13]</ref> but these objects are important to the safety of autonomous vehicles. <ref type="bibr" target="#b1">(2)</ref> We empirically found that these hard samples can increase the learning burden and affect the prediction of general objects. Thus, unified approaches can fail in both finding every object and predicting precise 3D locations. To this end, we propose a flexible detector that considers the difference among objects and estimates their 3D locations in an adaptive way. Since the estimation of an object's 3D location is usually decomposed into finding the projected 3D center and the object depth <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b51">52]</ref>, we also consider the flexibility from these two aspects.</p><p>To localize the projected 3D center, we divide objects according to whether their projected centers are "inside" or "outside" the image. Then we represent inside objects exactly as the projected centers and outside objects as delicately chosen edge points so that two groups of objects are handled by the inner and edge regions of the feature map respectively. Considering it is still difficult for convolutional filters to manage spatial-variant predictions, the edge fusion module is further proposed to decouple the feature learning and prediction of outside objects.</p><p>To estimate the object depth, we propose to combine different depth estimators with uncertainty estimation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. The estimators include direct regression <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b51">52]</ref> and geometric solutions from keypoints <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5]</ref>. We observe that computing depth from keypoints is usually an overdetermined problem, where simply averaging results from different keypoints <ref type="bibr" target="#b4">[5]</ref> can be sensitive to the truncation and occlusion of keypoints. As a result, we further split keypoints into M groups, each of which is exactly sufficient for solving the depth. To combine M keypoint-based estimators and the direct regression, we model their uncertainties and formulate the final estimation as an uncertaintyweighted average. The proposed combination allows the model to flexibly choose more suitable estimators for robust and accurate predictions.</p><p>Experimental results on KITTI <ref type="bibr" target="#b13">[14]</ref> dataset demonstrate that our method significantly outperforms all existing methods, especially for moderate and hard samples. The main contributions of this paper can be summarized in two aspects: (1) We reveal the importance to consider the difference among objects for monocular 3D object detection and propose to decouple the prediction of truncated objects;</p><p>(2) We propose a new formulation for object depth estimation, which utilizes uncertainties to flexibly combine independent estimators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Monocular 3D object Detection. Considering the difficulty in perceiving 3D environments from 2D images, most existing methods for monocular 3D object detection utilize extra information to simplify the task, which includes pretrained depth estimation modules <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>, annotated keypoints <ref type="bibr" target="#b1">[2]</ref> and CAD models <ref type="bibr" target="#b31">[32]</ref>. Mono3D <ref type="bibr" target="#b6">[7]</ref> first samples candidates based on the ground prior and scores them with semantic/instance segmentation, contextual information, object shape, and location prior. MonoPSR <ref type="bibr" target="#b20">[21]</ref> estimates the instance point cloud and enforces the alignment between the object appearance and the projected point cloud for proposal refinement. Pseudo-LiDAR <ref type="bibr" target="#b44">[45]</ref> lifts the monocular image into pseudo-LiDAR with estimated depth and then utilizes LiDAR-based detectors. AM3D <ref type="bibr" target="#b30">[31]</ref> proposes a multi-modal fusion module to enhance the pseudo-LiDAR with color information. PatchNet <ref type="bibr" target="#b29">[30]</ref> organizes pseudo-LiDAR into the image representation and utilizes powerful 2D CNN to boost the detection performance. Though these methods with extra information usually achieve better performance, they require more annotations for training and are usually less generalized.</p><p>Other purely monocular methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref> only utilize a single image for detection. Deep3DBox <ref type="bibr" target="#b33">[34]</ref> presents the MultiBin method for orientation estimation and uses geometric constraints of 2D bounding boxes to derive 3D bounding boxes. FQNet <ref type="bibr" target="#b26">[27]</ref> measures the fitting degree between projected 3D proposals and objects so that the best-fitted proposals are picked out. MonoGRNet <ref type="bibr" target="#b35">[36]</ref> directly predicts the depth of objects with sparse supervision and combines early features to refine the location estimation. M3D-RPN <ref type="bibr" target="#b2">[3]</ref> solves the problem with a 3D region proposal network and proposes the depth-aware convolutional layers to enhance extracted features. MonoPair <ref type="bibr" target="#b9">[10]</ref> considers the pair-wise relationships between neighboring objects, which are utilized as spatial constraints to optimize the results of detection. RTM3D <ref type="bibr" target="#b24">[25]</ref> predicts the projected vertexes of the 3D bounding box and solves 3D properties with nonlinear least squares optimization. Existing methods mostly neglect the difference among objects or only consider the general scale variance, which can suffer from predicting out-of-distribution objects and lead to downgraded performance. By contrast, our work explicitly decouples the heavily truncated objects with long-tail distribution for efficient learning and estimates the object depth by adaptively combining multiple depth estimators instead of utilizing one single method for all objects.</p><p>Uncertainty Estimation. Two major types of uncertainty are usually studied in Bayesian modeling <ref type="bibr" target="#b17">[18]</ref>. The epistemic uncertainty describes the uncertainty of the model parameters, while the aleatoric uncertainty can capture the noise of observations, whose applications in object detection have been explored in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15]</ref>. Gaussian YOLO <ref type="bibr" target="#b10">[11]</ref> models the uncertainty of predicted 2D boxes to rectify the detection scores. <ref type="bibr" target="#b14">[15]</ref> predicts the bounding box as a Gaussian distribution and formulates the regression loss as the KL divergence. MonoPair <ref type="bibr" target="#b9">[10]</ref> uses uncertainty to provide weights for the post-optimization between predicted 3D locations and pair-wise constraints. In this paper, we model the uncertainties of estimated depths from multiple estimators, which are used to quantify their contributions to the final combined prediction.</p><p>Ensemble Learning. Ensemble learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b38">39]</ref> generates multiple models strategically and combines their predictions for better performance. Traditional ensemble methods include bagging, boosting, stacking, gating network, and so on. <ref type="bibr" target="#b16">[17]</ref> uses a gating network to combine the mixture of experts for classification. <ref type="bibr" target="#b0">[1]</ref> proposes a tree-structured gate to hierarchically weight different experts for face alignment. Ensemble learning generally assumes the learners have identical structures but are trained with different samples or initializations, while our multiple depth estimators function in respectively different ways and are also supervised by substantially different loss functions. Therefore, we propose to formulate the combination as an uncertainty-guided average of all predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>Overview of our framework. First, the CNN backbone extracts feature maps from the monocular image as the input for multiple prediction heads. The image-level localization involves the heatmap and offsets, where the edge fusion modules are used to decouple the feature learning and prediction of truncated objects. The adaptive depth ensemble adopts four methods for depth estimation and simultaneously predicts their uncertainties, which are utilized to form an uncertaintyweighted prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Statement</head><p>The 3D detection of an object involves estimating its 3D location (x, y, z), dimension (h, w, l), and orientation ?. The dimension and orientation can be directly inferred from appearance-based clues, while the 3D location is converted to the projected 3D center x c = (u c , v c ) and the object depth z as shown in <ref type="figure" target="#fig_0">Figure 3</ref>(a) and <ref type="formula" target="#formula_0">(1)</ref>:</p><formula xml:id="formula_0">x = (u c ? c u )z f , y = (v c ? c v )z f<label>(1)</label></formula><p>where (c u , c v ) is the principle point and f is the focal length. To this end, the whole problem is decomposed into four independent subtasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Framework Overview</head><p>As shown in <ref type="figure">Figure 2</ref>, our framework is extended from CenterNet <ref type="bibr" target="#b51">[52]</ref>, where objects are identified by their representative points and predicted by peaks of the heatmap. Multiple prediction branches are deployed on the shared backbone to regress objects' properties, including the 2D bounding box, dimension, orientation, keypoints, and depth. The final depth estimation is an uncertaintyguided combination of the regressed depth and the computed depths from estimated keypoints and dimensions. We present the design of decoupled representative points for normal and truncated objects in Section 3.3 and then introduce the regression of visual properties in Section 3.4. Finally, the adaptive depth ensemble is detailed in Section 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Decoupled Representations of Objects</head><p>Existing methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b51">52]</ref> utilize a unified representation x r , the center of 2D bounding box x b , for every object. In such cases, the offset ? c = x c ? x b is regressed to derive the projected 3D center x c . We divide objects into two groups depending on whether their projected 3D centers are inside or outside the image and visualize the corresponding offsets ? c in <ref type="figure" target="#fig_0">Figure 3</ref>(b). Considering the sub-stantially different offsets of two groups, the joint learning of ? c can suffer from long-tail offsets and we therefore propose to decouple the representations and the offset learning of inside and outside objects. Inside Objects. For objects whose projected 3D centers are inside the image, they are directly identified by x c to avoid regressing the irregular ? c like <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25]</ref>. Though we still need to regress the discretization error ? in due to the downsampling ratio S of the backbone CNN as in <ref type="formula" target="#formula_1">(2)</ref>, it is much smaller than ? c and easier to regress.</p><formula xml:id="formula_1">? in = x c S ? x c S<label>(2)</label></formula><p>We follow <ref type="bibr" target="#b51">[52]</ref> to generate the ground-truth heatmap for inside objects with circular Gaussian kernels centered at x c . Outside Objects. To decouple the representation of outside objects, we propose to identify them by the intersection x I between the image edge and the line from x b to x c , as shown in <ref type="figure" target="#fig_1">Figure 4</ref>(a). It can be seen that the proposed intersection x I is more physically meaningful than simply clamping x b or x c to the boundary. The prediction of x I is achieved by the edge heatmap as shown in <ref type="figure" target="#fig_1">Figure 4</ref>(b), which is generated from a one-dimensional Gaussian kernel. We also compare our x I and the commonly used x b in <ref type="figure" target="#fig_1">Figure 4</ref>(c). Since 2D bounding boxes only capture the inside-image part of objects, the visual locations of x b can be confusing and even on other objects. By contrast, the intersection x I disentangles the edge area of the heatmap to focus on outside objects and offers a strong boundary prior to simplify the localization. Also, we regress the offsets from x I to the target x c as in <ref type="formula" target="#formula_2">(3)</ref>:</p><formula xml:id="formula_2">? out = x c S ? x I S<label>(3)</label></formula><p>Edge Fusion. Though the representations of inside and outside objects are decoupled in the interior and marginal regions of the output feature, it is still difficult for shared convolutional kernels to handle spatial-variant predictions. Thus, we propose an edge fusion module to further decouple the feature learning and prediction of outside objects. As shown in the right part of <ref type="figure">Figure 2</ref>, the module first extracts four boundaries of the feature map and concatenates them into an edge feature vector in clockwise order, which is then processed by two 1D convolutional layers to learn unique features for truncated objects. Finally, the processed vector is remapped to the four boundaries and added to the input feature map. When applied to the heatmap prediction, the edge features can specialize in predicting the edge heatmap for outside objects so that the localization of inside objects is not confused. For regressing the offsets, the significant scale difference between ? in and ? out as shown in <ref type="figure" target="#fig_0">Figure 3</ref>(b) can be resolved with the edge fusion module. Loss Functions. The penalty-reduced focal loss <ref type="bibr" target="#b25">[26]</ref> is utilized for heatmap prediction as in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>. We adopt L1 loss for regressing ? in and log-scale L1 loss for ? out because it is more robust to extreme outliers. The offset loss is computed as <ref type="formula" target="#formula_3">(4)</ref>:</p><formula xml:id="formula_3">L of f = |? in ? ? * in | if inside log (1 + |? out ? ? * out |) otherwise<label>(4)</label></formula><p>where ? in and ? out refer to predictions and ? * in and ? * out are ground-truth. Note that L of f is averaged separately for inside and outside objects due to the different formulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Visual Properties Regression</head><p>We elaborate on the regression of visual properties including the 2D bounding boxes, dimensions, orientations, and keypoints of objects in this section. 2D Detection. Since we do not represent objects as their 2D centers, we follow FCOS <ref type="bibr" target="#b43">[44]</ref> to regress the distances from the representative point x r = (u r , v r ), which refers to x b for inside and x I for outside objects, to four sides of the 2D bounding box. If we denote the left-top corner as (u 1 , v 1 ) and the right-bottom corner as (u 2 , v 2 ), the regression target is then:</p><formula xml:id="formula_4">l * = u r ? u 1 , r * = u 2 ? u r , t * = v r ? v 1 , b * = v 2 ? v r .<label>(5)</label></formula><p>GIoU loss <ref type="bibr" target="#b37">[38]</ref> is adopted for 2D detection since it is robust to scale changes. Dimension Estimation. Considering the small variance of object sizes within each category, we regress the relative changes with respect to the statistical average instead of absolute values. For each class c, the average dimension of the training set is denoted as (h c , w c , l c ). Assume the regressed log-scale offsets of dimensions are (? h , ? w , ? l ) and  the ground-truth dimensions are (h * , w * , l * ), the L1 loss for dimension regression is defined as:</p><formula xml:id="formula_5">L dim = k?{h,w,l} k c e ? k ? k *<label>(6)</label></formula><p>Orientation Estimation. The orientation can be represented as either the global orientation in the camera coordinate system or the local orientation relative to the viewing direction. For an object located at (x, y, z), its global orientation r y and local orientation ? satisfy <ref type="formula" target="#formula_6">(7)</ref>:</p><formula xml:id="formula_6">r y = ? + arctan(x/z)<label>(7)</label></formula><p>As shown in <ref type="figure" target="#fig_2">Figure 5</ref>, objects with the same global orientations but different viewing angles will have different local orientations and visual appearances. Thus, we choose to estimate the local orientation with MultiBin loss <ref type="bibr" target="#b5">[6]</ref>, which divides the orientation range into N o overlapping bins so that the network can determine which bin an object lies inside and estimate the residual rotation w.r.t the bin center. Keypoint Estimation. As shown in <ref type="figure" target="#fig_3">Figure 6</ref>, we define N k = 10 keypoints for each object which include the projections of eight vertexes {k i , i = 1...8}, bottom center k 9 and top center k 10 of the 3D bounding box. We regress the local offsets {? ki = k i ? x r , i = 1...N k } from x r to N k keypoints with L1 loss:</p><formula xml:id="formula_7">L key = N k i=1 I in (k i ) |? ki ? ? * ki | N k i=1 I in (k i )<label>(8)</label></formula><p>where ? * ki is the ground-truth and I in (k i ) indicates whether the keypoint k i is inside the image. <ref type="figure">Figure 7</ref>: The depth of a supporting line of the 3D bounding box can be computed with the object height and the line's pixel height. We split ten keypoints into three groups, each of which can produce the center depth independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Adaptive Depth Ensemble.</head><p>We formulate the estimation of object depth as an adaptive ensemble of M + 1 independent estimators, including direct regression and M geometric solutions from keypoints. We first introduce these depth estimators and then present how we combine them with uncertainties. Direct Regression. To directly regress the object depth, we follow <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b51">52]</ref> to transform the unlimited network output z o into the absolute depth z r with the inverse sigmoid transformation:</p><formula xml:id="formula_8">z r = 1 ?(z o ) ? 1, ?(x) = 1 1 + e ?x<label>(9)</label></formula><p>To jointly model the uncertainty of the regressed depth, we follow <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> to utilize a modified L1 loss for depth regression:</p><formula xml:id="formula_9">L dep = |z r ? z * | ? dep + log(? dep )<label>(10)</label></formula><p>where ? dep is the uncertainty of the regressed depth. When the model lacks confidence in its prediction, it will output a larger ? dep so that L dep can be reduced. The term log(? dep ) can avoid trivial solutions and encourage the model to be optimistic about accurate predictions. Depth From Keypoints. With known camera matrices, we can utilize the relative proportion between pixel height and estimated object height to compute the object depth, which is similar to <ref type="bibr" target="#b4">[5]</ref>. From our baseline model, the relative errors of predicted dimensions are 5.2%, 6.1%, and 11.8% for height, width, and length. Therefore, solving depth from height is not only independent of orientation estimation but suffers less from the error of dimension estimation. As shown in <ref type="figure">Figure 7</ref>, the estimated ten keypoints constitute five vertical supporting lines of the 3D bounding box. The depth z l of each vertical line can be computed from its pixel height h l and the object height H as <ref type="formula" target="#formula_0">(11)</ref>:</p><formula xml:id="formula_10">z l = f ? H h l<label>(11)</label></formula><p>where f is the camera's focal length. The depth of the center vertical line z c is exactly the object depth while averaging the depths of two diagonal vertical edges, namely z 1 and z 3 or z 2 and z 4 , can also get the object depth. Therefore, the estimated ten keypoints are divided into three groups and generate respectively independent depths denoted as the center depth z c , the diag 1 depth z d1 and the diag 2 depth z d2 . To further supervise the computed depths from keypoints and also model their uncertainties, we adopt the L1 loss with uncertainty as follows:</p><formula xml:id="formula_11">L kd = k?{c,d1,d2} |z k ? z * | ? k + I in (z k ) log(? k )<label>(12)</label></formula><p>where z * is ground-truth and I in (z k ) indicates whether all keypoints used for computing z k are inside the image. Removing the log uncertainty for "invalid" depths computed from invisible keypoints allows the model to be fully pessimistic so that these depths are down-weighted in the ensemble. Note that we also restrict the gradients from these invalid depths to only update the uncertainty. Uncertainty Guided Ensemble. Now that we have M + 1 predicted depths {z i , i = 1...M + 1} and their uncertainties {? i , i = 1...M + 1} from M + 1 independent estimators, we propose to compute the uncertainty-weighted average, namely soft ensemble, as expressed in <ref type="formula" target="#formula_0">(13)</ref>:</p><formula xml:id="formula_12">z sof t = M +1 i=1 z i ? i / M +1 i=1 1 ? i<label>(13)</label></formula><p>The soft ensemble can assign more weights to those more confident estimators while being robust to potential inaccurate uncertainties. We also consider the hard ensemble where the estimator with minimal uncertainty is chosen as the final depth estimation. The performances of two ensemble ways are compared in Section 4.5. Integral Corner Loss. As discussed in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b41">42]</ref>, the separate optimization of multiple subtasks cannot ensure the optimal cooperation among different components. Therefore, we also supervise the coordinates of eight corners {v i = (x i , y i , z i ), i = 1, ..., 8} from the predicted 3D bounding box, which is formed by the estimated dimension, orientation, offset, and soft depth z sof t , with L1 loss:</p><formula xml:id="formula_13">L corner = 8 i=1 |v i ? v * i |<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>The proposed method is evaluated on KITTI 3D Object Detection benchmark <ref type="bibr" target="#b13">[14]</ref>, which includes 7481 images for training and 7518 images for testing. We follow <ref type="bibr" target="#b8">[9]</ref> to split the training images into train (3712) and val (3769) sets.</p><p>Detection results are evaluated on three levels of difficulty: easy, moderate, and hard, which are defined by the bounding box height, occlusion, and truncation. All our reported results are produced by models that jointly detect multiclasses, including Car, Pedestrian, and Cyclist. Note that results for KITTI Bird's Eye View benchmark will be provided in the supplementary material for reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We adopt the same modified DLA-34 <ref type="bibr" target="#b50">[51]</ref> as our backbone network following <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b51">52]</ref>. All input images are padded to the same size of 384 ? 1280. Every prediction head attached to the backbone consists of one 3 ? 3 ? 256 conv layer, BatchNorm <ref type="bibr" target="#b15">[16]</ref>, ReLU, and another 1 ? 1 ? c o conv layer, where c o is the output size. The edge fusion module has similar settings except using 1D conv layer and empirically removing the ReLU activation. For MultiBin loss <ref type="bibr" target="#b33">[34]</ref>, we use four bins centered at [0, ? 2 , ?, ? ? 2 ]. The model is trained using AdamW <ref type="bibr" target="#b28">[29]</ref> optimizer with the initial learning rate as 3e-4 and weight decay as 1e-5. We train the model for 34k iterations with a batchsize of 7 on a single RTX 2080Ti GPU and the learning rate is divided by 10 at 22k and 30k iterations. The random horizontal flip is adopted as the only data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>The detection is evaluated by the average precision of 3D bounding boxes AP 3D . For the val set, we report both AP 3D | R11 and AP 3D | R40 for a comprehensive comparison with previous studies. For the test set, the AP 3D | R40 results from the test server are reported. The IoU thresholds for AP 3D are 0.7 for Car and 0.5 for Pedestrian and Cyclist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative Results</head><p>In <ref type="table" target="#tab_1">Table 1</ref>, we conduct a comprehensive comparison between our proposed method and existing arts on the val and test sets of KITTI benchmark for Car. Without bells and whistles, our method outperforms all prior methods that only take monocular images as input. For AP 3D | R40 on the val set, our method is 45%, 42% and 42% higher than the second-best method MonoPair <ref type="bibr" target="#b9">[10]</ref> on three levels of difficulty. For the test set, our proposed method surpasses all existing methods, including those with extra information. The significant improvement on hard samples demonstrates that our method can effectively detect those heavily truncated objects, which are crucial for practical applications. We further show the results of Pedestrian and Cyclist on the test set in <ref type="table" target="#tab_2">Table 2</ref>. Our method outperforms M3D-RPN <ref type="bibr" target="#b2">[3]</ref> and Movi3D <ref type="bibr" target="#b42">[43]</ref> while achieving comparable performance with MonoPair <ref type="bibr" target="#b9">[10]</ref>. Finally, our method is also much faster than most existing methods, allowing for real-time inference. To sum up, our proposed framework achieves a stateof-the-art trade-off between performance and latency.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Decoupled Representations</head><p>In <ref type="table">Table 3</ref>, we compare various representations for inside and outside objects and validate the improvement from separate offset losses, namely the decoupled loss, and the edge fusion module. The second row which represents all objects with x b is considered as the baseline. All models directly regress the object depth without ensemble.</p><p>We observe that: (1) Simply discarding outside objects can improve the performance compared to the baseline, demonstrating the necessity of decoupling outside objects.</p><p>(2) Identifying inside objects as their projected 3D centers x c is better than the 2D centers x b , possibly because the offsets from x b to x c are irregular and hard to learn. (3) The decoupled optimization of inside and outside offsets and the edge fusion module are crucial for the remarkable improvement on moderate and hard samples, where the heavily truncated objects belong. (4) Compared with x cc and x cb derived by clamping x c and x b to the image edge as shown in <ref type="figure" target="#fig_1">Figure 4(a)</ref>, the proposed intersection x I is a more effective representation for outside objects.  <ref type="table">Table 3</ref>: Ablation study on decoupled representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Object Depth Estimation</head><p>We compare different methods for object depth estimation in <ref type="table" target="#tab_5">Table 4</ref>. "Direct Regression" refers to the best model in <ref type="table">Table 3</ref> with our decoupled representations but without estimating keypoints. "Keypoints" replaces the depth branch with keypoint prediction and solves the object depth from geometry as in Section 3.5. The regressed depth performs slightly better than the keypoints-based solutions. The import of uncertainties significantly improves both methods because it allows the model to neglect difficult outliers and focus on most moderate objects. By contrast, our adaptive depth ensemble method simultaneously performs both predictions and further combines them with the uncertainty-guided weights, outperforming all individual methods by an obvious margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Depth Combination</head><p>To further understand the effectiveness of the proposed depth ensemble, we compare the performance of each es- <ref type="figure">Figure 8</ref>: Qualitative Results. We visualize the results of 3D object detection on KITTI val set, where predicted cars, pedestrians, and cyclists are represented in cyan, light pink, and red boxes. We use red ovals to emphasize those heavily truncated objects.  timator and the combined depth from the ensemble model in <ref type="table" target="#tab_7">Table 5</ref>. It can be observed that the joint learning consistently improves the performance of each depth estimator compared with results in <ref type="table" target="#tab_5">Table 4</ref>, which can owe to enhanced feature learning. The combined depth from soft ensemble outperforms every individual estimator, especially for the moderate level of Car and all levels of Pedestrian. The hard ensemble is inferior, possibly due to its sensitivity to the mismatch between the actual depth error and the estimated uncertainty. We also provide the performance from the oracle depth which means the most accurate estimator is always chosen for each object by an oracle. It can be considered as the ideal upper bound of our depth ensemble. To match a predicted object with a ground-truth object, we require their 2D IoU to be larger than 0.5. We notice that our soft ensemble is very close to the oracle performance on Pedestrian, demonstrating the effectiveness of our proposed combination method. On the other hand, the oracle performance for Car reveals the enormous potential of combining different depth estimators, which can be left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Qualitative Results</head><p>From the qualitative results shown in <ref type="figure">Figure 8</ref>, our proposed framework can produce superior performance for ordinary objects in various street scenes. As highlighted by the red ovals, we can also successfully detect some extremely truncated objects which are crucial for the safety  of autonomous driving, demonstrating the effectiveness of decoupling truncated objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed a novel framework for monocular 3D object detection which flexibly handles different objects. We observe the long-tail distribution of truncated objects and explicitly decouple them with the proposed edge heatmap and edge fusion module. We also formulate the object depth estimation as an uncertainty-guided ensemble of multiple approaches, leading to more robust and accurate predictions. Experiments on KITTI benchmark show that our method significantly outperforms all existing competitors. Our work sheds light on the importance of flexibly processing different objects, especially for the challenging monocular 3D object detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>(a) The 3D location is converted to the projected center and the object depth. (b) The distribution of the offsets ? c from 2D centers to projected 3D centers. Inside and outside objects exhibit entirely different distributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Comparison between x I and x b Representations of outside objects. (a) The intersection x I between the image edge and the line from x b to x c is used to represent the truncated object. (b) The edge heatmap is generated with 1D Gaussian distribution whose kernel size is proportional to the size of the 2D bounding box. (c) The always-on-edge intersection x I (cyan) is a better representation than the 2D center x b (green) for heavily truncated objects. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>r y , ?, and ? are the global orientation, local orientation, and the viewing angle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Keypoints include the projections of eight vertexes, top center and bottom center of the 3D bounding box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>21.09 17.26 28.31 15.76 12.24 16.50 10.74 9.52 PatchNet[30] depth -35.10 22.00 19.60 31.60 16.80 13.80 15.68 11.12 10.17</figDesc><table><row><cell>Methods</cell><cell>Extra</cell><cell>Time (ms)</cell><cell cols="3">Val, AP3D|R 11 Easy Mod Hard</cell><cell cols="3">Val, AP3D|R 40 Easy Mod Hard</cell><cell cols="3">Test, AP3D|R 40 Easy Mod Hard</cell></row><row><cell>MonoPSR[20]</cell><cell>depth, LiDAR</cell><cell>120</cell><cell cols="2">12.75 11.48</cell><cell>8.59</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>10.76</cell><cell>7.25</cell><cell>5.85</cell></row><row><cell>UR3D[48]</cell><cell>depth</cell><cell>120</cell><cell cols="7">28.05 18.76 16.55 23.24 13.35 10.15 15.58</cell><cell>8.61</cell><cell>6.00</cell></row><row><cell cols="6">AM3D[31] 32.23 DA-3Ddet[50] depth -depth, LiDAR -33.40 24.00 19.90</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">16.80 11.50</cell><cell>8.90</cell></row><row><cell>D4LCN[13]</cell><cell>depth</cell><cell>-</cell><cell cols="8">26.97 21.71 18.22 22.32 16.20 12.30 16.65 11.72</cell><cell>9.51</cell></row><row><cell>Kinem3D[4]</cell><cell>multi-frames</cell><cell>120</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="5">19.76 14.10 10.47 19.07 12.72</cell><cell>9.17</cell></row><row><cell>FQNet[27]</cell><cell>-</cell><cell>-</cell><cell>5.98</cell><cell>5.50</cell><cell>4.75</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2.77</cell><cell>1.51</cell><cell>1.01</cell></row><row><cell>MonoGRNet[36]</cell><cell>-</cell><cell>60</cell><cell cols="2">13.88 10.19</cell><cell>7.62</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>9.61</cell><cell>5.74</cell><cell>4.25</cell></row><row><cell>MonoDIS[42]</cell><cell>-</cell><cell>100</cell><cell cols="3">18.05 14.98 13.42</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>10.37</cell><cell>7.94</cell><cell>6.40</cell></row><row><cell>M3D-RPN[3]</cell><cell>-</cell><cell>160</cell><cell cols="5">20.27 17.06 15.21 14.53 11.07</cell><cell>8.65</cell><cell>14.76</cell><cell>9.71</cell><cell>7.42</cell></row><row><cell>SMOKE[28]</cell><cell>-</cell><cell>30</cell><cell cols="3">14.76 12.85 11.50</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>14.03</cell><cell>9.76</cell><cell>7.84</cell></row><row><cell>MonoPair[10]</cell><cell>-</cell><cell>57</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">16.28 12.30 10.42 13.04</cell><cell>9.99</cell><cell>8.65</cell></row><row><cell>RTM3D[25]</cell><cell>-</cell><cell>55</cell><cell cols="3">20.77 16.86 16.63</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">14.41 10.34</cell><cell>8.77</cell></row><row><cell>Movi3D[43]</cell><cell>-</cell><cell>45</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">14.28 11.13</cell><cell>9.68</cell><cell cols="2">15.19 10.90</cell><cell>9.26</cell></row><row><cell>Ours</cell><cell>-</cell><cell>35</cell><cell cols="9">28.17 21.92 19.07 23.64 17.51 14.83 19.94 13.89 12.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Quantitative results for Car on KITTI val/test sets, evaluated by AP 3D . "Extra" lists the required extra information for each method. We divide existing methods into two groups considering whether they utilize extra information and sort them according to their performance on the moderate level of the test set within each group.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Test, AP 3D | R 40</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell></cell><cell>Pedestrian</cell><cell></cell><cell></cell><cell>Cyclist</cell><cell></cell></row><row><cell></cell><cell>Easy</cell><cell>Mod</cell><cell cols="3">Hard Easy Mod</cell><cell>Hard</cell></row><row><cell>M3D-RPN[31]</cell><cell>4.92</cell><cell>3.48</cell><cell>2.94</cell><cell>0.94</cell><cell>0.65</cell><cell>0.47</cell></row><row><cell>Movi3D[43]</cell><cell>8.99</cell><cell>5.44</cell><cell>4.57</cell><cell>1.08</cell><cell>0.63</cell><cell>0.70</cell></row><row><cell>MonoPair[10]</cell><cell>10.02</cell><cell>6.68</cell><cell>5.53</cell><cell>3.79</cell><cell>2.12</cell><cell>1.83</cell></row><row><cell>Ours</cell><cell>9.43</cell><cell>6.31</cell><cell>5.26</cell><cell>4.17</cell><cell>2.35</cell><cell>2.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Quantitative results for Pedestrian and Cyclist on KITTI test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on object depth estimation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Quantitative analysis for the adaptive ensemble of depth estimators.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported in part by the National Natural Science Foundation of China under Grant U1713214, Grant U1813218, Grant 61822603, in part by Beijing Academy of Artificial Intelligence (BAAI), and in part by a grant from the Institute for Guo Qiang, Tsinghua University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Treegated deep mixture-of-experts for pose-robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Est?phe</forename><surname>Arnaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Dapogny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K?vin</forename><surname>Bailly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>T-BIOM</publisher>
			<biblScope unit="page" from="122" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Monocular 3d object detection via geometric reasoning on keypoints. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Barabanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Artemov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vyacheslav</forename><surname>Murashkin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">M3d-rpn: Monocular 3d region proposal network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="9287" to="9296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Kinematic 3d object detection in monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="135" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection with decoupled structured polygon estimation and heightguided depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaonary</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?line</forename><surname>Teuliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Chateau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2040" to="2049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2147" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Monopair: Monocular 3d object detection using pairwise spatial relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="12093" to="12102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gaussian yolov3: An accurate and fast object detector using localization uncertainty for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoong</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayoung</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyuk-Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Ensemble learning. The handbook of brain theory and neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dietterich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="110" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning depth-guided convolutions for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bounding box regression with uncertainty for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2888" to="2897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptive mixtures of local experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural computation</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="79" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection leveraging accurate proposals and shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection leveraging accurate proposals and shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11867" to="11876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12697" to="12705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Why m heads are better than one: Training a diverse ensemble of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<idno>abs/1511.06314</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stereo r-cnn based 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7644" to="7652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rtm3d: Real-time monocular 3d detection from object keypoints for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaici</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feidao</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="644" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollar. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep fitting degree scoring network for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1057" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Smoke: Singlestage monocular 3d object detection via keypoint estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>T?th</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="996" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking pseudo-lidar representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Accurate monocular 3d object detection via color-embedded 3d reconstruction for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengbo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="6851" to="6860" />
		</imprint>
	</monogr>
	<note>Wanli Ouyang, and Xin Fan</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Roi-10d: Monocular lifting of 2d detection to 6d pose and metric shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2069" to="2078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="7074" to="7082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Monogrnet: A geometric reasoning network for 3d object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengyi</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="8851" to="8858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Triangulation learning network: from monocular to stereo 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengyi</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7615" to="7623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Ensemble-based classifiers. Artificial intelligence review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Rokach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pv-rcnn: Pointvoxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10529" to="10538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Disentangling monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>L?pez-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Towards generalization across depth for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8445" to="8453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection with pseudo-lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2345" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Distancenormalized unified representation for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun Kim Xuepeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="91" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection via feature domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="17" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Objects as points. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
