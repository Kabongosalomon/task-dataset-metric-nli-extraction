<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Poisson Flow Generative Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Xu</surname></persName>
							<email>ylxu@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Liu</surname></persName>
							<email>zmliu@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Tegmark</surname></persName>
							<email>tegmark@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Poisson Flow Generative Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new "Poisson flow" generative model (PFGM) that maps a uniform distribution on a high-dimensional hemisphere into any data distribution. We interpret the data points as electrical charges on the z = 0 hyperplane in a space augmented with an additional dimension z, generating a high-dimensional electric field (the gradient of the solution to Poisson equation). We prove that if these charges flow upward along electric field lines, their initial distribution in the z = 0 plane transforms into a distribution on the hemisphere of radius r that becomes uniform in the r ? ? limit. To learn the bijective transformation, we estimate the normalized field in the augmented space. For sampling, we devise a backward ODE that is anchored by the physically meaningful additional dimension: the samples hit the (unaugmented) data manifold when the z reaches zero. Experimentally, PFGM achieves current state-of-the-art performance among the normalizing flow models on CIFAR-10, with an Inception score of 9.68 and a FID score of 2.35. It also performs on par with the state-of-the-art SDE approaches while offering 10? to 20? acceleration on image generation tasks. Additionally, PFGM appears more tolerant of estimation errors on a weaker network architecture and robust to the step size in the Euler method. The code is available at https: //github.com/Newbeeer/poisson_flow.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep generative models are a prominent approach for data generation, and have been used to produce high quality samples in image <ref type="bibr" target="#b0">[1]</ref>, text <ref type="bibr" target="#b1">[2]</ref> and audio <ref type="bibr" target="#b36">[35]</ref>, as well as improve semi-supervised learning <ref type="bibr" target="#b20">[20]</ref>, domain generalization <ref type="bibr" target="#b25">[25]</ref> and imitation learning <ref type="bibr" target="#b15">[15]</ref>. However, current deep generative models also have limitations, such as unstable training objectives (GANs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b17">17]</ref>) and low sample quality (VAEs <ref type="bibr" target="#b21">[21]</ref>, normalizing flows <ref type="bibr" target="#b6">[6]</ref>). New techniques <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b24">24]</ref> are introduced to stablize the training of CNN-based or ViT-based GAN models. Although recent advances on diffusion <ref type="bibr" target="#b16">[16]</ref> and scored-based models <ref type="bibr" target="#b34">[33]</ref> achieve comparable sample quality to GAN's without adversarial training, these models have a slow stochastic sampling process. <ref type="bibr" target="#b34">[33]</ref> proposes backward ODE samplers (normalizing flow) that speed up the sampling process but these methods have not yet performed on par with the SDE counterparts.</p><p>We present a new "Poisson flow" generative model (PFGM), exploiting a remarkable physics fact that generalizes to N dimensions. As illustrated in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>, motion in a viscous fluid transforms any planar charge distribution into a uniform angular distribution. Specifically, we interpret Ndimensional data points x (images, say) as positive electric charges in the z = 0 plane of an N + 1-dimensional space (see <ref type="figure" target="#fig_0">Fig. 1(a)</ref>) filled with a viscous liquid (say honey). A positive charge with z &gt; 0 will be repelled by the other charges and move in the direction of their repulsive force, eventually crossing an imaginary hemisphere of radius r. We show that, remarkably, if the the original charge distribution is let loose just above z = 0, this law of motion will cause a uniform distribution for their hemisphere crossings in the r ? ? limit. Our Poisson flow generative process reverses the forward process: we generate a uniform distribution of negative charges on the hemisphere, then track their motion back to the z = 0 plane, where they will be distributed as the data distribution. A Poisson flow can be viewed as a type of continuous normalizing flows <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b34">33]</ref> in the sense that it continuously maps between an arbitrary distribution and an easily sampled one: in the previous works an N -dimensional Gaussian and in PFGM a uniform distribution on an N -dimensional hemisphere. In practice, we implement the Poisson flow by solving a pair of forward/backward ordinary differential equations (ODEs) induced by the electric field ( <ref type="figure" target="#fig_0">Fig. 1(b)</ref>) given by the N -dimensional version of Coulomb's law (the gradient of the solution to the Poisson's equation with the data as sources). We will interchangeably refer to this gradient as the Poisson field, since electric fields normally refer to the special case N = 3.</p><p>The proposed generative model PFGM has a stable training objective and empirically outperforms previously state-of-the-art continuous flow methods <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b34">33]</ref>. As a different iterative method, PFGM offers two advantages compared to score-based methods <ref type="bibr" target="#b33">[32,</ref><ref type="bibr" target="#b34">33]</ref>. First, the ODE process of PFGM achieves faster sampling speeds than the SDE samplers in <ref type="bibr" target="#b34">[33]</ref>. while retaining comparable performance. Second, our backward ODE exhibits better generation performance than the reverse-time ODEs of VE/VP/sub-VP SDEs <ref type="bibr" target="#b34">[33]</ref>, as well as greater stability on a weaker architecture NSCNv2 <ref type="bibr" target="#b33">[32]</ref>. The rationale for robustness is that the time variables in these ODE baselines are strongly correlated with the sample norms during training time, resulting in a less error-tolerant inference. In contrast, the tie between the anchored variable and the sample norm in PFGM is much weaker.</p><p>Experimentally, we show that PFGM achieves current state-of-the-art performance on CIFAR-10 dataset in the normalizing flow family, with FID/Inception scores of 2.48 9.65 (w/ DDPM++ <ref type="bibr" target="#b34">[33]</ref>) and 2. <ref type="bibr">35 9</ref>.68 (w/ DDPM++ deep <ref type="bibr" target="#b34">[33]</ref>). It performs competitively with current state-of-the-art SDE samplers <ref type="bibr" target="#b34">[33]</ref> and provides 10? to 20? speed up across datasets. Notably, the backward ODE in PFGM is the only ODE-based sampler that can produce decent samples on its own on NCSNv2 <ref type="bibr" target="#b33">[32]</ref>, while other ODE baselines fail without corrections. In addition, PFGM demonstrates the robustness to the step size in the Euler method, with a varying number of function evaluations (NFE) ranging from 10 to 100. We further showcase the utility of the invertible forward/backward ODEs of the Poisson field on likelihood evaluation and image manipulations, and its scalability to higher resolution images on LSUN bedroom 256 ? 256 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related works</head><p>Poisson equation Let x ? R N and ?(x) ? R N ? R be a source function. We assume that the source function has a compact support, ? ? C 0 and N ? 3. The Poisson equation is</p><formula xml:id="formula_0">? 2 ?(x) = ??(x),<label>(1)</label></formula><p>where ?(x) ? R N ? R is called the potential function, and ? 2 ? ? N i=1</p><formula xml:id="formula_1">? 2 ?x 2 i</formula><p>is the Laplacian operator. It is usually helpful to define the gradient field E(x) = ???(x) and rewrite the Poisson equation as ? ? E = ?, known in physics as Gauss's law <ref type="bibr" target="#b11">[11]</ref>. The Poisson equation is widely used in physics, giving rise to Newton's gravitational theory <ref type="bibr" target="#b9">[9]</ref> and the electrostatic theory <ref type="bibr" target="#b11">[11]</ref>, when ?(x) is interpreted as mass density or electric charge density, respectively. E is the N -dimensional analog of the electric field. The Poisson equation Eq. (1) (with zero boundary condition at infinity) admits a unique simple integral solution <ref type="bibr" target="#b1">2</ref> :</p><formula xml:id="formula_2">?(x) = G(x, y)?(y)dy, G(x, y) = 1 (N ? 2)S N ?1 (1) 1 x ? y N ?2 ,<label>(2)</label></formula><p>where S N ?1 (1) is a geometric constant representing the surface area of the unit (N ? 1)-sphere <ref type="bibr" target="#b3">3</ref> , and G(x, y) is the extension of Green's function in N -dimensional space (details in Appendix A.3).</p><p>The negative gradient field of ?(x), referred as Poisson field of the source ?, is</p><formula xml:id="formula_3">E(x) = ???(x) = ? ? x G(x, y)?(y)dy, ? x G(x, y) = ? 1 S N ?1 (1) x ? y x ? y N .<label>(3)</label></formula><p>Qualitatively, the Poisson field E(x) points away from sources, or equivalently ?E(x) points towards sources, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. It is straightforward to check that when ?(x) ? ?(x ? y), we get ?(x) ? G(x, y) and E(x) ? ?? x G(x, y). This implies that G(x, y) and ?? x G(x, y) can be interpreted as the potential function and the gradient field generated by a unit point source, e.g., a point charge, located at y. When ?(x) takes general forms but has bounded support, simple asymptotics exist for x ? y . To the lowest order, E(x) = ? x G(x, y) y=0 ? x x N behaves as if it were generated by a unit point source at y = 0. In physics, the power law decay is considered to be long-range (compared to exponential decay) <ref type="bibr" target="#b11">[11]</ref>.</p><p>Particle dynamics in a Poisson field The Poisson field immediately defines a flow model, where the probability distribution evolves according to the gradient flow ?p t (x) ?t = ?? ? (p t (x)E(x)). The gradient flow is a special case of the Fokker-Planck equation <ref type="bibr" target="#b29">[28]</ref>, where the diffusion coefficient is zero. Intuitively we can think of p t (x) as represented by a population of particles. The corresponding (non-diffusion) case of the It? process is the forward ODE dx dt = E(x). We can interpret the trajectories of the ODE as particles moving according to the Poisson field E(x), with initial states drawn from p 0 . The physical picture of the forward ODE is a charged particle under the influence of electric fields in the overdamped limit (details in Appendix F).</p><p>The dynamics is also rescalable in the sense that the particle trajectory remains the same for dx dt = ?f (x)E(x) for f (x) &gt; 0, f (x) ? C 1 , because the time rescaling dt ? f (x(t))dt recovers dx dt = ?E(x). Note that the dynamics is stiff due to the power law factor in the denominator in Eq. (3), posing computational challenges. Luckily the rescalablility allows us to rescale E(x) properly to get new ODEs (formally defined later in Section 3.3) that are more amenable for sampling.</p><p>Generative Modeling via ODE Generative modeling can be done by transforming a base distribution to a data distribution via mappings defined by ODEs. The ODE-based samplers allow for adaptive sampling, exact likelihood evaluation and modeling of continuous-time dynamics <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b34">33]</ref>. Previous works broadly fall into two lines. <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b3">3]</ref> introduce a continuous-time normalizing flow model that can be trained with maximum likelihood by the instantaneous change-of-variables formula <ref type="bibr" target="#b4">[4]</ref>. For sampling, they directly integrate the learned invertiable mapping over time. Another work <ref type="bibr" target="#b34">[33]</ref> unifies the scored-based model <ref type="bibr" target="#b32">[31,</ref><ref type="bibr" target="#b33">32]</ref> and diffusion model <ref type="bibr" target="#b16">[16]</ref> into a general diffusion process, and uses the reverse-time ODE of the diffusion process for sampling. They show that the reverse-time ODE produces high quality samples with improved architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Poisson Flow Generative Models</head><p>In this section, we start with the properties of the Poisson flow in the augmented space and show how to draw samples from the data distribution by following the backward ODE of the Poisson flow (Section 3.1). We then discuss how to actually learn a normalized Poisson field from data samples through simulations of the forward ODE (Section 3.2) and present an equivalent backward ODE that allows for exponentially decay on z (Section 3.3). <ref type="bibr" target="#b1">2</ref> Eq. (2) is valid for N ? 3. When N = 2, the Green's function is G(x, y) = ?log( x ? y ) 2?. We assume N ? 3 since N is typically large in the relevant applications. <ref type="bibr" target="#b3">3</ref> The N -sphere with radius r is defined as {x ? R N +1 , x = r} No augmentation (2D) <ref type="figure" target="#fig_5">Figure 2</ref>: (a) Poisson field (black arrows) and particle trajectories (blue lines) of a 2D uniform disk (red). Left (no augmentation, 2D): all particles collapse to the disk center. Right (augmentation, 3D): particles hit different points on the disk. (b) Proof idea of Theorem 1. By Gauss's Law, the outflow flux d? out equals the inflow flux d? in . The factor of two in p(x)dA 2 is due to the symmetry of Poisson fields in z &lt; 0 and z &gt; 0.</p><formula xml:id="formula_4">Augmentation (3D) (a) d? in = p(x)dA/2 d? d? out = d?/S N (1) Gauss? s Law N dim O r ? ? d? in d? out = z S 5 S 4 S 3 S 1 S 2 (b)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Augment the data with additional dimension</head><p>We wish to generate samples x ? R N from a distribution p(x) supported on a bounded region. We may set the source ?(x) = p(x) ? C 0 4 and compute the resulting gradient field E(x) from Eq. (3).</p><p>Since ?E(x) points towards sources, the backward ODE dx dt = ?E(x) will take samples close to the sources. One may naively hope that the backward ODE is a generative model that recovers p(x). Unfortunately, the backward ODE has the problem of mode collapse. We illustrate this phenomenon with a 2D uniform disk. The reverse Poisson field ?E(x) on the 2D (x, y)-plane points towards the center of the disk O ( <ref type="figure" target="#fig_5">Fig. 2(a)</ref> left), so all particle trajectories (blue lines) will eventually hit O. If we instead add an additional dimension z ( <ref type="figure" target="#fig_5">Fig. 2</ref>(a) right), particles can hit different points on the disk and faithfully recover the data distribution.</p><p>Consequently, instead of solving the Poisson equation ? 2 ?(x) = ?p(x) in the original data space, we solve the Poisson equation in an augmented spacex = (x, z) ? R N +1 with an additional variable z ? R. We augment the training datax in the new space by setting z = 0 such thatx = (x, 0). As a consequence, the data distribution in the augmented space isp(x) = p(x)?(z), where ? is the Dirac delta function. By Eq. (3), the Poisson field by solving the new Poisson equation ? 2 ?(x) = ?p(x) has an analytical form:</p><formula xml:id="formula_5">?x ? R N +1 , E(x) = ???(x) = 1 S N (1) x ?? x ?? N +1p (?)d?<label>(4)</label></formula><p>The associated forward/backward ODEs of the Poisson field are dx dt = E(x), dx dt = ?E(x). Intuitively, theses ODEs uniquely define trajectories of particles between the z = 0 hyperplane and an enclosing hemisphere (cf. <ref type="figure" target="#fig_0">Fig. 1(a)</ref>). In the following theorem, we show that the backward ODE defines a transformation between the uniform distribution on an infinite hemisphere and the data distributionp(x) in the z = 0 plane. We present the formal proof to Appendix A, illustrated by <ref type="figure" target="#fig_5">Fig. 2(b)</ref>. The proof is based on the idea that when the radius of hemisphere r ? ?, the data distributionp(x) can be effectively viewed as a delta distribution at origin. Consequently, the Poisson field points in the radial direction at r ? ?, perpendicular to S + N (r) (Green arrows in <ref type="figure" target="#fig_5">Fig. 2(b)</ref>). Theorem 1. Suppose particles are sampled from a uniform distribution on the upper (z &gt; 0) half of the sphere of radius r and evolved by the backward ODE dx dt = ?E(x) until they reach the z = 0 hyperplane, where the Poisson field E(x) is generated by the sourcep(x). In the r ? ? limit, under some mild conditions detailed in Appendix A, this process generates a particle distributionp(x), i.e., a distribution p(x) in the z = 0 hyperplane. Proof sketch. Suppose the flux of the backward ODE connects a solid angle d? (on S + N (r)) with an area dA (on supp(p(x)). According to Gauss's law, the outflow flux d? out = d? S N (1) on the hemisphere (Green arrows in <ref type="figure" target="#fig_5">Fig. 2(b)</ref>) equals the inflow flux d? in = p(x)dA 2 on supp(p(x)) (Red arrows in <ref type="figure" target="#fig_5">Fig. 2(b)</ref>). d? in = d? out gives d? dA = p(x)S N (1) 2 ? p(x). Together, by change-ofvariable, we conclude that the final distribution in the z = 0 hyperplane is p(x).</p><p>The theorem states that starting from an infinite hemisphere, one can recover the data distributionp by following the inverse Poisson field ?E(x). We defer the formal proof and technical assumptions of the theorem to Appendix A. The property allows generative modeling by following the Poisson flow of ? 2 ?(x) = ?p(x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning the normalized Poisson Field</head><p>Given a set of training data D = {x i } n i=1 i.i.d sampled from the data distribution p(x), we define the empirical version of the Poisson field (Eq. (4)) as follows:</p><formula xml:id="formula_6">E(x) = c(x) n i=1x ?x i x ?x i N +1</formula><p>where the gradient field is calculated on n augmented datapoints</p><formula xml:id="formula_7">{x i = (x i , 0)} n i=1 , and c(x) = 1 ? n i=1 1</formula><p>x?x i N +1 is the multiplier for numerical stability. We further normalize the field to resolve the variations in the magnitude of the norm ??(x) ? 2 , and fit the neural network to the more amenable negative normalized field v(x) = ? ? N?(x) ??(x) ? 2 . The Poisson field is rescalable (cf. Section 2) and thus trajectories of its forward/backward ODEs are invariant under normalization. We denote the empirical field calculated on batch data B by? B and the negative normalized field as v</p><formula xml:id="formula_8">B (x) = ? ? N? B (x) ?? B (x) ? 2 .</formula><p>Similar to the scored-based models, we sample points inside the hemisphere by perturbing the augmented training data. Given a training point x ? D, we add noise to its augmented version</p><formula xml:id="formula_9">{x i = (x i , 0)} n i=1 to construct the perturbed point (y, z): y = x+ ? x ? (1 + ? ) m u, z = z (1 + ? ) m<label>(5)</label></formula><p>where = ( x , z ) ? N (0, ? 2 I N +1?N +1 ), u ? U(S N (1)) and m ? U[0, M ]. The upper limit M , standard deviation ? and ? are hyper-parameters. With fixed and u, the added noise increases exponentially with m. The rationale behind the design is that points farther away from the data support play a less important role in generative modeling, sharing a similar spirit with the choice of noisy scales in score-based models <ref type="bibr" target="#b33">[32,</ref><ref type="bibr" target="#b34">33]</ref>.</p><p>In practice, we sample the points by perturbing a mini-batch data B = {x i } B i=1 in each iteration. We uniformly sample the power m in [0, M ] for each datapoint. We select a large M (typically around 300) to ensure the perturbed points can reach a large enough hemisphere. We use a larger batch B L for the estimation of normalized field since the empirical normalized field is biased, which empirically gives better results. Denoting the set of perturbed points as {? i } B i=1 , we train the neural network f ? on these points to estimate the negative normalized field by minimizing the following loss:</p><formula xml:id="formula_10">L(?) = 1 B B i=1 ? f ? (? i ) ? v B L (? i ) ? 2 2</formula><p>We summarize the training process in Algorithm 1. In practice, we add a small constant ? to the denominator of the normalized field to overcome the numerical issue when ?i, x ?x i ? 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Backward ODE anchored by the additional dimension</head><p>After estimating the normalized field v, we can sample from the data distribution by the backward ODE dx = v(x)dt. Nevertheless, the boundary condition of the above ODE is unclear: the starting and terminal time t of the ODE are both unknown. To remedy the issue, we propose an equivalent backward ODE in which x evolves with the augmented variable z: </p><formula xml:id="formula_11">d(x, z) = ( dx dt dt dz dz, dz) = (v(x) x v(x) ?1 z , 1)dz</formula><formula xml:id="formula_12">B i=1 from B L Simulate the ODE: {? i = perturb(x i ) } B i=1</formula><p>Calculate the normalized field by</p><formula xml:id="formula_13">B L : v B L (? i ) = ? ? N? B L (? i ) (?? B L (? i ) ? 2 +?), ?i Calculate the loss: L(?) = 1 B ? B i=1 ? f ? (? i ) ? v B L (? i ) ? 2 2</formula><p>Update the model parameter:</p><formula xml:id="formula_14">? = ? ? ??L(?) end for return f ? Algorithm 2 perturb(x) Sample the power m ? U[0, M ] Sample the initial noise ( x , z ) ? N (0, ? 2 I (N +1)?(N +1) )</formula><p>Uniformly sample the vector from the unit ball u ? U(S N (1))</p><formula xml:id="formula_15">Construct training point y = x+ ? x ? (1 + ? ) m u, z = z (1 + ? ) m return? = (y, z)</formula><p>where v(x) x , v(x) z are the corresponding components of x, z in vector v(x). In the new ODE, we replace the time variable t with the physically meaningful variable z, permitting explicit starting and terminal conditions: when z = 0, we arrive at the data distribution and we can freely choose a large z max as the starting point in the backward ODE. The backward ODE is compatible with general-purpose ODE solvers, e.g., RK45 method <ref type="bibr" target="#b23">[23]</ref> and forward Euler method. The popular black-box ODE solvers, such as the one in Scipy library <ref type="bibr" target="#b38">[37]</ref>, typically use a common starting time for the same batch of samples. Since the distribution on the z = z max hyperplane is no longer uniform, we derive the prior distribution by radially projecting uniform distribution on the hemisphere with radius r = z max to the z = z max hyperplane:</p><formula xml:id="formula_16">p prior (x) = 2z N +1 max S N (z max )(? x ? 2 2 +z 2 max ) N +1 2 = 2z max S N (1)(? x ? 2 2 +z 2 max ) N +1 2</formula><p>where S N (r) is the surface area of N -sphere with radius r. The reason behind the radial projection is that the Poisson field points in the radial direction at r ? ?. The new backward ODE also defines a bijective transformation between p prior (x) on the infinite hyperplane (z max ? ?) and the data distributionp(x), analogous to Theorem 1. In order to sample from p prior (x), it is suffice to sample the norm (radius) from the distribution:</p><formula xml:id="formula_17">p radius (? x ? 2 ) ? ? x ? N ?1 2 (? x ? 2 2 +z 2 max ) N +1 2</formula><p>and then uniformly sample its angle. We provide detailed derivations and practical sampling procedure in Appendix A.4. We further achieve exponentially decay on the z dimension by introducing a new variable t ? :</p><formula xml:id="formula_18">[Backward ODE] d(x, z) = (v(x) x v(x) ?1 z z, z)dt ?<label>(6)</label></formula><p>The z component in the backward ODE, i.e., dz = zdt ? , can be solved by z = e t ? . Since z reaches zero as t ? ? ??, we instead choose a tiny positive number z min as the terminal condition. The corresponding starting/terminal time of the variable t ? are log z max log z min respectively. Empirically, this simple change of variable leads to 2? faster sampling with almost no harm to the sample quality. In addition, we substitue the predicted v(x) z with a more accurate one when z is small (Appendix B.2.3). We defer more details of the simulation of backward ODE to Appendix B.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Generative Modeling via the Backward ODE</head><p>In this section, we demonstrate the effectiveness of the backward ODE associated with PFGM on image generation tasks. In Section 4.1, we show that PFGM achieves currently best in class performance in the normalizing flow family. In comparison to the existing state-of-the-art SDE or MCMC approaches, PFGM exhibits 10? or 20? acceleration while maintaining competitive or </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Efficient image generation by PFGM</head><p>Setup For image generation tasks, we consider the CIFAR-10 <ref type="bibr" target="#b22">[22]</ref>, CelebA 64 ? 64 <ref type="bibr" target="#b42">[38]</ref> and LSUN bedroom 256 ? 256 <ref type="bibr" target="#b43">[39]</ref>. Following <ref type="bibr" target="#b33">[32]</ref>, we first center-crop the CelebA images and then resize them to 64 ? 64. We choose M = 291 (CIFAR-10 and CelebA) 356 (LSUN bedroom), ? = 0.01 and ? = 0.03 for the perturbation Algorithm 2, and z min = 1e ? 3, z max = 40 (CIFAR-10) 60 (CelebA 64 2 ) 100 (LSUN bedroom) for the backward ODE. We further clip the norms of initial samples into (0, 3000) for CIFAR-10, (0, 6000) for CelebA 64 2 and (0, 30000) for LSUN bedroom. We adopt the DDPM++ and DDPM++ deep architectures <ref type="bibr" target="#b34">[33]</ref> as our backbones. We add the scalar z (resp. predicted direction on z) as input (resp. output) to accommodate the additional dimension. We take the same set of hyper-parameters, such as batch size, learning rate and training iterations from <ref type="bibr" target="#b34">[33]</ref>. We provide more training details in Appendix B.1, and discuss how to set these hyper-parameters for general datasets in B.1.1 and B.2.1.</p><p>Baselines We compare PFGM to modern autoregressive model <ref type="bibr" target="#b37">[36]</ref>, GAN <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b24">24]</ref>, normalizing flow <ref type="bibr" target="#b19">[19]</ref> and EBM <ref type="bibr" target="#b8">[8]</ref>. We also compare with variants of score-based models such as DDIM <ref type="bibr" target="#b31">[30]</ref> and current state-of-the-art SDE/ODE methods <ref type="bibr" target="#b34">[33]</ref>. We denote the methods that use forward-time SDEs in <ref type="bibr" target="#b34">[33]</ref> such as Variance Exploding (VE) SDE/Variance Preserving (VP) SDE/ sub-Variance Preserving (sub-VP), and the corresponding backward SDE/ODE, as A-B, where A ? {VE, VP, sub-VP} and B ? {SDE, ODE}. We follow the model selection protocol in <ref type="bibr" target="#b34">[33]</ref>, which selects the checkpoint with the smallest FID score over the course of training every 50k iterations. Numerical Solvers The backward ODE (Eq. <ref type="formula" target="#formula_18">(6)</ref>) is compatible with any general purpose ODE solver. In our experiments, the default solver of ODEs is the black box solver in the Scipy library <ref type="bibr" target="#b38">[37]</ref> with the RK45 [7] method (RK45), unless otherwise specified. For VE/VP/subVP-SDEs, we use the predictor-corrector (PC) sampler introduced in <ref type="bibr" target="#b34">[33]</ref>. For VP/sub-VP-SDEs, we apply the predictoronly sampler, because its performance is on par with the PC sampler while requiring half computation.</p><p>Results For quantitative evaluation on CIFAR-10, we report the Inception <ref type="bibr" target="#b30">[29]</ref> (higher is better) and FID <ref type="bibr" target="#b13">[13]</ref> scores (lower is better) in <ref type="table" target="#tab_1">Table 1</ref>. We also include our preliminary experimental results on a weaker architecture NCSNv2 <ref type="bibr" target="#b33">[32]</ref> in Appendix D.2. We measure the inference speed by the average NFE (number of function evaluation). We also explicitly indicate which methods belong to the invertible flow family.</p><p>Our main findings are: (1) PFGM achieves the best Inception scores and FID scores among the normalizing flow models. Specifically, PFGM obtains an Inception score of 9.68 and a FID score of 2.48 using the DDPM++ deep architecture. To our best knowledge, these are the highest FID and Inception scores by flow models on CIFAR-10.</p><p>(2) PFGM achieves a 10? ? 20? faster inference speed than the SDE methods using the similar architectures, while retaining comparable sample quality. As shown in <ref type="table" target="#tab_1">Table 1</ref>  In our preliminary experiments on NCSNv2 architectures, we empirically observe that the VE/VP-ODEs have FID scores greater than 90 on CIFAR-10. In particular, VE/VP-ODEs can only generate decent samples when applying the Langevin dynamics corrector, and even then, their performances are still inferior to PFGM ( <ref type="table" target="#tab_10">Table 9</ref>, <ref type="table" target="#tab_1">Table 10</ref>). The poor performance on NCSNv2 stands in striking contrast to their high sample quality on NCSN++/DDPM++ in <ref type="bibr" target="#b34">[33]</ref>. It indicates that the VE/VP-ODEs are more susceptible to estimation errors than PFGM. We hypothesize that the strong norm-? correlation seen during the training of score-based models causes the problem.</p><p>For score-based models, the l 2 norms of perturbed training samples and the standard deviations ?(t) of Gaussian noises have strong correlation, e.g., l 2 norm ? ?(t) ? N for large ?(t) in VE <ref type="bibr" target="#b34">[33]</ref>. In contrast, as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, PFGM allocates high mass across a wide spectrum of the training sample norms. During sampling, VE/VP-ODEs could break down when the trajectories of backward ODEs deviate from the norm-?(t) relation to which most training samples pertain. The weaker NCSNv2 backbone incurs larger errors and thus leads to their failure. The PFGM is more resistant to estimate errors because of the greater range of training sample norms.</p><p>To further verify the hypothesis above, we split a batch of VE-ODE samples into cleaner and noisier samples according to visual quality ( <ref type="figure" target="#fig_12">Fig. 8(a)</ref>). In <ref type="figure">Fig. 5(a)</ref>, we investigate the relation for cleaner and noisier samples during the forward Euler simulation of VE-ODE when ?(t) &lt; 15. We can see that the trajectory of cleaner samples stays close to the norm-?(t) relation (the red dash line), whereas that of the noisier samples diverges from the relation. The Langevin dynamics corrector changes the trajectory of noisier samples to align with the relation. <ref type="figure">Fig. 5</ref>(b) further shows that the anchored variable z(t ? ) and the norms in the backward ODE of PFGM are not strongly correlated, giving rise to the robustness against the imprecise estimation on NCSNv2. We defer more details to Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effects of step size in the forward Euler method</head><p>In order to accelerate the inference speed of ODEs, we can increase the step size (decrease the NFEs) in numerical solvers such as the forward Euler method. It also enables the trade-off between sample quality and computational efficiency in real-world deployment. We study the effects of increasing step size on PFGM, VP-ODE and DDIM <ref type="bibr" target="#b31">[30]</ref> using the forward Euler method, with a varying NFE ranging from 10 to 100.</p><p>In <ref type="figure">Fig. 5(c)</ref>, we report the sample quality measured by FID scores on CIFAR-10. As expected, all the methods have higher FID scores when decreasing the NFE. We observe that the sample quality of PFGM degrades gracefully as we decrease the NFE. Our method shows significantly better robustness to step sizes than the VP-ODE, especially when only taking a few Euler steps. In addition, PFGM obtains better FID scores than DDIM on most NFEs except for 10 where PFGM is marginally worse. This suggests that the PFGM is a promising method for accommodating instantaneous resource availability, as high-quality samples can be generated in limited steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Utilities of ODE: likelihood evaluation and latent representation</head><p>Similar to the family of discrete normalizing flows <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b14">14]</ref> and continuous probability flow <ref type="bibr" target="#b34">[33]</ref>, the forward ODE in PFGM defines an invertible mapping between the data space and latent space with a known prior. Formally, we define the invertible forward M mapping by integrating the</p><formula xml:id="formula_19">corresponding forward ODE d(x, z) = (v(x) x v(x) ?1 z z, z)dt ? of Eq. (6): x(log z max ) = M(x(log z min )) ? x(log z min ) + log zmax log z min v(x(t ? )) x v(x(t ? )) ?1 z e t ? dt ?</formula><p>where log z min /log z max are the starting/terminal time in the forward ODE. The forward mapping transfers the data distribution to the prior distribution p prior on the z = z max hyperplane (cf. Section 3.3): p prior (x(log z max )) = M(p(x(log z min ))). The invertibility enables likelihood evaluation and creates a meaningful latent space on the z = z max hyperplane. In addition, we can adapt to the computational constraints by adjusting the step size or the precision in numerical ODE solvers. 3.49 Glow <ref type="bibr" target="#b19">[19]</ref> 3.35 Residual Flow <ref type="bibr" target="#b3">[3]</ref> 3.28 Flow++ <ref type="bibr" target="#b14">[14]</ref> 3.29 DDPM (L) <ref type="bibr" target="#b16">[16]</ref> ? 3.70 *</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DDPM++ backbone</head><p>VP-ODE <ref type="bibr" target="#b34">[33]</ref> 3.20 sub-VP-ODE <ref type="bibr" target="#b34">[33]</ref> 3.02 PFGM (ours) <ref type="bibr">3.19</ref> Likelihood evaluation We evaluate the data likelihood by the instantaneous change-of-variable formula <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b34">33]</ref>.</p><p>In <ref type="table" target="#tab_3">Table 2</ref>, we report the bits/dim on the uniformly dequantized CIFAR-10 test set and compare with existing baselines that use the same setup. We observe that PFGM achieves better likelihoods than discrete normalizing flow models, even without maximum likelihood training. Among the continuous flow models, sub-VP-ODE shows the lowest bits/dim, although its sample quality is worse than VP-ODE and PFGM ( <ref type="table" target="#tab_1">Table 1</ref>). The exploration of the seeming trade-off between likelihood and sample quality is left for future works.</p><p>Latent representation Since the samples are uniquely identifiable by their latents via the invertible mapping M, PFGM further supports image manipulation using its latent representation on the z = z max hyperplane. We include the results of image interpolation and the temperature scaling <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b34">33]</ref> to Appendix D.4 and Appendix D.5. For interpolation, it shows that we can travel along the latent space to obtain perceptually consistent interpolations between CelebA images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present a new deep generative model by solving the Poisson equation whose source term is the data distribution. We estimate the normalized gradient field of the solution in an augmented space with an additional dimension. For sampling, we devise a backward ODE that exponential decays on the physically meaningful additional dimension. Empirically, our approach has currently best performance over other normalizing flow baselines, and achieving 10? to 20? acceleration over the stochastic methods. Our backward ODE shows greater stability against errors than popular ODE-based methods, and enables efficient adaptive sampling. We further demonstrate the utilities of the forward ODE on likelihood evaluation and image interpolation. Future directions include improving the normalization of Poisson fields. More principled approaches can be used to get around the divergent near-field behavior. For example, we may exploit renormalization, a useful tool in physics, to make the Poisson field well-behaved in near fields. The proof idea of the uniqueness is similar to the uniqueness theorems in electrostatics. Suppose we have two different solutions ? 1 , ? 2 ? C 2 which satisfy</p><formula xml:id="formula_20">? 2 ? 1 (x) = ??(x), ? 2 ? 2 (x) = ??(x).<label>(7)</label></formula><p>We define?(x) ? ? 2 (x) ? ? 1 (x). Subtracting the above two equations gives</p><formula xml:id="formula_21">? 2? (x) = 0, ?x ? ?.<label>(8)</label></formula><p>By the vector differential identity we hav?</p><formula xml:id="formula_22">?(x)? 2? (x) = ? ? (?(x)??(x)) ? ??(x) ? ??(x),<label>(9)</label></formula><p>By the divergence theorem we have</p><formula xml:id="formula_23">? ? ? (?(x)??(x))d N x = ??? (x)??(x) ? d N ?1 S = 0,<label>(10)</label></formula><p>where d N ?1 S denotes an N ? 1 dimensional surface element at infinity, and the second equation holds due to zero boundary condition at infinity. Combining Eq. (8)(9)(10), we have</p><formula xml:id="formula_24">? ? ? (?(x)??(x))d N x = ? ??(x) 2 d N x = 0,<label>(11)</label></formula><p>since this is an integral of a positive quantity, we must have ??(x) = 0, or?(x) = c, ?x ? ?. This means ? 1 and ? 2 differ at most by a constant, but a constant does not affect gradients, so ?? 1 (x) = ?? 2 (x).</p><p>In our method section (Section 3.1), we augmented the original N -dimensional data with an extra dimension. The new data distribution in the augmented space isp(x) = p(x)?(z), where ? is the Dirac delta function. The support of the data distribution is in the z = 0 hyperplane. In the following lemma, we show the existence and uniqueness of the solution to ? 2 ?(x) = ?p(x) outside the data support.  For the uniqueness, suppose we have two different solutions ? 1 , ? 2 ? C 2 (R N +1 ? supp(p(x))) which satisfy</p><formula xml:id="formula_25">? 2 ? 1 (x) = ?p(x), ? 2 ? 2 (x) = ?p(x). (12) We define?(x) ? ? 2 (x) ? ? 1 (x). Subtracting the above two equations gives ? 2? (x) = 0, ?x ? R N +1 ? supp(p(x)).<label>(13)</label></formula><p>By the vector differential identity we hav?</p><formula xml:id="formula_26">?(x)? 2? (x) = ? ? (?(x)??(x)) ? ??(x) ? ??(x),<label>(14)</label></formula><p>By the divergence theorem we have</p><formula xml:id="formula_27">R N +1 ? ? (?(x)??(x))d N +1x = ?R N +1? (x)??(x) ? d N S = 0,<label>(15)</label></formula><p>where d N S denotes an N dimensional surface element at infinity, and the second equation holds due to zero boundary condition at infinity. Combining Eq. (13)(14)(15), we have</p><formula xml:id="formula_28">R N +1 ? ? (?(x)??(x))d N +1x = R N +1 ?supp(p(x)) ? ? (?(x)??(x))d N +1x = R N +1 ?supp(p(x)) ??(x) 2 d N +1x = 0,</formula><p>The first equation holds because Lebesgue measure of supp(p(x)) is zero. Since ??(x) 2 is an integral of a positive quantity, we must have ??(x) = 0, or?(x) = c, ?x ? R N +1 ? supp(p(x)). This means ? 1 and ? 2 differ at most by a constant function, but a constant does not affect gradients, so ?? 1 (x) = ?? 2 (x).</p><p>As illustrated in <ref type="figure">Fig. 6</ref>, there is a bijective mapping between the upper hemisphere of radius r and the z = 0 plane, where each pair of corresponding points is connected by an electric field line. We will now formally prove that, in the r ? ? limit, this mapping transforms the arbitrary charge distribution in the source plane (that generated the electric field) into a uniform distribution on the hemisphere. Theorem 2. Suppose particles are sampled from a uniform distribution on the upper (z &gt; 0) half of the sphere of radius r and evolved by the backward ODE dx dt = ?E(x) until they reach the z = 0 hyperplane, where the Poisson field E(x) is generated by the sourcep(x). In the r ? ? limit, under the conditions in Lemma 2, this process generates a particle distributionp(x), i.e., a distribution p(x) in the z = 0 hyperplane. Proof. By Lemma 2, we know that with zero boundary at infinity, the Poisson equation ? 2 ?(x) = ?p(x) has a unique solution ?(x) ? C 2 forx ? R N +1 ? supp(p(x)). Hence E(x) = ???(x) ? C 1 , guaranteeing the existence-uniqueness of the solution to the ODE dx dt = ?E(x) according to Theorem 2.8.1 in <ref type="bibr" target="#b28">[27]</ref>.</p><p>Consider the tube in <ref type="figure">Fig. 6</ref> connecting an area on dA in the z = ? 0 + hyperplane (S 3 ) to a solid angle d? on the hemisphere (S 1 ), with S 2 as its side. The tube is the space swept by dA following electric field E, so by definition the electric field is parallel to the tangent space of the tube sides S 2 . The bottom of the tube S 3 is located at z = ? 0 + , a bit above the z = 0 plane, so the tube does not enclose any charges. We note that the divergence of Poisson field is zero in R N +1 ? supp(p(x)):</p><formula xml:id="formula_29">? ? E(x) = ?? 2 ?(x) =p(x) = 0, ?x ? R N +1 ? supp(p(x))</formula><p>Denote the volume and surface of the tube as V and B. According to divergence theorem, E(x) ? dB = ? V ? ? E(x)dV = 0. Hence the net flux leaving the tube is zero:</p><formula xml:id="formula_30">? S1 + ? S2 + ? S3 = 0, ? Si ? Si E(x) ? dB (i = 1, 2, 3)<label>(16)</label></formula><p>There is no flux through the sides, i.e., ? S2 = 0, since E(x) is orthogonal to the surface element dB on the tube sides by definition. As a result, the flux ? S3 entering from below must equal the flux ? S1 leaving the other end. Denote the l 2 norm of the vector r as r. We first calculate the influx ? S3 . To do so, we study a Gaussian pillbox whose top, side and bottom are S 3 , S 4 and S 5 . S 3 and S 5 are located at z = and z = ? ( ? 0 + ). Denote the volume and surface of the pillbox as V ? and B ? . The pillbox contains charge p(x)dA, so according to Gauss's law</p><formula xml:id="formula_31">E(x) ? dB ? = ? V ? ? ? E(x)dV ? = ? V ?p (x)dV ? = p(x)dA, i.e., ? ? S3 + ? ? S4 + ? ? S5 = p(x)dA, ? ? Si ? Si E(x) ? dB ? (i = 3, 4, 5)<label>(17)</label></formula><p>The flux on the sides</p><formula xml:id="formula_32">? ? S4 ? ? 0, and ? ? S3 = ? ? S5 due to mirror symmetry of z = 0. So ? ? S3 = ? ? S5 = p(x)dA 2.</formula><p>Note on the S 3 surface, the outflux of the pillbox is exactly the influx of the tube, so we have:</p><formula xml:id="formula_33">? S3 = ?? ? S3 = ?p(x)dA 2,<label>(18)</label></formula><p>inserting which and ? S2 = 0 to Eq. <ref type="formula" target="#formula_0">(16)</ref> gives</p><formula xml:id="formula_34">? S1 = ?? S3 = p(x)dA 2.<label>(19)</label></formula><p>On the other hand, in the far-field limit r ? ?, since supp(p(x)) is bounded, the data distribution can be effectively seen as a point charge (see Appendix A.2). By Lemma 3, we have lim r?? E(r) = ? lim r?? ??(r) = r S N (1)r N +1 . The resulting outflux on the hemisphere is ? S1 = E r r N d? = d? S N (1)</p><p>where E r ? E(r) ? r r is the radial component of E. Comparing Eq. <ref type="bibr" target="#b19">(19)</ref> and Eq. (20) yields d? dA = p(x)S N (1) 2 ? p(x). In other words, the mapping from the z = 0 hyperplane to the hemisphere dilutes the charge density p(x) up to a constant factor. Thus by change-of-varible, we conclude that the mapping transforms the data distribution into a uniform distribution on the infinite hemisphere. Since the ODE is reversible, the backward ODE transforms the uniform distributoin on the infinite hemisphere to the distributionp(x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Multipole Expansion</head><p>We discuss the behaviors of the potential function in Poisson equation <ref type="figure" target="#fig_0">(Eq. (1)</ref>) under different scenarios, utilizing the multipole expansion. Suppose we have a unit point charge q = 1 located at x ? R N . We know that the potential function at another point y ? R N is ?(y ? x) = 1 y ? x N ?2 (ignoring a constant factor). Now we assume that x is close to the origin such that we can Taylor expand around x = 0:</p><formula xml:id="formula_36">?(y ? x) = ?(y) ? N ?=1 x ? ? ? (y) + 1 2 N ?=1 N ?=1 x ? x ? ? ?? (y) ? ...<label>(21)</label></formula><p>where</p><formula xml:id="formula_37">? ? (y) = ??(y ? x) ?x ? x=0 = (N ? 2) y ? y N ? ?? (y) = ? 2 ?(y ? x) ?x ? ?x ? x=0 = (N ? 2) N y ? y ? ? y 2 ? ?? y N +2<label>(22)</label></formula><p>In the case where the source is a distribution p(x), the potential ?(y) can again be Taylor expanded:</p><formula xml:id="formula_38">?(y) = q?(y) + N ?=1 q ? ? ? (y) + N ?=1 N ?=1 q ?? ? ?? (y) ? ...<label>(23)</label></formula><p>where</p><formula xml:id="formula_39">q = p(x)dx, q ? = p(x)x ? dx, q ?? = p(x)x ? x ? dx,<label>(24)</label></formula><p>which are called monopole, dipole and quadrupole in physics, respectively. The gradient field E(y) = ??(y) can be expanded in the same such that</p><formula xml:id="formula_40">E(y) = E (0) (y) + E (1) (y) + E (2) (y) + ...<label>(25)</label></formula><p>It is easy to check that E (i) (y) decays as 1 y N ?2+i , which means higher-order corrections decay faster than leading terms. So when y ? ?, only the monopole term E (0) (y) matters, which behaves like a point source.</p><p>In a more realistic setup, we only have a large but finite y , so the question is: under what condition is the point source approximation valid? We examine ? (0) , ? <ref type="bibr" target="#b0">(1)</ref> and ? (2) more carefully:</p><formula xml:id="formula_41">? (0) = 1 y N ?2 ? (1) = N ?=1 (N ? 2) y ? x ? y N = (N ? 2) x T y y N ? (2) = 1 2 N ?=1 N ?=1 (N ? 2) N y ? y ? ? y 2 ? ?? y N +1 x ? x ? = N ? 2 2 N (x T y) 2 ? x 2 y 2 y N +2<label>(26)</label></formula><p>Since ? <ref type="bibr" target="#b0">(1)</ref> is an odd function of x, integrating ? (1) over x leads to zero (samples are normalized to zero mean). In machine learning applications, N is usually a large number (although in physics N is merely 3). If y is a random vector of length y , then</p><formula xml:id="formula_42">x T y ? ( 1 ? N ? 1 N ) x y . So Eq. (26)</formula><p>can be approximated as</p><formula xml:id="formula_43">? (0) ? 1 y N ?2 , ? (2) ? ? N 2 x 2 y N<label>(27)</label></formula><p>Requiring ? ? (0) p(x)dx ? ? ? <ref type="bibr" target="#b1">(2)</ref> p(x)dx gives y 2 ? ? N Ep(x) x 2 . So the condition for the point source approximation to be valid is:</p><formula xml:id="formula_44">? = 2 y 2 ? N Ep(x) x 2 ? 1<label>(28)</label></formula><p>Based on this condition, we can partition space into three zones: (1) the far zone ? ? 1, where the point source approximation is valid; (2) the intermediate zone ? ? O(1), where the gradient field has moderate curvature; (3) the near zone ? ? 1, where the gradient field has high curvature. In practice, the initial value y is greater than 1000 (hence ? ? 1) with high probability on CIFAR-10 and CelebA datasets, incidating that the initial samples lie in the far zone and gradually move toward the near zone where y ? x (? ? 1).</p><p>We summarize above observations in the following lemma in the y ? ? limit: Lemma 3. Assume the data distribution p(x) ? C 0 has a compact support in R N , then the solution ? to the Poisson equation ? 2 ?(x) = ?p(x) with zero boundary condition at infinity satisfies lim ?x?2?? ??(x) = ? 1</p><formula xml:id="formula_45">S N ?1 (1) x ?x? N 2 .</formula><p>Proof. By Lemma 1, the gradient of the solution has the following form:</p><formula xml:id="formula_46">??(x) = ? x G(x, y)p(y)dy, ? x G(x, y) = ? 1 S N ?1 (1) x ? y x ? y N .</formula><p>Since p(x) has a bounded support, we assume max{? x ? 2 ? p(x) = 0} &lt; B. On the other hand, we have  Proof. It is convenient to denote r = x ? y, r = r and notice ?r ?x = r r. Firstly, we calculate ? x G(x, y):</p><formula xml:id="formula_47">lim ?x?2?? ? x G(x, y) = lim ?x?2?? ? 1 S N ?1 (1) x ? y x ? y N = lim ?x?2?? ? 1 S N ?1 (1) x x N for ?y such that ? y ? 2 &lt; B. Hence, lim ?x?2?? ??(x) = lim ?x?2?? ? x G(x, y)p(y)dy = lim ?x?2?? ? x G(x, y)p(y)dy = ? 1 S N ?1 (1) x ? x ? N</formula><formula xml:id="formula_48">? x G(x, y) = 1 (N ? 2)S N ?1 (1) ? x ( 1 r N ?2 ) = 1 (N ? 2)S N ?1 (1) ? ?r ( 1 r N ?2 )? x r = ? 1 S N ?1 (1) r r N (29)</formula><p>Then we calculate ? 2</p><p>x G(x, y):</p><formula xml:id="formula_49">? 2 x G(x, y) ? ? x ? ? x G(x, y) = ? 1 S N ?1 (1) ? x ? r r N = ? 1 S N ?1 (1) (? x ( 1 r N ) ? r + 1 r N ? r ? r) = ? 1 S N ?1 (1) (? N r N + N r N ) = ? 0 S N ?1 (1)r N<label>(30)</label></formula><p>which is 0 for r &gt; 0, but undermined for r = 0. So we are left with proving S (y)</p><formula xml:id="formula_50">? 2 x G(x, y)d N x = ?1,<label>(31)</label></formula><p>where S (y) denotes a ball centered at y with a radius ? 0 + . With the divergence theorem, we have</p><formula xml:id="formula_51">S (y) ? 2 x G(x, y)d N x = ?S (y) ? x G(x, y) ? d N ?1 B<label>(32)</label></formula><p>where the surface integral can be computed <ref type="bibr" target="#b34">(33)</ref> in which we used ?S (y) r ? d N ?1 B = S N ?1 ( ). Together, we conclude that</p><formula xml:id="formula_52">?S (y) ? x G(x, y) ? d N ?1 B = ?S (y) (? 1 S N ?1 (1) r r N ) ? d N ?1 B = ? 1 S N ?1 (1) S N ?1 ( ) N ?1 = ?1</formula><formula xml:id="formula_53">? 2 x G(x, y) = ??(x ? y)<label>(34)</label></formula><p>Next we show that ?(x) = ? G(x, y)?(y)dy solves ? 2 ?(x) = ??(x). Taking the Laplacian operator of both sides gives:</p><formula xml:id="formula_54">? 2 x ?(x) = ? 2 x G(x, y)?(y)dy = ? 2 x G(x, y)?(y)dy = ??(x ? y)?(y)dy (By Eq. (34)) = ??(x)</formula><p>In addition, we show that ?(x) is zero at infinity. Since ?(x) ? C 0 and has compact support, we know that ?(x) is bounded, and let ?(x) &lt; B.</p><formula xml:id="formula_55">lim ?x?2?? ?(x) = lim ?x?2?? G(x, y)?(y)dy ? B lim ?x?2?? supp(?) 1 (N ? 2)S N ?1 (1) 1 x ? y N ?2 dy = 0</formula><p>The last equality holds since supp(?) is a compact set. We obtain the prior distribution p prior by projecting the uniform distribution U(S + N (z max )) on the hemisphere S + N (z max ) to the z = z max hyperplane. In the following proposition, we show that the projected distribution is p prior (</p><formula xml:id="formula_56">x) = 2zmax S N (1)r N +1 . Proposition 1. The radial projection of U(S + N (z max )) on the hemisphere S + N (z max ) to the z = z max hyperplane is p prior (x) = 2zmax S N (1)r N +1 .</formula><p>Proof. We calculate the change-of-variable ratio by comparing two associate areas. As illustrated in <ref type="figure">Fig. 7</ref>, an area dA 1 on S + N (z max ) is projected to an area dA 3 on the hyperplane in the (x, z max ) direction, and we have U(S + N (z max ))dA 1 = p prior (x)dA 3 We aim to calculate the ratio dA 1 dA 3 below. We define the angle between (0, z max ) andx = (x, z max ) to be ?. We project dA 3 to the hyperplane orthogonal tox to get dA 2 = dA 3 cos? = dA 3 z max r where r ? x 2 = x 2 2 + z 2 max . Since dA 1 is parallel to dA 2 and they lie in the same cone from the origin O, we have dA 2 dA 1 = (r z max ) N . Combining all the results gives p prior (x) = U(S + N (z max ))</p><formula xml:id="formula_57">dA 1 dA 3 = U(S + N (z max )) dA 1 dA 2 dA 2 dA 3 = 2 S N (1)z N max ( z max r ) N z max r = 2z max S N (1)r N +1 .</formula><p>In order to sample from p prior (x), we first sample the norm (radius) R = x 2 from the distribution:</p><formula xml:id="formula_58">p radius (R) ? R N ?1 p prior (x) (p prior is isotropic) ? R N ?1 ( x 2 2 + z 2 max ) N +1 2 = R N ?1 (R 2 + z 2 max ) N +1 2<label>(35)</label></formula><p>and then uniformly sample its angle. Sampling from p prior encompasses three steps. We first sample a real number r 1 with parameters ? = N 2 , ? = 1 2 , i.e., R 1 <ref type="figure">? Beta(?, ?)</ref> Next, we set R 2 = R1</p><p>1?R1 such that R 2 is effectively sampled from the inverse beta distribution a(also known as beta prime distribution) with parameters ? = N 2 , ? = 1 2 . Finally, we set R 3 = z 2 max R 2 . To verify the pdf of R 3 is p radius , note that the pdf of inverse beta distribution is</p><formula xml:id="formula_59">p(R 2 ) ? R N 2 ?1 2 (1 + R 2 ) ? N 2 ? 1 2</formula><p>Next, by change-of-variable, the pdf of</p><formula xml:id="formula_60">R 3 = z 2 max R 2 is p(R 3 ) ? R N 2 ?1 2 (1 + R 2 ) ? N 2 ? 1 2 * 2R 3 z 2 max ? R 3 R N 2 ?1 2 (1 + R 2 ) N +1 2 = (R 3 z max ) N ?1 (1 + (R 2 3 z 2 max )) N +1 2 ? R N ?1 3 (1 + (R 2 3 z 2 max )) N +1 2 ? R N ?1 3 (z 2 max + R 2 3 ) N +1 2 ? p radius (R 3 ) (By Eq. (35))</formula><p>Hence we conclude that p(R 3 ) = p radius (R 3 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experimental Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Training</head><p>In this section we include more details about the training of PFGM and other baselines. We show the hyper-parameters settings for all the baselines (Appendix B.1.1). All the experiments are run on a single NVIDIA A100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.1 Additional Settings</head><p>PFGM We set the hyper-parameters ? = 5, the larger batch size for calculating normalize field B L = 2048 (CIFAR-10), 256 (CelebA), 64 (LSUN bedroom) in Algorithm 1, and M = 291 (CIFAR-10, CelebA) 356 (LSUN bedroom), ? = 0.01 and ? = 0.03 in Algorithm 2. We use the a batch size of B = 128 (CIFAR-10, CelebA) 32 (LSUN bedroom), the same Adam optimizer and exponential moving average in <ref type="bibr" target="#b34">[33]</ref>. We center the data around the origin. The initial z components in the normalized field are approximately zero with small initial z values in Algorithm 2. In this case, the trajectories of the forward ODE terminate at points that are unlikely traversed by the backward ODE, i.e., points with large ? x ? 2 and small z. In light of this, we heuristically confine the maximum sampling step to M = 200 (CIFAR-10, CelebA) 250 (LSUN bedroom) for points with the initial z smaller than 0.005. More principal solutions are left for future works. For selecting M in more general settings, we recommend the following rule-of-thumb. According to analysis in Section A.2, given a perturbation point (y, z) when setting the exponent m = M in Algorithm 2, we can ensure the point source approximation by</p><formula xml:id="formula_61">y 2 ? ? N E p(x) x 2 2<label>(36)</label></formula><p>where N is the data dimension and p(x) is the data distribution. By WLLN, we have x = ? N ?, and recall that y = x+ ? x ? (1 + ? ) M u where = ( x , z ) ? N (0, ? 2 I N +1?N +1 ), u ? U(S N (1)). Together, we conclude y ? ? N ?(1 + ? ) M . Substituting in Eq. (36), we have</p><formula xml:id="formula_62">M &gt; 1 2 log 1+? E p(x) x 2 2 ? N ? 2 = 1 2 ln E p(x) x 2 2 ? N ? 2 ln 1 + ?</formula><p>We empirically observe that setting M = 3 ? 291.</p><p>Since we are operating in the augmented space, we add minor modifications to the DDPM++/DDPM++ deep architectures to accommodate the extra dimension. More specifically, we replace the conditioning time variable in VP/sub-VP with the additional dimension z in PFGM as the input to the positional embedding. We also need to add an extra scalar output representing the z direction. To this end, we add an additional output channel to the final convolution layer and take the global average pooling of this channel to obtain the scalar. For LSUN bedroom dataset, we both experiments with the channel configurations suggested in NSCN++ <ref type="bibr" target="#b34">[33]</ref> and DDPM <ref type="bibr" target="#b16">[16]</ref>.</p><p>VE/VP/sub-VP We use the same set of hyper-parameters and the NCSN++/DDPM++ (deep) backbone and the continuous-time training objectives for forward SDEs in <ref type="bibr" target="#b34">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Sampling</head><p>We provide more details of PFGM and VE/VP sampling implementations in Appendix B.2.1. We further discuss two techniques used in PFGM ODE sampler: change-of-variable formula (Appendix B.2.2) and the substitution of ground-truth Poisson field direction on z (Appendix B.2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.1 Additional settings</head><p>PFGM For RK-45 sampler, we use the function implemented in scipy.integrate.solve ivp with atol=1e ? 4, rtol=1e ? 4. For forward Euler method, we discretize the ODE with constant step size determined by the number of steps, i.e., step size = (log z max ? log z min )/number of steps for the backward ODE (Eq. (6)). As in <ref type="bibr" target="#b0">[1]</ref>, we set the terminal value of z min = 1e ? 3. We choose z max = 40 (CIFAR-10), 60 (CelebA 64 2 ), 100 (LSUN bedroom) to satisfy the condition ? ? 1 by the multipole expansion analysis in Appendix A.2. The condition ensures that the data distribution can be viewed roughly as a point source at origin. For example, we set z max = 40 on CIFAR-10, and the corresponding ? is greater than 50 with high probability. The hyperparameters work well without further fine tuning. Hence, we hypothesize that PFGM is insensitive to the choice of hyperparameters in a reasonable range, as shown in <ref type="table" target="#tab_4">Table 3</ref>. We clip the norms of initial samples into (0, 3000) for CIFAR-10, (0, 6000) for CelebA and (0, 30000) for LSUN bedroom.</p><p>For selecting z max and clipping upper bound of norms for general datasets, we recommend the following rule-of-thumb. Recall that during the training perturbations (Eq. (5)), given a random initial value z ? N (0, ? 2 ), maximum z is </p><formula xml:id="formula_63">z = z (1 + ? ) M Hence we set z max = E[ z (1 + ? ) M ] = 2 ? ?(1 + ? ) M .</formula><formula xml:id="formula_64">z max = 2 ? ?(1 + ? ) M = 2 ?? E p(x) x 2 2 ? N 3 4 clipping upper value = ? N ?(1 + ? ) M = N ? E p(x) x 2 2 ? N 3 4</formula><p>where N is the data dimension and p(x) is the data distribution. These formulas are easier for practitioner to apply PFGM on new datasets.</p><p>VE/VP/sub-VP For the PC sampler in VE, we follow <ref type="bibr" target="#b34">[33]</ref> to set the reverse diffusion process as the predictor and the Langevin dynamics (MCMC) as the corrector. For VP/sub-VP, we drop the corrector in PC sampler since it only gives slightly better results <ref type="bibr" target="#b34">[33]</ref>. </p><formula xml:id="formula_65">d(x, z) = ( dx dt dt dz dz, dz) = (v(x) x v(x) ?1 z , 1)dz</formula><p>We further use the change-of-variable formula, i.e., t ? = ? log z, to achieve exponential decay on the z dimension:</p><formula xml:id="formula_66">d(x, z) = (v(x) x v(x) ?1 z z, z)dt ?</formula><p>The trajectories of the two ODEs above are the same when dt, dt ? ? 0. We compare the NFE and the sample quality of different ODEs in <ref type="table" target="#tab_5">Table 4</ref>. We measure the NFE/FID of generating 50000 CIFAR-10 samples with the RK45 method in Scipy package <ref type="bibr" target="#b38">[37]</ref>. The batch size is set to 1000. All the numbers are produced on a single NVIDIA A100 GPU. We observe that the ODE with the anchor variable t ? not only accelerates the vanilla by 2 times, but has almost no harm to the sample quality measured by FID score. Since the neural network cannot perfectly learn the ground-truth z direction, we replace the predicted f ? (x) z with the ground-truth direction when z is small. More specifically, givenx = (x, z) ? R N +1 , recall that the empirical field is?(</p><formula xml:id="formula_67">x) = c(x) ? n i=1x ?xi x?x i N +1 where c(x) = 1 ? n i=1 1 x?x i N +1 .</formula><p>Hence we can rewrite the empirical field as?</p><formula xml:id="formula_68">(x) = n i=1 w(x,x i )(x ?x i ) where ? n i=1 w(x,x i ) = ? n i=1 1 x?x i N +1 ? n j=1 1 x?x j N +1 = 1. Furthermore we have ?i, (x ?x i ) z = z ? 0 = z.</formula><p>Together, the z component in the empirical field is?(</p><formula xml:id="formula_69">x) z = ? n i=1 w(x,x i )(x ?x i ) z = z.</formula><p>The predicted normalized field (on x) is trained to approximate the normalized field (on x), i.e.,</p><formula xml:id="formula_70">f ? (x) x ? ? ? N?(x) x ( ??(x) x ? 2 2 +z 2 + ?) ? ? ? N?(x) x ( ??(x) x ? 2 2 + ?)</formula><p>The last approximation is due to</p><formula xml:id="formula_71">??(x) x ? 2 ? z. Solving for ??(x) x ? 2 , we get ??(x) x ? 2 ? ??f ? (x)x?2 ? N 1??f ? (x)x?2 ? N .</formula><p>Hence the z component in the normalized field after substituting the ground-truth</p><formula xml:id="formula_72">is?(x) z ( ??(x) x ? 2 2 +z 2 + ?) = z ( ( ??f ? (x)x?2 ? N 1??f ? (x)x?2 ? N ) 2 + z 2 + ?).</formula><p>In our experiments, we therefore replace the original prediction</p><formula xml:id="formula_73">f ? (x) z with ? ? N z ( ( ??f ? (x)x?2 ? N 1??f ? (x)x?2 ? N ) 2 + z 2 + ?)</formula><p>when z &lt; 5 1 0.1 during the backward ODE sampling for CIFAR-10/CelebA 64 2 /LSUN bedroom 256 2 . <ref type="table" target="#tab_6">Table 5</ref> reports the NFE and FID score w/o and w/ the above substitution. We observe that the usage of ground-truth z direction in the near field accelerates the sampling speed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Evaluation</head><p>We use FID <ref type="bibr" target="#b13">[13]</ref> and Inception scores <ref type="bibr" target="#b30">[29]</ref> to quantitatively measure the sample quality, and NFE (number of evaluation steps) for the inference speed. FID (Fr?chet Inception Distance) score is the Fr?chet distance between two multivariate Gaussians, whose means and covariances are estimated from the 2048-dimensional activations of the Inception-v3 <ref type="bibr" target="#b35">[34]</ref> network for real and generated samples respectively. Inception score is the exponential mutual information between the predicted labels of the Inception network and the images. We also report bits/dim for likelihood evaluation. It is computed by dividing the negative log-likelihood by the data dimension, i.e., bits/dim = ? log p prior (x) N .</p><p>For CIFAR-10, we compute the Fr?chet distance between 50000 samples and the pre-computed statistics of CIFAR-10 dataset in <ref type="bibr" target="#b13">[13]</ref>. For CelebA 64 ? 64, we follow the setting in <ref type="bibr" target="#b33">[32]</ref> where the distance is computed between 10000 samples and the test set. For model selection, we follow <ref type="bibr" target="#b33">[32]</ref> and pick the checkpoint with smallest FID every 50k iterations on 10k samples for computing all the scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Effects of Step Size: FID versus NFE</head><p>For preciseness, <ref type="table" target="#tab_7">Table 6</ref> reports the exact numbers in <ref type="figure">Fig. 5</ref>(c). </p><formula xml:id="formula_74">Since in the ODE d(x, z) = ?(v(x) x v(x) ?1 z z, z)dt ? of PFGM, the z variable is a function of t ? (z = e t ? )</formula><p>, we integrate the z in the Euler method to reduce the discretization error. The vanilla update with red boxes in <ref type="figure" target="#fig_12">Fig. 8</ref>(a) and the remaining images in <ref type="figure" target="#fig_12">Fig. 8(a)</ref> are cleaner samples. The samples within green boxes in <ref type="figure" target="#fig_12">Fig. 8(b)</ref> are noisier samples w/ corrector. Samples on the same spatial locations in the two figures are generated by identical initial latents.</p><formula xml:id="formula_75">from time t ? i to time t ? i+1 is (x i+1 , z i+1 ) = (x i , z i ) ? (v(x i ) x v(x i ) ?1 zi z i , z i )(t ? i+1 ? t ? i ), and the new update is (x i+1 , z i+1 ) = (x i , z i ) ? (v(x i ) x v(x i ) ?1 zi ? (a)</formula><p>The Gaussian kernels in score-based models are N (x, ?(t) 2 ) (VE) and N ( 1 ? ?(t) 2 x, ?(t) 2 ) (VP) <ref type="bibr" target="#b34">[33]</ref>. When ?(t) is large, the norms of perturbed samples are approximately ? N ?(t). The backward ODE could break down if the trajectories diverge from the norm-?(t) relation, as shown by the noisier samples' trajectories in <ref type="figure">Fig. 5(a)</ref>. In contrast, the norm distributions of PFGM is approximately</p><formula xml:id="formula_76">p(? x ?) ? ? x ? N ?2 2 (? x ? 2 2 +z 2 ) N 2</formula><p>when z is large (see deviation for p prior in Appendix A.4), which have a wider span for high density region (see <ref type="figure" target="#fig_2">Fig. 4</ref>). The weak correlation between norm and z makes PFGM more robust on the lighter NCSNv2 backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Extra Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 LSUN Bedroom 256 ? 256</head><p>We report the FID scores and NFEs for LSUN bedroom dataset in <ref type="table" target="#tab_8">Table 7</ref>. We adopt the code base of <ref type="bibr" target="#b34">[33]</ref> in our experiments. In <ref type="bibr" target="#b34">[33]</ref>, they experimented on the LSUN bedroom 256? 256 dataset only on VE-SDE using a deeper NCSN++ backbone. In our DDPM++ architecture, we directly borrow the configuration of channels from the NCSN++ architecture <ref type="bibr" target="#b34">[33]</ref> in each residual block (PFGM w/ NCSN++ channel). We further change z max to 100, as it empirically gives better sample quality.</p><p>We also evaluate the performance when using the configuration of channels in the DDPM <ref type="bibr" target="#b16">[16]</ref> architecture (PFGM w/ DDPM channel). We use the RK45 <ref type="bibr" target="#b7">[7]</ref> solver in the Scipy library <ref type="bibr" target="#b38">[37]</ref> for PFGM sampling. We report the FID score using the evaluation protocol in <ref type="bibr" target="#b5">[5]</ref>.  <ref type="table" target="#tab_8">Table 7</ref> shows that PFGM has comparable performance with VE-SDE when using DDPM channel, while achieving around 15? acceleration. We observe that PFGM achieves a better FID score using the similar configuration in the DDPM model, and converges faster -150k over the total 2.4M training iterations suggested in <ref type="bibr" target="#b34">[33]</ref>. Remarkably, the VE-ODE baseline -the method most comparable to ours -only produces noisy samples on this dataset. It suggests that PFGM is able to scale up to high resolution images when using advanced architectures. We also compare with the number reported in <ref type="bibr" target="#b16">[16]</ref> using similar architecture. Note that DDPM requires 1000 NFE during sampling, and doesn't possess invertibility compared to flow models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Results on NCSNv2 Architecture</head><p>In this section, we demonstrate the image generation on CIFAR-10 and CelebA 64 ? 64, using NCSNv2 architecture <ref type="bibr" target="#b33">[32]</ref>, which is the predecessor of NCSN++ and DDPM++ <ref type="bibr" target="#b34">[33]</ref> and has smaller capacity. Since the VE/VP-ODE has poor performance (FID greater than 90), with the RK45 solver, we also apply the forward Euler method (Euler) with fixed number of steps. We explicitly name the sampler, with forward Euler method as predictor and Langevin dynamics as corrector, as Euler w/ corrector. For Euler w/ corrector in VE/VP-ODE, we use the probability flow ODE (reverse-time ODE) as the predictor and the Langevin dynamics (MCMC) as the corrector. We borrow all the hyper-parameters from <ref type="bibr" target="#b34">[33]</ref> except for the signal-to-noise ratio. We empirically observe the new configurations in <ref type="table" target="#tab_9">Table 8</ref> give better results on the NCSNv2 architecture.</p><p>To accommodate the extra dimension z on NCSNv2, we concatenate the image with an additional constant channel with value z and thus the first convolution layer takes in four input channels. We also add an additional output channel to the final convolution layer and take the global average pooling of this channel to obtain the direction on z.  <ref type="table" target="#tab_10">Table 9</ref> reports the image quality measured by Inception/FID scores and the inference speed measured by NFE on CIFAR-10, using a weaker architecture NCSNv2 <ref type="bibr" target="#b33">[32]</ref>. We show that PFGM with the RK45 solver has competitive FID/Inception scores with the Langevin dynamics, which was the best model on the NCSNv2 architecture before, and requires 10? less NFE. In addition, PFGM performs better than all the other ODE samplers. Our method is more tolerant of sampling error. Among the compared ODEs, our backward ODE (Eq. <ref type="formula" target="#formula_18">(6)</ref>) is the only one that successfully generates high quality samples while the VE/VP-ODE fail w/o the Langevin dynamics corrector. The backward ODE still beats the baselines w/ corrector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2.2 CelebA</head><p>In <ref type="table" target="#tab_1">Table 10</ref>, we report the quality of images generated by models trained on CelebA 64 ? 64, as measured by the FID scores, and the sampling speed, as measured by NFE. We use this dataset as our preliminary experiments hence we only apply NCSNv2 <ref type="bibr" target="#b33">[32]</ref> for different baselines. As shown in <ref type="table" target="#tab_1">Table 10</ref>, PFGM achieves best FID scores than all the baselines on CelebA dataset, while accelerating the inference speed around 20?. Remarkably, PFGM outperforms the Langevin dynamics and reverse-time SDE samplers, which are usually considered better than their deterministic counterparts.</p><p>Remark: On the FID scores on CelebA 64 ? 64 One interesting observation is that the samples of PFGM (RK45) ( <ref type="figure">Fig. 9(b)</ref>) contain more obvious artifacts than Langevin dynamics ( <ref type="figure">Fig. 9(a)</ref>), although PFGM has a lower FID score on the same architecture. We hypothesize that the diversity of samples has larger effects on the FID scores than the artifacts. As shown in <ref type="figure">Fig. 9</ref>(a) and <ref type="figure">Fig. 9(b)</ref>, samples generated by PFGM have more diverse background colors and hair colors than samples of Langevin dynamics. In addition, we evaluate the performance of PFGM on the DDPM++ architecture. We show that the FID score can be further reduced to 3.68 using the more advanced DDPM++ architecture. By examining the generated samples of PFGM on DDPM++ ( <ref type="figure" target="#fig_0">Fig. 13</ref>), we observe that the samples are diverse and exhibit fewer artifacts than PFGM on NCSNv2. It suggests that by using a more powerful architecture like DDPM++, we can remove the artifacts while retaining the diversity in PFGM.</p><p>(a) Langevin dynamics <ref type="bibr" target="#b32">[31]</ref> (b) PFGM (RK45) <ref type="figure">Figure 9</ref>: Uncurated samples from Langevin dynamics <ref type="bibr" target="#b32">[31]</ref> and PFGM (RK45), both using the NCSNv2 architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Wall-clock Sampling Time</head><p>The main bottleneck of sampling time in each ODE step is the function evaluation of the neural network. Hence, for different ODE equations using similar neural network architectures, their inference times per ODE step are approximately the same.</p><p>We implement PFGM on the NCSNv2 <ref type="bibr" target="#b33">[32]</ref>, DDPM++ <ref type="bibr" target="#b34">[33]</ref>, and DDPM++ deep <ref type="bibr" target="#b34">[33]</ref> architectures, with sight modifications to account for the extra dimension z. In <ref type="table" target="#tab_1">Table 11</ref>, we report the sampling time per ODE step method with the DDPM++ backbone, as well as the total sampling time. We measure the sampling time of generating a batch of 1000 images on CIFAR-10. We compare PFGM, VP/sub-VP ODEs using the RK45 solver. As a reference, we also report the results of VP-SDE using the predictor-corrector sampler <ref type="bibr" target="#b34">[33]</ref>. All the numbers are produced on a single NVIDIA A100 GPU. As expected, ODEs using similar architectures and the same solver have nearly the same wall-clock time per ODE step. The table also shows that PFGM achieves the smallest total wall-clock sampling time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Image Interpolations</head><p>The invertibility of the ODE in PFGM enables the interpolations between pairs of images. As shown in <ref type="figure" target="#fig_0">Fig. 10</ref>, we adopt the spherical interpolations between the latent representations of the images in the first and last column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Temperature Scaling</head><p>To demonstrate more utilities of the meaningful latent space of PFGM, we include the experiments of temperature scaling on CelebA 64 ? 64 dataset. We linearly increase the norm of latent codes from 1000 to 6000 to get the samples in <ref type="figure" target="#fig_0">Fig. 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Extended Examples</head><p>We provide extended samples from PFGM on CIFAR-10 ( <ref type="figure" target="#fig_0">Fig. 12</ref>), CelebA 64 ? 64 ( <ref type="figure" target="#fig_0">Fig. 13</ref>) and LSUN bedroom 256 ? 256 ( <ref type="figure" target="#fig_0">Fig. 14)</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Physical Interpretation of the ODEs in PFGM</head><p>In Section 2, in order to move the particles along the electric lines, we set the time derivative of x to the Poisson field E(x): We give the interpretation of the ODEs from a physical perspective. Newton's law implies that the external force is proportional to the acceleration of the particle. In the overdamped limit, e.g., when the particle is moving in honey, the external force is instead proportional to the velocity of the particle, making the equation of motion a first-order ODE. Denoting the viscosity of the fluid as ?, the dynamics of the particle under the influence of the electric field of the source ?(x) is</p><formula xml:id="formula_77">m d 2 x dt 2 = ?? dx dt + qE(x),</formula><p>which has an overdamped limit dx dt = qE(x) when we set t ? ?t and ? ? ?. In this case, a particle with mass m = 1 and charge q = 1 would follow the electric field with velocity equal to E, justifying Eq. (37).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Limitations and Future Directions</head><p>In Section 3.2 we discuss the training paradigm of PFGM, including the normalized Poisson field and the discretized forward ODE. There are several potential improvements. First, the normalized field on mini-batch is biased. In this paper, we directly alleviate the bias by using a larger training batch. However, it does not solve the problem fundamentally. Some potential directions are incorporating more physical tools: we can exploit renormalization to make the Poisson field well-behaved in near fields. Another possibility is to replace a point charge with a quantum particle, whose position uncertainty fills the empty space among nearest neighbor data samples and makes the data manifold smoother.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Potential Social Impact</head><p>Generative models is a rapidly growing field of study with far-reaching implications for science and society. Our work proposes a new generative model that allows for high-quality samples, quick inference and adaptivity. Many downstream applications benefit from our PFGM models' powerful expressive capabilities, particularly those that need fast inference speed and good sample quality at the same time. The usage of these models might have both positive and negative outcomes depending on the downstream use case. For example, PFGM can be incorporated in producing good image/audio samples by the fast backward ODE. This, on the other hand, promotes deepfake technology and leads to social scams. Generative models are also brittle and susceptible to backdoor adversarial attacks on publicly available training data, causing unanticipated failure. Addressing the above concerns requires further research in providing robustness guarantees for generative models as well as close collaborations with researchers in socio-technical disciplines. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) 3D Poisson field trajectories for a heart-shaped distribution (b) The evolvements of a distribution (top) or an (augmented) sample (bottom) by the forward/backward ODEs pertained to the Poisson field.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Uncurated samples on datasets of increasing resolution. From left to right: CIFAR-10 32 ? 32, CelebA 64 ? 64 and LSUN bedroom 256 ? 256.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Sample norm distributions with varying time variables (? for VE-ODE and z for PFGM)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>10 Figure 5 :</head><label>105</label><figDesc>samples w/ corrector l2 norm = (t) N (a) Norm-?(t) in VE-ODE (Euler) (c) FID vs. NFE on CIFAR-(a) Norm-?(t) relation during the backward sampling of VE-ODE (Euler). (b) Norm-z(t ? ) relation during the backward sampling of PFGM (Euler). The shaded areas mean the standard deviation of norms. (c) Number of steps versus FID score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 Formal Proof of Theorem 1</head><label>11</label><figDesc>Before proceeding to Theorem 1, we show a technical lemma that guarantees the existence-uniqueness of the solution to the Poisson equation, under some mild conditions. Lemma 1. Given ? = R N , N ? 3, assume that the source function ? ? C 0 (?), and ? has a compact support. Then the the Poisson equation? 2 ?(x) = ??(x) on ? with zero boundary condition at infinity (lim ?x?2?? ?(x) = 0) has a unique solution ?(x) ? C 2 (?) up to a constant. Proof. For the existence of the solution, one can verify that the analytical construction using the extension of Green's function in N ? 3 dimensional space (Lemma 4), i.e., ?(x) = ? G(x, y)?(y)dy, G(x, y) = 1 (N ?2)S N ?1 (1) 1 x?y N ?2 , is one possible solution to the Poisson equation ? 2 ?(x) = ??(x).Since ? ? C 0 (?) and ? 2 ?(x) = ??(x), we conclude that ?(x) ? C 2 (?).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Lemma 2 .</head><label>2</label><figDesc>Assume the support of the data distribution in the augmented space (supp(p(x))) is a compact set on the z = 0 hyperplane, p(x) ? C 0 and N ? 3.The Poisson equation ? 2 ?(x) = ?p(x) with zero boundary condition at infinity (lim ?x?2?? ?(x) = 0) has a unique solution ?(x) ? C 2 for x ? R N +1 ? supp(p(x)), up to a constant. Proof. Similar to the proof in Lemma 1, one can easily verify that the analytical construction using Green's method, i.e., ?(x) = ? G(x,?)p(x)d?, G(x,?) = 1 (N ?1)S N (1) 1 x?? N ?1 , is one possible solution to the Poisson equation ? 2 ?(x) = ?p(x).Sincep(x) = 0 forx ? R N +1 ? supp(p(x)) and ? 2 ?(x) = ?p(x), we conclude that ?(x) ? C 2 (R N +1 ? supp(p(x))).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4 S 3 S 1 S 2 Figure 6 :</head><label>43126</label><figDesc>d? in = p(x)dA/2 d? d? out = d?/S N (1) Proof idea of Theorem 2. By Gauss's Law, the outflow flux d? out equals the inflow flux d? in . The factor of two in p(x)dA 2 is due to the symmetry of Poisson fields in z &lt; 0 and z &gt; 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2 A. 3</head><label>23</label><figDesc>Extension of Green's Function in N -dimensional Space In this section, we show that the function G(x, y) defined in Eq. (2) is the N -dimensional extension of the Green's function, ?(x) = ? G(x, y)?(y)dy solves the Poisson equation ? 2 ?(x) = ??(x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Lemma 4 .</head><label>4</label><figDesc>Assume the dimension N ? 3, and the source term satisfies ? ? C 0 (?), ? R N ? 2 (x)dx &lt; +?, lim ?x?2?? ?(x) = 0. The extension of Green's function G(x, y) = 1 (N ?2)S N ?1 (1) 1 x?y N ?2 solves the Poisson equation ? 2 x G(x, y) = ??(x ? y). In addition, with zero boundary condition at infinity (lim ?x?2?? ?(x) = 0), ?(x) = ? G(x, y)?(y)dy solves the Poisson equation ? 2 ?(x) = ??(x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>A. 4 Figure 7 :</head><label>47</label><figDesc>Proof for the Prior Distribution on z = z max Hyperplane Diagram of the deviation in Proposition 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>4 lnE p(x) x 2 2 ? N ? 2</head><label>422</label><figDesc>ln 1+? already gives good results, and the corresponding y ? 3000. For example, on CIFAR-10 datasets, N = 3072, ? = 0.03, ? = 0.01, E p(x) x 2 ? 900, we have M =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>For example, on CIFAR-10, ? = 0.03, M = 291, and z max ? 43. The clipping upper value is similarity derived, by setting it to E[ x (1 + ? ) M ] = ? N ?(1 + ? ) M ? 3000, where x ? N (0, ? 2 I N ?N ). By combining Eq. (36), we further have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Samples from VE-ODE (Euler) (b) Samples from VE-ODE (Euler w/ corrector) (a) Samples from VE-ODE (Euler w/o corrector). We highlight the noisier images with red boxes. The rest are cleaner images. (b) Samples from VE-ODE (Euler w/ corrector). We mark the noisier samples after correction with green boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>[q = 1 ,Figure 10 :Figure 11 :</head><label>11011</label><figDesc>forward ODE] dx dt = E(x), [q = ?1, backward ODE] dx dt = ?E(x)(37)Interpolation on CelebA 64 ? 64 by PFGM Temperature scaling on CelebA 64 ? 64 by PFGM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 12 :Figure 13 :Figure 14 :</head><label>121314</label><figDesc>CIFAR-10 samples from PFGM (RK45) CelebA 64 ? 64 samples from PFGM (RK45, NCSNv2 architecture) LSUN bedroom 256 ? 256 samples from PFGM (RK45) using DDPM channel configuration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Learning the normalized Poisson Field Input: Training iteration T , Initial model f ? , dataset D, constant ?, learning rate ?. for t = 1 . . . T do Sample a large batch B L from D and subsample a batch of datapoints B = {x i }</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>CIFAR-10 sample quality (FID, Inception) and number of function evaluation (NFE). Invertible? Inception ? FID ? NFE ?</figDesc><table><row><cell>PixelCNN [36]</cell><cell>4.60</cell><cell>65.9</cell><cell>1024</cell></row><row><cell>IGEBM [8]</cell><cell>6.02</cell><cell>40.6</cell><cell>60</cell></row><row><cell>ViTGAN [24]</cell><cell>9.30</cell><cell>6.66</cell><cell>1</cell></row><row><cell>StyleGAN2-ADA [17]</cell><cell>9.83</cell><cell>2.92</cell><cell>1</cell></row><row><cell>StyleGAN2-ADA (cond.) [17]</cell><cell>10.14</cell><cell>2.42</cell><cell>1</cell></row><row><cell>NCSN [31]</cell><cell>8.87</cell><cell>25.32</cell><cell>1001</cell></row><row><cell>NCSNv2 [32]</cell><cell>8.40</cell><cell>10.87</cell><cell>1161</cell></row><row><cell>DDPM [16]</cell><cell>9.46</cell><cell>3.17</cell><cell>1000</cell></row><row><cell>NCSN++ VE-SDE [33]</cell><cell>9.83</cell><cell>2.38</cell><cell>2000</cell></row><row><cell>NCSN++ deep VE-SDE [33]</cell><cell>9.89</cell><cell>2.20</cell><cell>2000</cell></row><row><cell>Glow [19]</cell><cell>3.92</cell><cell>48.9</cell><cell>1</cell></row><row><cell>DDIM, T=50 [30]</cell><cell>-</cell><cell>4.67</cell><cell>50</cell></row><row><cell>DDIM, T=100 [30]</cell><cell>-</cell><cell>4.16</cell><cell>100</cell></row><row><cell>NCSN++ VE-ODE [33]</cell><cell>9.34</cell><cell>5.29</cell><cell>194</cell></row><row><cell>NCSN++ deep VE-ODE [33]</cell><cell>9.17</cell><cell>7.66</cell><cell>194</cell></row><row><cell>DDPM++ backbone</cell><cell></cell><cell></cell><cell></cell></row><row><cell>VP-SDE [33]</cell><cell>9.58</cell><cell>2.55</cell><cell>1000</cell></row><row><cell>sub-VP-SDE [33]</cell><cell>9.56</cell><cell>2.61</cell><cell>1000</cell></row><row><cell>VP-ODE [33]</cell><cell>9.46</cell><cell>2.97</cell><cell>134</cell></row><row><cell>sub-VP-ODE [33]</cell><cell>9.30</cell><cell>3.16</cell><cell>146</cell></row><row><cell>PFGM (ours)</cell><cell>9.65</cell><cell>2.48</cell><cell>104</cell></row><row><cell>DDPM++ deep backbone</cell><cell></cell><cell></cell><cell></cell></row><row><cell>VP-SDE [33]</cell><cell>9.68</cell><cell>2.41</cell><cell>1000</cell></row><row><cell>sub-VP-SDE [33]</cell><cell>9.57</cell><cell>2.41</cell><cell>1000</cell></row><row><cell>VP-ODE [33]</cell><cell>9.47</cell><cell>2.86</cell><cell>134</cell></row><row><cell>sub-VP-ODE [33]</cell><cell>9.40</cell><cell>3.05</cell><cell>146</cell></row><row><cell>PFGM (ours)</cell><cell>9.68</cell><cell>2.35</cell><cell>110</cell></row></table><note>higher generation quality. Meanwhile, unlike existing ODE baselines that heavily rely on corrector to generate decent samples on weaker architectures, PFGM exhibits greater stability against error (Sec- tion 4.2). Finally, we show that PFGM is robust to the step size in the Euler method (Section 4.3), and its associated ODE allows for likelihood evaluation and image manipulation by editing the latent space (Section 4.4).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, PFGM requires NFEs of 110 whereas the SDE methods typically use 1000 ? 2000 inference steps. PFGM outperforms all the baselines on DDPM++ in all metrics. In addition, PFGM generally samples faster than other ODE baselines with the same RK45 solver. (3) The backward ODE in PFGM is compatible with architectures with varying capacities. PFGM consistently outperforms other ODE baselines on DDPM++ (Table 1) or NCSNv2 (Appendix D.2) backbones. (4) PFGM shows scalability to higher resolution datasets. In Appendix D.1, we show that PFGM are capable of scale-up to LSUN bedroom 256?256. In particular, PFGM has comparable performance with VE-SDE with 15? fewer NFE.InFig. 3, we visualize the uncurated samples from PFGM on CIFAR-10, CelebA 64 ? 64 and LSUN bedroom 256 ? 256. We provides more samples in Appendix E.</figDesc><table><row><cell cols="2">4.2 Failure of VE/VP-ODEs on NCSNv2 architecture</cell><cell></cell><cell></cell></row><row><cell>10 2</cell><cell>|| x || 2 10 3</cell><cell>10 4</cell><cell>z(t')</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Bits/dim on CIFAR-10</figDesc><table><row><cell>bits/dim ?</cell></row><row><cell>RealNVP [6]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>FID scores versus z max on PFGM w/ DDPM++</figDesc><table><row><cell>z max</cell><cell>30</cell><cell>40</cell><cell>50</cell></row><row><cell cols="4">FID score 2.49 2.48 2.48</cell></row><row><cell>B.2.2 Exponential Decay on z Dimension</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Recall that in Section 3.3, we replace the vanilla backward ODE with a new ODE anchored by z:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>NFE and FID scores of different backward ODEs in PFGM</figDesc><table><row><cell cols="3">Algorithm d(x, z) dz d(x, z) dt ?</cell></row><row><cell>NFE</cell><cell>242</cell><cell>104</cell></row><row><cell>FID score</cell><cell>2.53</cell><cell>2.48</cell></row><row><cell cols="3">B.2.3 Substitute the Predicted z Direction with the Ground-truth</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>NFE and FID scores of w/ and w/o substitution</figDesc><table><row><cell cols="3">Algorithm w/o substitution w/ substitution</cell></row><row><cell>NFE</cell><cell>134</cell><cell>104</cell></row><row><cell>FID score</cell><cell>2.48</cell><cell>2.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>The FID scores in Fig. 5(c) of different methods and NFE.</figDesc><table><row><cell>Method / NFE</cell><cell>10</cell><cell>20</cell><cell>50</cell><cell>100</cell></row><row><cell>VP-ODE</cell><cell cols="4">192.36 72.25 38.18 19.73</cell></row><row><cell>DDIM</cell><cell>13.36</cell><cell>6.48</cell><cell>4.67</cell><cell>4.16</cell></row><row><cell>PFGM</cell><cell>14.98</cell><cell>6.46</cell><cell>3.48</cell><cell>2.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>FID/NFE on LSUN bedroom 256 ? 256</figDesc><table><row><cell></cell><cell cols="2">FID ? NFE ?</cell></row><row><cell>StyleGAN [18]</cell><cell>2.65</cell><cell>1</cell></row><row><cell>DDPM [16]</cell><cell>6.86</cell><cell>1000</cell></row><row><cell>VE-SDE [33]</cell><cell>11.75</cell><cell>2000</cell></row><row><cell cols="2">PFGM w/ NCSN++ channel 17.01</cell><cell>134</cell></row><row><cell>PFGM w/ DDPM channel</cell><cell>13.66</cell><cell>122</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Signal-to-noise ratio of different dataset-method pairs</figDesc><table><row><cell>Dataset-Method</cell><cell cols="4">CIFAR-10 -VE CIFAR-10 -VP CelebA -VE CelebA -VP</cell></row><row><cell>signal-to-noise ratio</cell><cell>0.16</cell><cell>0.27</cell><cell>0.12</cell><cell>0.27</cell></row><row><cell>D.2.1 CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>CIFAR-10 sample quality (FID, Inception) and number of function evaluation (NFE). All the methods below the NCSNv2 backbone separator use the NCSNv2<ref type="bibr" target="#b33">[32]</ref> network architecture as the backbone.</figDesc><table><row><cell></cell><cell>Inception ?</cell><cell>FID ?</cell><cell>NFE ?</cell></row><row><cell>PixelCNN [36]</cell><cell>4.60</cell><cell>65.93</cell><cell>1024</cell></row><row><cell>IGEBM [8]</cell><cell>6.02</cell><cell>40.58</cell><cell>60</cell></row><row><cell>WGAN-GP [12]</cell><cell>7.86 ? .07</cell><cell>36.4</cell><cell>1</cell></row><row><cell>SNGAN [26]</cell><cell>8.22 ? .05</cell><cell>21.7</cell><cell>1</cell></row><row><cell>NCSN [31]</cell><cell>8.87 ? .12</cell><cell>25.32</cell><cell>1001</cell></row><row><cell>NCSNv2 backbone</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Langevin dynamics [32]</cell><cell>8.40 ? .07</cell><cell>10.87</cell><cell>1161</cell></row><row><cell>VE-SDE [33]</cell><cell>8.23 ? .02</cell><cell>10.94</cell><cell>1000</cell></row><row><cell>VP-SDE [33]</cell><cell>6.85 ? .01</cell><cell>44.05</cell><cell>1000</cell></row><row><cell>VE-ODE (Euler w/ corrector)</cell><cell>8.05 ? .03</cell><cell>11.33</cell><cell>1000</cell></row><row><cell>VP-ODE (Euler w/ corrector)</cell><cell>7.33 ? .07</cell><cell>37.74</cell><cell>1000</cell></row><row><cell>PFGM (Euler)</cell><cell>8.00 ? .09</cell><cell>11.78</cell><cell>200</cell></row><row><cell>PFGM (RK45)</cell><cell>8.30 ? .05</cell><cell>11.22</cell><cell>118</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>FID/NFE on CelebA 64 ? 64 FID ? NFE ?</figDesc><table><row><cell>NCSN [31]</cell><cell>26.89</cell><cell>1001</cell></row><row><cell>NCSNv2 backbone</cell><cell></cell><cell></cell></row><row><cell>Langevin dynamics [32]</cell><cell>10.23</cell><cell>2501</cell></row><row><cell>VE-SDE [33]</cell><cell>8.15</cell><cell>1000</cell></row><row><cell>VP-SDE [33]</cell><cell>34.52</cell><cell>1000</cell></row><row><cell cols="2">VE-ODE (Euler w/ corrector) 8.30</cell><cell>200</cell></row><row><cell cols="2">VP-ODE (Euler w/ corrector) 41.81</cell><cell>200</cell></row><row><cell>PFGM (Euler)</cell><cell>7.85</cell><cell>100</cell></row><row><cell>PFGM (RK45)</cell><cell>7.93</cell><cell>110</cell></row><row><cell>DDPM++ backbone</cell><cell></cell><cell></cell></row><row><cell>PFGM (RK45)</cell><cell>3.68</cell><cell>110</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Wall-clock sampling time (second) Method PFGM VP-ODE sub-VP-ODE VP-SDE (PC)</figDesc><table><row><cell>NFE</cell><cell>110</cell><cell>134</cell><cell>146</cell><cell>1000</cell></row><row><cell cols="2">Wall-clock time per step 0.526</cell><cell>0.522</cell><cell>0.520</cell><cell>0.491</cell></row><row><cell>Total wall-clock time</cell><cell>57.81</cell><cell>69.97</cell><cell>75.92</cell><cell>490.65</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">A probability distribution p(x) is a special case of "charge density" ?(x) because p(x) need to be nonnegative and integrates to unity. Here we focus on applications to probability distribution of data, which is the objective to be modeled in generative modeling.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t ? i+1 t ? i z(t ? )dt ? , ? t ? i+1 t ? i z(t ? )dt ? ). We empirically observe that the new update scheme significantly improve the FID score.C Failure of VE/VP-ODE on NCSNv2 backboneInFig. 5(a), we demonstrate the trajectories of cleaner samples/noisier samples/noisier samples w/ corrector. We visualize these three groups inFig. 8(a)andFig. 8(b). The noisier samples are marked</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We are grateful to Shangyuan Tong, Timur Garipov and Yang Song for helpful discussion. We would like to thank Octavian Ganea and Wengong Jin for reviewing an early draft of this paper. YX and TJ acknowledge support from MIT-DSTA Singapore collaboration, from NSF Expeditions grant (award 1918839) "Understanding the World Through Code", and from MIT-IBM Grand Challenge project. ZL and MT would like to thank the Center for Brains, Minds, and Machines (CBMM) for hospitality. ZL and MT are supported by The Casey and Family Foundation, the Foundational Questions Institute, the Rothberg Family Fund for Cognitive Science and IAIFI through NSF grant PHY-2019786.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno>abs/1809.11096</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<editor>Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rn-Henrik</forename><surname>Kristjanson Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacobsen</surname></persName>
		</author>
		<title level="m">Residual flows for invertible generative modeling. ArXiv, abs</title>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tian Qi Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">Kristjanson</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
		<idno>abs/1806.07366</idno>
		<title level="m">Neural ordinary differential equations. ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<title level="m">Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. ArXiv, abs/1605.08803</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A family of embedded runge-kutta formulae</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Dormand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="19" to="26" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Implicit generation and generalization in energy-based models. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Safko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Classical mechanics</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ffjord: Free-form continuous dynamics for scalable reversible generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">Kristjanson</forename><surname>Duvenaud</surname></persName>
		</author>
		<idno>abs/1810.01367</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Griffiths</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Introduction to electrodynamics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Flow++: Improving flowbased generative models with variational dequantization and architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno>abs/1902.00275</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Denoising diffusion probabilistic models. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>ArXiv, abs</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semisupervised learning with deep generative models. ArXiv, abs/1406</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5298</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes. CoRR, abs/1312</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6114</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Cifar-10 (canadian institute for advanced research)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Some practical runge-kutta formulas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>F Shampine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Computation</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="135" to="150" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Vitgan: Training gans with vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiwen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/2107.04589</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Semantic segmentation with generative models: Semi-supervised learning and strong out-of-domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8296" to="8307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno>abs/1802.05957</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A modern introduction to differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Ricardo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Fokker-planck equation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Risken</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1606.03498</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Denoising diffusion implicit models. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Generative modeling by estimating gradients of the data distribution. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Improved techniques for training score-based generative models. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Score-based generative modeling through stochastic differential equations. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SSW</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Oriol Vinyals, and Alex Graves. Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauli</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Gommers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Travis</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Haberland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeni</forename><surname>Burovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pearu</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warren</forename><surname>Weckesser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Bright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>St?fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Jarrod</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Millman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mayorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C J</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilhan</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Polat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Feng</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Laxalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Perktold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">Daniel</forename><surname>Cimrman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Henriksen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Quintero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><forename type="middle">M</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ant?nio</forename><forename type="middle">H</forename><surname>Archibald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pedregosa</surname></persName>
		</author>
		<editor>Paul van Mulbregt, Aditya Alessandro Pietro Alex Andreas Andreas Anthony Ant Vijaykumar Bardelli Rothberg Hilboll Kloeckner Sco, Aditya Vijaykumar, Alessandro Pietro Bardelli, Alex Rothberg, Andreas Hilboll, Andre Kloeckner, Anthony M. Scopatz, Antony Lee, Ariel S</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Nathan</forename><surname>Rokem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chad</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Fulton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Masson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clark</forename><surname>H?ggstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>Nicholson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><forename type="middle">V</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Pasechnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Olivetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrice</forename><surname>Wieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Lenders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gert</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><forename type="middle">A</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gert-Ludwig</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">E</forename><surname>Ingold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">R</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irvin</forename><surname>Audren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorg</forename><forename type="middle">P</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Dietrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">T</forename><surname>Silterra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janko</forename><surname>Webber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Slavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Buchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><forename type="middle">L</forename><surname>Kulick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sch?nberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Joscha Reimer</title>
		<imprint/>
		<respStmt>
			<orgName>Jos? Vin?cius de Miranda Cardoso</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan Luis Cano</forename><surname>Harrington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Rodr?guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Nunez-Iglesias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">Lee</forename><surname>Kuczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">Dr</forename><surname>Tritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Thoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Newville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>K?mmerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bolingbroke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Tartre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><forename type="middle">J</forename><surname>Pak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolai</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Nowaczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Shebanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pavlyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Per</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Perry</forename><surname>Brodtkorb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Mcgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Feldbauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Tygier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastiano</forename><surname>Sievert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Vigna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Robitaille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thouis</forename><forename type="middle">Raymond</forename><surname>Spura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Cera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiziano</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Zito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Krauss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><forename type="middle">O</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Halchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>V?zquez-Baeza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="261" to="272" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Scipy 1.0: fundamental algorithms for scientific computing in python</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">From facial parts responses to face detection: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3676" to="3684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<idno>abs/1506.03365</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
