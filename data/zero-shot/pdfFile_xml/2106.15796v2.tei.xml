<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Monocular 3D Object Detection: An Extrinsic Parameter Free Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsong</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzi</forename><surname>Zhu</surname></persName>
							<email>hongzi@sjtu.edu.cnheyuan</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinhong</forename><surname>Jiang</surname></persName>
							<email>jiangqinhong@senseauto.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Monocular 3D Object Detection: An Extrinsic Parameter Free Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Monocular 3D object detection is an important task in autonomous driving. It can be easily intractable where there exists ego-car pose change w.r.t. ground plane. This is common due to the slight fluctuation of road smoothness and slope. Due to the lack of insight in industrial application, existing methods on open datasets neglect the camera pose information, which inevitably results in the detector being susceptible to camera extrinsic parameters. The perturbation of objects is very popular in most autonomous driving cases for industrial products. To this end, we propose a novel method to capture camera pose to formulate the detector free from extrinsic perturbation. Specifically, the proposed framework predicts camera extrinsic parameters by detecting vanishing point and horizon change. A converter is designed to rectify perturbative features in the latent space. By doing so, our 3D detector works independent of the extrinsic parameter variations and produces accurate results in realistic cases, e.g., potholed and uneven roads, where almost all existing monocular detectors fail to handle. Experiments demonstrate our method yields the best performance compared with the other state-of-the-arts by a large margin on both KITTI 3D and nuScenes datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D object detection plays an important role in a variety of computer vision tasks, such as automated driving vehicles, autonomous drones, robotic manipulation, augmented reality applications, etc. Most existing 3D detectors require accurate depth-of-field information. To acquire such resource, majority of the methods resort to the LiDAR pipeline <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b60">60]</ref>, some to the radars solution <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref> or others to the multi-camera framework <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b50">51]</ref>. In this paper, we address this problem in a monocular camera setting and curate it specifically for automated driving scenarios With the difficulty in directly acquiring a depth of field information, monocular 3D detection (Mono3D) is an ill-posed and challenging * Co-corresponding authors <ref type="figure">Figure 1</ref>. The effect of extrinsic parameter perturbations on 3D detection task. When the vehicle undergoes a slight pose change on an uneven road, the 3D detection results are less accurate (second row). This happens often in realistic applications and the detection offset can be viewed more evidently in the bird-eye's view. task. However, Mono3D approaches have the advantage of low cost, low power consumption, and easy-to-deployment in real-world applications. Therefore, monocular 3D detection has received increasing attention over the past few years <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39]</ref>. Current Mono3D methods have achieved considerable high accuracy given a specifically fixed camera coordinate system. However, in real scenarios, the unevenness (perturbation) of the road surface often causes the camera extrinsic parameters to be disturbed, which introduces a significant algorithmic challenge. To the best of our knowledge, there are no 3D detection datasets that takes into account the camera pose change under perturbation.</p><p>As shown in <ref type="figure">Figure 1</ref>, current datasets or detectors assume there is no perturbation, i.e., the extrinsic parameters are set to be constant. Therefore the accurate 3D results are obtained (top row). However, as depicted in the bottom perturbation case, the object information viewed by the camera deviates from the real object information. This makes the detection results unreliable by recovering a large offset in form of both 3D boxes and bird-eye's view. Straightforward methods to address this problem are to design complementary branches or networks to improve the generalization ability, and yet this solution yields limited improvement <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b11">12]</ref>. Some approaches utilize vehicle CAD models or keypoints to reconstruct vehicle geometry <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b49">50]</ref>, while others exploit existing networks to predict pixel-level or instance-level depth map by mimicking stateof-the-art (SOTA) LiDAR 3D detection methods, namely pseudo-LiDAR methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Our work is inspired by the visual odometry methods that resolve camera pose change in adjacent frames from images <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b58">58]</ref>. Note that this idea differentiates from those that focus solely on detecting objects in the perturbation-prone camera coordinate system <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b2">3]</ref>. These approaches focus on some less critical issues regarding to realistic industrial applications. For example, the modeling of occlusive objects <ref type="bibr" target="#b10">[11]</ref>, depth branches <ref type="bibr" target="#b41">[42]</ref>, kinematic motion information (object orientation) <ref type="bibr" target="#b2">[3]</ref>, etc. Moreover, it is similar to human behavior patterns that one can naturally adapt to changing road gradients and gradually deduce the accurate position of objects even on potholes. Formulating our network to encode such learning patterns is feasible on a biological basis.</p><p>In this paper, we propose to leverage the extrinsic parameter change implicitly in the image. Our key idea is to estimate camera pose change w.r.t. the ground plane from images and optimize predicted 3D locations of objects guided by the camera extrinsic geometry constraint. We abbreviate the proposed framework as MonoEF (extrinsic parameter free detector). Specifically, a novel detector is proposed to extract the vanishing point and horizon information from the image to estimate the camera extrinsic corresponding to the image. The model is thus capable of capturing the extrinsic parameter perturbations to which the current image is subjected in the geometric space. During inference, we transform latent feature space using extrinsic parameters as seed to remove the effect of extrinsic perturbations on features fed from the input image. Note that the transformation network is learned in a supervised manner, which allows the image features to recover from camera perturbation. By doing so, we impose our detector exclusive from the effects of the extrinsic parameter. The resultant 3D locations are obtained via the extrinsic parameter-free predictor and projected back into the real-world coordinate system. Experiments on both the KITTI 3D benchmark <ref type="bibr" target="#b14">[15]</ref> and nuScenes dataset <ref type="bibr" target="#b3">[4]</ref> demonstrate that our method outperforms the SOTA methods by a large margin, especially for perturbative examples with a distinguished improvement. To sum up, the contributions of our paper are as follows:</p><p>? We introduce a novel Mono3D detector by capturing the perturbative information of the extrinsic parameters from monocular images to make the detector free from extrinsic fluctuation.</p><p>? We design a feature transformation network, using camera extrinsic parameters as seed, to recover the non-perturbative image information from the perturbative latent feature space.</p><p>? We propose an extrinsic module that complements the camera's pose in 3D object detection. Such a plug-andplay can be applied to existing detectors and pragmatic for industrial applications, e.g., autonomous driving scenarios.</p><p>The whole suite of the codebase will be released and the experimental results will be pushed to the public leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Monocular 3D Object Detection. The Monocular camera is in lacks 3D information compared with multi-beam LiDAR or stereo cameras. To overcome this difficulty and reconstruct the geometry and position of the object in world coordinates, most Mono3D methods can be roughly divided into three categories. In the first category <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b4">5]</ref>, auxiliary information is widely used like vehicle Computer-Aided Design (CAD) models or keypoints. By this means, extra labeling cost is inevitably required. In the second category <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51]</ref>, the prior knowledge like depth map by LiDAR point cloud, or disparity map by stereo cameras trained by external data is exploited. Usually, the inference time would increase significantly due to the prediction of these dense heat maps. Unlike the aforementioned work, methods in the third category only make use of the RGB image as input and remove the dependencies on extra labeling or pre-trained networks by external data. SMOKE <ref type="bibr" target="#b23">[24]</ref> predicts a 3D bounding box by combining a single keypoint estimation with regressed 3D variables based on CenterNet <ref type="bibr" target="#b59">[59]</ref>. M3D-RPN <ref type="bibr" target="#b1">[2]</ref> reformulates the monocular 3D detection problem as a standalone 3D region proposal network. Current SOTA results for monocular 3D object detection are from MonoPair <ref type="bibr" target="#b10">[11]</ref>, Center3D <ref type="bibr" target="#b41">[42]</ref>, and Kinematic3D <ref type="bibr" target="#b2">[3]</ref>. Among them, MonoPair <ref type="bibr" target="#b10">[11]</ref> improves the modeling of occlusive objects by considering the relationship of paired samples. Cen-ter3D <ref type="bibr" target="#b41">[42]</ref> carefully designs two modules for better depth prediction called LID and DepJoint. Kinematic3D <ref type="bibr" target="#b2">[3]</ref> proposes a novel method for monocular video-based 3D object detection which leverages kinematic motion to improve the precision of 3D localization.</p><p>However, all the object detectors mentioned above focus only on the information in the current camera coordinate system that ignores the effect of camera pose on detection. These methods do not work well when the camera's pose receives a disturbance w.r.t. ground plane due to rough terrain or acceleration of ego vehicle. Deep Monocular Odometry. With the success of deep neural networks, end-to-end learning-based methods <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b53">53]</ref> have been proposed to tackle the visual odometry problem. Recently, some methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b57">57]</ref> exploit CNNs to predict the scene depth and camera pose jointly, utilizing the geometric connection between the structure and the motion. This corresponds to <ref type="figure">Figure 2</ref>. System overview. The Extrinsic Regression module (blue block) predicts the ground plane as well as vanishing point. The pose information is thereby obtained and then fed into the Feature Transfer module (yellow block) as guidance for feature enhancement. By doing so, the original features (in gray color) after the backbone are transferred to a rectified set of features (in yellow color), immune to the extrinsic parameter perturbation. The Monocular 3D Detection module and coordinate alignment unit follow standard procedures <ref type="bibr" target="#b23">[24]</ref>.</p><p>learning Structure-from-Motion (SfM) in a supervised manner. To mitigate the requirement of data annotations, selfsupervised and un-supervised methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b58">58]</ref> have been proposed to tackle the SfM task. CC <ref type="bibr" target="#b35">[36]</ref> addresses the unsupervised learning of several interconnected problems in low-level vision: single view depth prediction, camera motion estimation, optical flow, and segmentation of a video into the static scene and moving regions. MonoDepth2 <ref type="bibr" target="#b15">[16]</ref> proposes a set of improvements, which together result in both quantitatively and qualitatively improved depth maps compared to competing for self-supervised methods. LTMVO <ref type="bibr" target="#b61">[61]</ref> presents a selfsupervised learning method for visual odometry with special consideration for consistency over longer sequences.</p><p>While these visual odometry methods are relatively good at detecting camera pose, they all rely on motion information on the time series, which will not be available in a typical Mono3D task based on single-frame images. Consequently, the lack of motion information in the time series prevents us from directly obtaining accurate camera pose information. However, We can still use similar ideas to detect changes in the ground plane and vanishing point from the image compared to the reference frame, and thus indirectly infer changes in the camera extrinsic parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">An Extrinsic Parameter Free Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>We adopt the one-stage anchor-free architecture as does in SMOKE <ref type="bibr" target="#b23">[24]</ref>. <ref type="figure">Figure 2</ref> depicts an overview of our framework. It contains a backbone network, an extrinsic regression network, a feature transfer network, and several taskspecific dense prediction branches. The backbone takes a monocular image of size (W s ?H s ?3) as input and outputs a feature map of size (W ? H ? 64) after down-sampling with an s-factor. The feature map is utilized for extrinsic parameter detection (top blue pipeline), and in parallel rectified by the transfer network based on extrinsic parameters (known as Pose as in the bottom yellow pipeline). For 2D and 3D detection, we follow standard procedures in this domain. There exist seven output branches with each having size of (W ? H ? m), where m is the output channel of each branch. The detection results need to be aligned by the predicted extrinsic parameters in order to get the final bounding box and position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Preliminary on Monocular Object Detection</head><p>The 2D object detection follows the design of Center-Net <ref type="bibr" target="#b59">[59]</ref>. A heatmap of size (W ? H ? c) is used to enable keypoint localization(u g , v g ) and its classification. The number of object categories c equals three on KITTI3D benchmark and ten on nuScenes dataset. The other two branches of size (W ? H ? 2) are adopted to regress the dimensions of the 2D bounding box (w b , h b ) and the offset</p><formula xml:id="formula_0">(? u , ? v ) from the center of the bounding box (u b , v b ) to the keypoint (u g , v g ) correspondingly.</formula><p>The 3D object detection focuses on the 3D information of an object in the local camera coordinate system instead of the global world coordinate system. The object center in local camera coordinate system can be represented as homogeneous coordinates c w = (x, y, z); its projection in the feature map is c o = (u, v, 1). Similar to <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b38">39]</ref>, we predict the offset (? u , ? v ) to the keypoint location (u g , v g ) and depth z in two separate branches. Denote the coordinates in form of congruent concept, we have:</p><formula xml:id="formula_1">z u v 1 T = P ? x y z T ,<label>(1)</label></formula><p>where P is the projection conversion matrix between the world coordinate system and the image coordinate system. <ref type="figure" target="#fig_0">Figure 3</ref>. Visualization of the extrinsic perturbation. The pose of the ego vehicle varies due to the unevenness of road surfaces, which is quite common in realistic scenarios. It causes the camera's viewport i to be inconsistent with ground viewport j. Therefore, the position of keypoints found from the heat map and depth map are shifted from (ui, vi) to (uj, vj) by extrinsic perturbation, leading to a confusion for the 3D prediction and thereby inaccurate results.</p><p>The projection matrix can be decomposed as:</p><formula xml:id="formula_2">P = K ? T,<label>(2)</label></formula><p>where K is referred to as the constant camera intrinsic matrix and T as the inconstancy extrinsic matrix w.r.t ground plane. Naturally we have c o = 1 z Pc w . The depth z and size (w, h, l) are regressed according to <ref type="bibr" target="#b12">[13]</ref>. As aforementioned in Section 3.1, in these branches, the regression components are trained with the L1 loss. Similar to <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b59">59]</ref>, we represent the orientation using eight scalars, where the orientation branch is trained using the multi-bin loss <ref type="bibr" target="#b29">[30]</ref>. Given a specific local camera coordinate system called viewport i, it is normally assumed that the viewport i is consistent with ground plane coordinate system called viewport j, so do most of Mono3D datasets. Suppose the 3D center of a selected object in viewport i is c w i = (x i , y i , z i ), and the 3D center on the feature map is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Theoretical Analysis</head><formula xml:id="formula_3">c o i = (u i , v i , 1)</formula><p>, corresponding to the case as depicted in <ref type="figure" target="#fig_0">Figure 3</ref>. If there is an extrinsic perturbation from the ground plane variation, the identical relation between camera viewport i and ground plane viewport j would no longer exist. We discriminate this process as perturbation. The perturbation matrix A can be described as:</p><formula xml:id="formula_4">A = ? ? cos ? r sin ? r 0 cos ? p sin ? r cos ? r cos ? p sin ? p ? sin ? p sin ? r ? sin ? p cos ? r cos ? p ? ? ,<label>(3)</label></formula><p>where ? p stands for pitch angle and ? r for roll angle of ego vehicle w.r.t. ground plane respectively. Now we are equipped with the extrinsic perturbation being introduced spatially, the center of the object c w i of camera viewport i is transformed to a point c w j in the ground plane viewport j, where c w j = z j P ?1 j c o j = Ac w i . On the feature map, the keypoint of the object shifts correspondingly from c o i to c o j . The transfer relationship matrix M of keypoints on the feature map can be represented by:</p><formula xml:id="formula_5">c o j = Mc o i = z i z j P j AP ?1 i c o i .<label>(4)</label></formula><p>This shift in image coordinates would cause confusion for the prediction of 3D position. Given the example in <ref type="figure" target="#fig_0">Figure 3</ref>, we use the changes on the depth hidden map to perform our analysis. If the model can know the changes that occur in the ground plane coordinate system, such as LiDAR-based methods, it will assume that the target has changed in height. However, for the camera, the height and depth of the target will both affect its position on the image. The camera assumes that the target vehicle remains on the ground coordinate system at all times, so it incorrectly determines that the change in the target on the image is caused by the depth. The offset of keypoints leads to large depth prediction errors.</p><p>These keypoint positions need to be rectified to compen-sate for offsets caused by the change of camera extrinsic parameter. For training, A in Equation <ref type="formula" target="#formula_4">(3)</ref> can be collected through the ground truth vehicle ego-pose information. The image feature w.r.t. viewport H j and labels c w j need to be first adjusted using matrices M ?1 and A ?1 separately in order to eliminate the effect of extrinsic perturbations A. The label obtained by 3D detector f ?i established at camera viewport i can be recorded as? w i . The L1 loss function under external parameter perturbation is changed t?</p><formula xml:id="formula_6">c w i = f ?i (f t (M ?1 , H j )) = f ?i (H i ), L(? j ) = A ?1 c w j ?? w i = c w i ?? w i ,<label>(5)</label></formula><p>where f t (?, ?) is the transfer network on the feature implicit space which maps the change on camera extrinsic parameters to the feature map. During inference, we first estimate camera extrinsic pa-rameters? from input image X j and recover the unperturbed feature hidden space? i from the perturbed feature hidden space H j usingM. The predicted 3D center? w j is derived from the 3D detector f ?i which is independent of varying camera extrinsic parameters?:</p><formula xml:id="formula_7">c w j =?f ?i (f t (M ?1 ,? j )) =?f ?i (? i )).<label>(6)</label></formula><p>For camera extrinsic parameters A and M, we propose the extrinsic regression network, which is introduced in Section 3.4. For feature transfer network f t , the design methodology and training process is described in Section 3.5. These modules are utilized to detect extrinsic perturbations of the image in viewport j, and further adopt the extrinsic information to rectify the feature map. In this way, the image features can be restored back to camera viewport i, and the 3D detection model no longer receives the negative impact from extrinsic perturbations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Camera Extrinsic Parameters Regression</head><p>In addition to the regular regression task, we also introduce a module of extrinsic parameter regression in Mono3D branches, which is shown in <ref type="figure">Figure 2</ref>.</p><p>Owing to the fact that extrinsic parameters are too implicit for a model to regress, we choose to predict intuitive and explicit features from the image at first. The horizon and vanishing point in the image are often used to help determine the vehicle's ego-pose information w.r.t ground plane in the deep visual odometry tasks. Specifically, the tilt of the horizon can indicate the change of roll angle, while the vertical movement of the vanishing point can indicate the change of pitch angle.</p><p>Following the SOTA odometry framework in <ref type="bibr" target="#b5">[6]</ref>, we represent a regression task with L1 loss as:</p><formula xml:id="formula_8">[? gp ,? vp ] = f vo (H j ), L vo = A ? g(? gp ,? vp ) .<label>(7)</label></formula><p>Here, f vo is the CNN architecture used for horizon and vanishing point detection, we follow <ref type="bibr" target="#b18">[19]</ref> and make modifications to the filters for the fully connected layers.? gp and y vp are the predicted ground plane and vanishing point results at viewport j. The mapping function g : (R 2 , R 2 ) ? A 4?4 is a mathematical calculation function from the horizon and vanishing point to the camera extrinsic matrix. The function f vo ensures that the model can give sufficiently accurate information about the extrinsic parameters. Finally, the regression loss L vo can be trained jointly with 2D and 3D detection branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Feature Transfer by Extrinsic Parameters</head><p>To overcome the pose variation of ego vehicle w.r.t ground plane and improve 3D detection performance, we propose a transfer network applying camera extrinsic corrections on the feature latent layers. Generally speaking, as shown in <ref type="figure" target="#fig_0">Figure 3</ref>, the design intention of the transfer network is to rectify the perturbed feature space H j in camera view j, so that the discrepancy between H j and the unperturbed one H i under camera view i is as small as possible. For example, we fix the shift of keypoints caused by extrinsic parameter perturbations. Suppose that in one image with unknown perturbation, the network predicts camera extrinsic parameters? = g(f vo (H j )) based on the strategy in Section 3.4.</p><p>After carefully analyzing the influence of perturbation on the image characteristics of low-dimensional features and high-dimensional features, we find out that their changing patterns are quite different. On the one hand, lowdimensional features like the position of corresponding edges and geometries are closely related to the camera's extrinsic parameters, specifically in terms of content information. On the other hand, high-dimensional features like the textures and illuminations remain unchanged, specifically in terms of style information. Inspired by the image style transfer method <ref type="bibr" target="#b16">[17]</ref>, we propose a feature transfer module working on the feature latent space.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 4</ref>, this module is divided into two parts. One is the transfer network f t , and the other is a pre-trained loss network ? using <ref type="bibr" target="#b39">[40]</ref>.</p><p>The transfer network. The input feature map H in for transfer network is provided by the previous backbone, which is equal to H j . The predicted poseM acts as a guidance information for transfer network, which provides structural information for feature maps in low dimensions. The output of transfer network H out will be input into loss network with content target H content = f b (M ?1 X j ) and style target H style = H j to calculate final features, where X j stands for disturbed image input, and f b stands for backbone network.</p><p>The loss network. The transfer network f t mainly considers content loss l content and style loss l style . Let ? m be the activation of the m-th layer of the network ? with the </p><formula xml:id="formula_9">l ?,m content (H out , H content ) = ? m (H out ) ? ? m (H content ) 2 2 c m h m w m .</formula><p>(8) Following <ref type="bibr" target="#b13">[14]</ref>, we define the Gram matrix G ? m to be the c m ? c m matrix whose elements are given by:</p><formula xml:id="formula_10">G ? m (H) c,c = hm h=1 wm w=1 ? m (H) h,w,c ? m (H) h,w,c c m h m w m .<label>(9)</label></formula><p>The Gram matrix can be computed by reshaping ? m (H) into a matrix ?, then G ? m (H) = ?? T /c m h m w m . The style reconstruction loss is then the squared Frobenius norm of the difference between the Gram matrices of the output and target feature maps:</p><formula xml:id="formula_11">l ?,m style (H in , H style ) = G ? m (H in ) ? G ? m (H style ) 2 F . (10)</formula><p>The l content penalizes the output feature map when it deviates in content from the target and l style penalizes differences in style. The joint total loss is defined as:</p><formula xml:id="formula_12">L total = ? 1 l content + ? 2 l style ,<label>(11)</label></formula><p>where ? 1 and ? 2 are hyper-parameters for tuning content loss and style loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Setup</head><p>We conduct experiments on the KITTI3D object detection dataset, KITTI odometry dataset and nuScenes dataset. The KITTI3D dataset does not collect camera extrinsic information, which means its T matrix is an identity matrix. We can only find vehicle ego-pose information from the KITTI odometry and nuScenes datasets.</p><p>For the evaluation and ablation study, we show experimental results from two different setups. Baseline is derived from SMOKE <ref type="bibr" target="#b23">[24]</ref> with an additional output branch for camera extrinsic parameters. MonoEF is the final proposed method integrating seven prediction branches, camera extrinsic parameter regression branch, and camera extrinsic amendment network.</p><p>For the rest of the detailed dataset statistics, training and inference structure, learning rules, evaluation metrics, etc., please refer to the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quantitative and Qualitative Results</head><p>We first show the performance of our proposed MonoEF on KITTI3D object detection benchmark * for car. Comparison results with other state-of-the-art (SOTA) monocular 3D detectors including M3D-RPN <ref type="bibr" target="#b1">[2]</ref>, SMOKE <ref type="bibr" target="#b23">[24]</ref>, MonoPair <ref type="bibr" target="#b10">[11]</ref>, PathNet <ref type="bibr" target="#b24">[25]</ref>, D4LCN <ref type="bibr" target="#b11">[12]</ref> and Kine-matic3D <ref type="bibr" target="#b2">[3]</ref> are shown in <ref type="table" target="#tab_0">Table 1</ref>. AP 2D and AOS are metrics for 2D object detection and orientation estimations following the benchmark. We achieve the highest score for all kinds of samples and rank in first place among those 3D monocular object detectors on other metrics, regardless our model is only comparable or a bit worse than SOTA detector MonoPair <ref type="bibr" target="#b10">[11]</ref> on AP 2D . Our method outperforms Kinematic3D for a large margin in AP 3D and AP BV , especially for Hard samples. The comparison of results fully proves the effectiveness of the proposed camera extrinsic amendment for images with unknown perturbations. <ref type="table">Table 2</ref> shows the performance on KITTI3D validation set for the car with and without camera extrinsic perturbation. Since the KITTI3D dataset is initially without perturbation information of the camera pose, we simulate the camera extrinsic parameter perturbation in the real world using an artificially set Gaussian function (pitch, roll ? N (0, 1)). We evaluate the related values of SOTA monocular detectors through their published detection models. It  <ref type="table">Table 2</ref>. AP40 scores(%) on KITTI3D validation set for car at 0.5 IoU threshold before and after camera extrinsic disturbance. The lower the decreased value, the better the performance. The original target coordinates are transformed according to the pitch and roll angle set by the artificial extrinsic perturbation. The input image is also processed using the projection transformation according to these angles.  can be noticed that the detection performance of all models is degraded more or less after the addition of the extrinsic perturbation. The other models are quite sensitive to extrinsic perturbations, with very severe performance degradation, while our model only has a slight performance drop. This demonstrates the effectiveness of our model in handling camera extrinsic perturbations. <ref type="figure" target="#fig_2">Figure 5</ref> shows the qualitative results on KITTI odometry dataset. In this dataset we can get the vehicle pose information, so we know the real-world extrinsic parameter perturbations to which the vehicle is subjected. The parts drawn with dashed lines indicate that our model has good performance against perturbations, especially in depth estimation.</p><p>Since there is no open source code the more challenging nuScenes dataset by time, we only evaluate our model on it, which is shown in <ref type="table" target="#tab_2">Table 3</ref>. From this dataset, we can Category</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Angular Error ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiple frames</head><p>CC <ref type="bibr" target="#b35">[36]</ref> 0.0320 MonoDepth2 <ref type="bibr" target="#b15">[16]</ref> 0.0312 LTMVO <ref type="bibr" target="#b61">[61]</ref> 0.0142</p><p>Single frame MonoEF 0.0287 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We conduct several ablation studies for different evaluation items and data settings. We only show results from M oderate samples here.</p><p>Time expense analysis. Other Mono3D models may require some additional operations to assist the prediction during inference, such as generating pseudo-lidar <ref type="bibr" target="#b2">[3]</ref>, generating pairs <ref type="bibr" target="#b10">[11]</ref>, etc. Compared to these methods, Mo-noEF is based on the SMOKE <ref type="bibr" target="#b23">[24]</ref> with modified extrinsic parameters and only needs to go through a backbone network during the inference process. We can see from <ref type="figure">Figure  1</ref> that our method also has a great advantage in time expense.</p><p>Camera pose detection. For camera extrinsic parameters regression study, we evaluate the angular errors of the MonoEF on the KITTI odometry verification sequence 08, comparing with SOTA monocular visual odometry methods including CC <ref type="bibr" target="#b35">[36]</ref>, MonoDepth2 <ref type="bibr" target="#b15">[16]</ref> and LTMVO <ref type="bibr" target="#b61">[61]</ref>. The evaluation results shown in <ref type="table" target="#tab_3">Table 4</ref> indicate that although our model is not specifically designed to implement visual odometry functionality, it is also possible to predict accurate camera poses and achieve SOTA performance on the KITTI odometry dataset. This ensures the accuracy of the camera extrinsic parameters regression.</p><p>Camera extrinsic amendment. In terms of the camera extrinsic amendment study, we perform performance comparison experiments on sequences of the KITTI odometry dataset shown in <ref type="table" target="#tab_4">Table 5</ref>. Because the odometry dataset does not contain a 3D detection label, we used the point cloud detection model 3DSSD <ref type="bibr" target="#b55">[55]</ref> to formulate the ground truth. For the detection task training on the single sequence and multi sequences, our model shows a substantial improvement on performance with the camera extrinsic  <ref type="table">Table 6</ref>. AP40 scores(%) evaluated on KITTI Odometry sequecnce 00 for SOTA methods, including M3D-RPN <ref type="bibr" target="#b1">[2]</ref>, Kinematic3D <ref type="bibr" target="#b23">[24]</ref> and SMOKE <ref type="bibr" target="#b2">[3]</ref>. +E.F. indicates that we apply the transfer network to feature maps by extrinsic regression network to the original method. amendment compared to the baseline. The improvement is more pronounced on a single sequence since the initial frame of different sequences in KITTI odometry dataset can not assure a consistent camera pose w.r.t. ground plane, which would confuse the extrinsic regression network. We apply our MonoEF to other SOTA detection models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b2">3]</ref> and achieve similar significant improvements, which is shown in <ref type="table">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a novel method for monocular 3D object detection with two camera-extrinsic-aware modules, namely the extrinsic regression net and the feature transfer net. By capturing the camera pose change from image w.r.t ground plane and performing a corresponding amendment for the naturally ill-posed Mono3D detection, our method is robust against camera extrinsic perturbation and helps model predict much more accurate depth results. Our model achieves the state-of-the-art performance on KITTI3D object detection benchmark using a monocular camera and proves its efficiency on KITTI odometry and nuScenes dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3</head><label>3</label><figDesc>depicts a concrete example of how an extrinsic perturbation can significantly impose poor prediction onto heat map and depth map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>The training process of the transfer network f t . The feature target is derived from a feature obtained by the backbone after a direct extrinsic parameter correction of the image. The pretrained loss network ? has two branches, one with the style target for high-dimensional losses computed on three layers and the other with the content target for low-dimensional losses computed only on the last layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative results on KITTI odometry dataset. The prediction 3D bounding boxes of SMOKE (the one above) and our model (the one below) are shown under camera extrinsic perturbation in the images. Green boxes and orange boxes in bird view mean ground truth and predictions of cars. A more pronounced difference in the predictions appears where the dashed line is circled. It can be seen from the figure that our model is effective against the perturbation of the external participants, especially for depth prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>14.49 12.75 92.94 87.02 77.12 93.21 87.51 77.66 14.83 12.89 91.65 86.11 76.45 96.61 93.55 83.55 AP40 scores(%) and runtime(s) on KITTI3D test set for car at 0.7 IoU threshold referred from the KITTI benchmark website. E, M and H represent Easy , Moderate and Hard samples.Our model not only ranks first on the 3D evaluation metrics but also keeps the run time fairly low and comparable as a simple one-stage detection. Corner information might be cropped and padded by feature transferring and correction so that the performance of 2D detection is slightly affected.</figDesc><table><row><cell>H</cell><cell>E</cell><cell>M</cell><cell>H</cell></row></table><note>feature map of shape (c m ? h m ? w m ). The content fea- ture reconstruction loss is the squared Euclidean distance between feature representations:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>http://www.cvlibs.net/datasets/kitti/eval object.php?obj benchmark=3d</figDesc><table><row><cell>Methods</cell><cell>Test Data</cell><cell>E</cell><cell>AP3D M</cell><cell>H</cell><cell>E</cell><cell>APBV M</cell><cell>H</cell><cell>E</cell><cell>AP2D M</cell><cell>H</cell></row><row><cell></cell><cell>original</cell><cell>64.91</cell><cell>50.53</cell><cell>41.73</cell><cell>69.28</cell><cell>53.64</cell><cell>45.12</cell><cell>91.88</cell><cell>93.11</cell><cell>77.24</cell></row><row><cell>M3D-RPN [2]</cell><cell>disturbed</cell><cell>39.72</cell><cell>31.08</cell><cell>25.73</cell><cell>48.37</cell><cell>38.55</cell><cell>32.22</cell><cell>92.20</cell><cell>93.14</cell><cell>77.13</cell></row><row><cell></cell><cell>decrease</cell><cell cols="5">-25.19 -19.45 -16.00 -20.91 -15.09</cell><cell>-12.9</cell><cell>0.32</cell><cell>0.03</cell><cell>-0.11</cell></row><row><cell></cell><cell>original</cell><cell>77.89</cell><cell>72.80</cell><cell>65.37</cell><cell>83.30</cell><cell>82.92</cell><cell>75.76</cell><cell>99.50</cell><cell>99.05</cell><cell>90.55</cell></row><row><cell>SMOKE [24]</cell><cell>disturbed</cell><cell>42.58</cell><cell>35.09</cell><cell>30.74</cell><cell>53.01</cell><cell>44.15</cell><cell>39.41</cell><cell>98.66</cell><cell>98.12</cell><cell>89.84</cell></row><row><cell></cell><cell>decrease</cell><cell cols="6">-35.31 -37.71 -34.63 -30.29 -38.77 -36.35</cell><cell>-0.85</cell><cell>-0.92</cell><cell>-0.71</cell></row><row><cell></cell><cell>original</cell><cell>61.54</cell><cell>45.60</cell><cell>37.77</cell><cell>68.32</cell><cell>51.68</cell><cell>39.31</cell><cell>97.35</cell><cell>89.1</cell><cell>71.51</cell></row><row><cell>D4LCN [12]</cell><cell>disturbed</cell><cell>41.77</cell><cell>29.22</cell><cell>25.78</cell><cell>59.9</cell><cell>43.45</cell><cell>36.06</cell><cell>85.38</cell><cell>76.64</cell><cell>60.03</cell></row><row><cell></cell><cell>decrease</cell><cell cols="3">-19.77 -16.38 -11.99</cell><cell>-8.42</cell><cell>-8.23</cell><cell>-3.25</cell><cell cols="3">-11.97 -12.46 -11.48</cell></row><row><cell></cell><cell>original</cell><cell>55.45</cell><cell>39.47</cell><cell>31.29</cell><cell>61.72</cell><cell>44.65</cell><cell>34.58</cell><cell>98.61</cell><cell>86.3</cell><cell>71.39</cell></row><row><cell>Kinematic3D [3]</cell><cell>disturbed</cell><cell>27.30</cell><cell>16.95</cell><cell>13.79</cell><cell>47.78</cell><cell>36.70</cell><cell>29.24</cell><cell>90.84</cell><cell>55.13</cell><cell>44.80</cell></row><row><cell></cell><cell>decrease</cell><cell cols="4">-28.15 -22.52 -17.50 -13.94</cell><cell>-7.95</cell><cell>-5.34</cell><cell>-7.77</cell><cell cols="2">-31.17 -26.59</cell></row><row><cell></cell><cell>original</cell><cell>77.55</cell><cell>72.83</cell><cell>72.01</cell><cell>82.33</cell><cell>82.80</cell><cell>75.61</cell><cell>99.56</cell><cell>99.19</cell><cell>90.62</cell></row><row><cell>MonoEF</cell><cell>disturbed</cell><cell>76.87</cell><cell>70.86</cell><cell>63.86</cell><cell>81.64</cell><cell>74.76</cell><cell>73.92</cell><cell>99.65</cell><cell>99.15</cell><cell>90.62</cell></row><row><cell></cell><cell>decrease</cell><cell>-0.68</cell><cell>-1.97</cell><cell>-8.16</cell><cell>-0.68</cell><cell>-8.04</cell><cell>-1.70</cell><cell>0.09</cell><cell>-0.04</cell><cell>-0.01</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Evaluation errors on the nuScenes test dataset. Our errors are lower on the three representative categories selected. In overall classes, our error is much lower than baseline.</figDesc><table><row><cell>Class</cell><cell cols="4">Methods ATE ? ASE ? AOE ?</cell></row><row><cell>car</cell><cell>baseline MonoEF</cell><cell>0.73 0.56</cell><cell>0.16 0.15</cell><cell>0.09 0.09</cell></row><row><cell>pedestrian</cell><cell>baseline MonoEF</cell><cell>0.85 0.71</cell><cell>0.32 0.31</cell><cell>1.48 0.99</cell></row><row><cell>motorcycle</cell><cell>baseline MonoEF</cell><cell>0.84 0.70</cell><cell>0.23 0.23</cell><cell>0.86 0.79</cell></row><row><cell>overall</cell><cell>baseline MonoEF</cell><cell>0.87 0.77</cell><cell>0.57 0.37</cell><cell>0.75 0.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Angular Error(deg/m) on KITTI Odometry validation sequence 08. Methods designed specifically for the odometry task use information from consecutive frames to detect pose, and we have achieved comparable detection accuracy by doing the detection only on a single frame. get ego car pose information. The table shows smoke of the more representative categories in the dataset, and we can see that our model's prediction errors on these categories have decreased compared to the baseline. Across all categories, our model reduced the overall ATE and ASE quite a lot. This demonstrates the enhancement of our model for the 3D detection task on the nuScenes dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>AP40 scores(%) evaluated on KITTI Odometry sequecnce 00 (trained on single sequence 00) and sequecnce 08 (trained on sequence 00-07 &amp; 09-10) for car.</figDesc><table><row><cell>Data</cell><cell cols="3">Methods AP3D APBV</cell><cell>AP2D</cell></row><row><cell>Single Seq.</cell><cell cols="2">baseline MonoEF 41.78 34.98</cell><cell>43.29 52.78</cell><cell>80.51 79.33</cell></row><row><cell>Multiple Seq.</cell><cell cols="2">baseline MonoEF 26.06 23.46</cell><cell>26.51 32.43</cell><cell>75.41 80.21</cell></row><row><cell>Methods</cell><cell></cell><cell cols="2">AP3D APBV</cell><cell>AP2D</cell></row><row><cell>M3D-RPN [2]</cell><cell></cell><cell>36.13</cell><cell>42.88</cell><cell>67.49</cell></row><row><cell cols="2">M3D-RPN [2] + E.F.</cell><cell>41.36</cell><cell>43.41</cell><cell>67.57</cell></row><row><cell>Kinematic3D [3]</cell><cell></cell><cell>41.44</cell><cell>43.51</cell><cell>65.70</cell></row><row><cell cols="2">Kinematic3D [3] + E.F.</cell><cell>48.92</cell><cell>51.39</cell><cell>66.92</cell></row><row><cell>SMOKE [24]</cell><cell></cell><cell>34.98</cell><cell>43.29</cell><cell>80.51</cell></row><row><cell cols="3">SMOKE [24] + E.F. (MonoEF) 41.78</cell><cell>52.78</cell><cell>79.33</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported in part by National Key R&amp;D Program of China (Grant No. 2018YFC1900700) and National Natural Science Foundation of China (Grants No. 61772340).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Codeslam-learning a compact, optimisable representation for dense visual slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bloesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Czarnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Davison</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">M3d-rpn: Monocular 3d region proposal network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV), October</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Kinematic 3d object detection in monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09548</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11027</idno>
		<title level="m">nuscenes: A multimodal dataset for autonomous driving</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaonary</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?line</forename><surname>Teuli?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Chateau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2040" to="2049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deepvp: Deep learning for vanishing point detection on 1 million street view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Kai</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaping</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Monocular 3d Object Detection for Autonomous Driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2147" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d Object Proposals Using Stereo Imagery for Accurate Object Class Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1259" to="1272" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Monopair: Monocular 3d object detection using pairwise spatial relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="12093" to="12102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning depth-guided convolutions for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1000" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Texture synthesis using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3828" to="3838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Lowlevel sensor fusion for 3d vehicle detection using radar range-azimuth heatmap and monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyeong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsuk</forename><surname>Kum</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00851,2020.1</idno>
		<title level="m">Deep learning on radar centric 3d object detection</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stereo r-cnn based 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7644" to="7652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Undeepvo: Monocular visual odometry through unsupervised deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbing</forename><surname>Gu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep Continuous Fusion for Multi-sensor 3d Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="663" to="678" />
			<date type="published" when="2018" />
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Smoke: Singlestage monocular 3d object detection via keypoint estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>T?th</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="996" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.04582</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Rethinking pseudo-lidar representation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Accurate monocular 3d object detection via color-embedded 3d reconstruction for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengbo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>Wanli Ouyang, and Xin Fan</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Vehicle detection with automotive radar using deep learning on range-azimuth-doppler tensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bence</forename><surname>Major</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fontijne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><forename type="middle">Teja</forename><surname>Sukhavasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhika</forename><surname>Gowaikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slawomir</forename><surname>Grzechnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sundar</forename><surname>Subramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Roi-10d: Monocular lifting of 2d detection to 6d pose and metric shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3d Bounding Box Estimation Using Deep Learning and Geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7074" to="7082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reconstructing vehicles from a single image: Shape priors for road scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Falak</forename><surname>Gv Sai Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K Madhava</forename><surname>Chhaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust object proposals re-ranking for object detection in autonomous driving using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Cuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Wook</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing: Image Communication</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="110" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgbd data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Monogrnet: A geometric reasoning network for monocular 3d object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengyi</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8851" to="8858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Triangulation learning network: From monocular to stereo 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengyi</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Balles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="12240" to="12249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Roarnet: A robust 3d object detection based on region approximation refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiwoo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwook</forename><surname>Paul Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2510" to="2515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Disentangling monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Lopez-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Ba-net: Dense bundle adjustment network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengzhou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Dorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiragkumar</forename><surname>Savani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.13423</idno>
		<title level="m">Cen-ter3d: Center-based monocular 3d object detection with joint depth understanding</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Deepv2d: Video to depth with differentiable structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Demon: Depth and motion network for learning monocular stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Refinedmpl: Refined monocular pseudolidar for 3d object detection in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean Marie Uwabeza</forename><surname>Vianney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubhra</forename><surname>Aich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09712</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Recurrent neural network for (un-) supervised learning of monocular video visual odometry and depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Pizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frahm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Deepvo: Towards end-to-end visual odometry with deep recurrent convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">End-to-end, sequence-to-sequence probabilistic visual odometry through deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">IJRR</biblScope>
			<biblScope unit="page" from="513" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07179</idno>
		<idno>arXiv: 1812.07179. 2</idno>
		<title level="m">Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3d Object Detection for Autonomous Driving</title>
		<imprint>
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04693</idno>
		<idno>arXiv: 1604.04693</idno>
		<title level="m">Subcategory-aware Convolutional Neural Networks for Object Proposals and Detection</title>
		<imprint>
			<date type="published" when="2017-03" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Multi-level Fusion Based 3d Object Detection from Monocular Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2345" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Guided feature selection for deep visual odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Zha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Beyond tracking: Selecting memory and refining poses for deep visual odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunkai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Zha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Radarnet: Exploiting radar for robust perception of dynamic objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runsheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.14366</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Point-based 3d single stage object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="11040" to="11048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huangying</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chamara</forename><surname>Saroj Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kejie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Deeptam: Deep tracking and mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<idno>arXiv: 1904.07850. 2</idno>
		<title level="m">Objects as Points</title>
		<imprint>
			<date type="published" when="2019-04" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Learning monocular visual odometry via self-supervised long-term modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc-Huy</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10983</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
