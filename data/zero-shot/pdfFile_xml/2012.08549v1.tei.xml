<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring Transfer Learning For End-to-End Spoken Language Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subendhu</forename><surname>Rongali</surname></persName>
							<email>srongali@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Amazon Alexa AI</orgName>
								<address>
									<region>New York</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beiye</forename><surname>Liu</surname></persName>
							<email>beiyeliu@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Amazon Alexa AI</orgName>
								<address>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Amazon Alexa AI</orgName>
								<address>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantine</forename><surname>Arkoudas</surname></persName>
							<email>arkoudk@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Amazon Alexa AI</orgName>
								<address>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengwei</forename><surname>Su</surname></persName>
							<email>chengwes@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Amazon Alexa AI</orgName>
								<address>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
							<email>waelhamz@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Amazon Alexa AI</orgName>
								<address>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring Transfer Learning For End-to-End Spoken Language Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Voice Assistants such as Alexa, Siri, and Google Assistant typically use a two-stage Spoken Language Understanding pipeline; first, an Automatic Speech Recognition (ASR) component to process customer speech and generate text transcriptions, followed by a Natural Language Understanding (NLU) component to map transcriptions to an actionable hypothesis. An end-to-end (E2E) system that goes directly from speech to a hypothesis is a more attractive option. These systems were shown to be smaller, faster, and better optimized. However, they require massive amounts of end-to-end training data and in addition, don't take advantage of the already available ASR and NLU training data. In this work, we propose an E2E system that is designed to jointly train on multiple speech-to-text tasks, such as ASR (speech-transcription) and SLU (speech-hypothesis), and text-to-text tasks, such as NLU (text-hypothesis). We call this the Audio-Text All-Task (AT-AT) Model and we show that it beats the performance of E2E models trained on individual tasks, especially ones trained on limited data. We show this result on an internal music dataset and two public datasets, FluentSpeech and SNIPS Audio, where we achieve state-of-the-art results. Since our model can process both speech and text input sequences and learn to predict a target sequence, it also allows us to do zero-shot E2E SLU by training on only text-hypothesis data (without any speech) from a new domain. We evaluate this ability of our model on the Facebook TOP dataset and set a new benchmark for zeroshot E2E performance. We will soon release the audio data collected for the TOP dataset for future research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>In recent years, there has been a dramatic surge in the adoption of voice assistants such as Amazon Alexa, Apple Siri, and Google Assistant. Customers use them for a variety of tasks such as playing music and online shopping.</p><p>These voice assistants are built on complex Spoken Language Understanding (SLU) systems that are typically too large to store on an edge device such as a mobile phone or a smart speaker. Hence, user traffic is routed through a cloud server to process requests. This has led to privacy concerns and fueled the push for tiny AI and edge processing, where the user requests are processed on the device itself.</p><p>Traditional SLU systems consist of a two-stage pipeline, an Automatic Speech Recognition (ASR) component that processes customer speech and generates a text transcription (ex. play the song watermelon sugar), followed by a Natural Language Understanding (NLU) component that maps the transcription to an actionable hypothesis consisting of intents and slots (ex. Intent: PlaySong, Slots: SongNamewatermelon sugar). An end-to-end (E2E) system that goes directly from speech to the hypothesis would help make the SLU system smaller and faster, allowing it to be stored on an edge device. It could potentially also be better optimized than a pipeline since it eliminates cascading errors.</p><p>However, E2E systems are not used in practice because they have some key issues. These systems are hard to build since they consist of large neural components such as transformers and require massive amounts of E2E training data. They also don't make use of the vastly available training data for the ASR and NLU components that could be used to enhance their performance, because the examples in these datasets may not be aligned to create an E2E training sample. Another issue is feature expansion, a scenario where a new domain, with new intents and slots, is added to the voice assistant's capabilities. Here, developers typically only have access to some synthetically generated text-hypothesis examples. Speech data isn't readily available and it is very expensive to collect. E2E models thus fail as they require lots of new audio and hypothesis data to learn this new domain.</p><p>In this work, we build an E2E model that mitigates these issues using transfer learning. We call it the Audio-Text All-Task (AT-AT) Model. AT-AT is an E2E transformerbased model that is jointly trained on multiple audio-totext and text-to-text tasks. Examples of these tasks include speech recognition (ASR), hypothesis prediction from speech (SLU), masked LM prediction (MLM), and hypothesis prediction from text (NLU). Our model achieves this by converting data from all these tasks into a single audio-totext or text-to-text format. <ref type="figure">Figure 1</ref> shows this joint training phase in detail. Our findings indicate that there is significant knowledge transfer taking place from multiple tasks, which in turn helps in downstream model performance. We see that the AT-AT pretrained model shows improved performance on SLU hypothesis prediction on internal data col-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Target Decoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Audio Encoder</head><p>Text Encoder</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AT-AT</head><p>PlayMusic( okay Artist( rihanna )Artist )PlayMusic it was a glorious morning to wake up to an old fashioned bar was close by PlayMusic( play Song( don't stop believing )Song )PlayMusic play don't stop believing SLU ASR MLM NLU <ref type="figure">Figure 1</ref>: Pretraining AT-AT with audio-to-text and text-to-text tasks. The audio and text inputs go to separate encoders but share a joint decoder, which decodes the target sequence based on the task. Task labels are passed as BOS tokens while decoding. lected from Alexa traffic. We also report state-of-the-art results on two public datasets: FluentSpeech <ref type="bibr" target="#b12">(Lugosch et al. 2019)</ref>, and SNIPS Audio <ref type="bibr" target="#b19">(Saade et al. 2018)</ref>.</p><p>Furthermore, since our model contains a text encoder, it can consume both audio and text inputs to generate a target sequence. By jointly training on both audio-to-text and textto-text tasks, we hypothesize that this model learns a shared representation for audio and text inputs. This allows us to simply train on new text-to-text data and get audio-to-text performance for free, giving us a way to do E2E hypothesis prediction in a zero-shot fashion during feature expansion. We test this approach on an internal dataset from Alexa traffic, and an external dataset, Facebook TOP <ref type="bibr" target="#b5">(Gupta et al. 2018)</ref>. Since TOP consists of only text data, we collected speech data for the test split using an internal tool at Amazon. We will soon release this dataset.</p><p>In summary, our contributions are as follows. ? We developed an E2E SLU model that is jointly trained on multiple audio-to-text and text-to-text tasks and shows knowledge transfer and SLU performance improvements. ? We report state-of-the-art results on two public SLU datasets, FluentSpeech and SNIPS Audio. ? We show how to perform zero-shot E2E hypothesis prediction with our model. ? We report a new benchmark for zeroshot E2E SLU on the Facebook TOP dataset and will soon release the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>The architecture of prior E2E SLU models is taken from neural speech recognition literature. Speech recognition was originally performed using hidden Markov models that predict acoustic features, followed by word-level language models <ref type="bibr" target="#b2">(Furui 2000)</ref>. More recently, deep learning models have become more popular for this task <ref type="bibr" target="#b7">(Hinton et al. 2012)</ref>. Deep learning models solve this task by posing it as a sequence-to-sequence problem <ref type="bibr" target="#b3">(Graves, rahman Mohamed, and Hinton 2013;</ref><ref type="bibr" target="#b14">Nassif et al. 2019)</ref>. With the success of transformer-based sequence-to-sequence models on text based tasks <ref type="bibr" target="#b20">(Vaswani et al. 2017)</ref>, researchers have explored and shown success in applying them for speech recognition <ref type="bibr" target="#b13">(Mohamed, Okhonko, and Zettlemoyer 2019;</ref><ref type="bibr">Karita et al. 2019)</ref>. Our architecture is based on these models.</p><p>Other end-to-end SLU models also closely resemble this sequence-to-sequence encoder-decoder framework <ref type="bibr" target="#b6">(Haghani et al. 2018;</ref><ref type="bibr" target="#b12">Lugosch et al. 2019)</ref>. The slot-filling task for SLU is formulated as a target text sequence by wrapping the target English tokens with intent and slot tags, which was shown to achieve state of the art results <ref type="bibr" target="#b18">(Rongali et al. 2020)</ref>. Our approach improves upon these models by introducing transfer learning. The transfer learning paradigm we adopt here is similar to prior efforts that use multiple tasks or pretraining to improve SLU performance <ref type="bibr" target="#b8">Jia et al. 2020)</ref>. The audio-text shared training idea also has prior work. However, these efforts require parallel audio-text data <ref type="bibr" target="#b0">(Denisov and Vu 2020)</ref>, or are evaluated on a simpler classification task (Sar?, Thomas, and Hasegawa-Johnson 2020).</p><p>Zeroshot E2E SLU, where we only have text NLU training data but no audio has also been explored. Recently, (Lugosch et al. 2020) approached this task using speech synthesis. They generate synthetic speech from text using a Text to Speech (TTS) system and use the resultant audio to train their models. While this approach is simple and intuitive, its success greatly depends on access to a good TTS system. We propose a method that can perform this task, end-to-end, without any TTS system, and can also be used in conjunction with a TTS system to further improve performance.</p><p>Finally, an important part of all these models is the representation of audio. The raw audio waveform is typically converted into higher level features before being passed to the actual models. While Mel-Frequency Cepstral Coefficitents (MFCC) have been the traditional choice for this conversion, Log-filterbank features (LFB) have become more popular recently (Fayek 2016). We use LFB features here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The AT-AT Model</head><p>In this section, we explain the design of our proposed Audio-Text All-Task (AT-AT) model. AT-AT is trained to jointly perform multiple audio-to-text and text-to-text tasks.</p><p>We hypothesize that AT-AT will benefit from potential knowledge transfer in a multi-task setting. This is in line with findings in a recent work <ref type="bibr" target="#b17">(Raffel et al. 2019</ref>) that converts a variety of text based natural language tasks into source and target text sequences and shows knowledge transfer by using a single shared sequence-to-sequence model.</p><p>AT-AT can also be used as a pretrained checkpoint to build end-to-end models on new datasets to achieve better performance. Finally, we believe that AT-AT is a powerful audiotext shared representation model that would allow us to do E2E zeroshot prediction using just text data.</p><p>When training AT-AT with audio tasks, the input audio signal is pre-processed to obtain a sequence of LFB features, which is taken as the source sequence. For text tasks, the source sequence is simply the text input tokens. The target consists of a sequence of tokens corresponding to the task being solved. For example, the target sequence is a sequence of words if the task is speech recognition. If the task is SLU or NLU hypothesis prediction, the target consists of the intent and slot tags as well as the words within them, a formulation based on recent work that solves this task as a sequence-to-sequence problem <ref type="bibr" target="#b18">(Rongali et al. 2020</ref>). An example set of source-target sequences for tasks is shown in <ref type="figure">Figure 1</ref>. We pass the task label as the beginning-ofsequence (BOS) token in the target decoder. This way, the model can conditionally decode the target sequence based on the observed input and the task being solved. Note that previous multi-task text-to-text models <ref type="bibr" target="#b17">(Raffel et al. 2019)</ref> add this information to the source sequence itself. Since our source sequence can be in the audio space, we add the task label at the start of the target sequence.</p><p>While the audio encoder trained on multiple audio-totext tasks presents an obvious transfer learning advantage in SLU, our reasons for incorporating a text encoder in this model are two-fold; first, we can add more text-to-text tasks in the pretraining phase, and second, more importantly, this would enable us to train on a task with only text-to-text data and expect good audio-to-text performance. AT-AT thus has the ability to do zero-shot end-to-end SLU by training on only annotated text data, an important ability that comes in handy during feature expansion, where new intents and slots need to be added to the model without any audio data available. This situation arises because the text data for new intents and slots can be synthetically generated but the audio data is not readily available and is expensive to collect.</p><p>A model can develop the zero-shot ability if the audio and text inputs share a common space from which the target sequence is generated. A common way to learn a shared space from two input sources is to explicitly impose an L2 loss penalty on the hidden state vectors of the two aligned input sources <ref type="bibr" target="#b0">(Denisov and Vu 2020)</ref>. This is however infeasible in our setup because the hidden states from the audio and text input sequences are not single vectors, but sequences of vectors of different lengths and resolution. While we can pool these vectors to get a single vector, doing so would result in a huge information bottleneck which makes the decoder incapable of decoding the target sequence well. We resolve this problem by avoiding the explicit vector alignment altogether, hence eliminating any need to pool the encoder hidden states. We use a single shared decoder to process the hidden state vectors of both the audio and text encoder. By constraining the complexity of this decoder, we force it to learn a shared representation between audio and text so that it can solve both tasks without solving them separately.</p><p>AT-AT consists of two phases of training: 1) the pretrain-  ing phase, where we train our model on multiple audio-totext and text-to-text tasks, and 2) the finetuning phase, where we finetune our model on a single downstream task. The architecture of AT-AT, these two phases, and our zeroshot endto-end approach are described below.</p><p>Architecture AT-AT has an architecture similar to many transformer-based speech recognition models proposed recently <ref type="bibr">(Karita et al. 2019;</ref><ref type="bibr" target="#b13">Mohamed, Okhonko, and Zettlemoyer 2019)</ref>, which contain an encoder-decoder framework to process a source audio sequence and decode the target text sequence. In addition to the audio encoder, our model also contains a text encoder to process text sequences.</p><p>The audio encoder consists of multiple convolutional and max pooling layers to contextually embed the audio LFB spectrogram. This is followed by a transformer encoder <ref type="bibr" target="#b20">(Vaswani et al. 2017</ref>). These convert the input audio sequence into a much shorter sequence of hidden states to be consumed by the decoder. For the text encoder, we use BERT <ref type="bibr" target="#b1">(Devlin et al. 2018)</ref>, which consists of an embedder to embed the input tokens and their positions, followed by a transformer encoder. The text encoder typically produces hidden states that are larger in size than the audio encoder so we use a projection layer to project the text hidden states down to match the dimensionality of the audio hidden states. Once this is done, both the text and audio sequences generate a sequence of hidden states of the same size.</p><p>We use a single transformer decoder to decode the targets from the sequence of encoder hidden states. Both the text and audio inputs go through the same generation process, which allows the model to learn a shared representation without any explicit loss penalty to align them.</p><p>We use byte-pair encoding (BPE) to split the target words into smaller pieces. We only split the target English words, not any tokens corresponding to intent and slot tags. The target sequence tokens are embedded using a standard embedding matrix. The transformer decoder consumes the current token embedding and performs a multi-head multi-layer attention over the encoder hidden states to generate a decoder hidden state. The decoder hidden state is passed through a generator layer that shares weights with the embedding ma-trix. The generator layer assigns a probability mass to each token in the target vocabulary, representing the probability of that token being generated next. Further details on this decoder framework are beyond the scope of this paper and can be found in <ref type="bibr" target="#b20">(Vaswani et al. 2017)</ref>. Note that instead of a fixed BOS token to start decoding as usual, we use the task label as the BOS token. <ref type="figure" target="#fig_0">Figure 2</ref> lays out these components.</p><p>Pretraining Phase The pretraining phase of AT-AT consists of training with multiple audio-to-text and text-to-text sequence-to-sequence tasks. Examples from all tasks are randomly sampled in each batch during pretraining. <ref type="figure">Figure 1</ref> shows the pretraining phase in action, where we train with three audio-to-text tasks: SLU hypothesis prediction (SLU), automatic speech recognition (ASR), masked-audio LM prediction (MLM), and one text-to-text task: NLU hypothesis prediction (NLU). For the MLM task, the audio corresponding to certain words is masked out and the model is trained to predict the whole target sequence. We perform audio-word alignment prior to the masking using an external tool; more details on this are in the datasets section. We require at least one audio-to-text and one text-to-text task if the model will be used to do zeroshot E2E prediction.</p><p>Finetuning Phase In the finetuning phase, we start from the pretrained model and train it on a specific downstream task, such as SLU. We hypothesize that pretraining with multiple tasks allows the model to transfer knowledge from different tasks, allowing it to be better regularized and obtain a warm start for optimization for the downstream task.</p><p>When the pretrained model is used as a starting point for new datasets with new intents and slots, unseen target token embeddings are randomly initialized. The model is first trained by freezing all pretrained parameters so that these new parameters get to a good optimization point. They are then gradually unfrozen over time as the model is finetuned.</p><p>Zeroshot End-to-End In the zeroshot scenario, we have access to a new annotated text-to-text dataset and we want to construct an E2E model capable of predicting the target sequence given audio input. It is a common occurrence in the feature expansion phase in voice assistants, where a new domain is added to the voice assistant's capabilities. For example, say a voice assistant is currently capable of handling user requests in music and shopping domains. We want to add the capability for it to handle requests in a new domain, say books, such as reading a book. In this case, developers usually write down some launch phrases and annotate them to perform a certain task in the new domain. The audio data for these phrases doesn't exist yet. The goal is to bootstrap an E2E model that can process audio data and generate the hypothesis by just training on the text data.</p><p>AT-AT allows us to do this easily when it is pretrained on a certain task from both audio and text inputs. In the voice assistant feature expansion case for example, the pretraining phase is carried out with an SLU task on existing domains, an NLU task on existing domains, and any other tasks we want to add such as ASR and MLM. Once the pretraining is complete, we simply finetune the model using the annotated text NLU data from the new domain and test on audio data.</p><p>While this is one way to train E2E models without audio data, another way is to simply generate the missing audio data using a Text-to-Speech (TTS) system and use it for training. This approach is however contingent on the availability of a good TTS system. With AT-AT, we can perform zeroshot prediction without a TTS system. Moreover, when we do have access to a TTS system, we can add the generated synthetic audio to the finetuning phase and finetune AT-AT on both the synthetic audio and text. We hypothesize that this is better than simple E2E training since the text NLU data helps train the language model within the decoder even better, allowing AT-AT to work harmoniously with synthetic audio to further improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Datasets</head><p>Our experiments are carried out on a combination of internal and publicly available datasets. We describe them here.</p><p>Internal SLU Data Our internal dataset is created by sampling utterances from user traffic of our voice assistant, Alexa. This is done in compliance with user commitments with regards to privacy and anonymity. We select only utterances from the music domain for the first set of experiments. This dataset contains about 3M training utterances, 100k validation, and 100k testing utterances comprising 23 intents and 95 slots. Each utterance here contains the audio, text transcript, and the SLU hypothesis.</p><p>For our low-resource experiments, we sample 10% of utterances from the above dataset and select the audio and hypotheses. We pick the text transcriptions from the rest to create data for the ASR task during AT-AT pretraining.</p><p>LibriSpeech ASR Data We also compile an ASR dataset by downloading all splits of the publicly available Lib-riSpeech dataset <ref type="bibr" target="#b15">(Panayotov et al. 2015)</ref>, giving us ?1000 hours of data. This data is comprised of multiple speakers reading sentences from audio books in the LibriVox project.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLM Data</head><p>We create the dataset for the MLM task by modifying the LibriSpeech dataset. We first use an external audio alignment tool, Gentle 1 that is built on the Kaldi framework <ref type="bibr" target="#b16">(Povey et al. 2011)</ref>. Once this is done, we mask 15% of the words in each transcript and the corresponding audio in the audio file. This masked audio is then processed to produce the LFB features to produce the audio input and the target sequence is the entire transcript.</p><p>Public SLU Datasets We also evaluate AT-AT on public SLU datasets to compare with the state-of-the-art results. We use two public datasets: FluentSpeech <ref type="bibr" target="#b12">(Lugosch et al. 2019)</ref>, and SNIPS Audio <ref type="bibr" target="#b19">(Saade et al. 2018)</ref> in our evaluation. The FluentSpeech dataset consists of target sequences that are 3-tuples, not sequences. We convert them into target sequences using some pre-processing rules to create data in the required format. There are about 23k train, 3k valid, and 4k test examples in this dataset.</p><p>The annotations in SNIPS are in the form of intents and slots, so can be trivially converted into target sequences in   We also construct a zeroshot dataset from the publicly available Facebook TOP <ref type="bibr" target="#b5">(Gupta et al. 2018)</ref> dataset. This is a challenging dataset that contains complex utterances with nested intents and slots. It contains ?32k train, 4k eval, and 9k test utterances. We want to evaluate the performance of AT-AT on this dataset to show its effectiveness in a complete domain shift. With TOP, we use the training and validation data splits as is. Using an internal utterance collection tool, we collected audio data for a fraction of the test split, about 1915 utterances from multiple speakers, to test zeroshot performance. We will soon release this dataset 2 .</p><p>For the zeroshot experiments, one of our baselines is an E2E model built by generating synthetic speech data from a TTS System. We use Amazon Polly 3 as our TTS system. We use 9 randomly selected speakers and the neural engine to create speech data for utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Details</head><p>We use 80-dim LFB features to process the audio signals. The target English words were tokenized using byte-pair encoding to obtain a final vocabulary of 5k.</p><p>We use a 2-layer 2D CNN with 256 final units and a transformer encoder with 12 layers, 4 heads, 256 units, and 2048 hidden units as our audio encoder. The text encoder is the standard BERT-base encoder <ref type="bibr" target="#b1">(Devlin et al. 2018)</ref>. The target decoder consists of a 256-dim tied embedding/generator matrix and a transformer decoder with 6 layers, 4 heads, 256 units, and 2048 hidden units. We use noam learning rate schedule with 4000 warm-up steps and an adam optimizer with learning rate 1. We use cross entropy loss with label smoothing ( = 0.1) as our loss function. During inference, we use beam search with a beam-size of 4.</p><p>When finetuning with gradual unfreezing, we use a learning rate multiplier of 0 for the first 500 steps, and 0.2, 0.5, 0.7 for the next 100 steps each, finally reaching 1 after 800 steps and training normally from there on. We didn't perform extensive hyper-parameter tuning for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AT-AT in Low Resource Settings</head><p>Our first set of experiments evaluate the effect of AT-AT multi-task training on improving the performance of an E2E model trained on a low-resource annotated dataset. To simulate the low resource setting, we take our internal music SLU dataset and sample 10% of the data to obtain the speech and SLU annotations. For the rest of the examples, we obtain the ASR transcripts to create the ASR dataset for the multi-task training. Our AT-AT model is pretrained on these two tasks. We evaluate this model's performance on the test set immediately after pretraining. We then perform the finetuning step on just the 10% SLU data and perform another evaluation.</p><p>Baselines We train two E2E models as baselines. These models have the same architecture as our AT-AT model, without the multi-task component or the text encoder. The first model is trained on the full internal music SLU dataset. The second model is trained on the extracted 10% dataset. We expect our AT-AT model, that makes use of the additional ASR data from music to recuperate any drop in performance between these two models.</p><p>Results <ref type="table" target="#tab_2">Table 1</ref> shows the results of these experiments. We report two metrics here, the semantic error rate (Se-mER), and the exact match (EM) accuracy. Exact match accuracy simply corresponds to the accuracy obtained by matching the entire predicted hypothesis to the gold hypothesis. SemER is a more slot-filling oriented metric that rewards partially correct hypotheses. It is an internal metric that is used to evaluate the performance of SLU models built for Alexa. Given the number of insertion (I), deletion (D), and substitution (S) errors in the predicted hypothesis, it is given by S+I+D # total slots + 1 (for intent) . We want a lower SemER and a higher EM accuracy. Due to internal regulations, we do not report the absolute numbers on internal datasets. For this experiment, we use the performance numbers of the E2E model trained on 100% data as the baseline and report the remaining numbers relative to it.</p><p>We observe that there is a big drop in performance when we train a model on 100% data vs 10% data. The SemER increases by 8.63 absolute points. However, our AT-AT model, pretrained with additional ASR data recuperates most of this performance, mitigating this increase in error to only 2.23 points. Finetuning on the SLU data further improves performance, giving us a error increase of just 1.45 points. We see  a similar trend in the exact match accuracy scores as well where our models lose the least number of accuracy points. These results show that multi-task training with additional ASR data is hugely beneficial in a low-resource scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Building Better E2E Models with AT-AT</head><p>The previous experiment showed that the performance of an E2E model trained on a low-resource dataset (10% data) can be improved by adding additional ASR data and training in a multi-task setting with AT-AT. In this experiment, we want to take this a step further and evaluate if we can improve the performance of a model trained on the full 100% dataset using any available external data. We pretrain AT-AT with the full 100% SLU dataset and in addition, include two more tasks: ASR and MLM. We use the LibriSpeech ASR and MLM datasets as described in the datasets section for these two tasks. Note that these datasets are from a completely different domain than music. We want to determine whether we can improve the performance of an E2E model by adding tasks from other domains with transferable knowledge. We evaluate our model in two settings. The first setting consists of pretraining with 2 tasks, SLU and ASR, followed by finetuning on the 100% SLU dataset. The second setting's pretraining phase consists of 3 tasks, SLU, ASR, and MLM, followed by finetuning again on the 100% SLU dataset. Our baseline is the E2E model trained on 100% music SLU data from the previous set of experiments. For context, we also report numbers from two 2-stage pipeline models for SLU. We use a production-level ASR system from Amazon for the first stage. For the second (NLU) stage, we experiment with a linear chain CRF and a pretrained BiLSTM + CRF (SOTA). The BiLSTM + CRF model beats transformerbased models for this dataset <ref type="bibr" target="#b18">(Rongali et al. 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We report the results of these experiments in Table 2. We again report relative numbers here since this is in an internal dataset. We use the performance of the E2E 100% model as the baseline. We see that adding LibriSpeech ASR data and pretraining with AT-AT improves SemER on the internal music SLU test set by 1.16 points, representing a significant relative error reduction. The exact match accuracy also improves by 1.4 absolute points here. With all three tasks, we see that the SemER improves by 1.01 points, slightly worse than the previous number. We believe the lack of further improvement from the MLM task might be because it doesn't contribute new information to the model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AT-AT on Public Datasets</head><p>In this set of experiments, we evaluate how AT-AT's pretraining can help improve performance on other datasets. We selected the publicly available FluentSpeech and SNIPS Audio datasets to compare to state-of-the-art models.</p><p>We use the AT-AT model pretrained with 2 tasks from the previous experiment and finetune it on the FluentSpeech and SNIPS datasets. We also trained end-to-end models from scratch on these two datasets. To perform an ablation on the AT-AT finetuning approach, we report an additional number on the SNIPS dataset, for a model that uses a pretrained AT-AT audio encoder. This model, compared to the full AT-AT model would give us an idea of how much the decoder pretraining helps, in addition to the encoder pretraining.</p><p>Baselines For the FluentSpeech dataset, we compare to two SOTA models. The first model is the best model from <ref type="bibr" target="#b12">(Lugosch et al. 2019)</ref>. It is a multi-layer RNN-based network, with lower layers trained to predict aligned word targets from the LibriSpeech dataset. The final task is formulated as a 3-way classification task, not a generation task like our AT-AT model. The second model is a transformer-based pretrained model from .</p><p>For the SNIPS Audio dataset, we compare with the two models reported in <ref type="bibr" target="#b19">(Saade et al. 2018)</ref>, SNIPS and Google. The SNIPS model consists of a pipe-lined approach with an acoustic model for ASR, followed by a language model, and slot tagging model for NLU. The Google model is from Google's DialogFlow cloud service 4 .</p><p>Results <ref type="table" target="#tab_5">Table 3</ref> reports the results on the FluentSpeech dataset. We report error rate on the complete hypothesis (Hy-pER), exact match accuracy on the hypothesis, and the exact match accuracy on the full target sequence. While the endto-end model doesn't perform too well from scratch, our AT-AT finetuned model beats the state-of-the-art model by 0.5 accuracy points. This corresponds to a 50% error reduction.    <ref type="table" target="#tab_7">Table 4</ref> contains the results on the SNIPS dataset. We report the exact match accuracy on the hypothesis and the full target sequence here. We see that our AT-AT pretrained models have the best performance on both the close-field and farfield sets with a 5-fold cross validation setup. They beat both Google and SNIPS models' numbers previously reported. We also see that the AT-AT model is vastly superior to an end-to-end model with a pretrained audio encoder. This is especially evident with the accuracy scores on the full target sequence where the AT-AT model beats it by 10-15 absolute points. Note that we weren't able to train an end-to-end model from scratch due to extremely small dataset size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zeroshot E2E with AT-AT</head><p>In the final experiments, we evaluate the performance of AT-AT on zeroshot end-to-end tasks. Here, we only have text training data and we want to evaluate on speech.</p><p>We first pretrained AT-AT on 4 tasks: SLU (speech-tohypothesis), ASR, MLM, and NLU (text-to-hypothesis). We use data from the internal music dataset (for SLU and NLU), and the LibriSpeech dataset for ASR and MLM. This model is then finetuned on the internal books dataset and the Facebook TOP dataset with the text NLU training data as described in the architecture section. We also finetune AT-AT in another setting, using text NLU training data along with the synthetic speech data from our TTS system. We want to show that in addition to performing zeroshot prediction without access to a TTS system, we can also work with an existing TTS system to further improve performance.</p><p>Baselines For the internal books dataset, we built an E2E model on real audio training data to obtain a rough upper bound and gauge the zeroshot performance. In addition to this, we also trained another E2E model on the synthetic dataset constructed from our TTS system.</p><p>For the TOP dataset, we don't have real audio training data, so our baseline was an E2E model trained on the synthetic training data. For a fair comparison to AT-AT, we use the same pretrained audio encoder for these E2E models.</p><p>Results Tables 5 and 6 report results of the zeroshot experiments. On the internal books dataset, we report relative numbers for SemER and EM accuracy. We use the performance numbers of an E2E model trained on real speech data as baselines. Using synthetic training data gives us a SemER of baseline + 5.05 points . AT-AT achieves a zeroshot Se-mER of baseline + 11.90 without access to a TTS system, a respectable number compared to the aforementioned model. AT-AT when finetuned with additional synthetic speech data beats an E2E model trained on only synthetic data, obtaining a SemER of baseline + 3.31 (lowest increase in error).</p><p>On the TOP dataset, we report all the recommended metrics given in the dataset but we are primarily interested in exact match accuracy. Note that our test set was compiled by recording speech for a fraction of the full test set. We observe the same trend here that we observe on the books dataset. An E2E model trained on synthetic data achieves an accuracy of 69.19 while AT-AT achieves 51.54. While there is a significant drop, it is to be noted that AT-AT sees absolutely no new labeled audio instances, giving it a significant disadvantage while switching input models during inference. It also doesn't require a TTS system for this training. The E2E model is however beaten by the AT-AT model finetuned with additional synthetic data, which achieves an accuracy of 70.60 (2% relative improvement).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We propose the Audio-Text All-Task (AT-AT) model that uses transfer learning to improve the performance on endto-end SLU. AT-AT beat the performance of E2E models on our internal music data, both in the full and low-resource settings. It also achieved state-of-the-art performance on the FluentSpeech (99.5% EM Accuracy) and SNIPS audio datasets (84.88% close-field, 74.64% far-field EM) with significant improvements over prior models. AT-AT also demonstrated its ability to perform zeroshot E2E SLU, without access to a TTS system, and by learning a shared audiotext representation without any explicit loss penalty to force the audio and text hidden states into the same space. We also showed how AT-AT can work in conjunction with a TTS system to further improve E2E performance. It achieves a zeroshot E2E EM Accuracy of 70.60 on the TOP dataset.</p><p>On a closing note, we would like to remark that AT-AT somewhat mimics actual human learning. We typically read a lot more words than we hear. But when we hear a word for the first time, we transfer our knowledge of that word from when we read it. AT-AT similarly learns to understand and perform NLU tagging from text and then applies this knowledge when it is given speech.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Model Components of AT-AT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results on the low-resource music dataset.</figDesc><table><row><cell>Method</cell><cell cols="2">SemER EM Accuracy</cell></row><row><cell>E2E Model w. 100% data</cell><cell>baseline</cell><cell>baseline</cell></row><row><cell>Prod ASR, linear chain CRF</cell><cell>+0.61</cell><cell>-1.02</cell></row><row><cell>Prod ASR, BiLSTM + CRF</cell><cell>-0.45</cell><cell>+0.70</cell></row><row><cell>AT-AT, SLU + ASR</cell><cell>-1.16</cell><cell>+1.39</cell></row><row><cell>AT-AT, SLU + ASR + MLM</cell><cell>-1.01</cell><cell>+1.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results on the full music dataset.Zeroshot SLU Datasets For our zeroshot experiments, we require text NLU training data and audio SLU test data on an unseen domain. We collect two datasets for this. First is an internal dataset that consists of utterances sampled from Alexa traffic in the books domain. We extract around 200k text NLU training examples, and 10k audio SLU test examples comprising 21 intents and 47 slots.</figDesc><table /><note>the required format. We use the smart-lights close-field and far-field datasets from the SNIPS dataset for our experiments and report results with 5-fold cross validation since there are no explicitly delineated train-test splits. These dataset are extremely small, each consisting of a total of 1660 examples.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results on the FluentSpeech dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Results on the SNIPS dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Results on the TOP dataset.</figDesc><table><row><cell>Method</cell><cell cols="2">SemER EM Accuracy</cell></row><row><cell>E2E Model w. Real Audio</cell><cell>baseline</cell><cell>baseline</cell></row><row><cell>E2E Model w. Syn. Audio</cell><cell>+5.05</cell><cell>-9.58</cell></row><row><cell>AT-AT zeroshot</cell><cell>+11.90</cell><cell>-15.14</cell></row><row><cell>AT-AT zeroshot + Syn. Audio</cell><cell>+3.31</cell><cell>-5.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Results on the books dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/lowerquality/gentle</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In Progress. Awaiting legal approval at Amazon. 3 https://aws.amazon.com/polly/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://cloud.google.com/dialogflow</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Pretrained semantic speech embeddings for end-to-end spoken language understanding via cross-modal teacher-student learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Denisov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Vu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01836</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<ptr target="https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html" />
	</analytic>
	<monogr>
		<title level="m">Fayek, H. M. 2016. Speech Processing for Machine Learning: Filter banks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Mel-Frequency Cepstral Coefficients (MFCCs) and What&apos;s In-Between</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
		<title level="m">Digital Speech Processing: Synthesis, and Recognition</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>Second Edition</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Speech recognition with deep recurrent neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic Parsing for Task Oriented Dialog using Hierarchical Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2787" to="2792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">From Audio to Semantics: Approaches to End-to-End Spoken Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Spoken Language Technology Workshop</title>
		<imprint>
			<biblScope unit="page" from="720" to="726" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for Acoustic Modeling in Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">82</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05671</idno>
		<title level="m">Large-scale Transfer Learning for Low-resource Spoken Language Understanding</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Someki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E Y</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<title level="m">A Comparative Study on Transformer vs RNN in Speech Applications. 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using Speech Synthesis to Train End-To-End Spoken Language Understanding Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8499" to="8503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Speech Model Pre-training for End-to-End Spoken Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ignoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Tomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ArXiv abs/1904.03670</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11660</idno>
		<title level="m">Transformers with convolutional context for ASR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Speech Recognition Using Deep Neural Networks: A Systematic Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Nassif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shahin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Attili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Azzeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shaalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="19143" to="19165" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Librispeech: An ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Motl?cek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Silovsk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stemmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vesel?</surname></persName>
		</author>
		<title level="m">The Kaldi Speech Recognition Toolkit</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ArXiv abs/1910.10683</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Don&apos;t Parse, Generate! A Sequence to Sequence Architecture for Task-Oriented Semantic Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rongali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamza</surname></persName>
		</author>
		<idno type="DOI">10.1145/3366423.3380064</idno>
		<ptr target="http://dx.doi.org/10.1145/3366423.3380064" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
		<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Training Spoken Language Understanding Systems with Non-Parallel Speech and Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caulier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doumouro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gisselbrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caltagirone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">; L</forename><surname>Primet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8109" to="8113" />
		</imprint>
	</monogr>
	<note>Spoken Language Understanding on the Edge. ArXiv abs/1810.12735. Sar?,</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Large-Scale Unsupervised Pre-Training for End-to-End Spoken Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7999" to="8003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
