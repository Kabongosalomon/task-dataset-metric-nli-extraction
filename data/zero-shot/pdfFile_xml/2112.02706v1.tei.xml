<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Achieving Forgetting Prevention and Knowledge Transfer in Continual Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixuan</forename><surname>Ke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
							<email>liub@uic.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianzu</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Amazon AWS AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Achieving Forgetting Prevention and Knowledge Transfer in Continual Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Continual learning (CL) learns a sequence of tasks incrementally with the goal of achieving two main objectives: overcoming catastrophic forgetting (CF) and encouraging knowledge transfer (KT) across tasks. However, most existing techniques focus only on overcoming CF and have no mechanism to encourage KT, and thus do not do well in KT. Although several papers have tried to deal with both CF and KT, our experiments show that they suffer from serious CF when the tasks do not have much shared knowledge. Another observation is that most current CL methods do not use pre-trained models, but it has been shown that such models can significantly improve the end task performance. For example, in natural language processing, fine-tuning a BERT-like pre-trained language model is one of the most effective approaches. However, for CL, this approach suffers from serious CF. An interesting question is how to make the best use of pre-trained models for CL. This paper proposes a novel model called CTR to solve these problems. Our experimental results demonstrate the effectiveness of CTR. 2 * Work was done prior to joining Amazon. <ref type="bibr" target="#b1">2</ref> The code of CTR can be found at https://github.com/ZixuanKe/PyContinual 35th</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper studies continual learning (CL) of a sequence of natural language processing (NLP) tasks in the task continual learning (Task-CL) setting. It aims to (i) prevent catastrophic forgetting (CF), and (ii) transfer knowledge across tasks. (ii) is particularly important because many tasks in NLP share similar knowledge that can be leveraged to achieve better accuracy. CF means that in learning a new task, the existing network parameters learned for the previous tasks may be modified, which degrades the performance of previous tasks <ref type="bibr" target="#b39">[40]</ref>. In the Task-CL setting, the task id is provided for each test case in testing so that the specific model for the task in the network can be applied to classify the test case. Another popular CL setting is class continual learning, which does not provide the task id during testing but it is for solving a different type of problems.</p><p>Most existing CL papers focus on dealing with CF <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b4">5]</ref>. There are also some papers that perform knowledge transfer. To achieve both objectives is highly challenging. To overcome CF in the Task-CL setting, we don't want the training of the new task to update the model parameters learned for previous tasks to achieve model separation. But to transfer knowledge across tasks, we want the new task to leverage the knowledge learned from previous tasks for learning a better model (forward transfer) and also want the new task to enhance the performance of similar previous tasks (backward transfer). This means it is necessary to update previous model parameters. This is a dilemma. Although several papers have tried to deal with both <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b36">37]</ref>, they were only tested using sentiment analysis tasks with strong shared knowledge. When tested with tasks that don't have much shared knowledge, they suffer from severe CF (see <ref type="bibr">Sec. 5.4)</ref>. Those existing papers that focus on dealing with CF do not do well with knowledge transfer as they have no explicit mechanism to facilitate the transfer.</p><p>Another observation about the current CL research is that most techniques do not use pre-trained models. But such pre-trained models or feature extractors can significantly improve the CL performance <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref>. An important question is how to make the best use of pre-trained models in CL. This paper studies the problem as well using NLP tasks, but we believe that the developed ideas are also applicable to computer vision tasks because most pre-trained models are based on the transformer architecture <ref type="bibr" target="#b59">[60]</ref>. We will see that the naive or the conventional way of directly adding the CL module on top of a pre-trained model is not the best choice (see <ref type="bibr">Sec. 5.4)</ref>.</p><p>In NLP, fine-tuning a BERT <ref type="bibr" target="#b7">[8]</ref> like pre-trained language model has been regarded as one of the most effective techniques in applications <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b56">57]</ref>. However, fine-tuning works poorly for continual learning. This is because the fine-tuned BERT for a task captures highly task-specific information <ref type="bibr" target="#b40">[41]</ref>, which is difficult to be used by other tasks. When fine-tuning for a new task, it has to update the already fine-tuned parameters for previous tasks, which causes serious CF (see <ref type="bibr">Sec. 5.4)</ref>. This paper proposes a novel neural architecture to achieve both CF prevention and knowledge transfer, which also deals with the CF problem with BERT fine-tuning. The proposed system is called CTR (Capsules and Transfer Routing for continual learning). CTR inserts a continual learning plug-in (CL-plugin) module in two locations in BERT. With the pair of CL-plugin modules added to BERT, we no longer need to fine-tune BERT for each task, which causes CF in BERT, and yet we can achieve the power of BERT fine-tuning. CTR has some similarity to Adapter-BERT <ref type="bibr" target="#b15">[16]</ref>, which adds adapters in BERT for parameter efficient transfer learning such that different end tasks can have their separate adapters (which are very small in size) to adapt BERT for individual end tasks and to transfer the knowledge from BERT to the end tasks. Then, there is no need to employ a separate BERT and fine-tuning it for each task, which is extremely parameter inefficient if many tasks need to be learned. An adapter is a simple 2-layer fully-connected network for adapting BERT to a specific end task. A CL-plugin is very different from an adapter. We do not use a pair of CL-plugin modules to adapt BERT for each task. Instead, CTR learns all tasks using only one pair of CL-plugin modules inserted into BERT. A CL-plugin is a full CL network that can leverage a pre-trained model and deal with both CF and knowledge transfer. Specifically, it uses a capsule <ref type="bibr" target="#b14">[15]</ref> to represent each task and a proposed transfer routing algorithm to identify and transfer knowledge across tasks to achieve improved accuracy. It further learns and uses task masks to protect task-specific knowledge to avoid forgetting. Empirical evaluations show that CTR outperforms strong baselines. Ablation experiments have also been conducted to study where to insert the CL-plugin module in BERT in order to achieve the best performance (see Sec. 5.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Catastrophic Forgetting: Existing work in CL mainly focused on overcoming CF using the following approaches. (1) Regularization-based approaches, such as those in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b68">69]</ref>, add a regularization in the loss to consolidate weights for previous tasks when learning a new task. (2) Replay-based approaches, such as those in <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b62">63]</ref>, retain some training data of old tasks and use them in learning a new task. The methods in <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b13">14]</ref> learn data generators and generate old task data for learning a new task. (3) Parameter isolation-based approaches, such as those in <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b9">10]</ref>, allocate model parameters dedicated to different tasks and mask them out when learning a new task. (4) Gradient projection-based approaches <ref type="bibr" target="#b67">[68]</ref> ensure the gradient updates occur only in the orthogonal direction to the input of old tasks and thus will not affect old tasks. Some recent papers used pre-trained models <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> and learn one class per task <ref type="bibr" target="#b17">[18]</ref>. Tackling CF only deals with model deterioration. These methods perform worse than learning each task separately. An empirical study of the cause of CF and the impact of task similarity on CF was done in <ref type="bibr" target="#b43">[44]</ref>.</p><p>Some NLP applications have also dealt with CF. For example, CL models have been proposed for sentiment analysis <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b42">43]</ref>, dialogue slot filling <ref type="bibr" target="#b52">[53]</ref>, language modeling <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b6">7]</ref>, language learning <ref type="bibr" target="#b30">[31]</ref>, sentence embedding <ref type="bibr" target="#b32">[33]</ref>, machine translation <ref type="bibr" target="#b24">[25]</ref>, cross-lingual modeling <ref type="bibr" target="#b34">[35]</ref>, and question answering <ref type="bibr" target="#b11">[12]</ref>. A dialogue CL dataset is also reported in <ref type="bibr" target="#b37">[38]</ref>.</p><p>Knowledge Transfer: Ideally, learning from a sequence of tasks should also allow multiple tasks to support each other via knowledge transfer. CAT <ref type="bibr" target="#b20">[21]</ref> (a Task-CL system) works on a mixed sequence of similar and dissimilar tasks and can transfer knowledge among similar tasks detected automatically. Progressive Network <ref type="bibr" target="#b47">[48]</ref> does forward transfer but it is for class continual learning (Class-CL).</p><p>Knowledge transfer in this paper is closely related to lifelong learning (LL), which aims to improve the new/last task learning without handling CF <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b4">5]</ref>. In the NLP area, NELL <ref type="bibr" target="#b2">[3]</ref> performs LL information extraction, and several other papers worked on lifelong document sentiment classification (DSC) and aspect sentiment classification (ASC). <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b60">[61]</ref> proposed two Naive Bayesian methods to help improve the new task learning. <ref type="bibr" target="#b63">[64]</ref> proposed a LL approach based on voting. <ref type="bibr" target="#b54">[55]</ref> used LL for aspect extraction. <ref type="bibr" target="#b42">[43]</ref> and <ref type="bibr" target="#b61">[62]</ref> used neural networks for DSC and ASC, respectively. Several papers also studied lifelong topic modeling <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref>. However, all these works do not deal with CF.</p><p>SRK <ref type="bibr" target="#b36">[37]</ref> and KAN <ref type="bibr" target="#b21">[22]</ref> try to deal with both CF and knowledge transfer in continual sentiment classification. However, they have two critical weaknesses: (i) Their RNN architectures cannot use plug-in or adapter modules to tune BERT, which significantly limits their power. (ii) Since they were mainly designed for knowledge transfer, they suffer from serious CF (see Sec. 5.4). B-CL <ref type="bibr" target="#b23">[24]</ref> uses the adapter idea <ref type="bibr" target="#b15">[16]</ref> to adapt BERT for sentiment analysis tasks, which are similar to each other. However, since its mechanism of dynamic routing for knowledge transfer is very week, its knowledge transfer ability is markedly poorer than CTR (see Sec. 5.4). CLASSIC <ref type="bibr" target="#b22">[23]</ref> is another recent work on continual learning for knowledge transfer, but its CL setting is domain continual learning. Its knowledge transfer method is based on contrastive learning.</p><p>AdapterFusion <ref type="bibr" target="#b41">[42]</ref> used adapters proposed in <ref type="bibr" target="#b15">[16]</ref>. It proposes a two-stage method to learn a set of tasks. In the first stage, it learns one adapter for each task independently using the task's training data. In the second stage, it uses the training data again to learn a good composition of the learned adapters in the first stage to produce the final model for all tasks. AdapterFusion basically tries to improve multi-task learning. It is not for continual learning and thus has no CF. As explained in Sec. 1, the CL-plugin concept in CTR is different from that of adapters for adapting BERT for each task. CL-plugins are continual learning systems that make use of a pre-trained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CTR Architecture</head><p>This section describes the general architecture of CTR. The details about its key component CLplugin is presented in the next section. Due to its good performance, BERT <ref type="bibr" target="#b7">[8]</ref> and its transformer <ref type="bibr" target="#b59">[60]</ref> architecture are used as the base in our model CTR. Since BERT fine-tuning is prone to CF (Sec. 1), we propose the CL plug-in idea, which is inspired by Adapter-BERT <ref type="bibr" target="#b15">[16]</ref>. CL-plugin is a full continual learning module designed to interact with a pre-trained model, in our case, BERT.</p><p>Inserting CL-plugins in BERT. A commonly used method of leveraging a pre-trained model is to add the end task module on top of the pre-trained model. However, as explained in Sec. 1, fine-tuning the pre-trained model can cause serious CF for CL. The CL system PCL <ref type="bibr" target="#b17">[18]</ref>, which uses this approach, has the pre-trained model frozen to avoid forgetting. But as we will see in Sec. 5.4, this is not the best choice. CTR inserts the proposed CL-plugin in two locations in BERT, i.e., in each transformer layer of BERT. We will also see in Sec. 5.4 that inserting only one CL-plugin in one location is sub-optimal. <ref type="figure" target="#fig_0">Figure 1</ref> gives the CTR architecture and we can see the two CL-plugins are added into BERT. In learning, only the two CL-plugins and the classification heads are trained. The components of the original pre-trained BERT are fixed.</p><p>Continual learning plug-in (CL-plugin). CL-plugin employs a capsule network (CapsNet) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b49">50]</ref> like architecture. In the classic neural network, a neuron outputs a scalar, real-valued activation as a feature detector. CapsNets replaces that with a vector-output capsule to preserve additional information. A simple CapsNet consists of two capsule layers. The first layer stores low-level feature maps, and the second layer generates the classification probability with each capsule corresponding to one class. CapsNet uses a dynamic routing algorithm to make each lower-level capsule to send its output to a similar (or "agreed", computed by dot product) higher-level capsule. This property can already be used to group similar tasks and their shareable features to produce a CL system (see the ablation study in Sec. 5.4). One of the key ideas of CL-plugin (see <ref type="figure" target="#fig_1">Figure 2</ref>(A)) is a transfer capsule layer with a new transfer routing algorithm to explicitly identify transferable features/knowledge from previous tasks to transfer to the new task. Additionally, transfer routing avoids the need for hyper-parameter tuning on the number of iterations of dynamic routing <ref type="bibr" target="#b49">[50]</ref> to update the agreements.  The architecture of our continual learning plug-in (CL-plugin) is shown in <ref type="figure" target="#fig_1">Figure 2</ref>(A). CL-plugin takes two inputs: (1) hidden states h (t) from the feed-forward layer inside a transformer layer and (2) task ID t, which is required by task continual learning (Task-CL). The outputs are hidden states with features suitable for the t-th task for classification. Inside CL-plugin, there are two modules: (1) knowledge sharing module (KSM) for identifying and transferring the shareable knowledge from similar previous tasks to the new task t, and (2) task specific module (TSM) for learning task specific neurons and their masks (which can protect the neurons from being updated by future tasks to deal with CF). Since TSM is differentiable, the whole system CTR can be trained end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Knowledge Sharing Module (KSM)</head><p>KSM achieves knowledge transfer among similar tasks via a task capsule layer (TK-Layer), a transfer capsule layer (TR-Layer), and a transfer routing mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Task Capsule Layer (TK-Layer)</head><p>Each capsule in the TK-Layer represents a task, and it prepares the low-level features derived from each task ( <ref type="figure" target="#fig_1">Figure 2(A)</ref>). As such, a capsule is added to the TK-Layer for each new task. This incremental growth is efficient and easy because these capsules are discrete and do not share parameters. Also, each capsule is simply a 2-layer fully connected network with a small number of parameters. Let h (t) ? R dt?de be the input of CL-plugin, where d t is the number of tokens, d e the number of dimensions, and t is the current task. In the TK-Layer, we have one capsule for each task. Assume we have learned t tasks so far. The capsule for the i-th (i ? t) task is</p><formula xml:id="formula_0">p (t) i = f i (h (t) ),<label>(1)</label></formula><p>where f i (?) = MLP i (?) denotes a 2-layer fully-connected network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Transfer Routing and Transfer Capsule Layer</head><p>Each capsule in the transfer capsule layer (TR-Layer) represents the transferable representation extracted from TK-Layer. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>(A), transfer routing between the lower-level capsules in TK-Layer and high-level capsules in TR-Layer has three components: pre-route vector generator (PVG), similarity estimator (SE) and task router (TR). Given the task capsules in the TK-Layer, we first transform the feature through a trainable weight matrix. We call the output of this transformation the pre-route vector. Each SE estimates the similarity between a previous task and the current task using the pre-route vector, resulting in a similarity score for each higher-level capsule. Additionally, each SE is augmented by a TR module, a differentiable task router acting as a gate. This router estimates a binary signal that decides whether to connect or disconnect the current route between the two consecutive capsule layers (i.e. TK-Layer and TR-Layer in CL-plugin). The binary signal estimated by TR can be seen as a differentiable binary attention. Conceptually, each SE and TR pair together learns the connectivity between capsules in a stochastic and differentiable manner, which can be seen as a task similarity-based connectivity search mechanism. This transfer routing identifies the shared features/knowledge from multiple task capsules and helps knowledge transfer across similar tasks. Next, we discuss the pre-route vector generator, similarity estimator and task router.</p><p>Pre-route Vector Generator (PVG). We first turns each transfer capsule p</p><formula xml:id="formula_1">(t) i into a pre-route vector, u (t) j|i = W ij p (t) i ,<label>(2)</label></formula><p>where W ij ? R ds?d k is the weight matrix, d s and d k are the dimensions of task capsule i (also representing a task) and transfer capsule j, and t is the current task. The number of transfer capsules n j is a hyperparameter detailed in Sec. 5.</p><p>Similarity Estimator (SE). Since tasks i and t are different, it is crucial to determine what in task i's representation is transferable. Inspired by <ref type="bibr" target="#b66">[67]</ref>, we use a convolution layer and activation units to compare task i with task t to determine the transferable proportion from the previous task i. In SE, we compute the task similarity as follows:</p><formula xml:id="formula_2">q (t) j|t = MaxPool(Relu(u (t) j|t * W q + b q )),<label>(3)</label></formula><formula xml:id="formula_3">a (t) j|i = MaxPool(Relu(u (t) j|i * W a + f a (q (t) j|t ) + b a )),<label>(4)</label></formula><p>where b a , b q ? R are the bias, W a , W q ? R de?dw are convolutions filters and d w is the windows size. We extract important features from the current task representation u (t) j|t via the convolution network in Eq. (3). The MaxPool helps remove the insignificant features to generate a fixed-size vector with the size equal to the number of filters n w . Similarly, we extract important features from the previous task i's representation u (t) j|i . Using the important features for the current and previous tasks, we compute a similarity score between them in Eq. (4) with ReLU activation. Note f a is a 1-layer fully-connected network to match the dimensions. As a result, a (t) j|i indicates how similar the representation of the i-th task is to the current task t. For those tasks with a very low a (t) j|i , their representations are less similar to the current task and thus has little transferable knowledge.</p><p>Task Router (TR). TR controls which previous task representation should flow to the next layer with the goal of letting only the transferable information to flow. Given the similarity a (t) j|i , TR estimates a binary decision signal ? (t) j|t ? {0:disconnect, 1:connect}. We first apply a convolution layer with 2 output channels and 1 ? 1 kernel size to generate un-normalized decision value. To estimate the binary decision, we need to generate a decision chosen from the set of two mutually exclusive and exhaustive events (disconnect and connect). In our work, we adopt the Gumbel-Softmax <ref type="bibr" target="#b18">[19]</ref> to help make the TR gate differentiable.</p><formula xml:id="formula_4">? (t) j|i = Gumbel_softmax(a (t) j|i * W ? + b ? ).<label>(5)</label></formula><p>Given the similarity a </p><formula xml:id="formula_5">v tran(t) j|i = a (t) j|i ? u (t) j|i , v (t) j = n+1 i=1 ? (t) ij =1 v tran(t) j|i .<label>(6)</label></formula><p>This makes sure only task capsules for tasks that are salient or similar to the new task are used, and the others task capsules are ignored (and thus protected) to learn more general shareable knowledge. As many NLP applications have similar tasks, such learning of task-sharing features can be very important. Note that in backpropagation, only the similar tasks with connected gate (? (t) ij =1) are updated, encouraging backward knowledge transfer of similar tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Task Specific Module (TSM)</head><p>We now discuss how to preserve task-specific knowledge of previous tasks to prevent forgetting (CF). To achieve this, we use task masks <ref type="figure" target="#fig_1">(Figure 2(B)</ref>). Specifically, we first detect the neurons used by each old task and then block off or mask out all the used neurons when learning a new task.</p><p>The task-specific module consists of differentiable layers (a 2-layer fully-connected network is used). Each layer's output is further applied with a task mask to indicate which neurons should be protected for that task to overcome CF and forbids gradient updates for those neurons during backpropagation for a new task. Those tasks with overlapping masks indicate some parameter sharing. Due to KSM, the features flowing into those overlapping neurons enable the related old tasks to also improve in learning the new task. Here we borrow the hard attention idea in <ref type="bibr" target="#b51">[52]</ref> and leverage the task ID embedding to train the task mask. Further details can be found in Supplementary.</p><p>Illustration. The task masking process is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>(B), which shows the learning process of three tasks. Before training, those solid cells with a 0 are the neurons that have been used by some previous tasks and should be protected (masked). Those empty cells are free neurons (not used). After training, those solid cells with a 1 are neurons that are important for the current task, which will be used as masks in the future. Those solid cells with a 0 are masked as they are important for previous tasks. Those non-solid cells with a 0 are neurons that are not used so far.</p><p>Let us walk through the learning process of the three tasks. After training task 0, we obtain its useful neurons indicated by the 1 entries. Before training task 1, those useful neurons for task 0 are first masked (those previous 1's entries are turned to 0's). After training task 1, two neurons with 1 are used by the task. When task 2 arrives, all used neurons by tasks 0 and 1 are masked before training, i.e., their entries are set to 0. After training task 2, we see that tasks 2 and 1 have a shared neuron (the cell with two colors, red and green), which is used by both of tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate CTR using three applications. We follow the continual learning (CL) evaluation method given in <ref type="bibr" target="#b28">[29]</ref>. CTR first learns a sequence of tasks. After a task is trained, the training data of the task is no longer accessible. After all tasks are learned, their task models are tested using their respective test sets. In training each task, we use its validation set to decide when to stop training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Three Applications and Their Datasets</head><p>The first two applications (and datasets) are used to show the knowledge transferability of CTR because their tasks are similar and have shared knowledge. Catastrophic forgetting (CF) is not a  <ref type="table">Train  352 245 283 271 162 677 228 343 118 175 191 212 153 176 484 362 194 3452 2163  Val.  44  31  35  34  20  85  29  43  15  22  24  26  19  22  61  45  24  150  150  Test  44  31  36  34  21  85  29  43  15  22  24  27  20  23  61  46</ref> 25 1120 638 <ref type="table">Table 1</ref>: Statistics of datasets for ASC. The datasets statistics for DSC and 20News have been described in the text. More detailed data statistics are given in <ref type="figure">Supplementary.</ref> major concern for them. The third application (and dataset) is mainly used to test CTR's ability to overcome CF as its tasks are very different and have little shared knowledge to transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Document Sentiment Classification (DSC).</head><p>This application is to classify each full product review into one of the two opinion classes (positive and negative). The training data of each task consists of reviews of a particular type of product. We adopt the text classification formulation in <ref type="bibr" target="#b7">[8]</ref>, where a [CLS] token is used to predict the opinion polarity.</p><p>We employ a set of 10 DSC datasets (reviews of 10 products) to produce sequences of tasks. The products are Sports, Toys, Tools, Video, Pet, Musical, Movies, Garden, Offices, and Kindle <ref type="bibr" target="#b21">[22]</ref>. Two experiments are conducted: (1) using small data in each task: 100 positive and 100 negative training reviews per task; (2) using the full data in each task: 2500 positive and 2500 negative training reviews per task <ref type="bibr" target="#b21">[22]</ref>. <ref type="formula" target="#formula_0">(1)</ref> is more useful in practice because labeling a large number of examples is very costly. The same validation reviews (250 positive and 250 negative) and the same test reviews (250 positive and 250 negative) are used in both experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Aspect Sentiment Classification (ASC).</head><p>It classifies a review sentence on the aspect-level sentiment (one of positive, negative, and neutral). For example, the sentence "The picture is great but the sound is lousy" about a TV expresses a positive opinion about the aspect "picture" and a negative opinion about the aspect "sound." We adopt the ASC formulation in <ref type="bibr" target="#b64">[65]</ref>, where the aspect term and sentence are concatenated via [SEP] in BERT. The opinion is predicted on top of the [CLS] token.</p><p>We employ a set of 19 ASC datasets (review sentences of 19 products) to produce sequences of tasks. Each dataset represents a task. The datasets are from 4 sources: (1) HL5Domains <ref type="bibr" target="#b16">[17]</ref> with reviews of 5 products; (2) Liu3Domains [32] with reviews of 3 products; (3) Ding9Domains <ref type="bibr" target="#b8">[9]</ref> with reviews of 9 products; and (4) SemEval14 with reviews of 2 products -SemEval 2014 Task 4 for laptop and restaurant. For (1), <ref type="bibr" target="#b1">(2)</ref> and <ref type="formula" target="#formula_2">(3)</ref>, we use about 10% of the original data as the validate data, another about 10% of the original data as the testing data. For (4), we use 150 examples from the training set for validation. To be consistent with existing research <ref type="bibr" target="#b58">[59]</ref>, sentences with conflict opinions about a single aspect are not used. Statistics of the 19 datasets are given in <ref type="table">Table 1</ref>.</p><p>3. Text classification using 20News data. This dataset <ref type="bibr" target="#b27">[28]</ref> has 20 classes and each class has about 1000 documents. The data split for train/validation/test is 1600/200/200. We created 10 tasks, 2 classes per task. Since this is topic-based text classification data, the classes are very different and have little shared knowledge. As mentioned above, this application (and dataset) is mainly used to show CTR's ability to overcome forgetting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>We setup 38 baselines, including both standalone and continual learning methods.</p><p>Multitask learning (MTL: Results of multitask learning is considered the upper-bound of those of continual learning. Here MTL fine-tunes BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Standalone (SDL) Baselines:</head><p>The SDL setting builds a model for each task independently using a separate network. It clearly has no knowledge transfer or forgetting. We have 4 baselines under SDL, BERT, BERT (Frozen), Adapter-BERT and W2V (word2vec embeddings). For BERT, we use trainable BERT, i.e., fine-tuning. BERT (Frozen) uses frozen BERT with a trainable CNN text classification network <ref type="bibr" target="#b25">[26]</ref> on top of it.; Adapter-BERT adapts the BERT as in <ref type="bibr" target="#b15">[16]</ref>, where only the adapter blocks are trainable; W2V uses embeddings trained on the Amazon review data in <ref type="bibr" target="#b65">[66]</ref> using FastText <ref type="bibr" target="#b10">[11]</ref>. For ASC, we adopt the ASC classification network in <ref type="bibr" target="#b66">[67]</ref>, which takes aspect term and review sentence as input. For DSC and 20News, we adopt the classification network in <ref type="bibr" target="#b7">[8]</ref>.</p><p>Continual Learning (CL) Baselines. CL setting includes 4 baselines with no forgetting handling (NFH) (corresponding to the 4 standalone baselines), and 25 baselines from 9 state-of-the art task continual learning (Task-CL) methods that deal with forgetting (CF). NFH baselines learn the tasks one by one with no awareness of forgetting/transfer.</p><p>The 12 state-of-the-art CL systems are: KAN <ref type="bibr" target="#b21">[22]</ref> and SRK <ref type="bibr" target="#b36">[37]</ref> are Task-CL methods for document sentiment classification. The rest were designed initially for image classification. Therefore, we replace their original MLP or CNN image classification network with CNN for text classification <ref type="bibr" target="#b25">[26]</ref>.</p><p>HAT <ref type="bibr" target="#b51">[52]</ref> is one of the best Task-CL methods with almost no forgetting. CAT <ref type="bibr" target="#b20">[21]</ref> is similar to HAT but can work with a mixed sequence. UCL <ref type="bibr" target="#b0">[1]</ref> is a recent Task-CL method. EWC <ref type="bibr" target="#b26">[27]</ref> is a popular regularization-based class continual learning (Class-CL) method, which was adapted for Task-CL by only training on the corresponding head of the specific task ID during training and only considering the corresponding head's prediction during testing. OWM <ref type="bibr" target="#b67">[68]</ref> is also a Class-CL method, which we also adapt to Task-CL. L2 <ref type="bibr" target="#b26">[27]</ref> is a classic regularization based Class-CL method, which we adapt to Task-CL. A-GEM <ref type="bibr" target="#b3">[4]</ref> is an efficient version of the replay Task-CL method GEM <ref type="bibr" target="#b35">[36]</ref>, which penalizes the previous task loss from being increased. DER++ [2] is a recent replay method using distillation to regularize the new task training and it can function as a Task-CL method. B-CL <ref type="bibr" target="#b23">[24]</ref> is similar to CTR but uses dynamic routing for knowledge transfer and it performs markedly worse than CTR. LAMOL <ref type="bibr" target="#b57">[58]</ref> is a pseudo-replay method based on GPT-2.</p><p>From the 10 systems, we created 10 baselines using W2V embeddings with the aspect term added before the sentence so that the Task-CL methods can take both aspect and the review sentence for ASC; 7 baselines using Adapter-BERT (SRK, KAN and CAT's architectures cannot work with adapters); and 10 baselines using BERT (Frozen) (which replaces W2V embeddings). The BERT formulation in Sec. 3 naturally takes both aspect and review sentence in the ASC case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Hyper-parameters</head><p>Unless otherwise stated, the same hyper-parameters are used in experiments for ASC, DSC and 20News datasets. For the knowledge sharing module (KSM), we employ 2 layers of fully connected network with dimensions 768 in the TK-Layer. We employ 3 transfer capsules. We experimented with 2 to 15 capsules and selected 3 based on the validation data accuracy. For the task specific module (TSM), we use 2000 dimensions as the final and the hidden layers of the TSM. The task ID embeddings have 2000 dimensions. A fully connected layer with softmax output is used as the classification heads in the last layer of the BERT, together with the categorical cross-entropy loss. dropout of 0.5 between fully connected layers. The training of BERT, Adapter-BERT and CTR follows that of <ref type="bibr" target="#b64">[65]</ref>. We adopt BERT BASE (uncased). The maximum input length is set to 128 which is long enough for both ASC and DSC. We use Adam optimizer and set the learning rate to 3e-5. We use 10 epochs for SemEval datasets and 30 epochs for the other datasets in the ASC application. For DSC, we use 20 epochs. For 20News, we use 10 epochs. These are selected based on the validation results. The batch size is set to 32 for all cases. For all the other CL baselines, we use the code provided by their authors and adapt them for text classification. We also adopt their original parameters, including learning rate, early stopping, and batch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation Results and Analysis</head><p>Since the order of the tasks in a sequence may impact the final results, we randomly sampled 5 task sequences and averaged their results. We compute both accuracy and Macro-F1, where Macro-F1 is the primary metric as the imbalanced classes (in ASC) introduce biases on accuracy. <ref type="table">Table 2</ref> gives the average results of all the 19 tasks (or datasets) for ASC and 10 tasks (or datasets) for DSC and 20News over the 5 random task sequences.</p><p>Overall Performance. <ref type="table">Table 2</ref> shows that CTR clearly outperforms all baselines.</p><p>(1). For the standalone (SDL) baselines, BERT (with fine-tuning) and Adapter-BERT perform similarly. W2V and BERT (Frozen) are poorer, which is understandable.</p><p>(2). Comparing SDL (standalone learning) and NFH (continual learning with no forgetting handling), we see NFH is much better than SDL for W2V and BERT (Frozen). This indicates ASC and DSC tasks have similarities and thus shared knowledge. Catastrophic forgetting (CF) is not a major issue for W2V and BERT (Frozen). However, for 20News, NFH variants have serious CF. NFH with BERT (fine-tuning) is much worse than SDL and Adapter-BERT, which we explained in Introduction.  <ref type="table">Table 2</ref>: Accuracy (Acc.) and Macro-F1 (MF1), averaged over 5 random sequences. The architectures of SRK, KAN and CAT cannot work with Adapter-BERT. "-" means not applicable. Due to the limited space, standard deviation, running time and network size are given in Supplementary.</p><p>(3). Unlike BERT and Adapter-BERT, CTR does very well in both forgetting avoidance and knowledge transfer (outperforming all baselines). For baselines, EWC, UCL, OWM and HAT, although they perform better than NFH, they are significantly poorer than CTR as they don't have methods to encourage knowledge transfer for ASC and DSC. KAN and SRK do knowledge transfer but they are weaker than many other CL methods. They perform very poorly for 20News as they have limited ability to overcome CF. CAT works well with large datasets, but is weak for small datasets.</p><p>(4). CTR's improvements over SDL variants for DSC (large) is less than for DCS (small). This is understandable because when the training data is large, learning a separate model is already good, and knowledge transfer is less important. <ref type="bibr" target="#b4">(5)</ref>. Compared with the SDL results, we can see that CTR has the least forgetting on 20News. <ref type="bibr" target="#b5">(6)</ref>. Compared to B-CL, CTR is markedly better in knowledge transfer. The forgetting rates (FR) of B-CL and CTR are both low. The comparison is in fact the same as comparing dynamics routing and transfer routing. We can see that the proposed transfer routing is dramatically better than dynamic routing for knowledge transfer. Additionally, transfer routing eliminates the need for hyper-parameter tuning on the number of iterations of dynamic routing <ref type="bibr" target="#b49">[50]</ref> to update the agreements. <ref type="bibr" target="#b6">(7)</ref>. CTR outperforms LAMOL in ASC and 20News even with the less powerful BERT model that CTR adopts. LAMOL outperforms BERT-based MTL in DSC. This may be because LAMOL is based on GPT-2, which is known to be more powerful than BERT (also shown in the LAMOL paper). For 20News, since its tasks are very different/dissimilar, there is little shared knowledge. Dealing with CF is the main issue. LAMOL has serious forgetting as its FR values show.  <ref type="table">Table 3</ref>: Ablation experiment results.</p><p>Effectiveness of Knowledge Transfer. Forward transfer is defined as the forward performance (CTR(forward) in <ref type="table">Table 2</ref>) subtracting the standalone (SDL) result. CTR(forward) is the test accuracy or MF1 of each task when it was first learned. Backward transfer is defined as the difference between the backward performance (CTR in <ref type="table">Table 2</ref>, the final result after all tasks are learned) and the forward performance. The average results of CTR (forward) and CTR are given in <ref type="table">Table 2</ref>. We observe that forward transfer of CTR is highly effective for the three datasets with similar tasks. For DSC, the less the data, the more effective is the transfer, which is reasonable. Backward transfer improves the accuracy and MF1 of ASC and DSC (small). For DSC (full), it is slightly weaker and for 20News, it is also slightly weaker due to a very small amount of forgetting, but the less than 0.0055 CF is negligible. Note that in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b45">46]</ref>, forward transfer is measured by comparing the test results of the new task on the current learned network and a random initialized network before/without training the new task (like zero-shot). This method indicates whether the learned network contains some useful knowledge for the new task. However, it does not tell how much forward transfer actually happens after learning the new task, which is more important and is what our method measures.</p><p>Overcoming Forgetting. To validate CTR's effectiveness in dealing forgetting with a sequence of dissimilar tasks, we compute the Forgetting Rate <ref type="bibr" target="#b33">[34]</ref>, FR = 1</p><formula xml:id="formula_7">t?1 t?1 i=1 A i,i ? A t,i , where A i,i</formula><p>is the forward accuracy of task i and A t,i is the accuracy of task i after training the last task t. We average over all tasks except the last one because the last task obviously has no forgetting. We report the forgetting rate FR (averaged over 5 random task sequences) for the 20News data on the two evaluation metrics in the last two columns of <ref type="table">Table 2</ref> (the other two datasets are mainly for knowledge transfer). CTR has very low FR values which indicate very little forgetting.</p><p>Ablation Study. The results of ablation experiments are in <ref type="table">Table 3</ref>. "-KSM;-TSM" means that the knowledge sharing module (KSM) and the task specific module (TSM) are not used, and we simply deploy the Adapter-BERT. "-KSM" means that the knowledge sharing module (KSM) is not used. "-TSM" means that the task specific module (TSM) is not used. "-TR/KSM" means that the task router (TR) in KSM is not used. We directly send the transferable representation v tran(t) j|i to the next layer. "dynamic routing" means that we replace our transfer routing (-(SE&amp;TR)/KSM) with dynamic routing <ref type="bibr" target="#b49">[50]</ref>, which is one of the most popular routing algorithms in capsule networks. "on top" means adding a CL-plugin on top of BERT. "after the first FF layer" means adding only one CL-plugin there in BERT. "after the other two FF layers" means adding only one CL-plugin module there in BERT. <ref type="table">Table 3</ref> shows that the full CTR system gives the best results, indicating every component contributes to the model and other options of adding CL-plugins are all poorer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper studied task continual learning (Task-CL) using the pre-trained model BERT to achieve both CF presentation and knowledge transfer. It proposed a novel technique called CTR to leverage the pre-trained BERT for CL. The key component of CTR is the CL-plugin inserted in BERT. A CL-plugin is a capsule network with a new transfer routing mechanism to encourage knowledge transfer among tasks and also to isolate task-specific knowledge to avoid forgetting. Experimental results using three NLP applications showed that CTR markedly improves the performance of both the new task and the old tasks via knowledge transfer and is also effective at overcoming catastrophic forgetting. One limitation of our work is the efficiency due to the use of capsules. Capsules try to represent a group of neurons in a vector reflecting properties of an entity. In NLP, an entity is a sentence/document which contains many tokens (e.g., 128) and features (e.g. 768 in BERT BASE ). Grouping them makes the capsule very large (e.g., 128 ? 768), which slows down training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architecture of BERT (left) and the proposed system CTR (right), which inserts two CL-plugins in BERT. Each CL-plugin module (far right) has two sub-modules and a skip connection: knowledge sharing sub-module (KSM) and task-specific sub-module (TSM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(A) CL-plugin Architecture. (B) Illustration of task masking. Cells/neurons in brown, green and red are respectively used by tasks 0, 1 and 2. Neurons with two colors are used by two tasks 4 Continual Learning Plug-in (CL-plugin)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>38.40 59.17 48.39 69.80 62.63 72.14 65.39 24.57 32.05 A-GEM 86.06 78.44 59.33 45.94 70.67 61.77 93.31 92.95 4.09 4.48 DER++ 84.27 75.08 72.29 66.28 86.70 85.46 60.44 49.67 10.54 12.16 KAN 85.49 77.38 77.27 72.34 82.32 81.23 73.07 69.97 15.52 18.87 SRK 84.76 78.52 78.58 76.03 83.99 82.66 79.64 77.89 12.06 13.52.43 67.26 62.76 73.03 71.50 69.56 65.50 23.12 27.39 A-GEM 45.88 28.21 62.89 55.96 71.22 69.94 60.29 50.40 40.22 51.20 DER++ 47.63 35.54 70.52 63.56 59.67 57.82 58.95 49.58 36.39 45.30 EWC 56.30 49.58 58.23 51.03 62.69 61.51 61.86 53.94 37.79 46.58 UCL 64.46 36.64 48.30 32.07 57.06 55.86 51.75 36.71.18 53.40 38.44 67.15 65.42 71.97 68.75 24.00 27.56 HAT 80.83 63.63 62.57 50.83 69.75 65.44 67.73 64.43 26.04 29.70 CAT 76.28 54.65 55.19 35.28 79.58 75.99 70.38 68.04 24.37 26.95</figDesc><table><row><cell>Scenarios</cell><cell>Category</cell><cell>Model</cell><cell>ASC Acc. MF1</cell><cell>DSC (small) Acc. MF1</cell><cell>DSC (full) Acc. MF1</cell><cell>20News Acc. MF1</cell><cell cols="2">20News (FR) Acc. MF1</cell></row><row><cell></cell><cell>BERT</cell><cell>MTL</cell><cell cols="4">91.91 88.11 85.05 84.03 89.77 89.28 96.77 96.77</cell><cell>-</cell><cell>-</cell></row><row><cell>Non-continual</cell><cell>BERT</cell><cell>SDL</cell><cell cols="4">85.84 76.35 78.04 74.17 87.84 86.80 96.49 96.48</cell><cell>-</cell><cell>-</cell></row><row><cell>Learning (SDL)</cell><cell>BERT (Frozen)</cell><cell>SDL</cell><cell cols="4">78.14 58.13 73.88 67.97 85.34 80.17 96.49 96.48</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Adapter-BERT</cell><cell>SDL</cell><cell cols="4">85.96 78.07 76.31 71.04 88.30 87.31 96.20 96.19</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>W2V</cell><cell>SDL</cell><cell cols="4">77.01 51.89 62.06 53.80 69.57 65.51 94.72 94.72</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>BERT</cell><cell>NFH</cell><cell cols="6">49.60 43.08 73.87 69.44 73.08 71.81 52.50 39.22 24.29 30.52</cell></row><row><cell></cell><cell>BERT (Frozen)</cell><cell>NFH</cell><cell cols="5">85.51 76.64 83.12 79.23 61.88 45.79 83.28 81.81 8.76</cell><cell>9.73</cell></row><row><cell></cell><cell>Adapter-BERT</cell><cell>NFH</cell><cell cols="6">54.03 44.81 63.76 53.95 64.94 63.40 68.29 61.70 30.59 3.79</cell></row><row><cell></cell><cell>W2V</cell><cell>NFH</cell><cell cols="5">82.69 73.56 65.16 57.48 70.40 68.03 90.74 90.59 4.30</cell><cell>4.47</cell></row><row><cell></cell><cell>BERT (frozen)</cell><cell>L2</cell><cell cols="6">56.04 97</cell></row><row><cell></cell><cell></cell><cell>EWC</cell><cell cols="5">86.37 74.52 82.38 78.41 72.77 65.76 80.26 78.60 3.50</cell><cell>3.03</cell></row><row><cell></cell><cell></cell><cell>UCL</cell><cell cols="5">83.89 74.82 80.12 74.13 74.76 69.48 94.65 94.63 0.48</cell><cell>0.48</cell></row><row><cell>Continual Learning (CL)</cell><cell></cell><cell>OWM HAT CAT</cell><cell cols="6">87.02 79.31 58.07 42.63 86.30 85.36 84.54 82.73 13.80 15.81 86.74 78.16 79.48 72.78 87.29 86.14 93.51 92.93 2.26 2.89 83.68 68.64 67.41 56.22 87.34 86.51 95.17 95.16 0.80 0.81</cell></row><row><cell></cell><cell>Adapter-BERT</cell><cell>L2</cell><cell cols="5">63.97 06 4.70</cell><cell>6.60</cell></row><row><cell></cell><cell></cell><cell>OWM</cell><cell cols="6">72.99 66.51 73.97 71.96 85.46 84.57 71.10 66.25 27.38 32.76</cell></row><row><cell></cell><cell></cell><cell>HAT</cell><cell cols="5">86.14 78.52 80.83 78.41 88.00 87.26 95.22 95.21 0.33</cell><cell>0.34</cell></row><row><cell></cell><cell></cell><cell>L2</cell><cell cols="6">60.36 39.13 54.34 43.19 57.71 48.00 59.54 54.40 7.83 11.89</cell></row><row><cell></cell><cell></cell><cell cols="6">A-GEM 81.33 63.35 69.80 60.07 77.67 70.75 90.72 90.60 3.94</cell><cell>4.08</cell></row><row><cell></cell><cell>W2V</cell><cell>DER++ KAN</cell><cell cols="6">83.27 69.93 77.51 73.13 74.79 66.68 89.28 89.19 4.32 72.06 40.01 57.13 43.75 69.35 64.78 57.92 51.65 20.98 27.02 4.42</cell></row><row><cell></cell><cell></cell><cell>SRK</cell><cell cols="5">71.01 39.63 64.47 55.93 69.65 65.25 61.07 58.47 7.26</cell><cell>8.81</cell></row><row><cell></cell><cell></cell><cell>EWC</cell><cell cols="5">84.16 72.29 64.82 57.20 70.00 65.11 91.86 91.80 2.64</cell><cell>2.71</cell></row><row><cell></cell><cell></cell><cell>UCL</cell><cell cols="5">84.41 75.99 56.23 41.34 70.56 67.01 90.61 90.46 4.53</cell><cell>4.70</cell></row><row><cell></cell><cell cols="7">OWM 82.70 B-CL 88.29 81.40 82.01 80.63 79.76 76.51 95.07 95.04 0.58</cell><cell>0.59</cell></row><row><cell></cell><cell>LAMOL</cell><cell></cell><cell cols="6">88.91 80.59 89.12 86.58 92.11 91.72 66.13 45.74 20.03 16.60</cell></row><row><cell></cell><cell cols="2">CTR (forward)</cell><cell cols="4">87.89 80.25 83.75 82.55 89.86 89.16 95.63 95.62</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>CTR</cell><cell></cell><cell cols="5">89.47 83.62 84.34 83.29 89.31 88.75 95.25 95.23 0.42</cell><cell>0.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>. The results of MTL (the upper bound) are only slightly better than CTR, which again shows that CTR is highly effective in overcoming forgetting and encouraging knowledge transfer.</figDesc><table><row><cell>Model</cell><cell>Acc.</cell><cell>ASC</cell><cell>MF1</cell><cell>DSC (small) Acc. MF1</cell><cell>DSC (full) Acc. MF1</cell><cell>20News Acc. MF1</cell></row><row><cell>CTR (-KSM;-TSM)</cell><cell cols="6">0.5403 0.4481 0.6376 0.5395 0.6494 0.6340 0.6829 0.6170</cell></row><row><cell>CTR (-TSM)</cell><cell cols="6">0.8312 0.7107 0.7085 0.6759 0.8545 0.8380 0.8275 0.8064</cell></row><row><cell>CTR (-KSM)</cell><cell cols="6">0.8614 0.7852 0.8083 0.7841 0.8800 0.8726 0.9522 0.9521</cell></row><row><cell>CTR (-TR/KSM)</cell><cell cols="6">0.8819 0.8155 0.8244 0.8119 0.8831 0.8762 0.9476 0.9469</cell></row><row><cell>CTR (dynamic routing) (B-CL)</cell><cell cols="6">0.8829 0.8140 0.8201 0.8063 0.7976 0.7651 0.9507 0.9504</cell></row><row><cell>CTR (on top)</cell><cell cols="6">0.8135 0.6390 0.7301 0.6875 0.8266 0.8105 0.8927 0.8920</cell></row><row><cell>CTR (after the first FF layer)</cell><cell cols="6">0.8741 0.8014 0.8300 0.8183 0.8699 0.8596 0.9381 0.9373</cell></row><row><cell cols="7">CTR (after the other two FF layers) 0.8662 0.7863 0.8269 0.8161 0.8714 0.8612 0.9339 0.9316</cell></row><row><cell>CTR</cell><cell cols="6">0.8947 0.8362 0.8434 0.8329 0.8931 0.8875 0.9525 0.9523</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by two National Science Foundation (NSF) grants (IIS-1910424 and IIS-1838770), a DARPA Contract HR001120C0023, and a Northrop Grumman research gift.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Uncertainty-based continual learning with adaptive regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4394" to="4404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Dark experience for general continual learning: a strong</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Buzzega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Boschini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07211</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">simple baseline. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Toward an architecture for never-ending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hruschka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient lifelong learning with A-GEM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lifelong machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="207" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lifelong learning for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="750" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y.</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-N</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02123</idno>
		<title level="m">Lifelong language knowledge distillation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<editor>J. Burstein, C. Doran, and T. Solorio</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A holistic lexicon-based approach to opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 international conference on web search and data mining</title>
		<meeting>the 2008 international conference on web search and data mining</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Pathnet: Evolution channels gradient descent in super neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Banarse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zwols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno>abs/1701.08734</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning word vectors for 157 languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Psycholinguistics meets continual learning: Measuring catastrophic forgetting in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Greco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bernardi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04229</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural topic modeling with continual lifelong learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Runkler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schuetze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3907" to="3917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic interference using conceptor-aided backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jaeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Transforming auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial neural networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of Machine Learning Research</title>
		<editor>K. Chaudhuri and R. Salakhutdinov</editor>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="2790" to="2799" />
			<date type="published" when="2019" />
			<publisher>PMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGKDD</title>
		<meeting>ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Continual learning by using information of each class holistically</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="7797" to="7805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep generative dual memory network for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kamra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1710.10368</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Continual learning of a mixed sequence of similar and dissimilar tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Continual learning with knowledge transfer for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML-PKDD</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Classic: Continual and contrastive learning of aspect sentiment classification tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adapting bert for continual learning of a sequence of aspect sentiment classification tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4746" to="4755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Regularized training objective for continued training for domain adaptation in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Khayrallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</title>
		<meeting>the 2nd Workshop on Neural Machine Translation and Generation</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="36" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<editor>A. Moschitti, B. Pang, and W. Daelemans</editor>
		<imprint>
			<biblScope unit="page" from="1746" to="1751" />
			<date type="published" when="2014" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clopath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<idno>abs/1612.00796</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Newsweeder: Learning to filter netnews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference on Machine Learning</title>
		<meeting>the Twelfth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="331" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Continual learning: A comparative study on how to defy forgetting in classification tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<idno>abs/1909.08383</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting by incremental moment matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4652" to="4662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Compositional language continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automated rule selection for aspect extraction in opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ungar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sedoc</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09187</idno>
		<title level="m">Continual learning for sentence representations using conceptors</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mnemonics training: Multi-class incremental learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12242" to="12251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Exploring fine-tuning techniques for pre-trained cross-lingual models via continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Winata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14218</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gradient episodic memory for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6467" to="6476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sentiment classification by leveraging the shared knowledge from a sequence of domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DASFAA</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="795" to="811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Crook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15504</idno>
		<title level="m">Continual learning in task-oriented dialogue systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Packnet: Adding multiple tasks to a single network by iterative pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7765" to="7773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology of learning and motivation</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1989" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">What happens to BERT embeddings during fine-tuning? CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Merchant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahimtoroghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tenney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adapterfusion: Non-destructive task composition for transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>R?ckl?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Using the past knowledge to improve sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-findings</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Anatomy of catastrophic forgetting: Hidden representations and task semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">V</forename><surname>Ramasesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning to learn without forgetting by maximizing transfer and minimizing interference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riemer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cases</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ajemian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Complementary learning for overcoming catastrophic forgetting using experience replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rostami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Pilly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3339" to="3345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<idno>abs/1606.04671</idno>
	</analytic>
	<monogr>
		<title level="j">Progressive neural networks. CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">ELLA: an efficient lifelong learning algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ruvolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eaton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="507" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3856" to="3866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beatson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Continual learning in generative adversarial nets. CoRR, abs/1705.08395</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting with hard attention to the task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Suris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karatzoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4555" to="4564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A progressive model to enable continual learning for semantic slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Continual learning with deep generative replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Lifelong learning CRF for supervised aspect extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="148" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Lifelong machine learning systems: Beyond learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lifelong Machine Learning, Papers from the 2013 AAAI Spring Symposium</title>
		<meeting><address><addrLine>Palo Alto, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Utilizing BERT for aspect-based sentiment analysis via constructing auxiliary sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<meeting><address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="380" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Lamol: Language modeling is all you need for lifelong language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y.</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Aspect level sentiment classification with deep memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<meeting><address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11" />
			<biblScope unit="page" from="214" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Forward and backward knowledge transfer for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="457" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Lifelong learning memory networks for aspect sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Big Data</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="861" to="870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Efficient meta lifelong-learning with limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>P?czos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Distantly supervised lifelong learning for large-scale social media sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affective Computing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="480" to="491" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">BERT post-training for review reading comprehension and aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<editor>J. Burstein, C. Doran, and T. Solorio</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2324" to="2335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Dual attention network for product compatibility and function satisfiability analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Aspect based sentiment analysis with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<editor>I. Gurevych and Y. Miyao</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2514" to="2523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Continuous learning of context-dependent processing in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Continual learning through synaptic intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3987" to="3995" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
