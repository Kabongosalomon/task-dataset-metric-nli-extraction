<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ANOSEG: ANOMALY SEGMENTATION NETWORK US- ING SELF-SUPERVISED LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jou</forename><forename type="middle">Won</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Sogang University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyeongbo</forename><surname>Kong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Media communication</orgName>
								<orgName type="institution">Pukyong National University</orgName>
								<address>
									<settlement>Busan</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye-In</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Sogang University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Gyun</forename><surname>Kim</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">LG Display</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suk-Ju</forename><surname>Kang</surname></persName>
							<email>sjkang@sogang.ac.kr1kbkong@pknu.ac.kr2ksglcd@lgdisplay.com3</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Sogang University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ANOSEG: ANOMALY SEGMENTATION NETWORK US- ING SELF-SUPERVISED LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Anomaly segmentation, which localizes defective areas, is an important component in large-scale industrial manufacturing. However, most recent researches have focused on anomaly detection. This paper proposes a novel anomaly segmentation network (AnoSeg) that can directly generate an accurate anomaly map using self-supervised learning. For highly accurate anomaly segmentation, the proposed AnoSeg considers three novel techniques: Anomaly data generation based on hard augmentation, self-supervised learning with pixel-wise and adversarial losses, and coordinate channel concatenation. First, to generate synthetic anomaly images and reference masks for normal data, the proposed method uses hard augmentation to change the normal sample distribution. Then, the proposed AnoSeg is trained in a self-supervised learning manner from the synthetic anomaly data and normal data. Finally, the coordinate channel, which represents the pixel location information, is concatenated to an input of AnoSeg to consider the positional relationship of each pixel in the image. The estimated anomaly map can also be utilized to improve the performance of anomaly detection. Our experiments show that the proposed method outperforms the state-of-the-art anomaly detection and anomaly segmentation methods for the MVTec AD dataset. In addition, we compared the proposed method with the existing methods through the intersection over union (IoU) metric commonly used in segmentation tasks and demonstrated the superiority of our method for anomaly segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Anomaly segmentation is the process that localizes anomaly regions. In the real world, since the number of anomaly data is very limited, conventional anomaly segmentation methods are trained using only normal data. Typically, many anomaly segmentation methods are based on anomaly detection techniques because the real dataset includes few anomaly images without the ground truth (GT) mask. Therefore, these methods are not trained directly on pixel-level segmentation and they are difficult to generate anomaly maps similar to GT masks.</p><p>Specifically, existing reconstruction-based methods using autoencoder (AE) <ref type="bibr" target="#b1">(An &amp; Cho (2015)</ref>; <ref type="bibr" target="#b2">Baur et al. (2018)</ref>; <ref type="bibr" target="#b18">Sakurada &amp; Yairi (2014)</ref>; <ref type="bibr" target="#b5">Chen et al. (2017)</ref>; <ref type="bibr" target="#b3">Bergmann et al. (2019)</ref>) and generative adversarial network (GAN) <ref type="bibr" target="#b0">(Akcay et al. (2018)</ref>; <ref type="bibr" target="#b7">Deecke et al. (2018)</ref>; <ref type="bibr" target="#b20">Schlegl et al. (2017)</ref>; <ref type="bibr" target="#b25">Zenati et al. (2018)</ref>) are trained to learn reconstruction of normal images and determine anomaly if the test sample has the high reconstruction error for an abnormal region. However, reconstructionbased methods often restore even non-complex anomaly regions, which degrade the performance on both anomaly detection and segmentation. Therefore, the anomaly map in <ref type="figure" target="#fig_0">Fig. 1(b)</ref> greatly differs from the corresponding GT mask. Alternative methods have been recently studied by using the high-level learned representation for anomaly detection and segmentation. These methods use a pretrained model to extract a holistic representation of a given image and compare it to the representation of a normal image. Also, several existing methods use patches, splitting a given image to perform anomaly segmentation. By extracting representations from an image patch, these methods compute the scores of the image patches and combine them to generate the final anomaly map. Therefore, the quality of anomaly maps is highly correlated with the patch size. The uninformed students (US) <ref type="bibr" target="#b4">(Bergmann et al. (2020)</ref>) in Figs. 1(c) and (d) are trained using a small patch size (17 x 17) and a large patch size (65 x 65), respectively. Therefore, as shown in <ref type="figure" target="#fig_0">Fig. 1(d)</ref>, US 65 x 65 is difficult to detect small anomaly regions. Patch SVDD <ref type="bibr" target="#b24">(Yi &amp; Yoon (2020)</ref>) and SPADE <ref type="bibr" target="#b6">(Cohen &amp; Hoshen (2020)</ref>) use feature maps of multiple scales to detect anomaly regions with various sizes. However, as shown in <ref type="figure" target="#fig_0">Figs. 1(e</ref>) and (f), these methods approximately detect anomaly regions. In addition, in GradCAM-based methods, <ref type="bibr">GradCAM (Selvaraju et al. (2017)</ref>) is used to generate anomaly maps to detect regions that influence the decision of the trained model <ref type="bibr" target="#b10">(Kimura et al. (2020)</ref>; <ref type="bibr" target="#b23">Venkataramanan et al. (2020)</ref>). CutPaste <ref type="bibr" target="#b12">(Li et al. (2021)</ref>) introduces a self-supervised framework using a simple effective augmentation that encourages the model to find local irregularities. CutPaste also performs anomaly localization through GradCAM by extending the model to use patch images after training the classifier. However, these methods are not aimed at anomaly segmentation and detect anomaly regions using a modified anomaly detection method. Generally, to improve the segmentation performance, a methodology that can be learned pixel-wise should be considered. Therefore, existing methods cannot clearly detect anomalies because it is difficult that directly use the pixel-wise loss such as a mean squared error typically used in the segmentation task.</p><p>To handle this problem, this paper proposes a new methodology that can directly learn the segmentation task. The proposed anomaly segmentation network (AnoSeg) can generate an anomaly map to segment the anomaly region that is unrelated to the normal class. The goal of AnoSeg is to generate an anomaly map that represents the normal class region within a given image for anomaly segmentation, unlike the existing methods to indirectly extract anomaly maps. For this goal, our AnoSeg proposes three following approaches. First, as shown in <ref type="figure">Fig. 2</ref>, AnoSeg uses the segmentation loss directly using the synthesized data generated through hard augmentation, which generates data shifted away from the input data distribution. Second, AnoSeg learns to generate the anomaly map and reconstruct normal images.</p><p>Also, an adversarial loss is applied by using a generated anomaly map and an input image. Unlike the existing GAN, the discriminator of AnoSeg determines whether the image is a normal class and whether the anomaly map is focused on the normal region. Since the anomaly map learns the normal sample distribution, AnoSeg has high generalization for unseen normal and anomaly regions even with a small number of normal samples.</p><p>Third, we propose the coordinate channel concatenation using a coordinate vector based on coordconv <ref type="bibr" target="#b14">(Liu et al. (2018)</ref>). Anomaly regions in a particular category often depend on the location information of a given image. Therefore, the proposed coordinate vector helps to understand the positional relationship of normal and anomaly regions in the input image. As a result, <ref type="bibr">Fig. 1(h)</ref> shows that the anomaly map of AnoSeg is very similar to GT even without thresholding. Moreover, we describe how to perform the anomaly detection using the generated anomaly map. By simply extending the model using an anomaly map to the existing GAN-based method <ref type="bibr" target="#b17">(Sabokrou et al. (2018)</ref>), we could achieve 96.4 area under ROC curve (AUROC) for image-level localization, which is a significant improvement over conventional state-of-the-art (SOTA) methods. As a result, the proposed method achieves SOTA performance on the MVTec Anomaly Detection (MVTec AD) dataset for anomaly detection and segmentation compared to conventional methods without using a pretrained model. The main contributions of this study are summarized as follows:</p><p>? We propose a novel anomaly segmentation network (AnoSeg) to directly generate an anomaly map. AnoSeg generates detailed anomaly maps using the holistic approaches to maximize segmentation performance.</p><p>? The proposed anomaly map can also be used in existing anomaly detection methods to improve the anomaly detection performance.</p><p>? In anomaly segmentation and detection, AnoSeg outperforms SOTA methods on the MVTec AD dataset in terms of intersection over union (IoU) and AUROC. Additional experiments using IoU metric also show that AnoSeg is robust for thresholding. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Anomaly detection is a research topic that has received considerable attention. Anomaly detection and segmentation are usually performed via unsupervised methods using the generative model for learning the distribution of a certain class. In these methods, GAN <ref type="bibr" target="#b8">(Goodfellow et al. (2014)</ref>) or VAE <ref type="bibr" target="#b11">(Kingma &amp; Welling (2014)</ref>) learned the distribution of a certain class and used the difference between a reconstructed image and an input for anomaly detection <ref type="bibr" target="#b1">(An &amp; Cho (2015)</ref>; <ref type="bibr" target="#b5">Chen et al. (2017)</ref>; <ref type="bibr" target="#b18">Sakurada &amp; Yairi (2014)</ref>; <ref type="bibr" target="#b17">Sabokrou et al. (2018)</ref>). In addition, initial deep learning-based anomaly segmentation methods focused on generative models such as GAN <ref type="bibr" target="#b20">(Schlegl et al. (2017)</ref>) and AE <ref type="bibr" target="#b3">(Bergmann et al. (2019)</ref>). However, these approaches could have high reconstruction performance for simple anomaly regions. Recently, methods using a representation of an image patch have shown great effectiveness in anomaly detection <ref type="bibr" target="#b24">(Yi &amp; Yoon (2020)</ref>; <ref type="bibr" target="#b6">Cohen &amp; Hoshen (2020)</ref>). In <ref type="bibr" target="#b4">Bergmann et al. (2020)</ref>, US was trained to mimic a pretrained teacher by dividing an image into patches. In recent studies <ref type="bibr" target="#b12">(Li et al. (2021)</ref>), an activation map that visualizes the region of interest through GradCAM <ref type="bibr" target="#b21">(Selvaraju et al. (2017)</ref>) was applied to anomaly detection. <ref type="bibr" target="#b10">Kimura et al. (2020)</ref> generated an activation map using GradCAM to focus only on the reconstruction loss of the ROI. <ref type="bibr" target="#b23">Venkataramanan et al. (2020)</ref> improved the detection performance using an activation map in the training process. <ref type="bibr" target="#b15">Liznerski et al. (2021)</ref> apply one-class classification on features extracted from a fully convolutional network and use receptive field upsampling with Gaussian smoothing to extract anomaly map. However, in these existing methods, it is difficult to apply the loss related to anomaly segmentation because the model does not directly generate an anomaly map by using the modified anomaly detection method. Our method is different from the conventional methods which use Grad-CAM to indirectly extract the activation map. Instead, the proposed method directly extracts and supervises the anomaly map. Therefore, the proposed method discriminates between anomaly and normal regions more accurately compared to previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED METHOD: ANOSEG</head><p>The proposed AnoSeg is a "holistic" approach which incorporates three techniques: self-supervised learning using hard augmentation, adversarial learning, and coordinate channel concatenation. The details are explained in the following sub-sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SELF-SUPERVISED LEARNING USING HARD AUGMENTATION</head><p>To train anomaly segmentation directly, an image with an anomaly region and its corresponding GT mask corresponding to the image are required. However, it is difficult to obtain these images and GT masks in the real case. Therefore, the proposed method uses hard augmentation <ref type="bibr" target="#b22">(Tack et al. (2020)</ref>) and Cutpaste <ref type="bibr" target="#b12">(Li et al. (2021)</ref>) to generate synthetic anomaly data and GT masks. Hard augmentation refers to generating samples shifted away from the original sample distribution. As <ref type="figure">Figure 2</ref>: Overview of the training process of the proposed AnoSeg. AnoSeg generates reconstructed images and anomaly maps. To directly generate anomaly maps, AnoSeg applies three novel techniques: hard augmentation, adversarial learning, and coordinate channel concatenation.</p><p>Figure 3: Our synthetic anomaly data augmentation. The synthetic anomaly data is generated by several hard augmentations and Cutpaste <ref type="bibr" target="#b12">(Li et al. (2021)</ref>). Synthetic anomaly data is generated by applying a rotation, perm, color jitter, and Cutpaste for each step. Hard augmentations are applied with a 50% chance.</p><p>confirmed in <ref type="bibr" target="#b22">Tack et al. (2020)</ref>, the hard augmented samples can be used as a negative samples. Therefore, as shown in <ref type="figure">Fig. 3</ref>, we use three types of hard augmentation: rotation, perm, and color jitter. Each augmentation is applied with a 50% chance. Then, like Cutpaste <ref type="bibr" target="#b12">(Li et al. (2021)</ref>), the augmented data is pasted into a random region of normal data to generate the synthetic anomaly data and corresponding masks for segmentation. Finally, the anomaly segmentation dataset is composed as follows:</p><formula xml:id="formula_0">x Seg = {x N or , x Ano } , A Seg = {A N or , A Ano } ,<label>(1)</label></formula><p>where x seg is a set of normal and synthetic anomaly images, in which x N or and x Ano are normal images and synthetic anomaly images, respectively. A seg is a set of normal and synthetic anomaly masks, in which A N or and A Ano are normal masks with all inner values set to one and synthetic anomaly masks, respectively.</p><p>Using the anomaly segmentation dataset with a pixel-level loss, we can directly train our AnoSeg. The anomaly segmentation loss L Seg is as follows:</p><formula xml:id="formula_1">L Seg = E A Seg ? A Seg 1 ,<label>(2)</label></formula><p>where A Seg indicates the generated anomaly map (normal and anomaly classes). The generated anomaly map has the same size as an input image and outputs a value in the range of [0, 1] for each pixel depending on the importance of the pixel of the input image. However, since the synthetic anomaly data are only subset of various anomaly data, it is difficult to generate a real anomaly maps that are unseen in training phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ADVERSARIAL LEARNING WITH RECONSTRUCTION</head><p>To improve the generality for various anomaly data, it is important to train normal region distribution accurately. Therefore, AnoSeg utilizes masked reconstruction loss that uses reconstruction loss only in normal regions to learn only the distribution of normal regions and avoid bias of the distribution of synthetic anomaly regions. Also, since the discriminator inputs a pair for an input image and its GT masks, the discriminator and generator can focus on normal region distribution. Thus, anomaly region cannot be reconstructed well and the detail of the anomaly map can also be improved. Loss functions for adversarial learning are as follows:</p><formula xml:id="formula_2">L Adv = min G max D {E [log(D(concat(x Seg , A Seg )))] + E [log(1 ? D(concat( x Seg , A Seg )))]},<label>(3)</label></formula><formula xml:id="formula_3">L Re = E x Seg * A Seg ? x Seg * A Seg 1 /E A Seg 1 ,<label>(4)</label></formula><p>where D, G, and concat are a discriminator, a generator, and a concatenation operation, respectively. In Section 5, we demonstrated the effectiveness of adversarial loss. In the typical segmentation task, the location information is the most important information because normal and anomaly regions can be changed depending on where they are located. To provide additional location information, we use a coordinate vector inspired by CoordConv <ref type="bibr" target="#b14">(Liu et al. (2018)</ref>). We first generate rank 1 matrices that are normalized to <ref type="bibr">[-1, 1]</ref>. Then, we concatenate these matrices with the input image as channels ( <ref type="figure" target="#fig_1">Fig. 4)</ref>. As a result, AnoSeg extracts features by considering the positional relationship of the input image. In ablation study, we demonstrated the effectiveness of coordinate channel concatenation. <ref type="figure">Figure 5</ref>: An overview of the proposed anomaly detection method. To obtain anomaly score, the pair of images reconstructed from the anomaly map and the anomaly detector (fake pair) are compared with the pair of the normal mask and the input image (real pair) using a discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">COORDINATE CHANNEL CONCATENATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">ANOMALY DETECTION USING PROPOSED ANOMALY MAP</head><p>In this section, we design a simple anomaly detector that adds the proposed anomaly map to the existing GAN-based detection method <ref type="bibr" target="#b17">(Sabokrou et al. (2018)</ref>).</p><p>The proposed anomaly detector performs anomaly detection by learning only normal data distribution. We simply concatenate the input image and anomaly map to use them as inputs of detector, and apply both an adversarial loss and a reconstruction loss. Then, we use the feature matching loss introduced in <ref type="bibr" target="#b19">(Salimans et al. (2016)</ref>) to stabilize the learning of the discriminator and extract the anomaly score. We include a detailed description of the training process for anomaly detection in Appendix A.</p><p>In the test process ( <ref type="figure">Fig. 5</ref>), the proposed anomaly detector obtains anomaly scores using the discriminator that has learned the normal data distribution. We first assume that the input image is normal, so the mask A N or with all inner values set to one is used in pairs with the input image. When the input image is really normal, a fake pair (anomaly map and reconstructed image) is similar to the real pair (normal mask and input image), so the anomaly detector has a low anomaly score. On the other hand, when the input image is abnormal, the fake pair is significantly different to the real pair, so it has a high anomaly score. To compare the real and fake pair, the reconstruction loss and the feature matching loss are used as follows:  <ref type="bibr" target="#b3">(Bergmann et al. (2019)</ref>). Full results for anomaly detection are added in where ? and ? are 1 and 0.1, respectively. A N or and L M SE represent a normal GT mask and the mean squared error, respectively.</p><formula xml:id="formula_4">Score = ?L M SE (f (concat(x Seg , A N or )), f (concat( x Seg , A Seg ))) + ?L M SE (x Seg , x Seg ),<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EVALUATION DATASETS AND METRICS</head><p>To verify the anomaly segmentation and detection performance of the proposed method, several evaluations were performed on the MVTec AD dataset <ref type="bibr" target="#b3">(Bergmann et al. (2019)</ref>  <ref type="formula" target="#formula_1">(2020)</ref>), we adopted the pixel-level and image-level AUROCs to quantitatively evaluate the performance of different methods for anomaly segmentation and detection, respectively.</p><p>In addition, we used IoU to evaluate anomaly segmentation. For the measurement of IoU, a threshold, which maximizes IoU, was applied in each method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">IMPLEMENTATION DETAILS</head><p>The encoder of AnoSeg consists of the convolution layers of ResNet-18 <ref type="bibr" target="#b9">(He et al. (2016)</ref>). The up-sampling layer of decoders consists of one transposed convolution layer and convolution layers. Two decoders of the AnoSeg are composed of five up-sampling layers and two convolution layer to generate an anomaly map and a reconstructed image. The structure of the anomaly detector is the same as the AnoSeg structure except for the decoder that generates the anomaly map. Detailed information on training process and the network architecture is described in Appendix B.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">EXPERIMENTS ON THE MVTEC AD DATASET</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">COMPARED METHODS</head><p>We compared the reconstruction-based method with the proposed method using autoencoder-L2 (AE L2 ). GradCAM-based methods (CAVGA <ref type="bibr" target="#b23">(Venkataramanan et al. (2020)</ref>) and Cutpaste <ref type="bibr" target="#b12">(Li et al. (2021)</ref>)) were also compared with the proposed method. Also, we compared the proposed method with the US Bergmann et al. (2020) using the representation of patch images. In our experiment, we compared the US trained with a patch size of 65 ? 65. The proposed method is also compared with FCDD <ref type="bibr" target="#b15">(Liznerski et al. (2021)</ref>) using receptive field upsampling. Finally, among the embedding similarity-based methods, the patch SVDD <ref type="bibr" target="#b24">(Yi &amp; Yoon (2020)</ref>) and SPADE <ref type="bibr" target="#b6">(Cohen &amp; Hoshen (2020)</ref>) were also used for the performance comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">QUANTITATIVE RESULTS</head><p>We evaluated the anomaly segmentation performance between the proposed method and the existing SOTA methods mentioned in section 4.3.1 using the MVTec AD dataset. As shown in Table 1, the proposed method consistently outperformed all other existing methods evaluated in AU-ROC. The reconstruction-based methods such as AE L2 used the reconstruction loss as the anomaly score. AE L2 had lower performance (0.82 AUROC) compared to the proposed method. CAVGA <ref type="bibr" target="#b23">(Venkataramanan et al. (2020)</ref>) and Cutpaste <ref type="bibr" target="#b12">(Li et al. (2021)</ref>) obtained anomaly maps using Grad-CAM (Selvaraju et al. <ref type="formula" target="#formula_0">(2017)</ref>), but these anomaly maps highly depend on the classification loss. In addition, compared to the methods using patch image representation such as US, the proposed method achieved higher performance. As a result, AnoSeg outperformed the conventional SOTA, such as Patch SVDD, SPADE, and Cutpaste, by 1% AUROC in anomaly segmentation.</p><p>In addition, we evaluated IoU, which is typically used as a metric for segmentation. <ref type="table" target="#tab_3">Table 2</ref> shows the quantitative comparison on IoU. AnoSeg achieved the highest performance compared to other methods in IoU. In particular, Patch SVDD and SPADE achieved 0.96 AUROC similar to AnoSeg in the evaluation of AUROC, but had lower IoU than the proposed method. This is because, unlike the existing method, the proposed method was directly trained for segmentation.  Additionally, we compared the AUROC and IoU metrics for the generated anomaly map in <ref type="figure" target="#fig_2">Fig. 6(a)</ref>.</p><p>In general, AUROC is affected by the detection performance of the anomaly regions. False positives for normal regions have relatively no impact on AUROC. In the Patch SVDD of <ref type="figure" target="#fig_2">Fig. 6(a)</ref>, there were abnormal regions that cannot be detected. Therefore, the anomaly map of Patch SVDD had lower AUROC compared to other methods. Although the anomaly maps of AnoSeg and SPADE visually show different anomaly maps, the same AUROC was calculated because most anomaly regions are detected in anomaly maps of AnoSeg and SPADE. However, IoU was affected by false positives in normal regions. Therefore, IoU of SPADE had lower performance compared to AUROC. The proposed AnoSeg achieved the highest performance for both IoU and AUROC. These results shows that the proposed method is superior in various aspects of anomaly segmentation.</p><p>We compared the anomaly detection performance between the proposed and existing methods using the method introduced in section 4.3.1. As shown in <ref type="table" target="#tab_0">Table 1</ref>, the proposed method achieved similar AUROC to existing SOTA methods (Full results are in Appendix A.3). Discriminator of anomaly detector learned representations of images and anomaly maps together. Therefore, with a simple anomaly detection method using the generated anomaly map, we achieve anomaly detection performance similar to that of the existing SOTA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">QUALITATIVE RESULTS</head><p>For the evaluation with existing methods, we visualized anomaly maps of existing and proposed methods in <ref type="figure" target="#fig_0">Fig. 1</ref>. The output image of AE L2 <ref type="bibr" target="#b3">(Bergmann et al. (2019)</ref>) was restored up to the anomaly image region and it was difficult to restore high-frequency regions of the normal image. Also, US 65?65 could detect large defects, but had poor detection performance for small defects. These results show that patch representations based methods are difficult to accurately localize defects for various sizes. Patch SVDD and SPADE extracted anomaly maps using feature extractions for different sizes to consider defects with various sizes. Therefore, the defects with different sizes could be detected, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. However, these anomaly maps had many false positives for normal regions and approximately detected anomaly regions. In contrast, as shown in <ref type="figure" target="#fig_3">Fig. 7</ref>, the proposed AnoSeg was trained to generate anomaly maps directly for anomaly segmentation using the segmentation loss. Therefore, the proposed method generated an anomaly map more similar to GT than the results of the existing methods as shown in <ref type="figure" target="#fig_2">Fig. 6</ref>. More comprehensive results on defect segmentation are given in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">ANALYSIS OF THRESHOLD SENSITIVITY</head><p>In this section, Patch SVDD and our AnoSeg were compared to verify the performance variation depending on the threshold of the proposed method. IoU was measured by dividing the anomaly score by 10000 units. <ref type="figure" target="#fig_2">Fig. 6(b)</ref> shows the performance change of AnoSeg, SPADE and Patch SVDD according to a threshold. As shown in <ref type="figure" target="#fig_2">Fig. 6(b)</ref>, the performance of AnoSeg did not significantly change significantly for different thresholds. Therefore, the anomaly map is shown similar to the GT mask even though thresholding was not applied in <ref type="figure" target="#fig_2">Fig. 6</ref>. On the other hand, <ref type="figure" target="#fig_2">Fig. 6(b)</ref> shows that Patch SVDD and SPADE had a significant change in performance when the threshold is changed around the threshold with the highest IoU. The result shows that our model is robust against thresholding. By setting the threshold between 0.2 and 0.8, AnoSeg could always achieve better results consistently than other SOTA solutions listed in <ref type="table" target="#tab_3">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ABLATION STUDY</head><p>We modified the generator structure (Section 4.2) to generate the only anomaly map and construct the base model with only Cutpaste applied. Then, we added modules incrementally on the base model, and evaluated with IoU and AUROC scores. The overall results show that the method using all modules improved by 5.4% and 10.2% for AUROC and IoU, respectively, compared to the base model. The effectiveness of each module is described below.</p><p>Hard augmentation We used images with several hard augmentations applied to train AnoSeg on anomaly regions. Hard augmentations generate samples away from the normal data distribution. Intuitively, synthetic anomaly data applied with hard augmentation can generate more diverse anomaly regions than Cutpaste. Therefore, AnoSeg detected more anomaly regions than the base model. As a result, AUROC and IOU were improved by 2.1% and 1.9% respectively.</p><p>Adversarial learning with reconstruction loss The proposed AnoSeg learns the normal region distribution through adversarial learning. We also use masked reconstruction loss in AnoSeg to apply reconstruction loss only for normal regions to avoid biasing synthetic anomaly regions. As shown in a of <ref type="figure" target="#fig_4">Fig. 8(a)</ref>, the base model is difficult to learn the normal data distribution. Therefore, the reconstructed image of base model partially restores the anomaly regions, and the base model detects anomaly regions as normal regions. In contrast, a model using adversarial learning learns the normal data distribution and can segment between normal and abnormal regions. Therefore, AnoSeg can generate detailed anomaly maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coordinate channel concatenation</head><p>To consider the additional location information while performing anomaly segmentation, we concatenated coordinate channels. In <ref type="figure" target="#fig_4">Fig. 8(b)</ref>, the effectiveness of coordinate channel concatenation is confirmed. The yellow cable in the input image changes the class property depending on the location. Therefore, these anomaly regions can be determine as normal if location information is insufficient. Because the base model that does not use the coordinate channel lacks location information, the yellow cable, which is an abnormal area, is reconstructed and determined as a normal area. AnoSeg provides additional location information by connecting the coordinate channel to the input image. As a result, as shown in <ref type="figure" target="#fig_4">Fig.8(b)</ref> anomaly regions that depend on location information were additionally detected, and AUROC and IOU were improved by 1.9% and 2.8% respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>This paper presented a novel anomaly segmentation network to directly generate an anomaly map. We proposed AnoSeg, a segmentation model using adversarial learning, and the proposed AnoSeg was directly trained for anomaly segmentation using synthetic anomaly data generated through hard augmentation. In addition, anomaly regions sensitive to positional relationships were more easily detected through coordinate vectors representing the pixel position information. Hence, our approach enabled AnoSeg to be trained to generate anomaly maps with direct supervision. We also applied this anomaly maps to existing methods to improve the performance of anomaly detection. Experimental results on the MVTec AD dataset using AUROC and IoU demonstrated that the proposed method is a specialized network for anomaly segmentation compared to the existing methods. A ANOMALY DETECTION USING PROPOSED ANOMALY MAP Here we provide detailed information for the training and loss functions of anomaly detector using the proposed anomaly map from Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 TRAINING PROCESS OF ANOMALY DETECTION METHOD</head><p>The proposed anomaly detection method uses an anomaly map generated from the AnoSeg along with the input image to learn the distribution of the normal image and the anomaly map. Therefore, the anomaly detector determines whether the anomaly map is focusing on the normal region of the input image while determining whether the input image is a normal image. Unlike AnoSeg, the proposed anomaly detection method does not use the synthetic anomaly x Ano as a real class in an adversarial loss because discriminator of anomaly detector only needs to learn the normal data distribution for anomaly detection. The loss function for learning the discriminator of the anomaly detector (L AD Adv ) is as follows: <ref type="bibr">(concat( x N or</ref> , A N or )))]</p><formula xml:id="formula_5">L AD Adv = min G max D {E [log(1 ? D</formula><p>+E <ref type="bibr">[log(D(concat(x N or</ref> , A N or )))]},</p><p>where x N or , A N or , x N or , and , A N or represent reconstructed a normal image, a anomaly map of AnoSeg, a normal image, and a normal mask, repectively.</p><p>Also, to help estimate the normal data distribution, we propose a synthetic anomaly classification loss that discriminates between synthetic data and normal data. As confirmed in <ref type="bibr" target="#b16">(Odena (2016)</ref>), the proposed synthetic anomaly classification loss improves the anomaly performance of the discriminator. This synthetic anomaly classification loss is defined as: <ref type="bibr">(concat(x N or</ref> , A N or )))].</p><formula xml:id="formula_7">L cls = E [log(1 ? D(concat(x Ano , A Ano )))] + E [log(D</formula><p>Then, we use the feature matching loss introduced in <ref type="bibr" target="#b19">(Salimans et al. (2016)</ref>) to stabilize the learning of the discriminator and extract the anomaly score. The high-level representations of the normal and reconstructed samples are expected to be identical. This loss is given as follows:</p><formula xml:id="formula_8">L f ea = E f (concat(x N or , A N or )) ? f (concat( x N or , A N or )) 2 ,</formula><p>where f (.) is the second to the last layer of the discriminator. <ref type="figure" target="#fig_5">Fig. 9</ref> shows an overview of the overall training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 QUANTITATIVE EVALUATION OF ANOMALY DETECTION IN THE MVTEC AD DATASET.</head><p>We describe the performance evaluation setting of the existing method that was not included in the main paper due to the length limitation. For performance comparison with existing methods, we used the results from existing literature, excluding the uninformed students method (US) <ref type="bibr" target="#b4">(Bergmann et al. (2020)</ref>). US method is only evaluated with PRO scores for anomaly segmentation without the provision of the AUROC for the anomaly segmentation and detection. Therefore, we re-implemented the  large patch size (patch size is 65 ? 65) version of the Student method and evaluated it on anomaly detection and segmentation. Tables 4 also shows the class-wise anomaly detection performances for the MVTec AD (AUROC) dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 ABLATION STUDY OF ANOMALY DETECTION METHOD</head><p>We evaluated the effectiveness of the individual components in the proposed anomaly detection method on the MVTec AD dataset, as shown in <ref type="table" target="#tab_6">Table 5</ref>. The base model used the same structure as the proposed model, and only the input images were fed except for the mask. The base model compared the features of the input image and the reconstructed image to calculate an anomaly score. However, since the reconstructed image often had anomaly regions restored, the base model has the low performance. The model that the feature matching loss is applied had slightly improved AUROC than the base model. The proposed anomaly detection method performed anomaly detection using input images and anomaly maps. Image-level AUROC was significantly increased by up to 15%. Hence, the model using an anomaly map as an input performed anomaly detection more sensitive than the conventional method using only an input image. Finally, to enhance the estimation of the normal data distribution, we added an anomaly classification loss. This loss helps in estimating the boundaries of the normal data distribution where synthetic anomaly data are separated. <ref type="table">Table 6</ref> shows the network structure of the proposed method. Each network is described by a list of layers including an output shape, a kernel size, a padding size, and a stride. In addition, batch normalization (BN) and activation function define whether BN is applied and which activation function is applied, respectively. The decoder used for image reconstruction has the same structure as the decoder for generating anomaly map, and AnoSeg uses two decoders. The structure of the proposed     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DETAILS ON THE NETWORK ARCHITECTURES</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparison of anomaly maps (before thresholding) of the proposed method with the SOTA methods in the MVTec-AD dataset. Except for the proposed method, anomaly maps of existing methods are normalized to [0, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Overall process of the coordinate channel concatenation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>(a) Comparison on AUROC and IoU using Anomaly map and (b) mean IoU change according to the threshold for each category. The x-axis and y-axis represent a threshold and IoU, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative results on the MVTec AD dataset for (first row) input image, (second row) GT mask, and (third row) proposed anomaly map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative results of the ablation study to illustrate the performance of the anomaly segmentation on the MVtec AD dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Overview of the training process of the proposed anomaly detection method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :Figure 12 :</head><label>1012</label><figDesc>IoU results for each category in the MVTec AD dataset according to the threshold change. (Green: AnoSeg, Orange: SPADE, Blue: Patch SVDD) IoU results for each category in the MVTec AD dataset according to the threshold change. (Green: AnoSeg, Orange: SPADE, Blue: Patch SVDD) Figure 13: Defect segmentation on MVTec AD dataset. For each sample image, there are an input image, the proposed anomaly map, and its GT mask from left to right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 14 :</head><label>14</label><figDesc>Defect segmentation on MVTec AD dataset. For each sample image, there are an input image, the proposed anomaly map, and its GT mask from left to right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 15 :</head><label>15</label><figDesc>Defect segmentation on MVTec AD dataset. For each sample image, there are an input image, the proposed anomaly map, and its GT mask from left to right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 16 :</head><label>16</label><figDesc>Defect segmentation on MVTec AD dataset. For each sample image, there are an input image, the proposed anomaly map, and its GT mask from left to right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 17 :</head><label>17</label><figDesc>Defect segmentation on MVTec AD dataset. For each sample image, there are an input image, the proposed anomaly map, and its GT mask from left to right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison of anomaly segmentation and detection in terms of pixel-level AUROC and image-level AUROC with the proposed method and conventional SOTA methods on the MVTec AD dataset</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 of</head><label>4</label><figDesc>Appendix A.3.</figDesc><table><row><cell></cell><cell></cell><cell cols="5">Anomaly Segmentation (Pixel-level AUROC)</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">AE L2 CAVGA</cell><cell>US</cell><cell>FCDD</cell><cell>Patch SVDD</cell><cell cols="3">SPADE Cutpaste Proposed</cell></row><row><cell>Bottle</cell><cell>0.86</cell><cell>0.89</cell><cell>0.94</cell><cell>0.97</cell><cell>0.98</cell><cell>0.98</cell><cell>0.98</cell><cell>0.99</cell></row><row><cell>Cable</cell><cell>0.86</cell><cell>0.85</cell><cell>0.91</cell><cell>0.90</cell><cell>0.97</cell><cell>0.97</cell><cell>0.90</cell><cell>0.99</cell></row><row><cell>Capsule</cell><cell>0.88</cell><cell>0.95</cell><cell>0.92</cell><cell>0.93</cell><cell>0.96</cell><cell>0.99</cell><cell>0.97</cell><cell>0.90</cell></row><row><cell>Carpet</cell><cell>0.59</cell><cell>0.88</cell><cell>0.72</cell><cell>0.96</cell><cell>0.93</cell><cell>0.98</cell><cell>0.98</cell><cell>0.99</cell></row><row><cell>Grid</cell><cell>0.90</cell><cell>0.95</cell><cell>0.85</cell><cell>0.91</cell><cell>0.96</cell><cell>0.94</cell><cell>0.98</cell><cell>0.99</cell></row><row><cell>Hazelnut</cell><cell>0.95</cell><cell>0.96</cell><cell>0.95</cell><cell>0.95</cell><cell>0.98</cell><cell>0.99</cell><cell>0.97</cell><cell>0.99</cell></row><row><cell>Leather</cell><cell>0.75</cell><cell>0.94</cell><cell>0.84</cell><cell>0.98</cell><cell>0.97</cell><cell>0.98</cell><cell>0.99</cell><cell>0.98</cell></row><row><cell>Metal nut</cell><cell>0.86</cell><cell>0.85</cell><cell>0.92</cell><cell>0.94</cell><cell>0.98</cell><cell>0.98</cell><cell>0.93</cell><cell>0.99</cell></row><row><cell>Pill</cell><cell>0.85</cell><cell>0.94</cell><cell>0.91</cell><cell>0.81</cell><cell>0.95</cell><cell>0.96</cell><cell>0.96</cell><cell>0.94</cell></row><row><cell>Screw</cell><cell>0.96</cell><cell>0.85</cell><cell>0.92</cell><cell>0.86</cell><cell>0.96</cell><cell>0.99</cell><cell>0.97</cell><cell>0.91</cell></row><row><cell>Tile</cell><cell>0.51</cell><cell>0.80</cell><cell>0.91</cell><cell>0.91</cell><cell>0.91</cell><cell>0.87</cell><cell>0.90</cell><cell>0.98</cell></row><row><cell>Toothbrush</cell><cell>0.93</cell><cell>0.91</cell><cell>0.88</cell><cell>0.94</cell><cell>0.98</cell><cell>0.98</cell><cell>0.98</cell><cell>0.96</cell></row><row><cell>Transistor</cell><cell>0.86</cell><cell>0.85</cell><cell>0.73</cell><cell>0.88</cell><cell>0.97</cell><cell>0.94</cell><cell>0.93</cell><cell>0.96</cell></row><row><cell>Wood</cell><cell>0.73</cell><cell>0.86</cell><cell>0.85</cell><cell>0.88</cell><cell>0.91</cell><cell>0.89</cell><cell>0.96</cell><cell>0.98</cell></row><row><cell>Zipper</cell><cell>0.77</cell><cell>0.94</cell><cell>0.91</cell><cell>0.92</cell><cell>0.95</cell><cell>0.97</cell><cell>0.99</cell><cell>0.98</cell></row><row><cell>Mean</cell><cell>0.82</cell><cell>0.89</cell><cell>0.88</cell><cell>0.92</cell><cell>0.96</cell><cell>0.96</cell><cell>0.96</cell><cell>0.97</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Anomaly Detection (Image-level AUROC)</cell><cell></cell><cell></cell></row><row><cell>Mean</cell><cell>0.71</cell><cell>0.82</cell><cell>0.84</cell><cell>-</cell><cell>0.92</cell><cell>0.86</cell><cell>0.95</cell><cell>0.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison of anomaly segmentation in term of mean IoU with the proposed and conventional SOTA methods on the MVTec AD dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Anomaly Segmentation (IoU)</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>CAVGA</cell><cell>US</cell><cell>Patch SVDD</cell><cell>SPADE</cell><cell>Proposed</cell></row><row><cell>Mean</cell><cell>0.470</cell><cell>0.244</cell><cell>0.427</cell><cell>0.483</cell><cell>0.542</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performance of various configurations on the MVTec AD dataset.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Ablation study (AUROC / IoU)</cell><cell></cell></row><row><cell>Method</cell><cell>Base model</cell><cell>+ Hard augmentation</cell><cell>+ Adversarial</cell><cell>+ Coordinate</cell></row><row><cell></cell><cell>(Cutpaste only)</cell><cell></cell><cell>learning</cell><cell>channel</cell></row><row><cell>Mean</cell><cell>0.923 / 0.492</cell><cell>0.942 / 0.503</cell><cell>0.951 / 0.527</cell><cell>0.970 / 0.542</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison of anomaly detection in terms of image-level AUROC with the proposed method and conventional SOTA methods on the MVTec AD dataset<ref type="bibr" target="#b3">(Bergmann et al. (2019)</ref>).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Anomaly Detection (Image-level AUROC)</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>AEL2</cell><cell>CAVGA</cell><cell>US</cell><cell>Patch SVDD</cell><cell>SPADE</cell><cell>Cutpaste</cell><cell>Proposed</cell></row><row><cell>bottle</cell><cell>0.80</cell><cell>0.91</cell><cell>0.85</cell><cell>0.99</cell><cell>-</cell><cell>0.98</cell><cell>0.98</cell></row><row><cell>Cable</cell><cell>0.56</cell><cell>0.67</cell><cell>0.90</cell><cell>0.90</cell><cell>-</cell><cell>0.81</cell><cell>0.98</cell></row><row><cell>Capsule</cell><cell>0.62</cell><cell>0.87</cell><cell>0.82</cell><cell>0.77</cell><cell>-</cell><cell>0.96</cell><cell>0.84</cell></row><row><cell>Carpet</cell><cell>0.50</cell><cell>0.78</cell><cell>0.86</cell><cell>0.93</cell><cell>-</cell><cell>0.93</cell><cell>0.96</cell></row><row><cell>Grid</cell><cell>0.78</cell><cell>0.78</cell><cell>0.60</cell><cell>0.95</cell><cell>-</cell><cell>0.99</cell><cell>0.99</cell></row><row><cell>Hazelnut</cell><cell>0.88</cell><cell>0.87</cell><cell>0.91</cell><cell>0.92</cell><cell>-</cell><cell>0.97</cell><cell>0.98</cell></row><row><cell>Leather</cell><cell>0.44</cell><cell>0.75</cell><cell>0.73</cell><cell>0.91</cell><cell>-</cell><cell>1.00</cell><cell>0.99</cell></row><row><cell>Metal nut</cell><cell>0.73</cell><cell>0.71</cell><cell>0.58</cell><cell>0.94</cell><cell>-</cell><cell>0.99</cell><cell>0.95</cell></row><row><cell>Pill</cell><cell>0.62</cell><cell>0.91</cell><cell>0.90</cell><cell>0.86</cell><cell>-</cell><cell>0.92</cell><cell>0.87</cell></row><row><cell>Screw</cell><cell>0.69</cell><cell>0.78</cell><cell>0.90</cell><cell>0.81</cell><cell>-</cell><cell>0.86</cell><cell>0.97</cell></row><row><cell>Tile</cell><cell>0.77</cell><cell>0.72</cell><cell>0.87</cell><cell>0.98</cell><cell>-</cell><cell>0.93</cell><cell>0.98</cell></row><row><cell>Toothbrush</cell><cell>0.98</cell><cell>0.97</cell><cell>0.81</cell><cell>1.00</cell><cell>-</cell><cell>0.98</cell><cell>0.99</cell></row><row><cell>Transistor</cell><cell>0.71</cell><cell>0.75</cell><cell>0.85</cell><cell>0.92</cell><cell>-</cell><cell>0.96</cell><cell>0.96</cell></row><row><cell>Wood</cell><cell>0.74</cell><cell>0.88</cell><cell>0.68</cell><cell>0.92</cell><cell>-</cell><cell>0.99</cell><cell>0.99</cell></row><row><cell>Zipper</cell><cell>0.80</cell><cell>0.94</cell><cell>0.90</cell><cell>0.98</cell><cell>-</cell><cell>0.99</cell><cell>0.99</cell></row><row><cell>Mean</cell><cell>0.71</cell><cell>0.82</cell><cell>0.84</cell><cell>0.92</cell><cell>0.86</cell><cell>0.95</cell><cell>0.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Anomaly detection performance of various configurations on the MVTec AD dataset.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Ablation study (Image-level AUROC)</cell><cell></cell></row><row><cell>Method</cell><cell>Base model</cell><cell>+ Feature matching loss</cell><cell>+ Input anomaly map</cell><cell>+ Anomaly classification loss</cell></row><row><cell>Mean</cell><cell>0.812</cell><cell>0.842</cell><cell>0.943</cell><cell>0.961</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>anomaly detector also has the same structure as that of AnoSeg. The structure of the AnnoSeg is also available in our code added in the supplementary material. The provided code contains pre-trained weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ANALYSIS OF THRESHOLD SENSITIVITY</head><p>In this section, we show the IoU results according to threshold changes for each category in the MVTec AD dataset. As shown in Figs. 10, 11, and 12, compared to SPADE and Patch SVDD, which are comparative methods, the performance difference of the proposed AnoSeg is not large according to the change in the threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D QUALITATIVE RESULTS ON THE MVTEC AD DATASET</head><p>We provided additional qualitative results of our method on the MVTec AD dataset in <ref type="bibr">Figs. 13,</ref><ref type="bibr">14,</ref><ref type="bibr">15,</ref><ref type="bibr">16,</ref><ref type="bibr">and 17</ref>. For each class, an Input image, a proposed anomaly map, and a GT mask are provided. The proposed AnoSeg had the highest performance even for anomaly regions with various sizes.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Ganomaly: Semi-supervised anomaly detection via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samet</forename><surname>Akcay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Variational autoencoder based anomaly detection using reconstruction probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwon</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungzoon</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Special Lecture on IE</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep autoencoding models for unsupervised anomaly segmentation in brain mr images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedikt</forename><surname>Wiestler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shadi</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="161" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mvtec ad -a comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9584" to="9592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4183" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Outlier detection with autoencoder ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saket</forename><surname>Sathe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charu</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Turaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 SIAM international conference on data mining</title>
		<meeting>the 2017 SIAM international conference on data mining</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="90" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Sub-image anomaly detection with deep pyramid correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02357</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint european conference on machine learning and knowledge discovery in databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adversarial discriminative attention for robust anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Kimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhajit</forename><surname>Chaudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minori</forename><surname>Narita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Munawar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuki</forename><surname>Tachibana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2172" to="2181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cutpaste: Self-supervised learning for anomaly detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsung</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9664" to="9674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Superpixel masking and inpainting for self-supervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
		<ptr target="https://www.bmvc2020-conference.com/assets/papers/0275.pdf" />
	</analytic>
	<monogr>
		<title level="m">31st British Machine Vision Conference 2020, BMVC 2020, Virtual Event</title>
		<meeting><address><addrLine>UK</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An intriguing failing of convolutional neural networks and the coordconv solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2018/file/60106888f8977b71e1f15db7bc9a88d1-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Explainable deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Liznerski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Billy</forename><forename type="middle">Joe</forename><surname>Franks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><forename type="middle">Robert</forename><surname>Muller</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=A5VV3UyIQz" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01583</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adversarially learned one-class classifier for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Khalooei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3379" to="3388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Anomaly detection using autoencoders with nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayu</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takehisa</forename><surname>Yairi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the MLSDA 2014 2nd Workshop on Machine Learning for Sensory Data Analysis</title>
		<meeting>the MLSDA 2014 2nd Workshop on Machine Learning for Sensory Data Analysis</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2016/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on information processing in medical imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Csi: Novelty detection via contrastive learning on distributionally shifted instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwoo</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongheon</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/8965f76632d7672e7d3cf29c87ecaa0c-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="11839" to="11852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention guided anomaly localization in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashanka</forename><surname>Venkataramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuan-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat Vikram</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahalanobis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Patch svdd: Patch-level svdd for anomaly detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Figure 11: IoU results for each category in the MVTec AD dataset according to the threshold change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houssam</forename><surname>Zenati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Sheng Foo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Lecouat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Manek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">Ramaseshan</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06222</idno>
	</analytic>
	<monogr>
		<title level="j">Orange: SPADE</title>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Green: AnoSeg</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Efficient gan-based anomaly detection. Patch SVDD</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
