<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FINQA: A Dataset of Numerical Reasoning over Financial Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Chen</surname></persName>
							<email>zhiyuchen@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charese</forename><surname>Smiley</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameena</forename><surname>Shah</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iana</forename><surname>Borova</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Langdon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reema</forename><surname>Moussa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Beane</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Hao</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Routledge</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
							<email>william@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Morgan</surname></persName>
						</author>
						<title level="a" type="main">FINQA: A Dataset of Numerical Reasoning over Financial Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The sheer volume of financial statements makes it difficult for humans to access and analyze a business's financials. Robust numerical reasoning likewise faces unique challenges in this domain. In this work, we focus on answering deep questions over financial data, aiming to automate the analysis of a large corpus of financial documents. In contrast to existing tasks on general domain, the finance domain includes complex numerical reasoning and understanding of heterogeneous representations. To facilitate analytical progress, we propose a new large-scale dataset, FINQA, with Question-Answering pairs over Financial reports, written by financial experts. We also annotate the gold reasoning programs to ensure full explainability. We further introduce baselines and conduct comprehensive experiments in our dataset. The results demonstrate that popular, large, pre-trained models fall far short of expert humans in acquiring finance knowledge and in complex multi-step numerical reasoning on that knowledge. Our dataset -the first of its kind -should therefore enable significant, new community research into complex application domains. The dataset and code are publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Financial analysis is a critical means of assessing business performance, and the consequences of poor analysis can involve costs of billions of dollars <ref type="bibr" target="#b13">(Jerven, 2013;</ref><ref type="bibr" target="#b20">MacKenzie, 2008)</ref>. To facilitate high quality, timely decision making, professionals -such as analysts or investors -perform complex quantitative analysis to select information from financial reports. Such analysis demands advanced expertise in reasoning among heterogeneous (structured and unstructured) data sources and performing complex numerical reasoning, for example, comparing financial ratios of profitability or growth. These challenges are compounded 1 https://github.com/czyssrs/FinQA by an exponentially expanding collection of company financial documents <ref type="bibr" target="#b21">(MacKenzie et al., 2012;</ref><ref type="bibr" target="#b17">Lange et al., 2016)</ref> such that it is genuinely unclear whether dedicated human effort can produce fiscal analysis of sufficient quality for current decision making. This poses an interesting question: can we automate such deep analysis of financial data?</p><p>A few NLP studies in Question Answering (QA) explored the numerical reasoning capabilities needed to answer questions correctly. For example, the DROP dataset <ref type="bibr" target="#b11">(Dua et al., 2019)</ref> focused on Wikipedia-based questions that require numerical reasoning, e.g., "Where did Charles travel to first, Castile or Barcelona?" needs a comparison between the times of two events. However, most prior work only targeted the general domain, where the questions involve much less calculation (mostly one-step calculation) than that of the financial domain. Financial QA is more challenging than classic QA <ref type="bibr" target="#b24">(Rajpurkar et al., 2018;</ref> because it requires the system to spot relevant information across heterogeneous sources, such as tables and unstructured texts, and then create a numerical reasoning path to connect all the information. It also takes substantial knowledge to ask meaningful financial questions. It is not clear how well the large language models, which performed well for general-domain QA, can be adapted to answer realistic, complex financial questions. This paper introduces FINQA, a expertannotated dataset that contains 8,281 financial QA pairs, along with their numerical reasoning processes. Eleven finance professionals collectively constructed FINQA based on the earnings reports of S&amp;P 500 companies <ref type="bibr" target="#b31">(Zheng et al., 2021)</ref>.  Expected life of options in years 6.3 6.3 6.3</p><p>Risk-free interest rate 5% 4% 4%</p><p>Page 91 from the annual reports of GRMN (Garmin Ltd.) The fair value for these options was estimated at the date of grant using a Black-Scholes option pricing model with the following weighted-average assumptions for <ref type="bibr">2006, 2005 and 2004:</ref> ? The total fair value of shares vested during <ref type="bibr">2006, 2005, and 2004 was $9,413, $8,249, and $6,418</ref>  ing processes answering these questions are made of many common calculations in financial analysis, such as addition, comparison, and table aggregation. To the best of our knowledge, FINQA is the first dataset of its kind to tackle complicated QA tasks based on the real-world financial documents.</p><p>We propose a retriever-generator QA framework to first retrieve supporting facts from financial reports, then to generate executable reasoning programs to answer the questions. Equipped with pretrained language models, such as BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> and RoBERTa <ref type="bibr" target="#b18">(Liu et al., 2019)</ref>, our proposed approach outperforms all other baselines and achieves an execution accuracy of 65.05%. Although our system outperforms the non-expert crowd (50.68%), the significant accuracy gap between the model and human experts (91.16%) motivates the need for future research.</p><p>The main contribution of this work is three-fold: ? We propose the task of QA over financial data to assist financial analysis. The task emphasizes an important phenomenon for the NLP community to study and analyze how the current pre-trained models perform on complex and specialized domains.</p><p>? We construct a new large-scale dataset, FINQA, with 8,281 examples written by financial experts, with fully annotated numerical reasoning programs.</p><p>? We experiment on various baselines and find that the models are still far behind expert performance, strongly motivating future research.  <ref type="bibr" target="#b2">(Amini et al., 2019)</ref>. The task is to generate the solution programs given a short input math problem. Existing models include <ref type="bibr" target="#b14">(Kim et al., 2020;</ref><ref type="bibr">Chen et al., 2020a,d)</ref>, etc.</p><p>Financial NLP. Financial NLP has become one of the major application domains attracting growing attentions. Previous works in finance domain include risk management to detect fraud <ref type="bibr" target="#b12">(Han et al., 2018;</ref><ref type="bibr" target="#b22">Nourbakhsh and Bang, 2019)</ref>, sentiment analysis to assist market prediction <ref type="bibr" target="#b9">(Day and Lee, 2016;</ref><ref type="bibr" target="#b26">Wang et al., 2013;</ref><ref type="bibr" target="#b0">Akhtar et al., 2017)</ref>, opinionated Question Answering <ref type="bibr" target="#b19">(Liu et al., 2020)</ref>, such as the FiQA 2 dataset built from forums and social media. Recent works attempt to develop pre-trained models specialized for finance domain <ref type="bibr" target="#b3">Araci, 2019)</ref>, and the downstream tasks are mostly sentiment classifications. To the best of our knowledge, there is no previous work and dataset on building QA systems of numerical reasoning on financial reports.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Definition</head><p>Problem Formulation. Presented with a financial report consisting of textual contents E and structured table T , given a question Q, the task is to generate the reasoning program G = {w 0 , w 1 , ...w n }, where w i is the program tokens defined by domain specific language (DSL), then it is executed to get the answer A:</p><formula xml:id="formula_0">P (A|T, E, Q) = P (G i |T, E, Q) (1)</formula><p>Where {G i } is all the correct programs to evaluate to the answer. For financial tables, there is typically a description header (blue header in <ref type="figure">Figure 1</ref>), which often gives the timing information; and each row has its name on the left. Some of the financial tables may demonstrate more complicated layouts, e.g., nested structures. As a first step for this direction, in this paper we only focus on the regular layout cases for simplicity. </p><p>Each operation takes a list of arguments args n . On consulting with financial experts, as most of the accounting and financial valuation theory primarily include linear algebra, we include 10 common types of operations in our dataset. There are 6 mathematical operations: add, subtract, multiply, divide, greater, exp, and 4 table aggregation operations The table operations take arguments of table row names. We use the special token #n to denote the result from the nth step. For example, in <ref type="figure">Figure 1</ref>, the program consists of 3 steps; The first and the second division steps take arguments from the table and the text, respectively, then the third step subtracts the results from the two previous steps. Refer to Appendix A for more details of the operations and the grammars.</p><p>Evaluations. Previous studies on QA with numerical reasoning only evaluate the execution accuracy, i.e., the final results from the generated programs, such as DROP <ref type="bibr" target="#b11">(Dua et al., 2019)</ref> and MathQA <ref type="bibr" target="#b2">(Amini et al., 2019)</ref>. However, the applications for the finance domain generally pose much higher requirements of explainability and transparency. Therefore, we also provide the gold programs for our dataset. Besides execution accuracy, we also propose to evaluate the accuracy of the generated programs. Specifically, we replace all the arguments in a program with symbols, and then we evaluate if two symbolic programs are mathematically equivalent. For example, the following two programs are equivalent programs: add(a 1 , a 2 ), add(a 3 , a 4 ), subtract(#0, #1) add(a 4 , a 3 ), add(a 1 , a 2 ), subtract(#1, #0)</p><p>Note that execution accuracy tends to overestimate the performance because sometimes the model just hit the correct answer by chance; While program accuracy tends to produce false negatives since some questions may have multiple correct programs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The FINQA Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Preparation</head><p>Data Source. We develop FINQA based on the publicly available earnings reports of S&amp;P 500 companies from 1999 to 2019, collected in the FinTabNet dataset <ref type="bibr" target="#b31">(Zheng et al., 2021</ref>). An earnings report is a set of pages in a PDF file that outlines the financials of a company, which usually contains tables and texts. The FinTabNet dataset has annotated the tables in each report.</p><p>Data Filtering. Realistic earnings reports contain many tables not suitable for numerical reasoning tasks. Equipped with the table annotations in FinTabNet, we filter the data as follows: First, we extract the pages in earnings reports with at most one table. Second, we exclude the tables with over 20 rows, over 2 description headers, or with other complex nested structures. We also exclude the tables with tedious contents, such as catalogs, which is common in FinTabNet. As stated in ?3, these over-complicated tables are out of the scope of this work. Finally, for the tables with 2 description headers, we merge them into a single header to simplify the representations. As a result, a total of 12,719 pages were selected for further annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Annotation Procedure</head><p>Recruiting Expert Annotators. We post job ads on UpWork 3 and hire eleven US-based experts with professional finance backgrounds (CPAs, MBAs, etc.) Each hire is interviewed using four example report pages and asked to compose example Q&amp;A pairs. After hiring, each annotator first goes through a training session to learn the task and the annotation interface (Appendix D). When the workers fully master the annotation process, we launch the official batches for them to work on.</p><p>An annotator can compose up to two questions for each given report page or skip if it is hard to compose any meaningful question. We pay around $2.0 for each question, which leads to an average hourly wage of $35.0. The whole data collection took around eight weeks.</p><p>We do not use popular micro-task platforms, such as Amazon Mechanical Turk (MTurk), because our preliminary studies show that many MTurk workers can not perform this task effectively. Our experiment with MTurk workers in ? 4.3 further echo this observation. As most existing QA datasets were constructed by MTurk workers <ref type="bibr" target="#b11">Dua et al., 2019;</ref><ref type="bibr" target="#b7">Chen et al., 2020c)</ref>, it requires substantial domain-specific knowledge to compose meaningful questions that are hard for computers to answer.</p><p>Annotation Task Design. For each page selected in ?4.1, the annotators are asked to (i) write a meaningful financial question, (ii) compose a reasoning program to answer the question, and (iii) to annotate the supporting fact. Each page is assigned to one or two experts for annotation. We detail each part as follows. (I) Financial question: For a given page of earnings reports, the annotators are asked first to compose a question that is "meaningful for financial analysis or learning insights of the company financial reports" and require numerical calculations to answer. We encourage the experts to write questions that require the information from both the text and the table to answer. (II) Reasoning program: After providing the question, the annotators are then asked to elaborate the operation steps to answer the question. Specifically, they compose a maximum of 5 steps of operation, where each operation has four slots: "operation", "argument1", "argument2", and "result". The "operation" is one of the ten predefined operations described in ?3. An "argument" is a number or a table's row name, either from the report or a previous step's result. For operations that only use one argument, such as table aggregation, workers can leave argument2 blank. The annotation interface (see Appendix D) automatically validates the inputs to ensure correctness. (III) Supporting fact:</p><p>We also ask the annotators to mark all the sentences in the text and the table rows that contain the information needed to answer the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Data Quality Assessment</head><p>External experts answer FINQA questions with a high accuracy and a high inter-annotator agreement. To validate the quality of the annotations, as well as to set up human expert performance upper bound, we hire another two financial professionals on UpWork. We randomly sample 200 examples from our dataset, and ask the professionals to answer the questions as well as write the operation steps, following the same procedure as in the dataset construction. The payment is $2.0 per question. For execution accuracy, they reach 92.25% and 90.06%, respectively (mean = 91.16%). For program accuracy, they reach 89.44% and 85.53% (mean = 87.49%). The agreements between the two annotators are 92.65% for execution accuracy, and 86.76% for program accuracy.</p><p>Non-expert crowd workers answer FINQA questions with a low accuracy. We also test how well non-expert MTurk workers can answer FINQA questions. We distribute the samples to MTurk 4 and take the similar process to distribute each example to two workers. We end up with an average execution accuracy of 50.68% and a program accuracy of 48.17%, which is far below the expert performance; the agreement rate is only around 60%. These results echo our preliminary study's observations for MTurk workers in ?4. has two pieces of facts; and 11.07% has more than two pieces of facts. For the examples with more than one piece of fact, we also calculate the maximum distances between all the same example's facts. 55.48% has a maximum distance of 3 or less sentences 5 ; 24.35% has a maximum distance of 4-6 sentences; and 20.17% has over 6 sentences.</p><p>Statistics of Reasoning Programs. In the programs, the most frequent operations, add, subtract, multiply, and divide, have the distributions of 14.98%, 28.20%, 5.82%, and 45.29%, respectively. The operation division has the highest frequency, as calculating ratios is common in financial analysis. In FINQA, 59.10% of the programs have 1 step, 32.71% have 2 steps, and the rest 8.19% have 3 or more steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Baseline Systems</head><p>In this section, we first describe our main baseline framework FinQANet in ?5.1, and then we introduce other baselines in ?5.2. <ref type="bibr">5</ref> For tables, we consider one row as one "sentence". </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Financial Report</head><p>Retrieved Facts <ref type="figure">Figure 2</ref>: The retriever retrieves supporting facts (text sentences or table rows) from the input financial report.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The FinQANet Framework</head><p>As a preliminary attempt on FINQA, we propose FinQANet, with a retriever to first retrieve the supporting facts from the input financial report, then a generator to generate the program to get the answer.</p><p>Retriever The full page of the financial report can go beyond 2,000 tokens, which cannot be coped with the current popular QA models <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>. Therefore we first retrieve the supporting facts from the input report. For the tables, we use templates to turn each row into sentences. For example, the last row of the table in <ref type="figure">Figure 1</ref> is represented as 'the risk-free interest rate of 2006 is 5%; ...'. We concatenate each supporting fact with the question and train a classifier using pre-trained LMs like BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>. Then we take the top n retrieved facts, reordered as they appear in the input report. This set of retriever results will serve as the input to the second phase. <ref type="figure">Figure 2</ref> illustrates the retrieving procedure. Another common strategy is sliding window <ref type="bibr" target="#b1">(Alberti et al., 2019)</ref>. We take the sliding window of a fixed size with a stride to go through the report, then the windows containing all the supporting facts are marked as positive. However, we observe in the experiments that the length of the input to the program generator in the second phase greatly influences the performance. The performance of using sliding window falls far behind the previous method.</p><p>Program Generator Given the retrieved supporting facts from the retriever, the program generator aims to generate the executable program to answer the question. <ref type="figure">Figure 3</ref> gives an overview of the program generator. The generated tokens come from 3 sources: 1) The input passage (retriever output) and the question tokens {e i }, like the numbers or the table row names.</p><p>2) The special tokens {s i } from the DSL, like the function names, predefined</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input encoder</head><p>Step memory embeddings 9413 add( ) 8249 #0 divide(</p><p>Step memory embeddings ) 8249</p><p>Step ]. An LSTM is used for decoding. At each decoding step T , the program token embeddings H are fed as the input; The decoder output h T is used to calculate the attention vector att p and att h over the input and the decoding history. Then a context vector c T combines all the contextual information:</p><formula xml:id="formula_2">c T = W c [att p ; att h ; h T ]<label>(3)</label></formula><p>Meanwhile, another attention vector att p over the input is applied to all the token embeddings:</p><formula xml:id="formula_3">H T = W h [H; H ? att p ]<label>(4)</label></formula><p>Different from other program tokens, the step memory tokens {m i } imply the reasoning path of the program. To make use of such structure information, at each decoding step indicating the end of one operation[args] unit, i.e., the step to generate the ending parentheses in our DSL, we compute another context vector a T :</p><formula xml:id="formula_4">a T = W a [att p ; att h ; h T ]<label>(5)</label></formula><p>Then the step memory token embedding corresponding to the current step is updated as a T . The final prediction is calculated with:</p><formula xml:id="formula_5">w T = sof tmax(H T ? c T )<label>(6)</label></formula><p>During inference time, based on the grammar of the DSL, we use masks at each decoding step to ensure the structural correctness of the generated programs. In the retriever phase, we take the top n retrieved results as the input to the program generator. Therefore, for the training of the program generator, we use the retriever result on the training set (combined with the gold facts if there is any wrong prediction) as the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Other Baselines</head><p>TF-IDF + Single Op. We use TF-IDF to retrieve the top 2 sentences from the input report. Since the most common case in our dataset is one-step program and the most common operation is division, we take the first number from each sentence and apply the division operation.</p><p>Retriever + Direct Generation. To demonstrate the necessity of generating the reasoning programs, we keep the architecture the same as our model, but directly generating the final results.</p><p>Retriever + Seq2seq. We use a Seq2seq architecture for the generator, similar to the Seq2seq baseline in the MathQA dataset <ref type="bibr" target="#b2">(Amini et al., 2019)</ref>. A bi-LSTM is used for encoding the input, and then an LSTM is used for decoding with attention.</p><p>Retriever + NeRd. The Neural Symbolic Reader(NeRd) <ref type="bibr" target="#b8">(Chen et al., 2020d)</ref> is also a pointergenerator based model for program generation, with the state of the art results on the MathQA dataset <ref type="bibr" target="#b2">(Amini et al., 2019)</ref>. Different from ours, it directly learns the program with nested format as a sequence, i.e., without the step memory tokens. This way the model is able to learn the program structures as patterns from very large-scale data (~40k for MathQA), but may fail on learning the reasoning paths. We keep the retriever part the same and compare with the generator part to demonstrate the usefulness of structure learning.</p><p>Pre-Trained Longformer. There are also works on modeling very long documents with thousands of characters, with the attention mechanism that scales linearly with sequence length, like the Longformer <ref type="bibr" target="#b4">(Beltagy et al., 2020)</ref>. To demonstrate the necessity of breaking up into the pipeline of retriever and program generator, we remove the retriever and directly use the pre-trained Longformer as the input encoder in the program generator, and encode the whole report. The table rows are linearized similar as in ?5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results</head><p>Experiment Setups. For the retriever, we use BERT-base as the classifier (other pre-trained models perform similarly). Since most of the examples in our dataset have 1 or 2 facts, and we find that longer inputs lower the performance of the program generator, we take the top 3 ranked facts as the retriever results. For the program generator, we experiment on using BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>, RoBERTa <ref type="bibr" target="#b18">(Liu et al., 2019)</ref>, and FinBert <ref type="bibr" target="#b3">(Araci, 2019)</ref> as the encoder, to test the performances of popular large pre-trained models. For all models, we use the Adam optimizer <ref type="bibr" target="#b15">(Kingma and Ba, 2015)</ref>. Check Appendix B for more details of training and parameter settings. <ref type="table" target="#tab_9">Table 2</ref> presents the results for all the baseline systems. We evaluate the execution accuracy (exe acc) and program accuracy (prog acc) as explained in ?3. For the BERT-based retriever, we have 89.66% recall for the top 3 retrieved facts and 93.63% recall for the top 5. Using TF-IDF results in 82.91% recall for the top 5 facts. We use the same retriever results for all retriever-generator based models. Directly generating the execution results gives nearzero scores, which indicates the necessity of generating the reasoning programs. If without using the retriever-generator pipeline, but directly applying an end-to-end pre-trained Longformer model, the performance falls far behind. Because longer inputs have more numbers which put more confusions on the program generator and thus make it harder to learn. Generally, the program generators using pre-trained models perform much better than the Seq2seq baseline, as there is language modeling knowledge that can also be used for the finance domain. And larger pre-trained models give better performance, as they tend to see more financial corpus during their pre-training. FinBert <ref type="bibr" target="#b3">(Araci, 2019)</ref> is a pre-trained model for the finance domain; its main downstream tasks are sentiment analysis.  The performance of using FinBert is no better than BERT-large, mostly because its pre-training corpus is limited (~30M words from news articles).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">QA Model Performance</head><p>Comparing FinQANet with the retriever + NeRd baseline <ref type="bibr" target="#b8">(Chen et al., 2020d)</ref>, it shows the improvements from learning the logical structure of the programs. We also run the program generator using the gold retriever result, shown as FinQANet-Gold. Another interesting observation is the comparisons with human performances. While there is still a large gap from the human expert upper bound, the best performing model already surpasses the general crowd performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Performance Breakdown</head><p>We conduct a set of performance breakdowns using the FinQANet (RoBERTa-large) model. what is the amount of credit lines that has been drawn in millions as of year-end 2016? [1] additionally , we have other committed and uncommitted credit lines of $ 746 million with major international banks and financial institutions to support our general global funding needs , including with respect to bank supported letters of credit, performance bonds and guarantees .</p><p>[2] approximately $ 554 million of these credit lines were available for use as of year-end 2016 . Question: what is the estimated percentage of revolving credit facility in relation with the total senior credit facility in millions? Gold program: <ref type="figure">multiply(1.4, const_1000)</ref>, divide(945.5, #0) Predicted program: divide(945.5, const_1000)</p><p>[1] we maintained a $ 1.4 billion senior credit facility with various financial institutions , including the $ 420.5 million term loan and a $ 945.5 million revolving credit facility .  <ref type="figure">Figure 4</ref>: Error cases. In these examples, the retriever results all correctly cover the gold facts; thus we only present the gold facts, gold program, and the predicted program to study the errors of the program generator. We give more error cases in Appendix C, including the cases for the retriever errors. Example 1: The financial knowledge to calculate the 'credit lines that has been drawn'.  Questions that need more than two steps to answer are challenging. The model has a low accuracy (22.78%) on the questions that need three or more steps. Meanwhile, not surprisingly, the questions that require only one step are the easiest.</p><p>Constants in programs. Many programs in FINQA contain constants as arguments. A constant is often used to convert an English number word to another. For example, we need first to use the constant "1,000" to convert "1.5 billion" to "1,500 million" so that it can be added with "50 million". A constant is also used to explicate the implicit numbers hidden in the language. For example, to calculate "the average for the year 2012, 2013, and 2014", the program needs to use the constant "3" as the denominator, which is not mentioned explicitly in the text. As shown in <ref type="table" target="#tab_10">Table 3</ref>, the programs with constants yield great challenges for our model, as the performance (43.88%) is much lower than that of the whole set (61.24%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Error Analysis</head><p>We sample 50 error cases from the results of the FinQANet (RoBERTa-large) model and analyze them manually. 15% of the errors are caused by the retriever, e.g., missing facts. Half of the rest are due to the lack of financial knowledge, such as the meaning of some terminology. And the rest half are primarily numerical reasoning errors, including complex programs with multiple steps, numerical unit conversions, or resolving the ordering and matching of the numbers and the years. Many error cases involve both the numerical reasoning problems and misunderstandings of financial knowledge. We show three representative error cases in <ref type="figure">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>This paper introduces FINQA, a new expertannotated QA dataset that aims to tackle numerical reasoning over real-world financial data. The questions in FINQA pose great challenge for existing models to resolve domain-specific knowledge, as well as to acquire complex numerical reasoning abilities. We propose baseline frameworks and conduct comprehensive experiments and analysis. The results show that current large pre-trained models still fall far behind the human expert performance. This encourages potential future work on developing pre-training tasks for such realistic, complex application domains. We believe FINQA should serve as a valuable resource for the research community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Ethical Considerations</head><p>Data Access and Licensing. We develop FINQA based on the publicly available earnings reports of S&amp;P 500 companies from 1999 to 2019, collected in the FinTabNet dataset <ref type="bibr" target="#b31">(Zheng et al., 2021)</ref>. The FinTabNet dataset is publicly available under the CDLA-Permissive 6 license, which permits us to create additional annotations on top of the data ("Enhanced Data", ?1.5 of CDLA) and publish the annotations ("Publish", ?1.9 of CDLA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Collection Process and Conditions.</head><p>For the annotation of our FINQA dataset on Upwork, we first launch interviews of the task introduction with 4 example questions, which is paid as $30, for them to try a few examples to get informed and familiar with the task. Then based on their consents to continue working on the large-scale job, we discuss with the workers to reach agreements on the compensation before starting the large-scale job. We pay around $2.0 per question, and the hourly rates are discussed and agreed upon with both sides based on the working speed of different workers. Among all eleven US-based hires, the average hourly rate is $35.0, and the minimum and maximum hourly rates are $20 and $50, respectively. The evaluation tasks follow the similar procedure, and each question is paid as $2.0.    <ref type="figure">20 sentences)</ref>... the ppaca effectively changes the tax treatment of federal subsidies paid to sponsors of retiree health benefit plans that provide a benefit that is at least actuarially equivalent to the benefits under medicare part d . the acts effectively make the subsidy payments taxable in tax years beginning after december 31 , 2012 and as a result , the company followed its original accounting for the underfunded status of the other postretirement benefits for the medicare part d adjustment and recorded a reduction in deferred tax assets and an increase in its regulatory assets amounting to $ 6348 and $ 6241 at december 31 , 2014 and 2013 , respectively . the following table summarizes the changes in the company 2019s gross liability , excluding interest and penalties , for unrecognized tax benefits: .  Figure 5: Error case study 1: The net change in the tax position is the sum of the increase and the decrease plus the penalties and interest. The model lacks this finance knowledge, thus the retriever fails to retrieve the correct table rows and sentences. Another challenging point is the table understanding, since in this case, it's hard to distinguish the retrieved two table rows for the year 2013 or 2014, using our method that regards each table row as basic unit. The model needs to look at the full table to get this global information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IRB (Institutional</head><p>Input Report K/2013/page_23.pdf-1 ? (abbreviate 12 sentences)... underlying gross margin declined by 180 basis points in 2012 as a result of cost inflation , net of cost savings , and the lower margin structure of the pringles business . underlying sga% ( sga % ) was consistent with 2011 . our underlying gross profit , underlying sga , and underlying operating profit measures are reconciled to the most comparable gaap measure as follows:    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The questions in FINQA, such as "Considering the weighted average fair value of options, what was the change of shares vested from 2005 to 2006?" (Figure 1) and "What was the net change in tax positions in 2014?", require information from both tables and unstructured texts to answer. The reason-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>in the table above does not include interest and penalties of $ 157 and $ 242 as of december 31 , 2014 and 2013 , respectively , which is recorded as a component of income tax expense . Question: what was the net change in tax positions in 2014? Gold program: add(53818, -36528), add(#0, 157) Retrieved evidence: [1]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Error case study 2: Complex numerical reasoning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Annotation interface: Display report.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Annotation interface: Annotator input fields.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2109.00122v3 [cs.CL] 7 May 2022</figDesc><table><row><cell></cell><cell>2006</cell><cell>2005</cell><cell>2004</cell></row><row><cell>Weighted average fair value of options granted</cell><cell cols="2">$20.01 $9.48</cell><cell>$7.28</cell></row><row><cell>Expected volatility</cell><cell cols="3">0.3534 0.3224 0.3577</cell></row><row><cell>Distribution yield</cell><cell>1.00%</cell><cell cols="2">0.98% 1.30%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>respectively. The aggregate intrinsic values of options outstanding and exercisable at December 30, 2006 were $204.1 million and $100.2 million, respectively. ( ? abbreviate 10 sentences ... ) Question: Considering the weighted average fair value of options , what was the change of shares vested from 2005 to 2006? An example from FINQA: The system needs to learn how to calculate the number of shares, then select relevant numbers from both the table and the text to generate the reasoning program to get the answer.</figDesc><table><row><cell cols="2">Answer: -400</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Calculations:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(</cell><cell>9413 20.01</cell><cell>) -</cell><cell>(</cell><cell>8249 9.48</cell><cell>)</cell><cell>= -400</cell></row><row><cell cols="2">Program:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">divide ( 9413, 20.01 )</cell><cell></cell><cell cols="3">divide ( 8249, 9.48 )</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">substract ( #0, #1 )</cell></row><row><cell>Figure 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>[args 1 ], op 2 [args 2 ]..., op n [args n ]</figDesc><table><row><cell>Domain Specific Language. In this work, we</cell></row><row><cell>use DSL consisting of mathematical operations</cell></row><row><cell>and table operations as executable programs. The</cell></row><row><cell>program consists of a sequence of operations:</cell></row><row><cell>op 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>table -</head><label>-</label><figDesc></figDesc><table><row><cell>max, table-min, table-sum,</cell></row><row><cell>table-average, that apply aggregation opera-</cell></row><row><cell>tions on table rows. The mathematical operations</cell></row><row><cell>take arguments of either numbers from the given</cell></row><row><cell>reports, or a numerical result from a previous step;</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Statistics of Supporting Facts. In FINQA, 23.42% of the questions only require the information in the text to answer; 62.43% of the questions only require the information in the table to answer; and 14.15% need both the text and table to answer. Meanwhile, 46.30% of the examples have one sentence or one table row as the fact; 42.63%</figDesc><table><row><cell>Examples (Q&amp;A pairs with program, fact)</cell><cell>8,281</cell></row><row><cell>Report pages</cell><cell>2,789</cell></row><row><cell>Vocabulary</cell><cell>22.3k</cell></row><row><cell>Avg. # sentences in input text</cell><cell>24.32</cell></row><row><cell>Avg. # tokens in input text</cell><cell>628.11</cell></row><row><cell>Avg. # rows in input table</cell><cell>6.36</cell></row><row><cell>Avg. # tokens in input table</cell><cell>59.42</cell></row><row><cell>Avg. # tokens in all inputs (text &amp; table)</cell><cell>687.53</cell></row><row><cell>Max. # tokens in all inputs (text &amp; table)</cell><cell>2,679</cell></row><row><cell>Avg. question length</cell><cell>16.63</cell></row><row><cell>Table 1: Statistics of FINQA.</cell><cell></cell></row><row><cell>4.4 Data Analysis</cell><cell></cell></row><row><cell cols="2">FINQA contains 8,281 examples. The data is re-</cell></row><row><cell cols="2">leased as training (6,251), validation (883), and</cell></row><row><cell cols="2">test (1,147) following an 75%/10%/15% split. The</cell></row><row><cell cols="2">three sets do not have overlapping input reports.</cell></row><row><cell cols="2">We quantitatively analyze some key properties of</cell></row><row><cell cols="2">FINQA. Table 1 shows the general statistics.</cell></row><row><cell></cell><cell>2.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>The fair value for these options was estimated at the date of grant using a Black-Scholes option pricing model with the following weighted-average assumptions for2006,  2005 and 2004:    The total fair value of shares vested during 2006, 2005 was $9,413, $8,249 respectively.</figDesc><table><row><cell>The total fair value of shares vested during</cell></row><row><cell>2006, 2005 was $9,413, $8,249 respectively.</cell></row><row><cell>The aggregate intrinsic values of options outstanding and</cell></row><row><cell>exercisable at December 30, 2006 were $204.1 million and $100.2</cell></row><row><cell>million, respectively.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>The program generator. The retriever results and the question are first encoded using pre-trained LMs. At each decoding step, the model can generate from the numbers or table row names from the input, the special tokens in the DSL, or the step memory tokens. At the end of the generation of each operation step, we update the step memory token embeddings.</figDesc><table><row><cell>#0</cell><cell>#1</cell><cell>...</cell><cell>Concat</cell><cell>Output space</cell><cell>Predicted token</cell><cell>#0</cell><cell>#1</cell><cell>...</cell><cell>Update memory</cell><cell>#0</cell><cell>memory embeddings #1 ...</cell></row><row><cell cols="3">Special token embeddings</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>add(</cell><cell>)</cell><cell>...</cell><cell></cell><cell>Attentions</cell><cell></cell><cell></cell><cell>...</cell><cell></cell><cell>...</cell><cell>...</cell><cell></cell></row><row><cell cols="2">Input embeddings</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LSTM</cell><cell></cell><cell></cell></row><row><cell>...</cell><cell>was</cell><cell>$</cell><cell>9413</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>decoder</cell><cell></cell><cell></cell></row><row><cell>...</cell><cell>was</cell><cell>$</cell><cell>9413</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Figure 3: constants, etc. 3) The step memory tokens {m i }</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">to denote the results from previous steps, like #0,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">#1 , etc. We first use pre-trained LMs to encode</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">{e i }, denote the output embeddings as {h e i }. The</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">embeddings of the special tokens and the step mem-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">ory tokens are randomly initialized and denoted as</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">{h s i } and {h m i } respectively. Denote all the token embeddings H = [h e i ; h s i ; h m i</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: The execution accuracy (Exe Acc) and program</cell></row><row><cell>accuracy (Prog Acc) for all the models. Although our best</cell></row><row><cell>system (61.24%) outperforms the non-expert crowd (50.68%),</cell></row><row><cell>the significant accuracy gap between the model and human</cell></row><row><cell>experts (91.16%) motivates the need for future research.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>shows all the results.</cell></row><row><cell>Necessity of using both table and text. We run</cell></row><row><cell>inferences taking facts only from a single source</cell></row><row><cell>from the retriever. Inferences on individual source</cell></row><row><cell>(table-only: 45.81%, text-only: 15.80%) are both</cell></row><row><cell>far behind the full results (61.24%).</cell></row><row><cell>The model performs the best on the table-only</cell></row><row><cell>questions. The model performs the best on table-</cell></row><row><cell>only questions (67.38%). Tables tend to have more</cell></row><row><cell>unified structures and might be easier for the model</cell></row><row><cell>to learn. Table 3 also shows that the questions</cell></row><row><cell>involving both tables and texts are the most chal-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Performance breakdown of FinQANet (RoBERTa-</cell></row><row><cell>large). The model benefits from using both table and text, as</cell></row><row><cell>inferences on individual source yield much lower performance.</cell></row><row><cell>FinQANet is better at answering table-only questions, and the</cell></row><row><cell>questions that require more steps to solve are indeed more</cell></row><row><cell>challenging to the model.</cell></row><row><cell>lenging ones for the model (43.80%).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>the J.P. Morgan Faculty research award. The authors are solely responsible for the contents of the paper and the opinions expressed in this publication do not reflect those of the funding agencies.</figDesc><table><row><cell>Name</cell><cell>Arguments</cell><cell cols="2">Output Description</cell></row><row><cell>add</cell><cell cols="3">number1, number2 number add two numbers: number1 + number2</cell></row><row><cell>subtract</cell><cell cols="3">number1, number2 number subtract two numbers: number1 ? number2</cell></row><row><cell>multiply</cell><cell cols="3">number1, number2 number multiply two numbers: number1 ? number2</cell></row><row><cell>divide</cell><cell cols="3">number1, number2 number multiply two numbers: number1/number2</cell></row><row><cell>exp</cell><cell cols="3">number1, number2 number exponential: number1 number2</cell></row><row><cell>greater</cell><cell cols="2">number1, number2 bool</cell><cell>comparison: number1 &gt; number2</cell></row><row><cell>table-sum</cell><cell>table header</cell><cell cols="2">number the summation of one table row</cell></row><row><cell cols="2">table-average table header</cell><cell cols="2">number the average of one table row</cell></row><row><cell>table-max</cell><cell>table header</cell><cell cols="2">number the maximum number of one table row</cell></row><row><cell>table-min</cell><cell>table header</cell><cell cols="2">number the minimum number of one table row</cell></row><row><cell></cell><cell cols="2">Review Board) Approval.</cell></row><row><cell cols="3">This project is approved by our Institutional Review</cell></row><row><cell cols="3">Board (IRB). The systems trained using our dataset</cell></row><row><cell cols="3">are primarily intended to be used as augmenting</cell></row><row><cell cols="3">human decision-making in financial analysis, but</cell></row><row><cell cols="2">not as a replacement of human experts.</cell><cell></cell></row></table><note>by</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 4 :</head><label>4</label><figDesc>Definitions of all operations</figDesc><table><row><cell>Baselines</cell><cell>Execution Accuracy (%)</cell><cell>Program Accuracy (%)</cell></row><row><cell>TF-IDF + Single Op</cell><cell>1.65</cell><cell>1.65</cell></row><row><cell>Retriever + Direct Generation</cell><cell>0.87</cell><cell>-</cell></row><row><cell>Pre-Trained Longformer (base)</cell><cell>23.83</cell><cell>22.56</cell></row><row><cell>Retriever + Seq2seq</cell><cell>18.76</cell><cell>17.52</cell></row><row><cell>Retriever + NeRd (BERT-base)</cell><cell>47.53</cell><cell>45.37</cell></row><row><cell>FinQANet (FinBert)</cell><cell>46.64</cell><cell>44.11</cell></row><row><cell>FinQANet (BERT-base)</cell><cell>49.91</cell><cell>47.15</cell></row><row><cell>FinQANet (BERT-large)</cell><cell>53.86</cell><cell>50.95</cell></row><row><cell>FinQANet (RoBerta-base)</cell><cell>56.27</cell><cell>53.49</cell></row><row><cell>FinQANet (RoBerta-large)</cell><cell>61.22</cell><cell>58.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 5 :</head><label>5</label><figDesc>Results on validation set</figDesc><table><row><cell>Input Report AWK/2014/page_121.pdf</cell></row><row><cell>? (abbreviate</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>table row</head><label>row</label><figDesc></figDesc><table><row><cell>: increases in current period tax positions: 27229 ;</cell></row><row><cell>[2] table row: increases in current period tax positions: 53818 ;</cell></row><row><cell>[3] table row: balance at december 31 2014: $ 195237 ;</cell></row><row><cell>Predicted program:</cell></row><row><cell>subtract(27229, 53818)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>Question: if 2014 underlying operating profit increases at the same pace as 2013 , what would it be , in millions? Gold program: divide(2098, 2014), multiply(2098, #0)Retrieved evidence:[1] underlying gross margin declined by 110 basis points in 2013 due to the impact of inflation , net of productivity savings , lower operating leverage due to lower sales volume , and the impact of the lower margin structure of the pringles business[2]  table row: ( dollars in millions ) The underlying operating profit ( d ) of 2013 is $ 2098 ; The underlying operating profit ( d ) of 2012 is $ 2014 ; The underlying operating profit ( d ) of 2011 is $ 2109 ; [3] during 2013 , we recorded $ 42 million of charges associated with cost reduction initiatives .</figDesc><table><row><cell>( dollars in millions )</cell><cell>2013</cell><cell>2012</cell><cell>2011</cell></row><row><cell>reported gross profit ( a )</cell><cell>$ 6103</cell><cell>$ 5434</cell><cell>$ 5152</cell></row><row><cell cols="2">? abbreviate 10 rows ...</cell><cell></cell><cell></cell></row><row><cell>underlying operating profit ( d )</cell><cell>$ 2098</cell><cell>$ 2014</cell><cell>$ 2109</cell></row><row><cell>Predicted program:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>divide(2098, 2098), multiply(2098, #0)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://sites.google.com/view/fiqa/home</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3"> UpWork (www.upwork.com)  is a platform where requesters can recruit skilled freelancers.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Three built-in worker qualifications are used: HIT Approval Rate (?95%), Number of Approved HITs (? 3000), and Locale (US Only) Qualification. We do not select any profession constraints. We pay $2.0 for each question.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">CDLA-Permissive: https://cdla.dev/sharing-1-0/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://github.com/hltcoe/turkle</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We thank the anonymous reviewers for their thoughtful comments. This research was supported</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: Operation Definitions</head><p>We describe all the operations in <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B: Experiment Details</head><p>All the validation results of the baselines are shown in <ref type="table">Table 5</ref>. The trainings of all models are conducted on TITAN RTX GPUs. All the implementation and pre-trained models are based on the huggingface transformers library. We use the Adam optimizer <ref type="bibr" target="#b15">(Kingma and Ba, 2015)</ref>. The parameter settings are the following: Retriever The learning rate is set as 3e-5, with batch size of 16. TF-IDF + Single Op We use the TF-IDF from the Scikit-learn library. FinQANet The learning rate is set as 1e-5. For Bert-base, Roberta-base, and finBert we use batch size of 32; For Bert-large and RoBerta-large we use batch size of 16 due to GPU memory constraints. Retriever + Seq2seq A bidirectional LSTM is used for encoding the input, then an LSTM is used for decoding with attention. Learning rate is set as 1e-3, hidden size as 100. Retriever + NeRd The parameter settings are the same as FinQANet. Pre-Trained Longformer We truncate the maximum input length as 2,000. The learning rate is set as 2e-5, with batch size of 16 due to GPU memory constraints.</p><p>For more modeling details refer to our released code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C: Case Studies</head><p>Here we provide more case studies with the full input reports. For all the examples the gold evidence is highlighted in blue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D: Annotation Interface</head><p>We use Turkle 7 to build our annotation platform, which is a Django-based web application that can run in a local server. <ref type="figure">Figure 7</ref> and <ref type="figure">Figure 8</ref> show our annotation interface. After the annotators finish one example, they will use the validation check button to automatically check the validity of their inputs.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A multilayer perceptron based ensemble technique for fine-grained financial sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shad</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepanway</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asif</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bhattacharyya</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d17-1057</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-09" />
			<biblScope unit="page" from="540" to="546" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A BERT baseline for the natural questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<idno>abs/1901.08634</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mathqa: Towards interpretable math word problem solving with operation-based formalisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aida</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saadia</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanchuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1245</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-02" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2357" to="2367" />
		</imprint>
		<respStmt>
			<orgName>Long and Short Papers</orgName>
		</respStmt>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Finbert: Financial sentiment analysis with pre-trained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dogu</forename><surname>Araci</surname></persName>
		</author>
		<idno>abs/1908.10063</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno>abs/2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mapping natural-language problems to formal-language solutions using structured neural representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kezhen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Smolensky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">D</forename><surname>Forbus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1566" to="1575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tabfact: A large-scale dataset for table-based fact verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hybridqa: A dataset of multi-hop question answering over tabular and textual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.91</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-20" />
			<biblScope unit="page" from="1026" to="1036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning for financial sentiment analysis on finance news providers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yuh</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Chou</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1109/ASONAM.2016.7752381</idno>
	</analytic>
	<monogr>
		<title level="m">ASONAM 2016</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-08-18" />
			<biblScope unit="page" from="1127" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA; Long and Short Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-02" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1246</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2368" to="2378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nextgen AML: distributed deep learning based language technologies to augment anti money laundering investigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utsab</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jer</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhua</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Burgin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dadong</forename><surname>Wan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-4007</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2018</title>
		<meeting>ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-15" />
			<biblScope unit="page" from="37" to="42" />
		</imprint>
	</monogr>
	<note>System Demonstrations. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Poor numbers: how we are misled by African development statistics and what to do about it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Jerven</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Cornell University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Point to the expression: Solving algebraic word problems using the expressionpointer transformer model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bugeun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung</forename><forename type="middle">Seo</forename><surname>Ki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gahgene</forename><surname>Gweon</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.308</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-11-16" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="3768" to="3779" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">MAWPS: A math word problem repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aida</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n16-1136</idno>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12" />
			<biblScope unit="page" from="1152" to="1157" />
		</imprint>
	</monogr>
	<note>The Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cultures of high-frequency trading: Mapping the landscape of algorithmic developments in contemporary financial markets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann-Christina</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lenglet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Seyfert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economy and Society</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="149" to="165" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Finbert: A pre-trained financial language representation model for financial text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Degen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/622</idno>
		<ptr target="ijcai.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="4513" to="4519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">An engine, not a camera: How financial models shape markets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Mackenzie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Mit Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Drilling through the allegheny mountains: Liquidity, materiality and high-frequency trading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Mackenzie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Beunza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Millo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Pablo</forename><surname>Pardo-Guerra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cultural economy</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="296" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A framework for anomaly detection using language modeling, and its applications to finance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armineh</forename><surname>Nourbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Bang</surname></persName>
		</author>
		<idno>abs/1908.09156</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Compositional semantic parsing on semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/p15-1142</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07-26" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1470" to="1480" />
		</imprint>
	</monogr>
	<note>The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for squad</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-2124</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07-15" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The web as a knowledge-base for answering complex questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-1059</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="641" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Financial sentiment analysis for risk prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan-Ju</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tse</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Ting</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Joint Conference on Natural Language Processing, IJC-NLP 2013</title>
		<meeting><address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10-14" />
			<biblScope unit="page" from="802" to="808" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing / ACL</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Are you for real? detecting identity fraud via dialogue interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1185</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="1762" to="1771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Finbert: A pretrained language model for financial communications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Christopher Siy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><surname>Uy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<idno>abs/2006.08097</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hotpotqa: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1259</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-31" />
			<biblScope unit="page" from="2369" to="2380" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spider: A largescale human-labeled dataset for complex and crossdomain semantic parsing and text-to-sql task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingning</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanelle</forename><surname>Roman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1425</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-31" />
			<biblScope unit="page" from="3911" to="3921" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Global table extractor (gte): A framework for joint table identification and cell structure recognition using visual context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Burdick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucian</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy Xin Ru</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conference for Applications in Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
