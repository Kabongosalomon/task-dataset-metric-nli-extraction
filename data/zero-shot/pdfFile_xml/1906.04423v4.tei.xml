<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NAS-FCOS: Fast Neural Architecture Search for Object Detection *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanning</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NAS-FCOS: Fast Neural Architecture Search for Object Detection *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The success of deep neural networks relies on significant architecture engineering. Recently neural architecture search (NAS) has emerged as a promise to greatly reduce manual effort in network design by automatically searching for optimal architectures, although typically such algorithms need an excessive amount of computational resources, e.g., a few thousand GPU-days. To date, on challenging vision tasks such as object detection, NAS, especially fast versions of NAS, is less studied. Here we propose to search for the decoder structure of object detectors with search efficiency being taken into consideration. To be more specific, we aim to efficiently search for the feature pyramid network (FPN) as well as the prediction head of a simple anchor-free object detector, namely FCOS [24], using a tailored reinforcement learning paradigm. With carefully designed search space, search algorithms and strategies for evaluating network quality, we are able to efficiently search a top-performing detection architecture within 4 days using 8 V100 GPUs. The discovered architecture surpasses state-of-the-art object detection models (such as Faster R-CNN, RetinaNet and FCOS) by 1.5 to 3.5 points in AP on the COCO dataset,with comparable computation complexity and memory footprint, demonstrating the efficacy of the proposed NAS for object detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object detection is one of the fundamental tasks in computer vision, and has been researched extensively. In the past few years, state-of-the-art methods for this task are based on deep convolutional neural networks (such as Faster R-CNN <ref type="bibr" target="#b19">[20]</ref>, RetinaNet <ref type="bibr" target="#b10">[11]</ref>), due to their impressive performance. Typically, the designs of object detection networks are much more complex than those for image classification, because the former need to localize and classify multiple objects in an image simultaneously while the latter only need to output image-level labels. Due to its complex structure and numerous hyper-parameters, designing effective object detection networks is more challenging and usually needs much manual effort.</p><p>On the other hand, Neural Architecture Search (NAS) approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32]</ref> have been showing impressive results on automatically discovering top-performing neural network architectures in large-scale search spaces. Compared to manual designs, NAS methods are data-driven instead of * NW, YG, HC contributed to this work equally. experience-driven, and hence need much less human intervention. As defined in <ref type="bibr" target="#b2">[3]</ref>, the workflow of NAS can be divided into the following three processes: 1) sampling architecture from a search space following some search strategies; 2) evaluating the performance of the sampled architecture; and 3) updating the parameters based on the performance.</p><p>One of the main problems prohibiting NAS from being used in more realistic applications is its search efficiency. The evaluation process is the most time consuming part because it involves a full training procedure of a neural network. To reduce the evaluation time, in practice a proxy task is often used as a lower cost substitution. In the proxy task, the input, network parameters and training iterations are often scaled down to speedup the evaluation. However, there is often a performance gap for samples between the proxy tasks and target tasks, which makes the evaluation process biased. How to design proxy tasks that are both accurate and efficient for specific problems is a challenging problem. Another solution to improve search efficiency is constructing a supernet that covers the complete search space and training candidate architectures with shared parameters <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref>. However, this solution leads to significantly increased memory consumption and restricts itself to small-to-moderate sized search spaces.</p><p>To our knowledge, studies on efficient and accurate NAS approaches to object detection networks are rarely touched, despite its significant importance. To this end, we present a fast and memory saving NAS method for object detection networks, which is capable of discovering top-performing architectures within significantly reduced search time. Our overall detection architecture is based on FCOS <ref type="bibr" target="#b23">[24]</ref>, a simple anchor-free one-stage object detection framework, in which the feature pyramid network and prediction head are searched using our proposed NAS method.</p><p>Our main contributions are summarized as follows.</p><p>? In this work, we propose a fast and memory-efficient NAS method for searching both FPN and head architectures, with carefully designed proxy tasks, search space and evaluation strategies, which is able to find top-performing architectures over 3, 000 architectures using 28 GPU-days only.</p><p>Specifically, this high efficiency is enabled with the following designs.</p><p>? Developing a fast proxy task training scheme by skipping the backbone finetuning stage;</p><p>? Adapting progressive search strategy to reduce time cost taken by the extended search space;</p><p>? Using a more discriminative criterion for evaluation of searched architectures.</p><p>? Employing an efficient anchor-free one-stage detection framework with simple post processing;</p><p>? Using NAS, we explore the workload relationship between FPN and head, proving the importance of weight sharing in head.</p><p>? We show that the overall structure of NAS-FCOS is general and flexible in that it can be equipped with various backbones including MobileNetV2, ResNet-50, ResNet-101 and ResNeXt-101, and surpasses stateof-the-art object detection algorithms using comparable computation complexity and memory footprint.</p><p>More specifically, our model can improve the AP by 1.5 ? 3.5 points on all above models comparing to their FCOS counterparts.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Object Detection</head><p>The frameworks of deep neural networks for object detection can be roughly categorized into two types: one-stage detectors <ref type="bibr" target="#b11">[12]</ref> and two-stage detectors <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>. Two-stage detection frameworks first generate classindependent region proposals using a region proposal network (RPN), and then classify and refine them using extra detection heads. In spite of achieving top performance, the two-stage methods have noticeable drawbacks: they are computationally expensive and have many hyperparameters that need to be tuned to fit a specific dataset.</p><p>In comparison, the structures of one-stage detectors are much simpler. They directly predict object categories and bounding boxes at each location of feature maps generated by a single CNN backbone.</p><p>Note that most state-of-the-art object detectors (including both one-stage detectors <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref> and two-stage detectors <ref type="bibr" target="#b19">[20]</ref>) make predictions based on anchor boxes of different scales and aspect ratios at each convolutional feature map location. However, the usage of anchor boxes may lead to high imbalance between object and non-object examples and introduce extra hyper-parameters. More recently, anchor-free one-stage detectors <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref> have attracted increasing research interests, due to their simple fully convolutional architectures and reduced consumption of computational resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Neural Architecture Search</head><p>NAS is usually time consuming. We have seen great improvements from 24, 000 GPU-days <ref type="bibr" target="#b31">[32]</ref> to 0.2 GPUday <ref type="bibr" target="#b27">[28]</ref>. The trick is to first construct a supernet containing the complete search space and train the candidates all at once with bi-level optimization and efficient weight sharing <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref>. But the large memory allocation and difficulties in approximated optimization prohibit the search for more complex structures.</p><p>Recently researchers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b22">23]</ref> propose to apply singlepath training to reduce the bias introduced by approximation and model simplification of the supernet. DetNAS <ref type="bibr" target="#b1">[2]</ref> follows this idea to search for an efficient object detection architecture. One limitation of the single-path approach is that the search space is restricted to a sequential structure. Single-path sampling and straight through estimate of the weight gradients introduce large variance to the optimization process and prohibit the search for more complex structures under this framework. Within this very simple search space, NAS algorithms can only make trivial decisions like kernel sizes for manually designed modules.</p><p>Object detection models are different from single-path image classification networks in their way of merging multi-level features and distributing the task to parallel prediction heads. Feature pyramid networks (FPNs) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27]</ref>, designed to handle this job, plays an important role in modern object detection models. NAS-FPN <ref type="bibr" target="#b3">[4]</ref> targets on searching for an FPN alternative based on one-stage framework RetinaNet <ref type="bibr" target="#b11">[12]</ref>. Feature pyramid architectures are sampled with a recurrent neural network (RNN) controller. The RNN controller is trained with reinforcement learning (RL). However, the search is very time-consuming even though a proxy task with ResNet-10 backbone is trained to evaluate each architecture.</p><p>Since all these three kinds of research ( <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref> and ours) focus on object detection framework, we demonstrate the differences among them that DetNAS <ref type="bibr" target="#b1">[2]</ref> aims to search for the designs of better backbones, while NAS-FPN <ref type="bibr" target="#b3">[4]</ref> searches the FPN structure, and our search space contains both FPN and head structure.</p><p>To speed up reward evaluation of RL-based NAS, the work of <ref type="bibr" target="#b16">[17]</ref> proposes to use progressive tasks and other training acceleration methods. By caching the encoder features, they are able to train semantic segmentation decoders with very large batch sizes very efficiently. In the sequel of this paper, we refer to this technique as fast decoder adaptation. However, directly applying this technique to object detection tasks does not enjoy similar speed boost, because they are either not in using a fully-convolutional model <ref type="bibr" target="#b10">[11]</ref> or require complicated post processing that are not scalable with the batch size <ref type="bibr" target="#b11">[12]</ref>.</p><p>To reduce the post processing overhead, we resort to a recently introduced anchor-free one-stage framework, namely, FCOS <ref type="bibr" target="#b23">[24]</ref>, which significantly improve the search efficiency by cancelling the processing time of anchor-box matching in RetinaNet.</p><p>Compared to its anchor-based counterpart, FCOS significantly reduces the training memory footprint while being able to improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>In our work, we search for anchor-free fully convolutional detection models with fast decoder adaptation. Thus, NAS methods can be easily applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>We base our search algorithm upon a one-stage framework FCOS due to its simplicity. Our training tuples {(x, Y )} consist of input image tensors x of size (3 ? H ? W ) and FCOS output targets Y in a pyramid representation, which is a list of tensors y l each of size</p><formula xml:id="formula_0">((K + 4 + 1) ? H l ? W l )</formula><p>where H l ?W l is feature map size on level p of the pyramid. (K + 4 + 1) is the output channels of FCOS, the three terms are length-K one-hot classification labels, 4 bounding box regression targets and 1 centerness factor respectively.</p><p>The network g : x ?? in original FCOS consists of three parts, a backbone b, FPN f and multi-level subnets we call prediction heads h in this paper. First backbone b : x ? C maps the input tensor to a set of intermediate-</p><formula xml:id="formula_1">leveled features C = {c 3 , c 4 , c 5 }, with resolution (H i ? W i ) = (H/2 i ? W/2 i ). Then FPN f : C ? P maps the features to a feature pyramid P = {p 3 , p 4 , p 5 , p 6 , p 7 }.</formula><p>Then the prediction head h : p ? y is applied to each level of P and the result is collected to create the final prediction. To avoid overfitting, same h is often applied to all instances in P .</p><p>Since objects of different scales require different effective receptive fields, the mechanism to select and merge intermediate-leveled features C is particularly important in object detection network design. Thus, most researches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20]</ref> are carried out on designing f and h while using widelyadopted backbone structures such as ResNet <ref type="bibr" target="#b6">[7]</ref>. Following this principle, our search goal is to decide when to choose which features from C and how to merge them.</p><p>To improve the efficiency, we reuse the parameters in b pretrained on target dataset and search for the optimal structures after that. For the convenience of the following statement, we call the network components to search for, namely f and h, together the decoder structure for the objection detection network.</p><p>f and h take care of different parts of the detection job. f extracts features targeting different object scales in the pyramid representations P , while h is a unified mapping applied to each feature in P to avoid overfitting. In practice, people seldom discuss the possibility of using a more diversified f to extract features at different levels or how many layers in h need to be shared across the levels. In this work, we use NAS as an automatic method to test these possibilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Search Space</head><p>Considering the different functions of f and h, we apply two search space respectively. Given the particularity of FPN structure, a basic block with new overall connection and f 's output design is built for it. For simplicity, sequential space is applied for h part.</p><p>We replace the cell structure with atomic operations to provide even more flexibility. To construct one basic block, we first choose two layers x 1 , x 2 from the sampling pool X at id1, id2, then two operations op1, op2 are applied to each of them and an aggregation operation agg merges the two output into one feature. To build a deep decoder structure, we apply multiple basic blocks with their outputs added to the sampling pool. Our basic block bb t : </p><formula xml:id="formula_2">X t?1 ? X t at</formula><formula xml:id="formula_3">X t?1 to X t = X t?1 ? {x t }, where x t is the output of bb t .</formula><p>The candidate operations are listed in <ref type="table" target="#tab_0">Table 1</ref>. We include only separable/depth-wise convolutions so that the decoder can be efficient. In order to enable the decoder to apply convolutional filters on irregular grids, here we have also included deformable 3 ? 3 convolutions <ref type="bibr" target="#b30">[31]</ref>. For the aggregation operations, we include element-wise sum and concatenation followed by a 1 ? 1 convolution.</p><p>The decoder configuration can be represented by a sequence with three components, FPN configuration, head configuration and weight sharing stages. We provide detailed descriptions to each of them in the following sections. The complete diagram of our decoder structure is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">FPN Search Space</head><p>As mentioned above, the FPN f maps the convolutional features C to P . First, we initialize the sampling pool as X 0 = C. Our FPN is defined by applying the basic block 7 times to the sampling pool, f :</p><formula xml:id="formula_4">= bb f 1 ? bb f 2 ? ? ? ? ? bb f 7 .</formula><p>To yield pyramid features P , we collect the last three basic block outputs {x 5 , x 6 , x 7 } as {p 3 , p 4 , p 5 }.</p><p>To allow shared information across all layers, we use a simple rule to create global features. If there is some dangling layer x t which is not sampled by later blocks {bb f i |i &gt; t} nor belongs to the last three layers t &lt; 5, we use element-wise add to merge it to all output features</p><formula xml:id="formula_5">p * i = p i + x t , i ? {3, 4, 5}.<label>(1)</label></formula><p>Same as the aggregation operations, if the features have different resolution, the smaller one is upsampled with bilinear interpolation.</p><p>To be consistent with FCOS, p 6 and p 7 are obtained via a 3 ? 3 stride-2 convolution on p 5 and p 6 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Prediction Head Search Space</head><p>Prediction head h maps each feature in the pyramid P to the output of corresponding y, which in FCOS and RetinaNet, consists of four 3 ? 3 convolutions. To explore the potential of the head, we therefore extend a sequential search space for its generation. Specifically, our head is defined as a sequence of six basic operations. Compared with candidate operations in the FPN structures, the head search space has two slight differences. First, we add standard convolution modules (including conv1x1 and conv3x3) to the head sampling pool for better comparison. Second, we follow the design of FCOS by replacing all the Batch Normalization (BN) layers to Group Normalization (GN) <ref type="bibr" target="#b24">[25]</ref> in the operations sampling pool of head, considering that head needs to share weights between different levels, which causes BN invalid. The final output of head is the output of the last (sixth) layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Searching for Head Weight Sharing</head><p>To add even more flexibility and understand the effect of weight sharing in prediction heads, we further add an index i as the location where the prediction head starts to share weights. For every layer before stage i, the head h will create independent set of weights for each FPN output level, otherwise, it will use the global weights for sharing purpose.</p><p>Considering the independent part of the heads being extended FPN branch and the shared part as head with adaptive-length, we can further balance the workload for each individual FPN branch to extract level-specific features and the prediction head shared across all levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Search Strategy</head><p>RL based strategy is applied to the search process. We rely on an LSTM-based controller to predict the full configuration. We consider using a progressive search strategy rather than the joint search for both FPN structure and prediction head part, since the former requires less computing resources and time cost than the latter. The training dataset is randomly split into a meta-train D t and meta-val D v subset. To speed up the training, we fix the backbone network and cache the pre-computed backbone output C. This makes our single architecture training cost independent from the depth of backbone network. Taking this advantage, we can apply much more complex backbone structures and utilize high quality multilevel features as our decoder's input. We find that the process of backbone finetuning can be skipped if the cached features are powerful enough. Speedup techniques such as Polyak weight averaging are also applied during the training.</p><p>The most widely used detection metric is average precision (AP). However, due to the difficulty of object detection task, at the early stages, AP is too low to tell the good ar-chitectures from the bad ones, which makes the controller take much more time to converge. To make the architecture evaluation process easier even at the early stages of the training, we therefore use negative loss sum as the reward instead of average precision:</p><formula xml:id="formula_6">R(a) = ? (x,Y )?Dv (L cls (x, Y |a) + L reg (x, Y |a) + L ctr (x, Y |a))<label>(2)</label></formula><p>where L cls , L reg , L ctr are the three loss terms in FCOS. Gradient of the controller is estimated via proximal policy optimization (PPO) <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Searching Phase</head><p>We design a fast proxy task for evaluating the decoder architectures sampled in the searching phase. PASCAL VOC is selected as the proxy dataset, which contains 5715 training images with object bounding box annotations of 20 classes. Transfer capacity of the structures can be illustrated since the search and full training phase use different datasets. The VOC training set is randomly split into a meta-train set with 4, 000 images and a meta-val set with 1715 images. For each sampled architecture, we train it on metatrain and compute the reward (2) on meta-val. Input images are resized to short size 384 and then randomly cropped to 384 ? 384. Target object sizes of interest are scaled correspondingly. We use Adam optimizer with learning rate 8e?4 and batch size 200. Polyak averaging is applied with the decay rates of 0.9. The decoder is evaluated after 300 iterations. As we use fast decoder adaptation, the backbone features are fixed and cached during the search phase. To enhance the cached backbone features, we first initialize them with pre-trained weights provided by open-source im- plementation of FCOS 1 and then finetune on VOC using the training strategies of FCOS. Note that the above finetuning process is only performed once at the begining of the search phase.</p><p>A progressive strategy is used for the search of f and h. We first search for the FPN part and retain the original head. All operations in the FPN structure have 64 output channels. The decoder inputs C are resized to fit output channel width of FPN via 1 ? 1 convolutions. After this step, a searched FPN structure is fixed and the second stage searching for the head will be started based on it. Most parameters for searching head are identical to those for searching FPN structure, with the exception that the output channel width is adjusted from 64 to 128 to deliver more information.</p><p>For the FPN search part, the controller model nearly converged after searching over 2.8K architectures on the proxy task as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Then, the top-20 best performing architectures on the proxy task are selected for the next full training phase. For the head search part, we choose the best searched FPN among the top-20 architectures and pre-fetch its features. It takes about 600 rounds for the controller to nearly converge, which is much faster than that for searching FPN architectures. After that, we select for full training the top-10 heads that achieve best performance on the proxy task. In total, the whole search phase can be finished within 4 days using 8 V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Full Training Phase</head><p>In this phase, we fully train the searched models on the MS COCO training dataset, and select the best one by evaluating them on MS COCO validation images. Note that our training configurations are exactly the same as those in FCOS for fair comparison. Input images are resized to short size 800 and the maximum long side is set to be 1333. The models are trained using 4 V100 GPUs with batch size 16 for 90K iterations. The initial learning rate is 0.01 and reduces to one tenth at the 60K-th and 80K-th iterations. The improving tricks are applied only on the final model (w/improv).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Search Results</head><p>The best FPN structure is illustrated in <ref type="figure">Fig. 3</ref>. The controller identifies that deformable convolution and concate-1 https://tinyurl.com/FCOSv1 nation are the best performing operations for unary and aggregation respectively. From <ref type="figure">Fig. 4</ref>, we can see that the controller chooses to use 4 operations (with two skip connections), rather than the maximum allowed 6 operations. Note that the discovered "dconv + 1x1 conv" structure achieves a good trade-off between accuracy and FLOPs. Compared with the original head, our searched head has fewer FLOPs/Params (FLOPs 79.24G vs. 89.16G, Params 3.41M vs. 4.92M) and significantly better performance (AP 38.7 vs. 37.4).</p><p>We use the searched decoder together with either lightweight backbones such as MobileNet-V2 <ref type="bibr" target="#b20">[21]</ref> or more powerful backbones such as ResNet-101 <ref type="bibr" target="#b6">[7]</ref> and ResNeXt-101 <ref type="bibr" target="#b25">[26]</ref>. To balance the performance and efficiency, we implement three decoders with different computation budgets: one with feature dimension of 128 (@128), one with 256 (@256) and another with FPN channel width 128 and prediction head 256 (@128-256). The results on the COCO test-dev with short side being 800 is shown in <ref type="table" target="#tab_2">Table 2</ref>. The searched decoder with feature dimension of 256 (@256) surpasses its FCOS counterpart by 1.5 to 3.5 points in AP under different backbones. The one with 128 channels (@128) has significantly reduced parameters and calculation, making it more suitable for resource-constrained environments. In particular, our searched model with 128 channels and MobileNetV2 backbone suparsses the original FCOS with the same backbone by 0.8 AP points with only 1/3 FLOPS. The third type of decoder (@128-256) achieves a good balance between accuracy and parameters. Note that our searched model outperforms the strongest FCOS variant by    number of parameters with other models are illustrated in <ref type="figure">Fig. 7</ref> and <ref type="figure">Fig. 8</ref> respectively.</p><p>In order to understand the importance of weight sharing in head, we add the number of layers shared by weights as an object of the search. <ref type="figure" target="#fig_4">Fig. 5</ref> shows a trend graph of head weight sharing during search. We set 50 structures as a statistical cycle. As the search deepens, the proportion of fully shared structures increases, indicating that on the multi-scale detection model, head weight sharing is a necessity.</p><p>We also demonstrate the comparison with other NAS methods for object detection in <ref type="table">Table 3</ref>. Our method is able to search for twice more architectures than DetNAS <ref type="bibr" target="#b1">[2]</ref> per GPU-day. Note that the AP of NAS-FPN <ref type="bibr" target="#b3">[4]</ref> is achieved by stacking the searched FPN 7 times, while we do not stack our searched FPN. Our model with ResNeXt-101 (64x4d) as backbone outperforms NAS-FPN by 1.3 AP points while using only 1/3 FLOPs and less calculation cost.</p><p>We further measure the correlation between rewards obtained during the search process with the proxy dataset and APs attained by same architectures trained on COCO. Specifically, we randomly sample 15 architectures from all the searched structures trained on COCO with batch size   <ref type="table">Table 3</ref> -Comparison with other NAS methods. For NAS-FPN, the input size is 1280 ? 1280 and the search cost should be timed by their number of TPUs used to train each architecture. Note that the FLOPs and AP of NAS-FPN @256 here are from <ref type="figure" target="#fig_0">Figure 11</ref> in NAS-FPN <ref type="bibr" target="#b3">[4]</ref>, and NAS-FPN 7@256 stacks the searched FPN structure 7 times. The input images are resized such that their shorter size is 800 pixels in DetNASNet <ref type="bibr" target="#b1">[2]</ref> and our models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mobile-FPN-RetinaNet@256</head><p>Mobile-FPN-FCOS@256</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mobile-NAS-FCOS (ours) @128</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mobile-NAS-FCOS(ours)@128-256</head><p>Mobile-NAS-FCOS (ours) @256</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R50-FPN-RetinaNet @256</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R50-FPN-FCOS @256</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R101-FPN-RetinaNet @256</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R101-FPN-FCOS @256</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R101-NAS-FCOS(ours)@256</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R50-NAS-FCOS(ours)@256</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R50-NAS-FCOS(ours)@128</head><p>R50-NAS-FCOS(ours)@128-256 <ref type="figure">Figure 7</ref> -Diagram of the relationship between FLOPs and AP with different backbones. Points of different shapes represent different backbones. NAS-FCOS@128 has a slight increase in precision which also gains the advantage of computation quantity. One with 256 channels obtains the highest precision with more computation complexity. Using FPN channel width 128 and prediction head 256 (@128-256) offers a trade-off. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mobile-FPN-RetinaNet@256</head><p>Mobile-FPN-FCOS@256</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mobile-NAS-FCOS(ours)@128</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mobile-NAS-FCOS(ours)@128-256</head><p>Mobile-NAS-FCOS (ours) @256 <ref type="figure">Figure 8</ref> -Diagram of the relationship between parameters and AP with different backbones. Adjusting the number of channels in the FPN structure and head helps to achieve a balance between accuracy and parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R50-FPN-RetinaNet @256</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R50-FPN-FCOS @256</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R50-NAS-FCOS(ours)@128</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R50-NAS-FCOS(ours)@128-256</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R50-NAS-FCOS(ours)@256</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R101-FPN-RetinaNet @256</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R101-FPN-FCOS @256</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R101-NAS-FCOS(ours)@256</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>16.</head><p>Since full training on COCO is time-consuming, we reduce the iterations to 60K. The model is then evaluated on the COCO 2017 validation set. As visible in <ref type="figure" target="#fig_5">Fig. 6</ref>, there is a strong correlation between search rewards and APs obtained from COCO. Poor-and well-performing architectures can be distinguished by the rewards on the proxy task very well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Design of Reinforcement Learning Reward</head><p>As we discussed above, it is common to use widely accepted indicators as rewards for specific tasks in the search, such as mIOU for segmentation and AP for object detection. However, we found that using AP as reward did not show a clear upward trend in short-term search rounds (blue curve in <ref type="figure" target="#fig_8">Fig. 9</ref>). We further analyze the possible reason to be that the controller tries to learn a mapping from the decoder to the reward while the calculation of AP itself is complicated, which makes it difficult to learn this mapping within a limited number of iterations. In comparison, we clearly see the increase of AP with the validation loss as RL rewards (red curve in <ref type="figure" target="#fig_8">Fig. 9</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Effectiveness of Search Space</head><p>To further discuss the impact of the search spaces f and h, we design three experiments for verification. One is to search f with the original head being fixed, one is to search h with the original FPN being fixed and another is to search the entire decoder (f +h). As shown in <ref type="table" target="#tab_5">Table 4</ref>, it turns out that searching f brings slightly more benefits than searching  h only. And our progressive search which combines both f and h achieves a better result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Impact of Deformable Convolution</head><p>As aforementioned, deformable convolutions are included in the set of candidate operations for both f and h, which are able to adapt to the geometric variations of objects. For fair comparison, we also replace the whole standard 3 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have proposed to use Neural Architecture Search to further optimize the process of designing object detection networks. It is shown in this work that top-performing detectors can be efficiently searched using carefully designed proxy tasks, search strategies and model evaluation metrics. The experiments on COCO demonstrates the efficiency of our discovered model NAS-FCOS and its flexibility to be used with various backbone architectures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 -</head><label>1</label><figDesc>A conceptual example of our NAS-FCOS decoder. It consists of two sub networks, an FPN f and a set of prediction heads h which have shared structures. One notable difference with other FPN-based one-stage detectors is that our heads have partially shared weights. Only the last several layers of the predictions heads (marked as yellow) are tied by their weights. The number of layers to share is decided automatically by the search algorithm. Note that both FPN and head are in our actual search space; and have more layers than shown in thisfigure.Here the figure is for illustration only.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 -</head><label>2</label><figDesc>Performance of reward during the proxy task, which has been growing throughout the process, indicating that the model of reinforcement learning works.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 -Figure 4 -</head><label>34</label><figDesc>Our discovered FPN structure. C 2 is omitted from this figure since it is not chosen by this particular structure during the search process. Our discovered Head structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 -</head><label>5</label><figDesc>Trend graph of head weight sharing during search. The coordinates in the horizontal axis represent the number of the statistical period. A period consists of 50 head structures. The vertical axis represents the proportion of heads that fully share weights in 50 structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 -</head><label>6</label><figDesc>Correlation between the search reward obtained on the VOC meta-val dataset and the AP evaluated on COCO-val.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 -</head><label>9</label><figDesc>Comparison of two different RL reward designs. The vertical axis represents AP obtained from the proxy task on the validation dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>? 3 convolutions with deformable 3 ? 3</head><label>33</label><figDesc>convolutions in FPN structure of the original FCOS and repeat them twice, making the FLOPs and parameters nearly equal to our searched model. The new model is therefore called DeformFPN-FCOS. It turns out that our NAS-FCOS model still achieves better performance (AP = 38.9 with FPN search only, and AP = 39.8 with both FPN and Head searched) than the DeformFPN-FCOS model (AP = 38.4) under this circumstance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 -</head><label>1</label><figDesc>time step t transforms the sampling pool Unary operations used in the search process.</figDesc><table><row><cell>ID</cell><cell>Description</cell></row><row><cell>0</cell><cell>separable conv 3 ? 3</cell></row><row><cell>1</cell><cell>separable conv 3 ? 3 with dilation rate 3</cell></row><row><cell>2</cell><cell>separable conv 5 ? 5 with dilation rate 6</cell></row><row><cell>3</cell><cell>skip-connection</cell></row><row><cell>4</cell><cell>deformable 3 ? 3 convolution</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1.4 AP points (46.1 vs. 44.7) with slightly smaller FLOPs and Params. The comparison of FLOPs and</figDesc><table><row><cell>Decoder</cell><cell>Backbone</cell><cell cols="2">FLOPs (G) Params (M)</cell><cell>AP</cell></row><row><cell>FPN-RetinaNet @256</cell><cell>MobileNetV2</cell><cell>133.4</cell><cell>11.3</cell><cell>30.8</cell></row><row><cell>FPN-FCOS @256</cell><cell>MobileNetV2</cell><cell>105.4</cell><cell>9.8</cell><cell>31.2</cell></row><row><cell>NAS-FCOS (ours) @128</cell><cell>MobileNetV2</cell><cell>39.3</cell><cell>5.9</cell><cell>32.0</cell></row><row><cell>NAS-FCOS (ours) @128-256</cell><cell>MobileNetV2</cell><cell>95.6</cell><cell>9.9</cell><cell>33.8</cell></row><row><cell>NAS-FCOS (ours) @256</cell><cell>MobileNetV2</cell><cell>121.8</cell><cell>16.1</cell><cell>34.7</cell></row><row><cell>FPN-RetinaNet @256</cell><cell>R-50</cell><cell>198.0</cell><cell>33.6</cell><cell>36.1</cell></row><row><cell>FPN-FCOS @256</cell><cell>R-50</cell><cell>169.9</cell><cell>32.0</cell><cell>37.4</cell></row><row><cell>NAS-FCOS (ours) @128</cell><cell>R-50</cell><cell>104.0</cell><cell>27.8</cell><cell>37.9</cell></row><row><cell>NAS-FCOS (ours) @128-256</cell><cell>R-50</cell><cell>160.4</cell><cell>31.8</cell><cell>39.1</cell></row><row><cell>NAS-FCOS (ours) @256</cell><cell>R-50</cell><cell>189.6</cell><cell>38.4</cell><cell>39.8</cell></row><row><cell>FPN-RetinaNet @256</cell><cell>R-101</cell><cell>262.4</cell><cell>52.5</cell><cell>37.8</cell></row><row><cell>FPN-FCOS @256</cell><cell>R-101</cell><cell>234.3</cell><cell>50.9</cell><cell>41.5</cell></row><row><cell>NAS-FCOS (ours) @256</cell><cell>R-101</cell><cell>254.0</cell><cell>57.3</cell><cell>43.0</cell></row><row><cell>FPN-FCOS @256</cell><cell>X-64x4d-101</cell><cell>371.2</cell><cell>89.6</cell><cell>43.2</cell></row><row><cell>NAS-FCOS (ours) @128-256</cell><cell>X-64x4d-101</cell><cell>361.6</cell><cell>89.4</cell><cell>44.5</cell></row><row><cell>FPN-FCOS @256 w/improvements</cell><cell>X-64x4d-101</cell><cell>371.2</cell><cell>89.6</cell><cell>44.7</cell></row><row><cell cols="2">NAS-FCOS (ours) @128-256 w/improvements X-64x4d-101</cell><cell>361.6</cell><cell>89.4</cell><cell>46.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 -</head><label>2</label><figDesc>Results on test-dev set of MS COCO after full training. R-50 and R-101 represents ResNet backbones and X-64x4d-101 represents ResNeXt-101 (64 ? 4d). All networks share the same input image resolution. FLOPs and parameters are being measured on 1088 ? 800, which is the median of the input size on COCO. For RetinaNet and FCOS, we use official models provided by the authors. For our NAS-FCOS, @128 and @256 means that the decoder channel width is 128 and 256 respectively. @128-256 is the decoder with 128 FPN width and 256 head width. The same improving tricks used on the newest FCOS version are used in our model for fair comparison.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 -</head><label>4</label><figDesc>Comparisons between APs obtained under different search space with ResNet-50 backbone.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhong</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detnas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10979</idno>
		<title level="m">Neural architecture search on object detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Hendrik</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05377</idno>
		<title level="m">Neural architecture search: A survey</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nas-Fpn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Single path oneshot neural architecture search with uniform sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00420</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Panoptic feature pyramid networks. arXiv: Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Foveabox: Beyond anchor-based object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03797</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An end-to-end network for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast neural architecture search of compact semantic segmentation models via auxiliary cells</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Nekrasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melody</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">Yolov3: An incremental improvement. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<title level="m">Proximal policy optimization algorithms. arXiv: Comp. Res. Repository</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Single-path NAS: Designing hardwareefficient convnets in less than 4 hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Stamoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhou</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Lymberopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodhi</forename><surname>Priyantha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Marculescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02877</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">FCOS: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01355</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05431</idno>
		<title level="m">Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pyramid feature attention network for saliency detection. arXiv: Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangqian</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongpeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04919</idno>
		<title level="m">BayesNAS: A bayesian approach for neural architecture search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Feature selective anchor-free module for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00621</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11168</idno>
		<title level="m">Deformable convnets v2: More deformable, better results</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
