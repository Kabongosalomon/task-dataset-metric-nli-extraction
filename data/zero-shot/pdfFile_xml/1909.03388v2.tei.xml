<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">L DMI : A Novel Information-theoretic Loss Function for Training Deep Nets Robust to Label Noise</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Xu</surname></persName>
							<email>xuyilun@pku.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Kong</surname></persName>
							<email>yuqing.kong@pku.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
							<email>yizhou.wang@pku.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">The Center on Frontiers of Computing Studies, Computer Science Dept</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Computer Science Dept</orgName>
								<orgName type="department" key="dep2">Deepwise AI Lab</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">L DMI : A Novel Information-theoretic Loss Function for Training Deep Nets Robust to Label Noise</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurately annotating large scale dataset is notoriously expensive both in time and in money. Although acquiring low-quality-annotated dataset can be much cheaper, it often badly damages the performance of trained models when using such dataset without particular treatment. Various methods have been proposed for learning with noisy labels. However, most methods only handle limited kinds of noise patterns, require auxiliary information or steps (e.g., knowing or estimating the noise transition matrix), or lack theoretical justification. In this paper, we propose a novel information-theoretic loss function, L DMI , for training deep neural networks robust to label noise. The core of L DMI is a generalized version of mutual information, termed Determinant based Mutual Information (DMI), which is not only information-monotone but also relatively invariant. To the best of our knowledge, L DMI is the first loss function that is provably robust to instance-independent label noise, regardless of noise pattern, and it can be applied to any existing classification neural networks straightforwardly without any auxiliary information. In addition to theoretical justification, we also empirically show that using L DMI outperforms all other counterparts in the classification task on both image dataset and natural language dataset include Fashion-MNIST, CIFAR-10, Dogs vs. Cats, MR with a variety of synthesized noise patterns and noise amounts, as well as a real-world dataset Clothing1M. Codes are available at https://github.com/Newbeeer/L_DMI.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks, together with large scale accurately annotated datasets, have achieved remarkable performance in a great many classification tasks in recent years (e.g., <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b16">18]</ref> ). However, it is usually money-and time-consuming to find experts to annotate labels for large scale datasets. While collecting labels from crowdsourcing platforms like Amazon Mechanical Turk is a potential way to get annotations cheaper and faster, the collected labels are usually very noisy. The noisy labels hampers the performance of deep neural networks since the commonly used cross entropy loss is not noise-robust. This raises an urgent demand on designing noise-robust loss functions.</p><p>Some previous works have proposed several loss functions for training deep neural networks with noisy labels. However, they either use auxiliary information <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b27">29]</ref> (e.g., having an additional set of clean data or the noise transition matrix) or steps <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b31">33]</ref> (e.g. estimating the noise transition matrix), or make assumptions on the noise <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b46">48]</ref> and thus can only handle limited kinds of the noise patterns (see perliminaries for definition of different noise patterns).</p><p>One reason that the loss functions used in previous works are not robust to a certain noise pattern, say diagonally non-dominant noise, is that they are distance-based, i.e., the loss is the distance between the classifier's outputs and the labels (e.g. 0-1 loss, cross entropy loss). When datapoints are labeled by a careless annotator who tends to label the a priori popular class (e.g. For medical images, given the prior knowledge is 10% malignant and 90% benign, a careless annotator labels "benign" when the underline true label is "benign" and labels "benign" with 90% probability when the underline true label is "malignant".), the collected noisy labels have a diagonally non-dominant noise pattern and are extremely biased to one class ("benign"). In this situation, the distanced-based losses will prefer the "meaningless classifier" who always outputs the a priori popular class ("benign") than the classifier who outputs the true labels.</p><p>To address this issue, instead of using distance-based losses, we propose to employ informationtheoretic loss such that the classifier, whose outputs have the highest mutual information with the labels, has the lowest loss. The key observation is that the "meaningless classifier" has no information about anything and will be naturally eliminated by the information-theoretic loss. Moreover, the information-monotonicity of the mutual information guarantees that adding noises to a classifier's output will make this classifier less preferred by the information-theoretic loss.</p><p>However, the key observation is not sufficient. In fact, we want an information measure I to satisfy I(classifier 1's output; noisy labels) &gt; I(classifier 2's output; noisy labels) ?I(classifier 1's output; clean labels) &gt; I(classifier 2's output; clean labels).</p><p>Unfortunately, the traditional Shannon mutual information (MI) does not satisfy the above formula, while we find that a generalized information measure, namely, DMI (Determinant based Mutual Information), satisfies the above formula. Like MI, DMI measures the correlation between two random variables. It is defined as the determinant of the matrix that describes the joint distribution over the two variables. Intuitively, when two random variables are independent, their joint distribution matrix has low rank and zero determinant. Moreover, DMI is not only information-monotone like MI, but also relatively invariant because of the multiplication property of the determinant. The relative invariance of DMI makes it satisfy the above formula.</p><p>Based on DMI, we propose a noise-robust loss function L DMI which is simply L DMI (data; classifier) ?= ? log[DMI(classifier's output; labels)].</p><p>As shown in theorem 4.1 later, with L DMI , the following equation holds:</p><p>L DMI (noisy data; classifier) = L DMI (clean data; classifier) + noise amount, and the noise amount is a constant given the dataset. The equation reveals that with L DMI , training with the noisy labels is theoretically equivalent with training with the clean labels in the dataset, regardless of the noise patterns, including the noise amounts.</p><p>In summary, we propose a novel information theoretic noise-robust loss function L DMI based on a generalized information measure, DMI. Theoretically we show that L DMI is robust to instanceindependent label noise. As an additional benefit, it can be easily applied to any existing classification neural networks straightforwardly without any auxiliary information. Extensive experiments have been done on both image dataset and natural language dataset including Fashion-MNIST, CIFAR-10, Dogs vs. Cats, MR with a variety of synthesized noise patterns and noise amounts as well as a real-world dataset Clothing1M. The results demonstrate the superior performance of L DMI .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>A series of works have attempted to design noise-robust loss functions. In the context of binary classification, some loss functions (e.g., 0-1 loss <ref type="bibr" target="#b20">[22]</ref> , ramp loss <ref type="bibr" target="#b1">[3]</ref> , unhinged loss <ref type="bibr" target="#b38">[40]</ref> , savage loss <ref type="bibr" target="#b21">[23]</ref> ) have been proved to be robust to uniform or symmetric noise and Natarajan et al. <ref type="bibr" target="#b24">[26]</ref> presented a general way to modify any given surrogate loss function. Ghosh et al. <ref type="bibr" target="#b5">[7]</ref> generalized the existing results for binary classification problem to multi-class classification problem and proved that MAE (Mean Absolute Error) is robust to diagonally dominant noise. Zhang et al. <ref type="bibr" target="#b46">[48]</ref> showed MAE performs poorly with deep neural network and they combined MAE and cross entropy loss to obtain a new loss function. Patrini et al. <ref type="bibr" target="#b27">[29]</ref> provided two kinds of loss correction methods with knowing the noise transition matrix. The noise transition matrix sometimes can be estimated from the noisy data <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b31">33]</ref> . Hendrycks et al. <ref type="bibr" target="#b10">[12]</ref> proposed another loss correction technique with an additional set of clean data. To the best of our knowledge, we are the first to provide a loss function that is provably robust to instance-independent label noise without knowing the transition matrix, regardless of noise pattern and noise amount.</p><p>Instead of designing an inherently noise-robust function, several works used special architectures to deal with the problem of training deep neural networks with noisy labels. Some of them focused on estimating the noise transition matrix to handle the label noise and proposed a variety of ways to constrain the optimization <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b42">44]</ref> . Some of them focused on finding ways to distinguish noisy labels from clean labels and used example re-weighting strategies to give the noisy labels less weights <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b30">32]</ref> . While these methods seem to perform well in practice, they cannot guarantee the robustness to label noise theoretically and are also outperformed by our method empirically.</p><p>On the other hand, Zhang et al. <ref type="bibr" target="#b44">[46]</ref> have shown that deep neural networks can easily memorize completely random labels, thus several works propose frameworks to prevent this overfitting issue empirically in the setting of deep learning from noisy labels. For example, teacher-student curriculum learning framework <ref type="bibr" target="#b12">[14]</ref> and co-teaching framework <ref type="bibr" target="#b8">[10]</ref> have been shown to be helpful. Multi-task frameworks that jointly estimates true labels and learns to classify images are also introduced <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b43">45]</ref> . Explicit and implicit regularization methods can also be applied <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b45">47]</ref> . We consider a different perspective from them and focus on designing an inherently noise-robust function.</p><p>In this paper, we only consider instance-independent noise. There are also some works that investigate instance-dependent noise model (e.g. <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b22">24]</ref> ). They focus on the binary setting and assume that the noisy and true labels agree on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem settings</head><p>We denote the set of classes by C and the size of C by C. We also denote the domain of datapoints by X . A classifier is denoted by h ? X ? ? C , where ? C is the set of all possible distributions over C. h represents a randomized classifier such that given x ? X , h(x) c is the probability that h maps x into class c. Note that fixing the input x, the randomness of a classifier is independent of everything else.</p><p>There are N datapoints {x i } N i=1 . For each datapoint x i , there is an unknown ground truth y i ? C. We assume that there is an unknown</p><formula xml:id="formula_0">prior distribution Q X,Y over X ? C such that {(x i , y i )} N i=1 are i.i.d. samples drawn from Q X,Y and Q X,Y (x, y) = Pr[X = x, Y = y].</formula><p>Note that here we allow the datapoints to be "imperfect" instances, i.e., there still exists uncertainty for Y conditioning on fully knowing X.</p><p>Traditional supervised learning aims to train a classifier h * that is able to classify new datapoints into their ground truth categories with access to {(x i , y i )} N i=1 . However, in the setting of learning with noisy labels, instead, we only have access to</p><formula xml:id="formula_1">{(x i ,? i )} N i=1</formula><p>where? i is a noisy version of y i . We use a random variable? to denote the noisy version of Y and T Y ?? to denote the transition distribution between Y and , i.e.</p><formula xml:id="formula_2">T Y ?? (y,?) = Pr[? =? Y = y]. We use T Y ?? to represent the C ? C matrix format of T Y ?? .</formula><p>Generally speaking <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b46">48]</ref> , label noise can be divided into several kinds according to the noise transition matrix T Y ?? . It is defined as class-independent (or uniform) if a label is substituted by a uniformly random label regardless of the classes, i.e.</p><formula xml:id="formula_3">Pr[? =c Y = c] = Pr[? =c ? Y = c], ?c,c ? ? c (e.g. T Y ?? = 0.7 0.3 0.3 0.7 )</formula><p>. It is defined as diagonally dominant if for every row of T Y ?? , the magnitude of the diagonal entry is larger than any non-diagonal entry, i.e.</p><formula xml:id="formula_4">Pr[? = c Y = c] &gt; Pr[? = c Y = c], ?c ? c (e.g. T Y ?? = 0.7 0.3 0.2 0.8 ). It is defined as diagonally non-dominant if it is not diagonally dominant (e.g. the example mentioned in introduction, T Y ?? = 1 0 0.9 0.1 ).</formula><p>We assume that the noise is independent of the datapoints conditioning on the ground truth, which is commonly assumed in the literature <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b46">48]</ref> , i.e., Assumption 3.1 (Independent noise). X is independent of? conditioning on Y .</p><p>We also need that the noisy version? is still informative.</p><formula xml:id="formula_5">Assumption 3.2 (Informative noisy label). T Y ?? is invertible, i.e., det(T Y ?? ) ? 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Information theory concepts</head><p>Since Shannon's seminal work <ref type="bibr" target="#b33">[35]</ref> , information theory has shown its powerful impact in various of fields, including several recent deep learning works <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b15">17]</ref> . Our work is also inspired by information theory. This section introduces several basic information theory concepts.</p><p>Information theory is commonly related to random variables. For every random variable W 1 , Shannon's entropy H(W 1 ) ?= ? w1 Pr[W = w 1 ] log Pr[W = w 1 ] measures the uncertainty of W 1 . For example, deterministic W 1 has lowest entropy. For every two random variables W 1 and W 2 , Shannon</p><formula xml:id="formula_6">mutual information MI(W 1 , W 2 ) ?= ? w1,w2 Pr[W 1 = w 1 , W 2 = w 2 ] log Pr[W =w1,W =w2]</formula><p>Pr[W1=w1] Pr[W2=w2] measures the amount of relevance between W 1 and W 2 . For example, when W 1 and W 2 are independent, they have the lowest Shannon mutual information, zero.</p><p>Shannon mutual information is non-negative, symmetric, i.e., MI(W 1 , W 2 ) = MI(W 2 , W 1 ), and also satisfies a desired property, information-monotonicity, i.e., the mutual information between W 1 and W 2 will always decrease if either W 1 or W 2 has been "processed". Fact 3.3 (Information-monotonicity <ref type="bibr" target="#b4">[6]</ref> ). For all random variables</p><formula xml:id="formula_7">W 1 , W 2 , W 3 , when W 3 is less informative for W 2 than W 1 , i.e., W 3 is independent of W 2 conditioning W 1 , MI(W 3 , W 2 ) ? MI(W 1 , W 2 ).</formula><p>This property naturally induces that for all random variables W 1 , W 2 ,</p><formula xml:id="formula_8">MI(W 1 , W 2 ) ? MI(W 2 , W 2 ) = H(W 2 )</formula><p>since W 2 is always the most informative random variable for itself.</p><p>Based on Shannon mutual information, a performance measure for a classifier h can be naturally defined. High quality classifier's output h(X) should have high mutual information with the ground truth category Y . Thus, a classifier h's performance can be measured by MI(h(X), Y ).</p><p>However, in our setting, we only have access to the i.i.d. samples of h(X) and? . A natural attempt is to measure a classifier h's performance by MI(h(X),? ). Unfortunately, under this performance measure, the measurement based on noisy labels MI(h(X),? ) may not be consistent with the measurement based on true labels</p><formula xml:id="formula_9">MI(h(X), Y ). (See a counterexample in Supplementary Material B.) That is, ?h, h ? , MI(h(X), Y ) &gt; MI(h ? (X), Y ) ? MI(h(X),? ) &gt; MI(h ? (X),? ).</formula><p>Thus, we cannot use Shannon mutual information as the performance measure for classifiers. Here we find that, a generalized mutual information, Determinant based Mutual Information (DMI) <ref type="bibr" target="#b14">[16]</ref> , satisfies the above formula such that under the performance measure based on DMI, the measurement based on noisy labels is consistent with the measurement based on true labels. Definition 3.4 (Determinant based Mutual Information <ref type="bibr" target="#b14">[16]</ref> ). Given two discrete random variables W 1 , W 2 , we define the Determinant based Mutual Information between W 1 and W 2 as</p><formula xml:id="formula_10">DMI(W 1 , W 2 ) = det(Q W1,W2 )</formula><p>where Q W1,W2 is the matrix format of the joint distribution over W 1 and W 2 .</p><p>DMI is a generalized version of Shannon's mutual information: it preserves all properties of Shannon mutual information, including non-negativity, symmetry and information-monotonicity and it is additionally relatively invariant. DMI is initially proposed to address a mechanism design problem <ref type="bibr" target="#b14">[16]</ref> . Lemma 3.5 (Properties of DMI <ref type="bibr" target="#b14">[16]</ref> ). DMI is non-negative, symmetric and information-monotone. Moreover, it is relatively invariant: for all random variables W 1 ,</p><formula xml:id="formula_11">W 2 , W 3 , when W 3 is less informative for W 2 than W 1 , i.e., W 3 is independent of W 2 conditioning W 1 , DMI(W 2 , W 3 ) = DMI(W 2 , W 1 ) det(T W1?W3 ) where T W1?W3 is the matrix format of T W1?W3 (w 1 , w 3 ) = Pr[W 3 = w 3 W 1 = W 1 ].</formula><p>Proof. The non-negativity and symmetry follow directly from the definition, so we only need to prove the relatively invariance. Note that</p><formula xml:id="formula_12">Pr Q W 2 ,W 3 [W 2 = w 2 , , W 3 = w 3 ] = w1 Pr Q W 1 ,W 2 [W 2 = w 2 , W 1 = w 1 ] Pr[W 3 = w 3 W 1 = w 1 ].</formula><p>as W 3 is independent of W 2 conditioning on W 1 . Thus,</p><formula xml:id="formula_13">Q W2,W3 = Q W2,W1 T W1?W3</formula><p>where Q W2,W3 , Q W2,W1 , T W1?W3 are the matrix formats of Q W2,W3 , Q W2,W1 , T W1?W3 , respectively. We have det(Q W2,W3 ) = det(Q W2,W1 ) det(T W1?W3 ) because of the multiplication property of the determinant (i.e. det(AB) = det(A) det(B) for every two matrices A, B). Therefore,</p><formula xml:id="formula_14">DMI(W 2 , W 3 ) = DMI(W 2 , W 1 ) det(T W1?W3 ) .</formula><p>The relative invariance and the symmetry imply the information-monotonicity of DMI. When W 3 is less informative for W 2 than W 1 , i.e., W 3 is independent of W 2 conditioning on W 1 ,</p><formula xml:id="formula_15">DMI(W 3 , W 2 ) = DMI(W 2 , W 3 ) = DMI(W 2 , W 1 ) det(T W1?W3 ) ? DMI(W 2 , W 1 ) = DMI(W 1 , W 2 )</formula><p>because of the fact that for every square transition matrix T, det(T) ? 1 <ref type="bibr" target="#b32">[34]</ref> .</p><p>Based on DMI, an information-theoretic performance measure for each classifier h is naturally defined as DMI(h(X),? ). Under this performance measure, the measurement based on noisy labels DMI(h(X),? ) is consistent with the measurement based on clean labels DMI(h(X), Y ), i.e., for every two classifiers h and h ? ,</p><formula xml:id="formula_16">DMI(h(X), Y ) &gt; DMI(h ? (X), Y ) ? DMI(h(X),? ) &gt; DMI(h ? (X),? ).</formula><p>4 L DMI : An Information-theoretic Noise-robust Loss Function</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Method overview</head><p>Our loss function is defined as</p><formula xml:id="formula_17">L DMI (Q h(X),? ) ?= ? log(DMI(h(X),? )) = ? log( det(Q h(X),? ) )</formula><p>where Q h(X),? is the joint distribution over h(X),? and Q h(X),? is the C ? C matrix format of Q h(X),? . The randomness h(X) comes from both the randomness of h and the randomness of X.</p><p>The log function here resolves many scaling issues 2 . <ref type="figure" target="#fig_1">Figure 1</ref> shows the computation of L DMI . In each step of iteration, we sample a batch of datapoints and their noisy labels</p><formula xml:id="formula_18">{(x i ,? i )} N i=1 .</formula><p>We denote the outputs of the classifier by a matrix O. Each column of O is a distribution over C, representing for an output of the classifier. We denote the noisy labels by a 0-1 matrix L. Each row of L is an one-hot vector, representing for a label. i.e.  We define U ?= 1 N OL, i.e.,</p><formula xml:id="formula_19">O ci = h(x i ) c , L ic = 1[? i =c],</formula><formula xml:id="formula_20">U cc ?= 1 N N i=1 O ci L ic = 1 N N i=1 h(x i ) c 1[? i =c].</formula><p>We have EU cc = Pr[h(X) = c,? =c] = Q h(X),? (c,c) (E means expectation, see proof in Supplementary Material B). Thus, U is an empirical estimation of Q h(X),? . By abusing notation a little bit, we define</p><formula xml:id="formula_21">L DMI ({(x i ,? i )} N i=1 ; h) = ? log( det(U)</formula><p>) as the empirical loss function. Our formal training process is shown in Supplementary Material A. and in fact, training using noisy labels is the same as training using clean labels in the dataset except a constant shift,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Theoretical justification</head><formula xml:id="formula_22">L DMI (Q h(X),? ) = L DMI (Q h(X),Y ) + ?; information-monotone for every two classifiers h, h ? , if h ? (X) is less informative for Y than h(X), i.e. h ? (X) is independent of Y conditioning on h(X), then L DMI (Q h(X),? ) ? L DMI (Q h(X),Y ).</formula><p>Proof. The relatively invariance of DMI (Lemma 3.5) implies</p><formula xml:id="formula_23">DMI(h(X),? ) = DMI(h(X), Y ) det(T Y ?? ) . Therefore, L DMI (Q h * (X),? ) = L DMI (Q h * (X),Y ) + log( det(T Y ?? ) ).</formula><p>Thus, the information-monotonicity and the noise-robustness of L DMI follows and the constant ? = log( det(T Y ?? ) ) ? 0.</p><p>The legal property follows from the information-monotonicity of L DMI as h * (X) = Y is the most informative random variable for Y itself and the fact that for every square transition matrix T , det(T ) = 1 if and only if T is a permutation matrix <ref type="bibr" target="#b32">[34]</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate our method on both synthesized and real-world noisy datasets with different deep neural networks to demonstrate that our method is independent of both architecture and data domain. We call our method DMI and compare it with: CE (the cross entropy loss), FW (the forward loss <ref type="bibr" target="#b27">[29]</ref> ), GCE (the generalized cross entropy loss <ref type="bibr" target="#b46">[48]</ref> ), LCCN (the latent class-conditional noise model <ref type="bibr" target="#b42">[44]</ref> ). For the synthesized data, noises are added to the training and validation sets, and test accuracy is computed with respect to true labels. For our method, we pick the best learning rate from {1.0 ? 10 ?4 , 1.0 ? 10 ?5 , 1.0 ? 10 ?6 } and the best batch size from {128, 256} based on the minimum validation loss. For other methods, we use the best hyperparameters they provided in similar settings. The classifiers are pretrained with cross entropy loss first. All reported experiments were repeated five times. We implement all networks and training procedures in Pytorch <ref type="bibr" target="#b26">[28]</ref> and conduct all experiments on NVIDIA TITAN Xp GPUs. <ref type="bibr" target="#b1">3</ref> The explicit noise transition matrices are shown in Supplementary Material C. Due to space limit, we defer some additional experiments to Supplementary Material D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">An explanation experiment on Fashion-MNIST</head><p>To compare distance-based and information-theoretic loss functions as we mentioned in the third paragraph in introduction, we conducted experiments on Fashion-MNIST <ref type="bibr" target="#b40">[42]</ref> . It consists of 70,000 28 ? 28 grayscale fashion product image from 10 classes, which is split into a 50, 000-image training set, a 10, 000-image valiadation set and a 10, 000-image test set. For clean presentation, we only compare our information-theoretic loss function DMI with the distance-based loss function CE here and convert the labels in the dataset to two classes, bags and clothes, to synthesize a highly imbalanced dataset (10% bags, 90% clothes). We use a simple two-layer convolutional neural network as the classifier. Adam with default parameters and a learning rate of 1.0 ? 10 ?4 is used as the optimizer during training. Batch size is set to 128.</p><p>We synthesize three cases of noise patterns: (1) with probability r, a true label is substituted by a random label through uniform sampling. (2) with probability r, bags ? clothes, that is, a true label of the a priori less popular class, "bags", is flipped to the popular one, "clothes". This happens in real world when the annotators are lazy. (e.g., a careless medical image annotator may be more likely to label "benign" since most images are in the "benign" category.) (3) with probability r, clothes ? bags, that is, the a priori more popular class, "clothes", is flipped to the other one, "bags". This happens in real world when the annotators are risk-avoid and there will be smaller adverse effects if the annotators label the image to a certain class. (e.g. a risk-avoid medical image annotator may be more likely to label "malignant" since it is usually safer when the annotator is not confident, even if it is less likely a priori.) Note that the parameter 0 ? r ? 1 in the above three cases also represents the amount of noise. When r = 0, the labels are clean and when r = 1, the labels are totally uninformative. Moreover, in case <ref type="formula">(2)</ref> and <ref type="formula">(3)</ref>, as r increases, the noise pattern changes from diagonally dominant to diagonally non-dominant. As we mentioned in the introduction, distance-based loss functions will perform badly when the noise is non-diagonally dominant and the labels are biased to one class since they prefer the meaningless classifier h 0 who always outputs the class who is the majority in the labels. (?x, h 0 (x) = "clothes" and has accuracy 90% in case <ref type="bibr" target="#b0">(2)</ref> and ?x, h 0 (x) = "bags" and has accuracy 10% in case <ref type="formula">(3)</ref>). The experiment results match our expectation. CE performs similarly with our DMI for diagonally dominant noises. For non-diagonally dominant noises, however, CE only obtains the meaningless classifier h 0 while DMI still performs pretty well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiments on CIFAR-10, Dogs vs. Cats and MR</head><p>CIFAR-10 [1] consists of 60,000 32 ? 32 color images from 10 classes, which is split into a 40, 000image training set, a 10, 000-image validation set and a 10, 000-image test set. Dogs vs. Cats <ref type="bibr" target="#b0">[2]</ref> consists of 25, 000 images from 2 classes, dogs and cats, which is split into a 12, 500-image training set, a 6, 250-image validation set and a 6, 250-image test set. MR <ref type="bibr" target="#b25">[27]</ref> consist of 10, 662 one-sentence movie reviews from 2 classes, positive and negative, which is split into a 7, 676-sentence training set, a 1, 919-sentence validation set and a 1, 067-sentence test set. We use ResNet-34 <ref type="bibr" target="#b9">[11]</ref> , VGG-16 <ref type="bibr" target="#b34">[36]</ref> , WordCNN <ref type="bibr" target="#b13">[15]</ref> as the classifier for CIFAR-10, Dogs vs. Cats, MR, respectively. SGD with a momentum of 0.9, a weight decay of 1.0 ? 10 ?4 and a learning rate of 1.0 ? 10 ?5 is used as the optimizer during training for CIFAR-10 and Dogs vs. Cats. Adam with default parameters and a learning rate of 1.0 ? 10 ?4 is used as the optimizer during training for MR. Batch size is set to 128.</p><p>We use per-pixel normalization, horizontal random flip and 32 ? 32 random crops after padding with 4 pixels on each side as data augmentation for images in CIFAR-10 and Dogs vs Cats. We use the same pre-processing pipeline in <ref type="bibr" target="#b13">[15]</ref> for sentences in MR. Following <ref type="bibr" target="#b42">[44]</ref> , the noise for CIFAR-10 is added between the similar classes, i.e. truck ? automobile, bird ? airplane, deer ? horse, cat ? dog, with probability r. The noise for Dogs vs. Cats is added as cat ? dog with probability r. The noise for MR is added as positive ? negative with probability r.</p><p>As shown in <ref type="figure" target="#fig_4">Figure 3</ref>, our method DMI almost outperforms all other methods in every experiment and its accuracy drops slowly as the noise amount increases. GCE has great performance in diagonally dominant noises but it fails in diagonally non-dominant noises. This phenomenon matches its theory: it assumes that the label noise is diagonally dominant. FW needs to pre-estimate a noise transition matrix before training and LCCN uses the output of the model to estimate the true labels. These tasks become harder as the noise amount grows larger, so their performance also drop quickly as the noise amount increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiments on Clothing1M</head><p>Clothing1M <ref type="bibr" target="#b41">[43]</ref> is a large-scale real world dataset, which consists of 1 million images of clothes collected from shopping websites with noisy labels from 14 classes assigned by the surrounding text provided by the sellers. It has additional 14k and 10k clean data respectively for validation and test. We use ResNet-50 <ref type="bibr" target="#b9">[11]</ref> as the classifier and apply random crop of 224 ? 224, random flip, brightness and saturation as data augmentation. SGD with a momentum of 0.9, a weight decay of 1.0 ? 10 ?3 is used as the optimizer during training. We train the classifier with learning rates of 1.0 ? 10 ?6 in the first 5 epochs and 0.5 ? 10 ?6 in the second 5 epochs. Batch size is set to 256. As shown in <ref type="table">Table 5</ref>, DMI also outperforms other methods in the real-world setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Discussion</head><p>We propose a simple yet powerful loss function, L DMI , for training deep neural networks robust to label noise. It is based on a generalized version of mutual information, DMI. We provide theoretical validation to our approach and compare our approach experimentally with previous methods on both synthesized and real-world datasets. To the best of our knowledge, L DMI is the first loss function that is provably robust to instance-independent label noise, regardless of noise pattern and noise amount, and it can be applied to any existing classification neural networks straightforwardly without any auxiliary information.</p><p>In the experiment, sometimes DMI does not have advantage when the data is clean and is outperformed by GCE. GCE does a training optimization on MAE with some hyperparameters while sacrifices the robustness a little bit theoretically. A possible future direction is to employ some training optimizations in our method to improve the performance.</p><p>The current paper focuses on the instance-independent noise setting. That is, we assume conditioning on the latent ground truth label Y ,? and X are independent. There may exist Y ? ? Y such that Y and X are independent conditioning on Y ? . Based on our theorem, training using? is also the same as training using Y ? . However, without any additional assumption, when we only has the conditional independent assumption, no algorithm can distinguish Y ? and Y . Moreover, the information-monotonicity of our loss function guarantees that if Y is more informative than Y ? with X, the best hypothesis learned in our algorithm will be more similar with Y than Y ? . Thus, if we assume that the actual ground truth label Y is the most informative one, then our algorithm can learn to predict Y rather than other Y ? s. An interesting future direction is to combine our method with additional assumptions to give a better prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Training Process</head><p>Algorithm 1 The training process with L DMI Randomly sample a batch of samples</p><formula xml:id="formula_24">Require: A training dataset D = {(x i ,? i )} D i=1 , a validation dataset V = {(x i ,? i )} V i=1 ,</formula><formula xml:id="formula_25">B v = {(x i ,? i )} N i=1</formula><p>from the validation dataset <ref type="bibr">12:</ref> Compute the validation loss:</p><formula xml:id="formula_26">L ? L DMI (B v ; h ? ) 13:</formula><p>if L &lt; L * then 14:</p><p>Update the minimum validation loss: L * ? L </p><formula xml:id="formula_27">U cc ?= 1 N N i=1 O ci L ic = 1 N N i=1 h(x i ) c 1[? i =c].</formula><p>Proof. Recall that the randomness of h(X) comes from both h and X and the randomness of h is independent of everything else. Claim B.2. Under the the performance measure based on Shannon mutual information, the measurement based on noisy labels MI(h(X),? ) is not consistent with the measurement based on true labels MI(h(X), Y ). i.e., for every two classifiers h and h ? ,</p><formula xml:id="formula_28">EU cc = E 1 N N i=1 h(x i ) c 1[? i =c] = E X,? h(X) c 1[? =c] (i.i.d. samples) = x,? Pr[X = x,? =?]h(x) c 1[? =c]</formula><formula xml:id="formula_29">I(h(X), Y ) &gt; I(h ? (X), Y ) ? I(h(X),? ) &gt; I(h ? (X),? ).</formula><p>Proof. See a counterexample:</p><p>The matrix format of the joint distribution If we use Shannon mutual information as the performance measure,</p><formula xml:id="formula_30">Q h(X),Y is Q h(X),Y = 0.1 0.4 0.2 0.3 , the matrix format of the joint distribution Q h ? (X),Y is Q h ? (X),</formula><formula xml:id="formula_31">MI(h(X), Y ) = 2.4157 ? 10 ?2 , MI(h ? (X), Y ) = 2.2367 ? 10 ?2 , MI(h(X),? ) = 3.2085 ? 10 ?3 , MI(h ? (X),? ) = 3.2268 ? 10 ?3 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Thus we have MI</head><formula xml:id="formula_32">(h(X), Y ) &gt; MI(h ? (X), Y ) but MI(h(X),? ) &lt; MI(h ? (X),? ). Therefore, MI(h(X), Y ) &gt; MI(h ? (X), Y ) ? MI(h(X),? ) &gt; MI(h ? (X),? ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Noise Transition Matrices</head><p>Here we list the explicit noise transition matrices. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Experiments</head><p>For clean presentation, we only include the comparison between CE and DMI in section 5.1 and attach comparisons with other methods here. In the experiments in section 5.2, noise patterns are divided into two main cases, diagonally dominant and diagonally non-dominant and uniform noise is a special case of diagonally dominant noise. Thus, we did not emphasize the uniform noise results in section 5.2 and attach them here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4: Additional experiments</head><p>We also compared our method to MentorNet (the sample reweighting loss <ref type="bibr" target="#b12">[14]</ref> ) and VAT (the regularization loss <ref type="bibr" target="#b23">[25]</ref> ). For clean presentation, we only attach them here. Our method still outperforms these two additional baselines in most of the cases. 4  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2</head><label></label><figDesc>?(c det(A) ) ?A = c det(A) (A ?1 ) T while ? log(c det(A) ) ?A = (A ?1 ) T , ? matrix A and ? constant c.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The computation of L DMI in each step of iteration</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Theorem 4 .</head><label>4</label><figDesc>1 (Main Theorem). With Assumption 3.1 and Assumption 3.2, L DMI is legal if there exists a ground truth classifier h * such that h * (X) = Y , then it must have the lowest loss, i.e., for all classifier h, L DMI (Q h * (X),? ) ? L DMI (Q h(X),? ) and the inequality is strict when h(X) is not a permutation of h * (X), i.e., there does not exist a permutation ? ? C ? C s.t. h(x) = ?(h * (x)), ?x ? X ; noise-robust for the set of all possible classifiers H, arg min h?H L DMI (Q h(X),? ) = arg min h?H L DMI (Q h(X),Y )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Test accuracy (mean and std. dev.) on Fashion-MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Test accuracy (mean) on CIFAR-10, Dogs vs. Cats and MR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>7 : 8 :</head><label>78</label><figDesc>a classifier modeled by deep neural network h ? , the running epoch number T , the learning rate ? and the batch size N . 1: Pretrain the classifier h ? on the dataset D with cross entropy loss 2: Initialize the best classifier: h ? * ? h ? 3: Randomly sample a batch of samples B v = {(x i ,? i )} N i=1 from the validation dataset 4: Initialize the minimum validation loss: L * ? L DMI (B v ; h ? ) 5: for epoch t = 1 ? T do 6: for batch b = 1 ? ?D B? do Randomly sample a batch of samples B t = {(x i ,? i )} N i=1 from the training dataset Compute the training loss: L ? L DMI (B t ; h ? )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>=x</head><label></label><figDesc>Pr[X = x,? =c]h(x) c = x Pr[X = x,? =c] Pr[h(X) = c X = x](definition of randomized classifier)= x Pr[X = x,? =c] Pr[h(X) = c X = x,? =c] (fixing x, the randomness of h is independent of everything else) = Pr[h(X) = c,? =c].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>6 .</head><label>6</label><figDesc>Given these conditions, Q h(X),? = 0.24 0.26 0.28 0.22 and Q h ? (X),? = 0.40 0.40 0.12 0.08 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>On? r 2 r 2 r 2 1 ? r 2 ;</head><label>22</label><figDesc>Fashion-MNIST, case (1): T Y ?? = 1 On Fashion-MNIST, case (2): T Y ?? = 1 ? r r 0 1 ;On Fashion-MNIST, case (3):T Y ?? = 1 0 r 1 ? r ; On CIFAR-10, T Y ?? = vs. Cats, T Y ?? = 1 0 r 1 ? r . On MR, T Y ?? = 1 0 r 1 ? r .For Fashion-MNIST case (1), r = 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0, 9 are diagonally dominant noises. For other cases, r = 0.0, 0.1, 0.2, 0.3, 0.4 are diagonally dominant noises and r = 0.5, 0.6, 0.7, 0.8, 0, 9 are diagonally non-dominant noises.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">: Test accuracy (mean) on Clothing1M</cell></row><row><cell>Method</cell><cell>CE</cell><cell>FW</cell><cell>GCE LCCN DMI</cell></row><row><cell cols="4">Accuracy 68.94 70.83 69.09 71.63 72.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Test accuracy on CIFAR-10 (mean ? std. dev.)</figDesc><table><row><cell>r</cell><cell>CE</cell><cell>MentorNet</cell><cell>VAT</cell><cell>FW</cell><cell>GCE</cell><cell>LCCN</cell><cell>DMI</cell></row><row><cell>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9</cell><cell>93.29 ? 0.18 91.63 ? 0.32 90.36 ? 0.24 88.79 ? 0.40 84.76 ? 0.98 74.81 ? 3.37 64.61 ? 0.72 59.15 ? 0.64 57.65 ? 0.28 57.46 ? 0.08</cell><cell>92.13 ? 1.22 91.35 ? 0.83 90.06 ? 0.52 88.47 ? 0.61 84.12 ? 1.29 78.43 ? 0.39 71.33 ? 0.13 66.28 ? 0.76 65.67 ? 0.57 59.49 ? 0.40</cell><cell>92.25 ? 0.1 91.4 ? 0.68 91.19 ? 0.31 88.97 ? 0.41 84.09 ? 0.46 75.07 ? 0.66 65.02 ? 0.63 58.92 ? 1.49 57.78 ? 0.32 57.19 ? 1.25</cell><cell>93.12 ? 0.16 91.54 ? 0.15 90.10 ? 0.22 88.77 ? 0.36 84.78 ? 1.53 77.2 ? 4.19 71.98 ? 1.83 67.59 ? 1.64 62.22 ? 1.80 58.23 ? 0.25</cell><cell>93.43 ? 0.24 91.96 ? 0.09 90.87 ? 0.16 89.67 ? 0.21 86.6 ? 0.47 78.53 ? 1.93 71.14 ? 0.78 67.10 ? 0.82 62.56 ? 0.72 58.91 ? 0.46</cell><cell>92.47 ? 0.36 91.88 ? 0.23 91.05 ? 0.43 89.88 ? 0.40 89.33 ? 0.58 88.30 ? 0.38 86.89 ? 0.51 77.50 ? 0.60 74.62 ? 1.16 61.32 ? 1.87</cell><cell>93.37 ? 0.20 92.08 ? 0.08 91.51 ? 0.17 91.12 ? 0.30 90.41 ? 0.32 89.45 ? 0.99 89.03 ? 0.69 88.82 ? 0.89 87.46 ? 0.79 85.94 ? 0.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Test accuracy on Dogs vs. Cats (mean ? std. dev.)</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Source codes are available at https://github.com/Newbeeer/L_DMI.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">VAT can not be applied to MR dataset.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to express our thanks for support from the following research grants: 2018AAA0102004, NSFC-61625201, NSFC-61527804.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Cats competition</title>
		<ptr target="https://www.kaggle.com/c/dogs-vs-cats.2013" />
		<imprint/>
	</monogr>
	<note>Dogs vs</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Support vector machines with the ramp loss and the hard margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Paul</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations research</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Max-mig: an information theoretic approach for joint learning from crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning with bounded instance-and label-dependent label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kotagiri</forename><surname>Ramamohanarao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03768</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Information theory and statistics: A tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imre</forename><surname>Csisz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shields</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends? in Communications and Information Theory</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="417" to="528" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Ben-Reuven</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Masking: A new perspective of noisy supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5836" to="5846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8527" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using trusted data to train deep networks on labels corrupted by severe noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10456" to="10465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05055</idno>
		<title level="m">Mentornet: Regularizing very deep neural networks on corrupted labels</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dominantly truthful multi-task peer prediction, with constant number of tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM-SIAM Symposium on Discrete Algorithms (SODA20)</title>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Water from two rocks: Maximizing the mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Schoenebeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM Conference on Economics and Computation</title>
		<meeting>the 2018 ACM Conference on Economics and Computation</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="177" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cleannet: Transfer learning for scalable image classifier training with label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5447" to="5456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Classification with noisy labels by importance reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="447" to="461" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudanthi</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bailey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02612</idno>
		<title level="m">Dimensionality-driven learning with noisy labels</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Noise tolerance under risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naresh</forename><surname>Manwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1146" to="1151" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the design of loss functions for classification: theory, robustness to outliers, and savageboost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Masnadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Shirazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1049" to="1056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning from binary labels with instance-dependent corruption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagarajan</forename><surname>Natarajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.00751</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagarajan</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambuj</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1196" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting on association for computational linguistics</title>
		<meeting>the 43rd annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Tensors and dynamic neural networks in python with strong gpu acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mixture proportion estimation via kernel embeddings of distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harish</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambuj</forename><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2052" to="2060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09050</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A rate of convergence for mixture proportion estimation, with application to learning from noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="838" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Non-negative matrices and Markov chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Seneta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2080</idno>
		<title level="m">Training convolutional networks with noisy labels</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5552" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Toward robustness against label noise in training deep discriminative neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5596" to="5605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning with symmetric label noise: The importance of being unhinged</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning from noisy large-scale datasets with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="839" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Safeguarded dynamic label regression for noisy supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Probabilistic end-to-end noise correction for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.07788</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8778" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
