<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pre-train and Learn: Preserving Global Information for Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-11">Nov. 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
						</author>
						<title level="a" type="main">Pre-train and Learn: Preserving Global Information for Graph Neural Networks</title>
					</analytic>
					<monogr>
						<title level="j" type="main">JOURNAL OF COMPUTER SCIENCE AND TECHNOLOGY</title>
						<imprint>
							<biblScope unit="volume">36</biblScope>
							<biblScope unit="issue">6</biblScope>
							<biblScope unit="page" from="1420" to="1430"/>
							<date type="published" when="2021-11">Nov. 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1007/s11390-020-0142-x</idno>
					<note type="submission">Received October 10, 2019 ; revised May 12, 2020.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph neural network</term>
					<term>Network embedding</term>
					<term>Representation learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs) have shown great power in learning on graphs. However, it is still a challenge for GNNs to model information faraway from the source node. The ability to preserve global information can enhance graph representation and hence improve classification precision. In the paper, we propose a new learning framework named G-GNN (Global information for GNN) to address the challenge. First, the global structure and global attribute features of each node are obtained via unsupervised pre-training, which preserve the global information associated to the node. Then, using the pre-trained global features and the raw attributes of the graph, a set of parallel kernel GNNs is used to learn different aspects from these heterogeneous features. Any general GNN can be used as a kernal and easily obtain the ability of preserving global information, without having to alter their own algorithms. Extensive experiments have shown that state-of-the-art models, e.g. GCN, GAT, Graphsage and APPNP, can achieve improvement with G-GNN on three standard evaluation datasets. Specially, we establish new benchmark precision records on Cora (84.31%) and Pubmed (80.95%) when learning on attributed graphs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semi-supervised learning on graphs is popular in various real world applications, since labels are often expensive and difficult to collect. In the recent years, Graph neural networks (GNNs) have shown great power in the semi-supervised learning on attributed graphs. GNN often contains multiple layers, and the nodes collect information from the neighborhood iteratively, layer by layer. The representative methods include graph convolutional network (GCN) <ref type="bibr" target="#b0">[1]</ref>, Graphsage <ref type="bibr" target="#b1">[2]</ref>, graph attention networks <ref type="bibr" target="#b2">[3]</ref> and so on.</p><p>Due to the multi-layer message aggregation scheme, GNN is easy to be over-smoothing after a few propagation steps <ref type="bibr" target="#b3">[4]</ref>. That is to say, the node representations tend to be identical and lack of distinction. In general, GNNs can afford only 2 layers to collect information within 2 hops of neighborhood, otherwise the oversmoothing problem will deteriorate the performance.</p><p>Several previous works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">6]</ref> aimed to address the problem by expanding the size of utilized neighborhood. For example, N-GCN <ref type="bibr" target="#b4">[5]</ref> trains multiple instances of GCNs over node pairs discovered at different distances in random walks. PPNP/APPNP <ref type="bibr" target="#b6">[6]</ref> introduces the teleport probability of personalized PageRank. These solutions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">6]</ref> are purely based on semisupervised learning, which have their natural bottleneck in modeling global information. As the reception field is getting larger and more nodes are involved, more powerful models are required to explore the complex relationships among nodes. However, the labels for training are quite sparse in the semi-supervised learning, and hence cannot afford to train models with high complexity well. Hence, the existing works often have to restrain the complexity of models, which limits their ability in learning global information.</p><p>The unsupervised learning methods based on random walk, e.g. Deepwalk <ref type="bibr" target="#b7">[7]</ref> and Node2vec <ref type="bibr" target="#b8">[8]</ref>, can be used to obtain the global structure information of each node. These methods first sample node sequences that contain the structure regularity of the network, then try to maximize the likelihood of neighborhood node pairs within certain distance, e.g. 10 hops. Hence, they can capture global structure features without label information.</p><p>In this paper, we propose a new learning schema of pre-training and learning to address the global information preserving problem in semi-supervised learning. First, instead of improving the aggregation function via semi-supervision, we obtain the global structure and global attribute features by pre-training the graph with random walk strategy in the unsupervised learning. Second, we design a GNN based framework to conduct semi-supervised classification by learning from the pre-trained features and the original graph.</p><p>The two-stage schema of pretraining-and-learning has several advantages. First, the global information modeling procedure is decoupled with the subsequent semi-supervised learning method. Therefore, the modeling of the global information no longer suffers from the sparse supervision or over-smoothing problem. Moreover, a general GNN can enhance its global information preserving ability by applying our learning framework, without having to altering its algorithm. Second, the proposed framework takes the advantage of both random walk and GNN, which can not only utilize global information but also aggregate local information well.</p><p>Moreover, the framework allows GNN to be applied to plain graphs without attributes, since the unsupervised structure features can be used as graph attributes directly.</p><p>In all, our contributions are as follows.</p><p>? We propose a learning framework named Global information for Graph Neural Network (G-GNN) for semi-supervised classification on graphs. The proposed pretraining-and-learning schema allows GNN models to use global information for learning, without altering their algorithms. Moreover, the schema enables GNN to be applied to plain graphs.</p><p>? We design the global information as the global structure and global attribute features to each node, and propose a set of parallel GNNs to learn different aspects from the pretrained global features and the original graph.</p><p>? Our method achieves state-of-the-art results in semi-supervised learning on both plain and attributed graphs. Specially, the precisions of attributed graph learning on Cora (84.31%) and</p><p>Pubmed (80.95%) are the new benchmark results.</p><p>The rest of the paper is organized as follows. In section 2, the preliminaries are given. We introduce our method in section 3. Section 4 presents the experiments. Section 5 briefly summarizes related work.</p><p>Finally, we conclude our work in section 6. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Definition</head><p>First, we will give the formal definition of attributed/plain graph, and the problem we are going to solve.</p><formula xml:id="formula_0">Let G = (V, E) be a graph, where V = {v 1 , v 2 ...v n }</formula><p>denotes the node set, E denotes the edges with the ad-</p><formula xml:id="formula_1">jacency matrix A ? R n?n . If A ij is not equal to 0,</formula><p>there is a link from v i to v j with weight A ij . If G is an attributed graph, there is a corresponding attribute matrix X ? R n?f , where the ith row denotes v i 's attributes and f denotes the total amount of attributes.</p><p>If G is a plain network, no X is provided. The graph contains label information Y ? R n?c , where the ith row denotes v i 's one hot label vector. The amount of labels is c.</p><p>During the training stage, the provided data is the entire adjacency matrix A and the node attributes X.</p><p>Only labels of the training nodes V train ? V are given.</p><p>The task of the semi-supervised learning in attributed graph is to predict the rest of the node labels Y Vtrain .</p><p>X is not provided for plain graph learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Neural Networks</head><p>We will introduce how general GNNs solve the semisupervised learning problem. Note that current GNNs can only be applied to attributed graphs. Therefore, we assume X is given here.</p><p>Among the huge family of GNNs, Graph convolutional network (GCN) <ref type="bibr" target="#b0">[1]</ref> is a simple and pioneering method. Let? = A + I n be the adjacency matrix with self-connections, where I n is an identity matrix.</p><p>The self-loops will allow GCN to consider attributes of the represented nodes when aggregating the neigbhood's attributes. Let? =D ?1/2?D?1/2 be the nomalized adjacency matrix, whereD denotes the diago-nal degree matrix whereD ii = j? ij . The two-layer GCN produces hidden states by aggregating neighborhood attributes iteratively, as in <ref type="bibr" target="#b0">(1)</ref>.</p><formula xml:id="formula_2">H GCN =?Relu(?XW 0 )W 1 .<label>(1)</label></formula><p>where H GCN ? R n?c and Relu(.) is an activation function commonly used in neural networks. Each row in H GCN denotes the final hidden states of a node, and each row corresponds to a prediction catagory. W 0 and W 1 are the trainable weight matrices. After that, the classification probability on each class Z GCN is obtained via sof tmax(.), a normalization function commonly used in machine learning, as in <ref type="bibr" target="#b1">(2)</ref>.</p><formula xml:id="formula_3">Z GCN = sof tmax(H GCN ).<label>(2)</label></formula><p>Finally, a loss function is applied to measure the difference between the predict probability and the ground truth labels.</p><p>Many of the following studies aim to improve the aggregation function, such as assigning alternative weights to the neighborhood nodes <ref type="bibr" target="#b2">[3]</ref>, adding skip connections <ref type="bibr" target="#b1">[2]</ref>, introducing teleport probability <ref type="bibr" target="#b6">[6]</ref> and so on. These methods can be viewed as a transform from the original X and A to the final hidden states H, as in <ref type="bibr" target="#b2">(3)</ref>.</p><formula xml:id="formula_4">GN N (X, A) : X, A ? H.<label>(3)</label></formula><p>From (1), we can see only 2-hops of local information can be used. The over-smoothing problem prevents from adding more layers, so the global information is difficult to be integrated in. The input attributes X are necessary in the general learning framework of GNN.</p><p>Hence, GNN cannot be applied to plain graphs directly.</p><p>In the next section, we will show how to solve these problems with the proposed pretraining-and-learning schema.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Framework of G-GNN</head><p>In the section, we first give an overview of G-GNN within the context of attributed graph. Second, the method to obtain the global features is introduced.</p><p>Third, a parallel GNN based method is proposed to learn from all these features. Finally, we show how to extend G-GNN to plain network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>The overview of the G-GNN is shown in <ref type="figure">Fig. 1</ref>.</p><p>First, the global structure feature matrix X (s) and at- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unsupervised Learning of Global Features</head><p>Herein, we propose to learn the unsupervised features of graphs based on random walk strategy. Each node can utilize information within k steps of random walk, where k is often set to 10. Small world phenomenon suggests that the average distance between nodes will grow logarithmically with the size of the graph nodes <ref type="bibr" target="#b9">[9]</ref>, and the undirected average distance of a very large Web graph is only 6.83 steps <ref type="bibr" target="#b10">[10]</ref>. Hence, 10 steps of random walk can already capture the global information of the graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Global Structure Features</head><p>Similar to Deepwalk <ref type="bibr" target="#b7">[7]</ref>, the structure features are learned by first sampling the context-source node pairs, and then maximizing their co-occurrence probability.</p><p>Note that graph attributes X are not used here. We apply random walks to the graph G to obtain short truncated node sequences. These sequences contain the structure regularity of the original graph. In the node sequences, the neighborhood nodes within certain distance to the source node v are considered as v's context nodes, which are denoted as N (v) ? V .</p><p>To maximize the likelihood of the observed sourcecontext pairs, we try to minimize the following objective function:</p><formula xml:id="formula_5">v?V u?N (v) e X (s) v ?X (s) u k?V e X (s) v ?X (s) k .</formula><p>X (s) ? R n?ds denotes the global structure feature matrix and d s denotes the dimension of the feature vectors.</p><formula xml:id="formula_6">X (s) v , X (s) u and X (s) k</formula><p>denote the global structure feature vectors of v, u and k respectively. The calculation of the denominator is computational expensive since it is required to traverse the entire node set. We approximate it using negative sampling <ref type="bibr" target="#b11">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Global Attribute Features</head><p>The global attribute features are obtained by maximizing the likelihood of the context attributes. The underlying idea is that if the context attributes can be recovered from the source node, the relationship has already been preserved by the learning model.</p><p>For each sampled context node u ? N (v), some attributes of u are sampled as the context attributes of v.</p><p>In this paper, we sample one attribute for one context node. Let CA(v) be the sampled context attributes of v, and T be the set of all attributes and |T | is the total number of attributes. We try to minimize the following objective.</p><p>v?V t?CA(v)</p><formula xml:id="formula_7">e X (a) v ?St k?T e X (a) v ?S k .</formula><p>where X (a) ? R n?da denotes the global attribute feature matrix, S ? R da?|T | denotes the parameters to predict the attributes and d a denotes the dimension of Zhu et al. <ref type="bibr" target="#b13">[12]</ref> proposed an unsupervised graph learning method that utilizes the context attributes.</p><p>The node representations are learned by jointly optimizing two objective functions that preserve the neighborhood nodes and attributes. The mainly difference of our work is that we learn two feature vectors for each node separately, which provide richer information for the following learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Parallel Graph Neural Networks</head><p>As is shown in <ref type="figure">Fig. 1</ref>, we propose a parallel model with kernels of GNN to learn from these input matrices of X (s) , X (a) and X. The learning is semi-supervised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Learning from the Heterogeneous Features</head><p>The motivation of applying multiple parallel GNN kernels to these feature matrices is as follows. First, the features are quite heterogeneous, especially when some of them are learned via pre-training. The parallel kernels can learn different aspects from these features respectively. Second, the three feature matrices are highly correlated. For example, X (a) is obtained partly based on X. X (s) and X (a) are sampled based on the identical random walk method. It is difficult to learn the complex relationships among them. The parallel setting allows to learn from these features separately, which will make the optimization easier. Indeed, the parallel schema is successful in some previous papers, such as multi-head attention <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">13]</ref> and N-GCN <ref type="bibr" target="#b4">[5]</ref>.</p><p>First, because the amplitude at each dimension of the pre-trained H (s) and H (a) often varies a lot, it is better to make a normalization. For each row h in H <ref type="bibr">(s)</ref> or H (a) , we make the following transformation, where mean denotes the average function and std denotes the standard derivation function.</p><formula xml:id="formula_8">h = h ? mean(h) std(h) .</formula><p>Then, several kernels of GNN are proposed to learn from the three feature matrices, as in (4)- <ref type="bibr" target="#b6">(6)</ref>. GN N ( * ) is the learning kernel of the G-GNN framework. Common GNN-based models that fulfill <ref type="formula" target="#formula_4">(3)</ref> can be used as a kernel, such as GCN <ref type="bibr" target="#b0">[1]</ref>, Graphsage <ref type="bibr" target="#b1">[2]</ref>, APPNP <ref type="bibr" target="#b6">[6]</ref> and so on. Hence, G-GNN can easily benefit from the strong learning capacities of these kernels.</p><formula xml:id="formula_9">H (s) = GN N (X (s) , A).<label>(4)</label></formula><formula xml:id="formula_10">H (a) = GN N (X (a) , A).<label>(5)</label></formula><formula xml:id="formula_11">H (o) = GN N (X, A).<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Combining the Hidden States</head><p>A simple way to obtain the final hidden state matrix is to linearly combine the three obtained hidden state matrices, where ? and ? are coefficients between 0 and 1.</p><formula xml:id="formula_12">H = ?H (s) + ?H (a) + H (o)<label>(7)</label></formula><p>Then a softmax function is applied to H, as in <ref type="formula" target="#formula_3">(2)</ref>, to get the prediction probability matrix Z, where Z ij denotes the probability that node v i 's label is the class j.</p><p>The coefficients of ? and ? are used to turn down the effect of the pre-trained features, which are essential for optimization. <ref type="bibr" target="#b16">[14]</ref> suggests the pre-training is useful for initializing the network in a region of the parameter space where optimization is easier. In training, easy samples often contribute more to the loss and dominate the gradient updating <ref type="bibr" target="#b17">[15]</ref>. Similarly, we find the easytrained components of GN N (X (s) ) and GN N (X (a) ) also dominate the learning procedure. If no weight strategy is used, GN N (X) merely contributes to the results and hence the performance is far from promising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Training</head><p>We minimize the cross-entropy loss function between Z and the ground-truth labels Y to train the model <ref type="bibr" target="#b4">[5]</ref>.</p><formula xml:id="formula_13">min diag(V train )[Y ? log Z]</formula><p>where ? denotes Hadamard product, and diag(V train ) denotes a diagonal matrix, with entry at (i, i) set to 1</p><p>if v i ? V train and 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Learning on Plain Graphs</head><p>Plain graphs contain no attributes X. It does not affect the obtaining of the global structure feature matrix X (s) . Then the final hidden states is:</p><formula xml:id="formula_14">H = H (s) = GN N (X (s) , A).</formula><p>Hence, learning on plain graphs also follows the pretraining-and-learning schema, where some components that depend on the graph attributes are removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Discussion</head><p>In the paper, we choose a parallel framework to learn from these heterogeneous attributes. But are there any alternative technique choices?</p><p>The pretrained vectors can be integrated with the classification model in different stages, e.g. early/middle/late fusion.  Therefore, we choose late fusion as our learning framework, which not only gives the best performance, but also can improve the global information utilization ability of general GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we conduct experiments to answer the following research questions * :</p><p>? Q1: How does G-GNN perform in comparison with state-of-the-art GNN kernels on attributed graphs?</p><p>? Q2: Are all the designed components in G-GNN helpful for achieving stronger learning ability?</p><p>? Q3: How does G-GNN perform in comparison with state-of-the-art learning methods on plain graphs?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiments on Attributed Graphs (Q1)</head><p>Herein, we will answer Q1 by comparing G-GNN with different GNN kernels. Note that the codes of all the GNN methods are based on the implementation released by DGL ? . All results are the average values of 10 experiments with different random initialization seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets and Baselines</head><p>The statistics of the datasets used in this study are shown in <ref type="table" target="#tab_0">Table 1</ref>. The three standard attributed graph benchmark datasets of Cora, Citeseer and Pubmed <ref type="bibr" target="#b18">[16]</ref> are widely used in various GNN studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">17]</ref>. In the citation graphs, nodes denote papers and links denote undirected citations. Node attributes are the extracted elements of bag-of-words representation of the documents. The class label is the research area of each paper. CoraFull is an extended version of Cora <ref type="bibr" target="#b20">[18]</ref>.</p><p>Following <ref type="bibr" target="#b20">[18]</ref>, we randomly split the train/valid/test dataset with 1:1:8.</p><p>The following baselines are compared in this paper.</p><p>? Graph convolutional network (GCN) <ref type="bibr" target="#b0">[1]</ref>: It is a simple type of GNN introduced in details in section 2. We use dropout technique to avoid overfitting <ref type="bibr" target="#b21">[19]</ref>, where the probability is 0.5. We set the number of training epoches to 300, the number of layers to 2, and the dimension of hidden states to <ref type="bibr" target="#b18">16</ref>. The self-loops are used.</p><p>? Graphsage <ref type="bibr" target="#b1">[2]</ref>: It is a general framework by sampling and aggregating features from a node's local neighborhood. We use the mean aggregate. We set the dropout rate to 0.5, the number of training epoches to 200, the number of layers to 2, and the dimension of hidden states to 16.</p><p>? APPNP <ref type="bibr" target="#b6">[6]</ref>: APPNP is designed with a new propagation procedure based on personalized PageRank, and hence can also model the long-distance information to a source node. We set the dropout rate to 0.5, the number of training epoches to 300, the number of propagation steps to 10, the teleport probability to 0.1 and the dimension of hidden states to 64.</p><p>? Graph attention network (GAT) <ref type="bibr">[</ref> All models are optimized with Adam <ref type="bibr" target="#b22">[20]</ref> where the initial learning rate is 0.01 and the weight decay is 0.0005 per epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Training Details</head><p>In the unsupervised learning of global features, we </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Results</head><p>The results are shown in <ref type="table">Table 2</ref>. From the table, it is found that all baseline kernels with global information achieve substantial gains on the classifi- In all, the results validate the effectiveness of the pretraining-and-learning schema, which can significantly improve the global information preserving ability of GNN based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Properties Analysis (Q2)</head><p>The parameter sensitivity and ablation analysis are given. Herein, we mainly use GCN as the kernel and the setting is attributed graph learning.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Parameter Sensitivity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Ablation Analysis</head><p>First, the effectiveness of parallel learning method is investigated. Note that the simplest way to utilize all three feature matrices X (s) , X (a) and X is to concatenate them first, and then feed the concatenated feature matrix to a single GNN kernel, so called early fusion in subsection 3.5. <ref type="table" target="#tab_1">Table 3</ref> compares the results of GCN, early fusion and G-GCN. We can find that early fusion has already outperforms GCN, which demonstrates that the pretraining-and-learning schema of G-GCN can well utilize the global information. G-GCN makes further improvement than the method of simple concatenation, which validates the effectiveness of parallel learning method. In all, both pre-trained features can contribute to improve the results of our proposed model. However, the amount of improvement depends on specific datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments on Plain Graphs (Q3)</head><p>We will answer Q3 by comparing G-GNN with other plain graph learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Experiment Setup</head><p>We conduct the task of semi-supervised classification on the two datasets of Cora and Citeseer. The Four semi-supervised learning methods on plain graphs are used as baselines.</p><p>? Iterative Classification Algorithm (ICA) <ref type="bibr" target="#b18">[16]</ref>:</p><p>ICA iteratively propagates observed labels to nearby unlabeled nodes, until the assignments to the labels stabilize. Here we use k-nearest neighbor as the classification algorithm.</p><p>? Planetoid-G <ref type="bibr" target="#b19">[17]</ref>: The method trains an embedding for each instance to jointly predict the class label and the neighborhood context in the graph.</p><p>? MMDW <ref type="bibr" target="#b25">[23]</ref>: The method jointly optimizes the max-margin classifier and the aimed social representation learning model.</p><p>? PNE <ref type="bibr" target="#b26">[24]</ref>: The method embeds nodes and labels into the same latent space.</p><p>Some other unsupervised or semi-supervised methods, such as LP <ref type="bibr" target="#b27">[25]</ref>, Deepwalk <ref type="bibr" target="#b7">[7]</ref>, Line <ref type="bibr" target="#b28">[26]</ref> and LSHM <ref type="bibr" target="#b29">[27]</ref> are excluded from comparison since the baseline methods have demonstrated that they are outperformed by the baseline methods <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b25">23,</ref><ref type="bibr" target="#b26">24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Results</head><p>The results are shown in <ref type="table" target="#tab_3">Table 5</ref>. G-GCN (plain) achieves the highest precision on more than half of the total data points (10 out of 18). Specially, the advantage is more obvious when the training ratio is arising.</p><p>When the training ratio is small, e.g. less than 30%,  Note: X (s) , X (a) , X are the global structure, the global attribute, and the raw graph features respectively. The model is G-GCN.</p><p>Each line gives the results of G-GCN with the corresponding feature matrices. Hence, the model in the first line is equivalent to GCN, and the one of the last line is corresponding to the full G-GCN model. the categories with less instances may provide very few training instances, which makes GNN difficult to pass message from these nodes. We believe this is the reason why G-GCN is less powerful when training ratio is small.</p><p>In all, the results show that the learning framework of G-GNN can be successfully applied to plain graphs, and achieve similar or better results than state-of-theart methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>There are a lot of efforts in recent literature to develop neural network learning algorithms on graphs.</p><p>The most prominent one may be Graph convolution networks (GCN) <ref type="bibr" target="#b0">[1]</ref>. GCN is based on the first-order approximation of spectral graph convolutions. Graphsage <ref type="bibr" target="#b1">[2]</ref> is new neighborhood aggregation algorithms by concatenating the node's features in addition to pooled neighborhood information. Graph attention model (GAT) <ref type="bibr" target="#b15">[13]</ref> was proposed to assign different neighborhoods with different weights based on multihead attention mechanism. In FastGCN <ref type="bibr" target="#b30">[28]</ref>, graph convolutions is interpreted as integral transforms of embedding functions under probability measures, which has faster training speed and comparable precision. P-PGNNs <ref type="bibr" target="#b31">[29]</ref> can capture nodes' position within graph structure, which first sample sets of anchor nodes and then learn a non-linear distance-weighted aggregation scheme over the anchor-sets.</p><p>Several related studies tried to expand the reception field of GNN and increase the neighborhood available at each node. PPNP/APPNP <ref type="bibr" target="#b6">[6]</ref> improves the message passing algorithms based on personalized PageRank. <ref type="bibr" target="#b32">[30]</ref> proposes jumping knowledge networks that can that flexibly leverage different neighborhood ranges for each node. N-GCN <ref type="bibr" target="#b4">[5]</ref> trains multiple instances of GCNs over node pairs discovered at different distances in random walks, and learns a combination of these in-stance outputs. However, because the semi-supervised settings lack enough training, these methods have to control the model complexity carefully, which limits the learning ability in exploring the global information.</p><p>For example, our experiments have shown that G-GNN with kernel of APPNP can still achieve promising improvement.</p><p>Some studies also try to introduce unsupervised learning in GNNs to alleviate the insufficient supervision problem. <ref type="bibr" target="#b33">[31]</ref> proposes an auto-encoder architecture that learns a joint representation of both local graph structure and available node features for the multi-task learning of link prediction and node classification. GraphNAS <ref type="bibr" target="#b23">[21]</ref> first generates variable-length strings that describe the architectures of graph neural networks, and then maximizes the expected accuracy of the generated architectures on a validation data based on reinforcement learning. However, these methods do not consider to utilize global information of the graphs.</p><p>The main difference of our work is that we use unsupervised learning to capture the global information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In the paper, we proposed a novel framework named G-GNN, which is able to conduct semi-supervised learning on both plain and attributed graphs. The proposed framework takes the advantage of both random walk and GNN, which can not only utilize global information but also aggregate local information well.</p><p>Therefor, the existing GNN models can be used as kernels of G-GNN, to obtain the ability of preserving global information. Extensive experiments show that our framework can improve the learning ability of existing GNN models.</p><p>For future work, we plan to test some more complicated methods that combine the hidden states, and study other unsupervised methods that can produce global features more suitable for the learning ability of GNN.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Regular PaperThis work was partially supported by the Natural Science Foundation of the Jiangsu Higher Education Institutions of China under grant No 18kJB510010, Social Science Foundation of Jiangsu Province under grant No 19TQD002, and National Nature Science Foundation of China (NSFC) under grant number 61976114.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>First 2</head><label>2</label><figDesc>Author et al.: Shortened Title Within 45 Characters 3 Preliminary</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>tribute feature matrix X (a) are learned in an unsupervised way. Next, X (s) , X (a) and the original attribute matrix X are fed to a parallel GNN based model, to learn their corresponding hidden states. Finally, the final hidden states are the weighted sum of the 3 hidden states H (s) , X (a) and H (o) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>First 5 Fig. 1 .</head><label>51</label><figDesc>Author et al.: Shortened Title Within 45 Characters An overview of the G-GNN framework. the attribute feature vectors. X (a) v denotes the global attribute feature vector of v. S k and S t denote the corresponding parameter vectors of k and t in S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>6 J</head><label>6</label><figDesc>.Comput. Sci. &amp; Technol., January 2018, Vol., No.    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>???</head><label></label><figDesc>Early fusion: Early fusion is quite simple, where the pretrained and raw attributes are combined in the input layer. For example, we can concatenate the attributes and then feed them to a GNN model. Middle fusion: Middle fusion is a bit more complicated. It is required to design specifical propagation and aggregation functions of GNN to integrate the features in each layer. Late fusion: The late fusion is the current version of parallel framework. The attributes are fed to different GNNs and the outputs are combined in the final layer. In practice, early fusion can already improve the precision, but the improvement is not so big as late fusion. See table 3 in section 4.2.2 for details. Middle fusion requires to modify the inner propagation or aggregation functions of the specifical GNN. Hence, the framework will lose generality to apply to other GNNs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>First</head><label></label><figDesc>Author et al.: Shortened Title Within 45 Characters 7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>conduct 10 iterations of random walk start from each node. The walk length is 100. For each source node, the nearby nodes within 10 steps are considered as the neighborhood nodes. The dimensions of both the global structure and attribute vectors are 8. The number of negative sampling is 64. In the semi-supervised learning, the three baseline models are used as kernels of G-GNN, and the corresponding models are named as G-GCN, G-Graphsage and G-APPNP. The parameters are exactly the same as those in the baseline methods. We search ? and ? between 0.001 to 0.05. The test results are reported when the best valid results are obtained.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 2 .</head><label>2</label><figDesc>cation task. For example, G-GCN outperforms GCN with 2.24%, 0.26%, 1.79% and 1.76% of precision on the four datasets respectively. The results demonstrate that the learning framework of G-GNN can effectively and consistently enhance the learning ability of the corresponding GNN kernels. APPNP is also designed for enlarging the reception field and can utilize global information. APPNP outperforms the other baseline models. Although the improvement is not so large as those in G-GCN, G-APPNP still significantly outperforms its kernel of APPNP with 0.40%, 0.07%, 1.27% and 1.48% on the four datasets respectively. The result shows that even for a propagation method which can powerfully utilize global information, our learning schema of G-GNN can still bring considerable precision gains. We believe the advantage comes from the pretraining-and-learning schema, since our global information is obtained via pre-training and no longer suffers from the limitation brought by weak supervision. G-APPNP achieves the best results on Cora, Cite-First Author et al.: Shortened Title Within 45 Characters 9 Precision w.r.t. ? and ? on different datasets. (a) Citeseer. (b) Cora. (c) Pubmed. seer and Cora. Note that its precisions on Cora (84.31%) and Pubmed (80.95%) are the new state-ofthe-art results. To the best of our knowledge, the previous best results are GraphNAS (84.20%) [21] on Cora, and MixHop (80.80%) [22] on Pubmed. On the large dataset of CoraFull, G-GAT outperforms all the other methods with precision of 63.42%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 2</head><label>2</label><figDesc>shows the precision w.r.t. ? and ?. Generally, different datasets require different ? and ? to achieve the best precision, and ? and ? are often around 0.01. The precision will decrease quickly if we continue to increase the two parameters. The result shows that it is very necessary to introduce the two hyper-parameters to turn down the impact of the pre-trained features. In fact, the component of GN N (X, A) will contribute almost nothing without the weight method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 3 Fig. 3 .</head><label>33</label><figDesc>shows the precision w.r.t. the dimension of the global features. The highest precision is achieved when the dimension is around 8 to 16. Precision w.r.t. Dimension of global features on Cora.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>results of the baseline methods are cited from their original papers. Pubmed is excluded from comparison since it is not used in the baseline papers.The training data is the entire plain graph (X is excluded), and part of the node labels. We use 0.1, 0.2 ... 0.9 of node labels to train the model respectively, and report the classification accuracy on the rest of data.All results are the average values of 10 experiments with different random split.Our proposed model is G-GCN (plain), where the GCN is used as the kernel. In the unsupervised training, the dimension of global structure vectors is 32. In the semi-supervised learning, the dimension of the hidden states is 256. No dropout is used. The rest of parameters are the same as those in 4.1.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>First</head><label></label><figDesc>Author et al.: Shortened Title Within 45 Characters</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The statistics of the datasets Dataset #Nodes #Edges #Attributes #Classes #Training nodes #Valid nodes #Test nodes # denotes the number of something, e.g. #Nodes is the number of nodes.</figDesc><table><row><cell>3]: GAT is</cell></row></table><note>* The data, code and pre-trained vectors to reproduce our results are released on https://github.com/zhudanhao/G-GNN ? https://github.com/dmlc/dgl/tree/master/examples/pytorch</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>The</figDesc><table><row><cell cols="4">classification precision (%) on different feature</cell></row><row><cell></cell><cell>fusion settings</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="3">Cora Citeseer Pubmed</cell></row><row><cell>GCN</cell><cell>81.47</cell><cell>71.01</cell><cell>79.10</cell></row><row><cell>Early Fusion</cell><cell>83.39</cell><cell>70.78</cell><cell>80.64</cell></row><row><cell cols="2">Late Fusion (G-GCN) 83.71</cell><cell>71.27</cell><cell>80.88</cell></row><row><cell cols="4">Second, we investigate the effect of the global features,</cell></row><row><cell cols="4">as shown in Table 4. The results show that both pre-</cell></row><row><cell cols="4">trained feature matrices can help to increase the model</cell></row><row><cell cols="4">precision. The global structure features are more help-</cell></row><row><cell cols="4">ful on Cora, but less effective than global attribute fea-</cell></row><row><cell cols="4">tures on Pubmed and Citeseer. On the datasets of Cora</cell></row><row><cell cols="4">and Pubmed, the highest precisions are obtained when</cell></row><row><cell cols="4">all the features are used. However, in Citeseer, the</cell></row><row><cell cols="4">global structure features cannot help to increase the</cell></row><row><cell cols="4">performance if the global attribute features are used.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>The classification precision (%) when using different feature matrices Feature Matrices Cora Citeseer Pubmed</figDesc><table><row><cell>X</cell><cell>81.47</cell><cell>71.01</cell><cell>79.10</cell></row><row><cell>X + X (s)</cell><cell>83.20</cell><cell>71.05</cell><cell>79.35</cell></row><row><cell>X + X (a)</cell><cell>82.77</cell><cell>71.27</cell><cell>80.73</cell></row><row><cell cols="2">X + X (s) + X (a) 83.71</cell><cell>71.27</cell><cell>80.88</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Classification precision (%) on plain graphs 34.95 37.59 38.36 40.92 42.79 46.23 48.96 57.05 Planetoid-G [17] 54.10 57.92 58.15 62.31 64.22 68.11 70.16 70.34 72.12 PNE [24] 54.79 60.87 64.67 66.95 68.59 70.00 72.06 73.41 74.76 MMDW [23] 55.60 60.97 63.18 65.08 66.93 69.52 70.47 70.87 70.95 G-GCN (Plain) 54.24 60.31 64.16 66.41 69.36 70.77 72.12 74.41 75.89 Note: The results of PNE<ref type="bibr" target="#b26">[24]</ref> and MMDW<ref type="bibr" target="#b25">[23]</ref> are cited from their original papers.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>Training percent 40% 50% 60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell></row><row><cell></cell><cell>ICA [16]</cell><cell cols="7">44.96 47.21 47.40 48.23 48.82 50.18 52.43 58.05 66.04</cell></row><row><cell>Cora</cell><cell cols="8">Planetoid-G [17] 74.45 79.16 80.72 82.63 84.59 83.34 84.23 84.67 85.10</cell></row><row><cell></cell><cell>PNE [24]</cell><cell cols="7">77.58 81.22 82.94 84.54 84.73 85.55 86.15 86.39 87.76</cell></row><row><cell></cell><cell>MMDW [23]</cell><cell cols="7">74.94 80.83 82.83 83.68 84.71 85.51 87.01 87.27 88.19</cell></row><row><cell></cell><cell cols="2">G-GCN (Plain) 76.88</cell><cell>80.5</cell><cell cols="5">82.65 85.06 85.57 86.23 87.67 87.05 89.16</cell></row><row><cell></cell><cell>ICA [16]</cell><cell>33.54</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Citeseer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">J. Comput. Sci. &amp; Technol., January 2018, Vol., No.Xin-Yu Dai received the B.Eng.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>and Ph.D. degrees in computer science, Nanjing University, Nanjing, in 1999 and 2005, respectively. He has been on leave from Nanjing University from August 2010 to September 2011 to visit EECS Department and Statistics Department at UC Berkeley. He is currently a professor with the School of Artificial Intelligence at Nanjing University. His research interests include natural language processing and knowledge engineering.</p><p>Jia-Jun Chen is a professor of Department of Computer Science and Technology, Nanjing University, and the director of Natural Language Processing Group.</p><p>Prof. Chen received his Ph.D., M.S. and B.S. in computer science from Nanjing University, Nanjing, in 1985, 1988, 1998 respectively. His research interest is on natural language processing, machine translation, information extraction, and text classification.</p><p>http://arxiv.org/ps/1910.12241v2</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Semi?supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<ptr target="https://arxiv.org/abs/1609.02907" />
		<imprint>
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in neural information processing systems</title>
		<meeting>Advances in neural information processing systems</meeting>
		<imprint>
			<date type="published" when="2017-12" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<ptr target="https://arxiv.org/abs/1710.10903" />
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeper insights into graph aonvolutional networks for semi?supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the thirty-second AAAI conference on artificial intelligence (AAAI-18)</title>
		<meeting>the thirty-second AAAI conference on artificial intelligence (AAAI-18)</meeting>
		<imprint>
			<date type="published" when="2018-02" />
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu?el?haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><forename type="middle">J</forename><surname>N?gcn</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Multi?scale graph convolution for semi?supervised node classification</title>
		<idno type="arXiv">arXiv:1802.08888</idno>
		<ptr target="https://arxiv.org/abs/1802.08888" />
		<imprint>
			<date type="published" when="2018-08" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gunnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05997</idno>
		<ptr target="https://arxiv.org/abs/1810.05997" />
		<imprint>
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skiena</forename><forename type="middle">S</forename><surname>Deepwalk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the 20th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014-08" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016-08" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Internet: Diameter of the world?wide web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">?l</forename><surname>Barabasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="issue">6749</biblScope>
			<biblScope unit="page">130</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph structure in the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Broder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Maghoul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tomkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wiener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer networks</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1?6</biblScope>
			<biblScope unit="page" from="309" to="320" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International conference on neural information processing systems</title>
		<meeting>International conference on neural information processing systems</meeting>
		<imprint>
			<date type="published" when="2013-12" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">First</forename><surname>Author</surname></persName>
		</author>
		<title level="m">Shortened Title Within 45 Characters</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">PCANE: Preserving context attributes for network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><forename type="middle">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<title level="m">Pacific?Asia conference on knowledge discovery and data mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-04" />
			<biblScope unit="page" from="156" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in neural information processing systems</title>
		<meeting>Advances in neural information processing systems</meeting>
		<imprint>
			<date type="published" when="2017-12" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Greedy layer?wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in neural information processing systems</title>
		<meeting>Advances in neural information processing systems</meeting>
		<imprint>
			<date type="published" when="2007-12" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">?y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Collective classification in network data. AI magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">93</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Revisiting semisupervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08861</idno>
		<ptr target="https://arxiv.org/abs/1603.08861" />
		<imprint>
			<date type="published" when="2016-03" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gunnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03815</idno>
		<ptr target="https://arxiv.org/abs/1707.03815" />
		<imprint>
			<date type="published" when="2017-03" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<ptr target="https://arxiv.org/abs/1412.6980" />
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Graphnas: Graph neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09981</idno>
		<ptr target="https://arxiv.org/abs/1904.09981" />
		<imprint>
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu?el?haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixhop</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00067</idno>
		<ptr target="https://arxiv.org/abs/1905.00067" />
		<title level="m">Higher?order graph convolution architectures via sparsified neighborhood mixing</title>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Max?margin deepwalk: discriminative learning of network representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International joint conference on artificial intelligence</title>
		<meeting>International joint conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2016-07" />
			<biblScope unit="page" from="3889" to="3895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">PNE: Label embedding enhanced network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Pacific?Asia conference on knowledge discovery and data mining</title>
		<meeting>Pacific?Asia conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2017-04" />
			<biblScope unit="page" from="547" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the 20th international conference on machine learning (ICML-03)</title>
		<meeting>the 20th international conference on machine learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003-08" />
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Line: Large?scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the 24th international conference on world wide web</title>
		<meeting>the 24th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2015-05" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning latent representations of nodes for classifying in heterogeneous social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the 7th ACM international conference on Web search and data mining</title>
		<meeting>the 7th ACM international conference on Web search and data mining</meeting>
		<imprint>
			<date type="published" when="2014-02" />
			<biblScope unit="page" from="373" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><forename type="middle">C</forename><surname>Fastgcn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10247</idno>
		<ptr target="https://arxiv.org/abs/1801.10247" />
		<title level="m">Fast learning with graph convolutional networks via importance sampling</title>
		<imprint>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Position-aware graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04817</idno>
		<ptr target="https://arxiv.org/abs/1906.04817" />
		<imprint>
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03536</idno>
		<ptr target="https://arxiv.org/abs/1806.03536" />
		<title level="m">Representation learning on graphs with jumping knowledge networks</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to make predictions on graphs with autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2018 IEEE 5th International conference on data science and advanced analytics (DSAA)</title>
		<meeting>2018 IEEE 5th International conference on data science and advanced analytics (DSAA)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-10" />
			<biblScope unit="page" from="237" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">He is currently a librarian in Jiangsu Police Institute. His research interests include network representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dan-Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">computer science in Nanjing University</title>
		<meeting><address><addrLine>Changzhou; Nanjing; Nanjing</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>Zhu received his B.S. degree in Management from Hohai University ; Management from Nanjing University</orgName>
		</respStmt>
	</monogr>
	<note>graph learning and natural language processing</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
