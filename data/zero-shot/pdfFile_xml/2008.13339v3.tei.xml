<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Bidirectional Tree Tagging Scheme for Joint Medical Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xukun</forename><surname>Luo</surname></persName>
							<email>luoxukun@pku.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cn</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cn</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Ma</surname></persName>
							<email>mameng@pku.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Wang</surname></persName>
							<email>pwang@pku.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Bidirectional Tree Tagging Scheme for Joint Medical Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Index Terms-medical relation extraction, sequential tagging</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Joint medical relation extraction refers to extracting triples, composed of entities and relations, from the medical text with a single model. One of the solutions is to convert this task into a sequential tagging task. However, in the existing works, the methods of representing and tagging the triples in a linear way failed to the overlapping triples, and the methods of organizing the triples as a graph faced the challenge of large computational effort. In this paper, inspired by the tree-like relation structures in the medical text, we propose a novel scheme called Bidirectional Tree Tagging (BiTT) to form the medical relation triples into two two binary trees and convert the trees into a word-level tags sequence. Based on BiTT scheme, we develop a joint relation extraction model to predict the BiTT tags and further extract medical triples efficiently. Our model outperforms the best baselines by 2.0% and 2.5% in F1 score on two medical datasets. What's more, the models with our BiTT scheme also obtain promising results in three public datasets of other domains.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Medical relation extraction is an important task for building knowledge graphs in the medical domain. Relation extraction (RE) refers to extracting the relations between entity pairs contained in unstructured text such as electronic health records (EHRs) and medication package inserts (MPIs).</p><p>The early studies on RE are mainly fallen on pipeline methods, i.e., first identify the entities in a sequence by the named entity recognition (NER) module, and then classify the relation for each entity pair by the relation classification (RC) module <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b35">[36]</ref>. However, in the above methods, the error in NER module would be introduced into the subsequent RC module. Thus, the joint methods were put forward to solve this error propagation problem. These studies can be roughly divided into several paradigms <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b39">[40]</ref>, among them, one stream of work is to convert the RE task into the sequential tagging task. The first work on tagging scheme <ref type="bibr" target="#b39">[40]</ref> is developed to organize the relation triples in a linear way and turn them into a tagging sequence with the same length s as the input sentence. As <ref type="bibr" target="#b39">[40]</ref> needs to compute the probability of different tags for each token, the computational complexity is O(s|R|), where |R| is the size of the predefined relation set R. However, <ref type="bibr" target="#b39">[40]</ref> failed to the overlapping triples, i.e., EntityPairOverlap (EPO) and SingleEntityOverlap (SEO) <ref type="bibr" target="#b36">[37]</ref>. Take the first sentence in <ref type="figure" target="#fig_0">Figure 1</ref> as an example, the tagging scheme of <ref type="bibr" target="#b39">[40]</ref> is unable to represent both Birth-Place and President in a single tag for each word. After that, some tagging schemes <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b31">[32]</ref> are proposed to handle the overlapping problem. They formed the relation triples into a graph represented by an adjacency matrix, and obtained a tagging sequence of length s 2 . Though the improved tagging sequences were able to accommodate more information, the computation complexity of training is increased from O(s|R|) to O(s 2 |R|).</p><p>Besides, most of existing medical RE frameworks are based on the RE models specialized to the generic domains. However, the relation structure in the medical domain has different characteristics from other domains. Specifically, from EHRs and MPIs, we found that there are many tree-like relation structures in the medical text. For example, in the second sentence of <ref type="figure" target="#fig_0">Figure 1</ref>, a drug (nimodipine) corresponds to two effects (hypoxemia and hypotension). Furthermore , drug contains multiple ingredients and can treat multiple symptoms in a MPI.</p><p>Inspired by this discovery, in our paper, we employ a forest rather than a graph to represent relation triples in the medical sentence, and convert the forest into a tagging sequence of length 8s. Specifically, our scheme called Bidirectional Tree Tagging (BiTT) is divided into three steps. First, the triples with the same relation in a sentence are grouped together. Second, the entities and relations in a group are modeled into two binary trees according to the order in which they appear in the sentence. Finally, we establish a mapping between the binary tree and token-level sequence tags so that they can be converted to each other. Our BiTT tags can represent more triples with low computation complexity of O(s|R|).</p><p>The key contributions are summarized as:</p><p>? We propose a novel tagging scheme called BiTT, which apply the forest structure to represent the relation triples in the medical text; ? Based on our BiTT scheme, a joint medical RE model is developed for automatically predicting BiTT tags and extract relation triples; ? The models based on BiTT achieve solid results on two medical datasets and three public generic datasets, which is a unique benefit obtained by effectively handling the overlap issue through the BiTT scheme.</p><p>II. RELATED WORK RE is a core task for text mining and information extraction. Depending on the structural diversity of neural networks, a number of pipeline RE models <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b35">[36]</ref> were proposed with improved effect. The joint RE methods were put forward to solve the error propagation problem of the pipeline methods. Some works <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b18">[19]</ref> extended the RC module to share the encoder representation with the NER module, mitigated the problem of error propagation but still extracted entities and relations in a pipeline way. Some works <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b39">[40]</ref> studied on the sequential tagging based methods, which turned the extracting tasks into a sequence labeling problem, bridging the information gap between the RC and NER steps. They focused on solving the overlapping triple problem and made great progress. However, these works could not deal with some special cases yet or cost much time. Some works <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> directly generated the relational triples by the seq2seq framework. The seq2seq based works did not require complex tagging schemes, yet the maximum length of generated sequence was hard to determine and the decoding step cost much time.</p><p>Medical RE is a sub-field of RE, and researchers usually adapted the generic RE models through the challenges in the medical domain. For example, <ref type="bibr" target="#b16">[17]</ref> integrated the entityrelated information, such as part-of-speech (POS) and medical ontology, into a joint RE model to improve the performance of medical entity recognition. <ref type="bibr" target="#b15">[16]</ref> proposed the trainable character embedding in the model to solve the out-of-vocabulary (OOV) problem in the medical text. And <ref type="bibr" target="#b22">[23]</ref> exploited the medical knowledge of medicines, and incorporated the medical knowledge database into a RE model for Chinese medicine instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>In this section, we firstly present our fine-grained division for the sentences with overlapping triples. Then we illustrate how to convert these sentences to BiTT tag sequences and extract triples from the BiTT tags. Finally, we introduce our joint RE model for predicting BiTT tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Fine-grained Division</head><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, <ref type="bibr" target="#b36">[37]</ref> categorized the sentences with overlapping triples into EPO and SEO. A sentence belongs to Initialize l = an array of all EN i 's location pair in S;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Add the elements in l to L; 6: end for 7: Sort L from small to large according to beginning index; <ref type="bibr">8:</ref> while L is not empty do 9:</p><p>Initialize a tree T with the root L 1 , remove L 1 from L; <ref type="bibr">10:</ref> for i = 2; i ? length(L); i + + do <ref type="bibr">11:</ref> if L i not in T and there is a valid relation between L i and the node in T then <ref type="bibr">12:</ref> Remove L i from L and add it to T ; <ref type="bibr">13:</ref> end if <ref type="bibr">14:</ref> end for <ref type="bibr">15:</ref> Add T to F ; 16: end while 17: /* Transform the forest to a binary tree. */ 18: Initialize an empty stack S t , push the root nodes in F to S t in order; <ref type="bibr">19:</ref> Initialize a binary tree B whose root node is F 1 's root; 20: for i = 2; i ? length(F ); i + + do <ref type="bibr">21:</ref> Add F i 's root to B as the right child of F i?1 's root; <ref type="bibr" target="#b21">22</ref>: end for 23: while S t is not empty do <ref type="bibr">24:</ref> Initialize node cur = P op(S t ), C = the children array of node cur in F ; <ref type="bibr">25:</ref> Add C 1 to B as the left child of node cur ; <ref type="bibr">26:</ref> for i = 2; i ? length(C); i + + do <ref type="bibr">27:</ref> Add C i to B as the right child of C i?1 ; <ref type="bibr">28:</ref> end for <ref type="bibr">29:</ref> Push the children of node cur to S t in order; <ref type="bibr">30:</ref> end while EPO if there are some triples whose entity sets ({e 1 , e 2 }) are the same, and belongs to SEO if there are some triples whose entity sets are different and contain at least one overlapping entity. Note that a sentence can pertain to the intersection of EPO and SEO. To separately handle the tree-like relation structures in the medical text, we further divide SEO into ExcludeLoopSentences (ELS) and IncludeLoopSentences (ILS) based on the existence of relation loops. A sentence belongs to ILS if there are some triples whose entity sets satisfy following conditions: (1) they are different; (2) each of them has at least one overlapping entity; (3) some of them contain two overlapping entities. A sentence belongs to ELS if it pertains to SEO but is not an ILS sentence. Note that there is at least one loop in the relation graph of an ILS sentence and no loop in the relation graph of an ELS sentence without considering the ( edges' direction. Based on the statistics of the datasets from different domains in <ref type="table" target="#tab_2">Table I</ref>, most of the medical sentences with overlapping triples belong to ELS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Bidirectional Tree Tagging Scheme</head><p>We propose a novel tagging scheme called BiTT, which uses two binary tree structures to incorporate the three kinds of sentence in Section III-A. We show an example in <ref type="figure">Figure  2</ref> to detail our handling approach and algorithm for different classes.</p><p>1) EPO Handling: Though there are some triples whose entity sets are the same in the EPO sentences, their relation categories are different from each other. Hence we group the corresponding triples together with the same relation category in a sentence and label them respectively. For example, as shown in <ref type="figure">Figure 2</ref>, (America, Capital, W ashington) with the relation category Capital is divided into an independent group, and triples with the relation category Contains are aggregated into another group. Note that although the triple groups are labeled respectively, all triples are predicted simultaneously by the joint RE model to preserve their correlations.</p><p>2) ELS Handling: The most remarkable feature for the triples in an ELS sentence is that, there is no loop in its relation graph, thus the triples can be completely represented as a forest. We handle the ELS sentences with a Tree Tagging scheme, which consists of two steps.</p><p>Relation-to-Tree The pseudo code is presented in Algorithm 1. First, obtain the positions of all entities in a sentence and construct a relation forest according to the entities' forward appearing order. Take the first entity as the root of the first tree T 1 , and then recurrently go through other entities from left to right, add the entities having relations with T 1 's nodes to T 1 gradually. When the remaining entities can no longer be added to the current tree, select the first entity of remains as a new tree T i 's root and apply previous operations on T i until there is no entity left. After that, all trees are aggregated into a forest F . Second, we convert F into a binary tree B. For an entity node e in F , e's first child becomes e's left child in B, and right adjacent brother becomes e's right child in B. For example, the node Washington is the first child of The White House and the left brother of America in the forward forest in <ref type="figure">Figure 2</ref>. Therefore, Washington becomes the left child of The White House and America becomes Washington's right child in the forward binary tree. Note that we add the annotation Brother on an edge in B if the nodes connected by the edge used to be two root nodes in F . Besides, every edge is directional, from e 1 to e 2 in a specific triple.</p><p>Tree-to-Tag We assign a tag for every single word in the sentence according to the binary relation tree B from Relationto-Tree step. If a word does not belong to any entity, its tag will be "O". If a word is a part of an entity node e, its tag will be a combination of following four parts: e and its parent in B. P 2 = Root when e is the root of B. And P 2 = Brother when the annotation on the edge is also Brother. Except, P 2 = (Child 2 , Role 2 ), where Child 2 ? {l(lef t), r(right)} indicates if e is the left or right child of its parent, and Role 2 ? {1(e 1 ), 2(e 2 )} shows the entity role of e pointed out by the direction of the edge. ? Part 3 (P 3 ) indicates the information on the edge between e and its left child in B. P 3 = N U LL when e has no left child. Except that, P 3 = Role 3 , where Role 3 ? {1(e 1 ), 2(e 2 )}. ? Part 4 (P 4 ) indicates the information on the edge between e and its right child in B. P 4 = N U LL when e has no right child. P 4 = Brother when the annotation on the edge is also Brother. Except, P 4 = Role 4 , where Role 4 ? {1(e 1 ), 2(e 2 )}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) ILS Handling:</head><p>Since there is at least one loop in the relation graph of an ILS sentence, the triples can not be absolutely represented by a single forest. To solve the problem, we simply build another forest according to the entities' backward appearing order, then convert it to a backward binary tree, and obtain the backward tree tags for a sentence. We apply the forward tags and the backward tags to accommodate more triples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Tags to Triples</head><p>For the forward tags, first find all root nodes in the forest by the condition of P 2 = Root or P 2 = Brother. Then we start from these root nodes, recursively match P 3 or P 4 of the nodes already in forest with other nodes' P 2 to reconnect the edges between the node pairs. If a parent node can match more than one node as its left (or right) child, we select the nearest node behind of it, since we take the entities' appearing order into account when building the forest in the first step of the Tree Tagging scheme in Section III-B2. What's more, when rebuilding the relation tree, if a child or brother node is missed, our algorithm simply ignore it and discard its sub-tree. After reconstructing the forest, we can recursively extract the relational triples in the sentence from it.</p><p>The operation on the backward tags is the same as that on the forward tags. Except that when a parent node can match more than one node as its left (or right) child, we select the nearest node before it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Joint Relation Extraction Model</head><p>In this section, we introduce our joint RE model in four parts, i.e., the text embedding module, the encoder module, the decoder module and our loss function respectively.</p><p>1) Text Embedding: For a given word w in the input sentence, the representation e ? R d of w from the text embedding module is concatenate by four parts:</p><formula xml:id="formula_0">e = Linear([e w ; e l ; e c ; e p ])<label>(1)</label></formula><p>where e w ? R dw is the word embedding, e l ? R d l is the pretrained contextualized word embedding from language models such as BERT <ref type="bibr" target="#b3">[4]</ref>, e c ? R dc is the character embedding, and e p ? R dp is the part-of-speech (POS) embedding.</p><p>2) Encoder: The encoder module aims to extract a context vector representation for each word in the sentence. In this paper, we adopt the Bi-directional Long Short-Term Memory (Bi-LSTM) layers and the multi-head self-attention layers as the encoder.</p><p>A Bi-LSTM layer consists of a forward LSTM <ref type="bibr" target="#b8">[9]</ref> and a backward one. Denote the word embedding vectors of a sentence with s words as V = [e 1 , . . . , e s ]. The output o t and the hidden state h t of the t-th word w t from LSTM are:</p><formula xml:id="formula_1">o t , h t = lstm block(e t , h t?1 )<label>(2)</label></formula><p>where lstm block(?) is the function of a memory block in LSTM. We concatenate the two hidden states corresponding to the same word together as the Bi-LSTM hidden state. Thus, the Bi-LSTM hidden state? t of the t-th word w t is:</p><formula xml:id="formula_2">h t = [ ? ? h t , ? ??? ? h s?t+1 ]<label>(3)</label></formula><p>where ? ? h t and ? ??? ? h s?t+1 are respectively the hidden states of w t in the forward LSTM and the backward one.</p><p>The multi-head self-attention layer, the main component in the Transformer <ref type="bibr" target="#b26">[27]</ref> encoder, is represented mathematically as follow:</p><formula xml:id="formula_3">M ultiHead(H) = [head 1 (H); . . . ; head h (H)]<label>(4)</label></formula><formula xml:id="formula_4">head i (H) = Atten(HW Q i , HW K i , HV Q i ) (5) Atten(Q, K, V) = sof tmax( QK T ? d k )V<label>(6)</label></formula><p>Here H ? R s?d h denotes the hidden state matrix [? 1 , . . . ,? s ] of the input sentence from the last Bi-LSTM layer. W Q i , W K i and V Q i are learnable matrices. h indicates the number of heads and d k = d h /h means the size of the key vector.</p><p>3) Decoder: The decoder module is designed to parse the word representations and then predict the BiTT tags for each word. In this paper, it consists of a series of Linear layers. And there are two alternative construction methods for the Linear layers to predict BiTT tags, i.e., one-head and multihead. Take a forward tree tag as example, the former means that concatenate the four parts of the tag as one label and predict it by a single Linear layer, like the work in <ref type="bibr" target="#b39">[40]</ref>. The latter predicts these four parts separately with four Linear layers, requiring fewer parameters. In this study, we employ the multi-head mechanism to reduce the computational costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Loss Function:</head><p>The loss function for training the forward or backward tags is a weighted bias objective function defined by:</p><formula xml:id="formula_5">L = ? 1 L 1 + ? 2 L 2 + ? 3 L 3 + ? 4 L 4<label>(7)</label></formula><p>where L j (j ? {1, 2, 3, 4}) are the same bias cross entropy functions as <ref type="bibr" target="#b39">[40]</ref> for the four parts of BiTT tags respectively, ? j are the weights for L j . We obtain L f for the forward tags and L b for the backward tags by Eq. <ref type="formula" target="#formula_5">(7)</ref>, then define the total loss L T of our framework as follow:</p><formula xml:id="formula_6">L T = L f + ?L b (8)</formula><p>where ? is a weight hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setting</head><p>Datasets We evaluate our BiTT scheme on five datasets, including two medical datasets (ADE <ref type="bibr" target="#b5">[6]</ref> and CMeIE <ref type="bibr" target="#b7">[8]</ref>) and three datasets (NYT <ref type="bibr" target="#b23">[24]</ref>, WebNLG <ref type="bibr" target="#b6">[7]</ref> and DuIE <ref type="bibr" target="#b13">[14]</ref>) from other domains. ADE, collected from the English medical reports, contains 4.2k samples with the relation Adverse-Effect. Following previous work <ref type="bibr" target="#b33">[34]</ref>, the samples with overlapping entities are filtered out and the 10-fold cross validation is performed for ADE in our experiment. CMeIE is a Chinese medical dataset constructed by several rounds of manual annotation. It consists of 28k sentences and 44 relations derived from medical textbooks and clinical practices.</p><p>NYT contains 66.1k sentences with 24 relation categories from New York Times news. We adopt the preprocessed version released by <ref type="bibr" target="#b36">[37]</ref>. WebNLG is an English dataset created for Natural Language Generation (NLG) task. It was adapted for RE task and released by <ref type="bibr" target="#b36">[37]</ref>, which contains 6.2k sentences and 246 relation types. DuIE, consisting of 214.7k sentences with 49 relation categories, is a big Chinese dataset released by Baidu Inc. for RE. Since the testing set of DuIE is not available, we randomly separate the training set into a new validation set (20k samples) and a new training set (153.1k samples), and take the old validation set as the testing set (21.6k samples). <ref type="table" target="#tab_2">Table I</ref> illustrates some statistics of the samples with overlapping triples in the five datasets. It indicates that the overlapping triple problem is very common in all datasets. Besides, in the medical datasets, the samples with overlapping triples have a high proportion of ELS samples , reaching more than 87%. However, this pattern may not apply to the datasets in other domains, e.g., the sentences with overlapping triples have a low proportion of ELS sentences at only 42.2%.</p><p>Metrics We employ micro Precision (Prec), Recall (Rec) and F1 score (F1) to evaluate the performance of our models. We consider a predicted triple (e 1 , r, e 2 ) as a correct one only if e 1 , e 2 and r are all correct. Note that some original achievements of baselines evaluate the performance by Partial Matching, which means that (e 1 , r, e 2 ) is simply regarded as a correct triple if the first (or last) tokens of two entities (e 1 and e 2 ) and r are correct.</p><p>Implementation Details For the medical datasets, we apply the model called BiTT-Med mentioned in Section III-D for prediction of BiTT tagging sequence and extraction of relation triples. In the text embedding module, the hidden sizes of different embedding vectors are d w = d c = d p = 100 and d l = d = 768. The pre-trained GloVe <ref type="bibr" target="#b21">[22]</ref> vectors are used to initialize the word embedding e w . The contextualized word embedding e l is fixed and initialized by the vectors from the pre-trained BERT-base encoder. The character embedding is randomly initialized and computed by an LSTM <ref type="bibr" target="#b11">[12]</ref> to cope with the OOV problem. And the POS tagging sequence of the input sentence is generated by SpaCy <ref type="bibr" target="#b9">[10]</ref>. In the encoder module, the number of Bi-LSTM layers in the encoder module  <ref type="bibr" target="#b13">[14]</ref> 15,672 94,891 11,780 109,675 0.865 is 2 and the hidden size is 768. The number of multi-head selfattention layers is 2 and the number of head is h = 8. As for the loss function, in order to balance the loss of each part of the BiTT tags, we adopt ? 1 = 8/6, ? 2 = 8/8, ? 3 = 8/5 and ? 4 = 8/6 in Eq. <ref type="formula" target="#formula_5">(7)</ref>, since the number of optional labels in the four parts are 6, 8, 5 and 6 after adding the tag "O" and the padding tag respectively. And we set ? = 1 in Eq. <ref type="bibr" target="#b7">(8)</ref>. Besides, the maximum length of an input sentence, the batch size and the learning rate are set to 128, 8 and 1e-4 respectively. And the training operation is terminated when the F1 on validation set no longer rises. Besides, in <ref type="table" target="#tab_2">Table II</ref>, some baselines on CMeIE dataset is upgraded by ourselves for a further comparison. We simply substitute our text embedding module for the word embedding layers of the original achievements their papers. For the other three datasets, due to the large amount of training data and the absence of medical proper nouns in the generic datasets, we simplify BiTT-Med and obtain two new models called BiTT-LSTM and BiTT-BERT. Through these two models we can evaluate the performance of the BiTT scheme on generic domains. Specifically, we replace the text embedding layer of BiTT-Med with a learnable GloVe embedding in BiTT-LSTM and an unfixed BERT-base encoder in BiTT-BERT. The warm up rate in BiTT-BERT is set to 0.1. And we remove the multi-head self-attention layers of BiTT-Med in these two models. What's more, similar to the experiments on the medical datasets, in <ref type="table" target="#tab_2">Table III</ref>, we apply the BERT-base encoder as the word embedding layer of some baseline models and report the results. Note that the evaluation metrics in original NovelTagging and GraphRel are much looser than that for their upgraded version with BERT in our experiment, which makes the results of upgraded models with BERT look worse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baseline Models</head><p>For comparison, we employ 13 models as baselines, which can be roughly divided into one-stage models and two-stage models. The one-stage models output both entities and relations at the same time, including Neural Joint <ref type="bibr" target="#b16">[17]</ref>, NovelTagging <ref type="bibr" target="#b39">[40]</ref>, Rel-Metric <ref type="bibr" target="#b25">[26]</ref>, PA <ref type="bibr" target="#b2">[3]</ref>, GraphRel <ref type="bibr" target="#b4">[5]</ref>, <ref type="table">Table-</ref>Sequence <ref type="bibr" target="#b28">[29]</ref>, TPLinker <ref type="bibr" target="#b31">[32]</ref>, and PFN <ref type="bibr" target="#b33">[34]</ref>. The two-stage models identify the entities firstly and then classify the relations of all entity pairs, or identify the head entities first and then find out the tail entities based on the relation categories for each head entity. The corresponding representative methods include Multi-head <ref type="bibr" target="#b0">[1]</ref>, Multi-head + AT <ref type="bibr" target="#b1">[2]</ref>, CopyRE <ref type="bibr" target="#b36">[37]</ref>, CopyMTL <ref type="bibr" target="#b37">[38]</ref> and CasRel <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results</head><p>Compared Results on Medical Datasets <ref type="table" target="#tab_2">Table II</ref>  Compare Results on Common Datasets To further evaluate the performance of our BiTT scheme on the joint RE task, we apply our BiTT-LSTM and BiTT-BERT models on three generic datasets. <ref type="table" target="#tab_2">Table III</ref> shows the comparison of our models with baselines on NYT, WebNLG and DuIE. For BiTT-LSTM, it achieves ideal F1 scores on NYT and WebNLG, which are 71.1% and 73.8%. BiTT-LSTM outperforms CopyMTL-One by 7.6% and 11.1% in Rec on NYT and WebNLG. There is an impressive Rec gap between BiTT-LSTM and the baselines without BERT encoder, which verifies the utility of BiTT scheme when dealing with overlapping triples. For BiTT-BERT, it also achieves solid F1 scores close to CasRel and TPLinker on NYT, WebNLG and DuIE datasets, which are 88.9% , 86.2% and 78.0% respectively. And BiTT-BERT achieve the best Prec on three datasets, which are 89.7%, 89.1% and 75.7%. In addition, we note that the sequential tagging based models (NovelTagging, CasRel, TPLinker and BiTT-BERT) achieve higher Prec than other models. It proves </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Efficiency of BiTT based models</head><p>Our BiTT based models can predict the BiTT tags with low computation complexity of O(s|R|), thus have higher efficiency compared to graph-based and two-stage models. First, we compare our BiTT-BERT model with the one-stage baselines. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, in our experiments on NYT, most of baseline models with BERT-base encoder converge at a similar epoch, i.e., about the 15th epoch. Besides, the number of decoder parameters in our BiTT-BERT (48.9M) is almost the same as that in NovelTagging (47.3M) and much less than that in other one-stage baseline models, i.e., GraphRel-2p (106.4M) and TPLinker (1293.3M). This indicates that with the same encoder, the converge speed of our framework is  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study</head><p>As is shown in <ref type="table" target="#tab_2">Table IV</ref>, to explore the effects of the handling methods for overlapping triples in BiTT scheme and our decoder architecture, we perform the ablation tests based on BiTT-BERT and the NYT dataset. BiTT-BERT "w/o Group" means that the "EPO Handling" described in Section III-B1 is dropped while replaced by adding the relation categories information to P 2 , P 3 and P 4 of the BiTT tags. BiTT-BERT "w/o Bidirectional" implies that we only build the forward forest in a sentence and then generate the forward tags, BiTT-BERT "w/o Multi-head" indicates that the multihead structure in our framework is substituted by the one-head structure, which is illustrated in the decoder module of our joint RE model in Section III-D.</p><p>From <ref type="table" target="#tab_2">Table IV</ref>, we have observed some impressive facts when comparing these three ablation tests with BiTT-BERT. First, the grouping operation greatly improves the performance of our framework on not only EPO (17.2%) but also ELS (11.1%) and ILS (17.2%), since it reduces a complex relation graph into several simpler ones in a sentence. And it also reduces the categories of BiTT tags, since the information of relation categories and entity types does not need to be added to the tags. Second, the backward forest can effectively supplement the triples that cannot be totally represented by a single forward forest, which provides an F1 score boost of 10.8% for ILS sentences. The backward forest also boosts the F1 score on EPO and ELS sentences by 1.0% and 2.8%. This indicates that when the sub-tree of a node is discarded due to the wrong prediction of the forward forest, the backward forest can supplement the information of the missing node. Last, compared with the one-head structure, the multi-head structure can decrease the parameters of the decoder module, thus lessen the computation burden and the training time in an epoch. Besides, the F1 score of multi-head is 1.2% higher than one-head. This indicates that multi-head does not make the interaction of the four parts of BiTT labels less, but makes the labels with fewer occurrences to be trained more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, motivated by the tree-like relation structures in the medical text, we propose a Bidirectional Tree Tagging (BiTT) scheme to label the overlapping entities and relations in medical sentences with solid accuracy and high efficiency.</p><p>We build up a jointly RE model called BiTT-Med and two simplified versions (BiTT-LSTM and BiTT-BERT) for experiments. The results on two public medical datasets and three generic datasets demonstrate that our proposal outperforms several baselines, especially when processing the ELS cases.</p><p>Our future work aims to improve the BiTT scheme and the RE models in following aspects. (1) When extracting results from BiTT tags, we will try reconstructing a binary forest instead of a binary tree to reduce the error propagation if a node is dropped. (2) More rule restrictions for BiTT will be proposed to make it more robust. (3) We will apply more powerful pre-trained encoders to the extraction framework for better performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Trump was born in America in 1946 and became President of the state in 2016.Birth-PlacePresidentEasily reversible hypoxemia and hypotension induced by nimodipine.Drug-EventsDrug-EventsHewitt was born in washington, the capital of America. Examples of the sentences with overlapping triples, including EPO, ELS and ILS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The training curves of different models with BERT-base encoder on the NYT dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I STATISTICS</head><label>I</label><figDesc>OF THE TRAINING SET AND THE TESTING SET OF THE FIVE DATASETS. ELS Ratio INDICATES ELS / OVERLAP SAMPLES.</figDesc><table><row><cell>Dataset</cell><cell>EPO</cell><cell>ELS</cell><cell>ILS</cell><cell>Overlap Samples</cell><cell>ELS Ratio</cell></row><row><cell>ADE [6]</cell><cell>118</cell><cell>1,216</cell><cell>159</cell><cell>1,391</cell><cell>0.874</cell></row><row><cell>CMeIE [8]</cell><cell>381</cell><cell>8,805</cell><cell>457</cell><cell>9,213</cell><cell>0.956</cell></row><row><cell>NYT [24]</cell><cell cols="2">17,004 10,740</cell><cell>2,006</cell><cell>25,422</cell><cell>0.422</cell></row><row><cell>WebNLG [7]</cell><cell>622</cell><cell>2,894</cell><cell>1,294</cell><cell>3,957</cell><cell>0.731</cell></row><row><cell>DuIE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II THE</head><label>II</label><figDesc>PERFORMANCE COMPARISON OF DIFFERENT METHODS ON ADE AND CMEIE. ? INDICATES THAT THE RESULTS ON CMEIE IS UPGRADED FROM ORIGINAL ACHIEVEMENTS BY OUR TEXT EMBEDDING MODULE. THE MAIN ENCODERS APPLIED IN DIFFERENT MODELS: L = LSTM, L+C = LSTM + CNN, ALB = ALBERT-XXLARGE-V1, BB = BERT-BASE, BB+G = BERT-BASE + GCN.</figDesc><table><row><cell>Model</cell><cell>Encoder</cell><cell>Prec</cell><cell>Rec</cell><cell>F1</cell></row><row><cell>ADE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Neural Joint [17]</cell><cell>L</cell><cell>64.0</cell><cell>62.9</cell><cell>63.4</cell></row><row><cell>Multi-head [1]</cell><cell>L</cell><cell>72.1</cell><cell>77.2</cell><cell>74.5</cell></row><row><cell>Multi-head + AT [2]</cell><cell>L</cell><cell>-</cell><cell>-</cell><cell>75.5</cell></row><row><cell>Rel-Metric [26]</cell><cell>L+C</cell><cell>77.4</cell><cell>77.3</cell><cell>77.3</cell></row><row><cell>Table-Sequence [29]</cell><cell>ALB</cell><cell>-</cell><cell>-</cell><cell>80.1</cell></row><row><cell>PFN [34]</cell><cell>Bb</cell><cell>-</cell><cell>-</cell><cell>80.0</cell></row><row><cell>BiTT-Med (Ours)</cell><cell>Bb</cell><cell>83.1</cell><cell>81.3</cell><cell>82.1</cell></row><row><cell>CMeIE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NovelTagging [40] ?</cell><cell>Bb</cell><cell>51.4</cell><cell>17.1</cell><cell>25.6</cell></row><row><cell>GraphRel-1p [5] ?</cell><cell>Bb+G</cell><cell>31.2</cell><cell>26.0</cell><cell>28.4</cell></row><row><cell>GraphRel-2p [5] ?</cell><cell>Bb+G</cell><cell>28.5</cell><cell>23.1</cell><cell>25.5</cell></row><row><cell>CasRel [30] ?</cell><cell>Bb</cell><cell>53.5</cell><cell>28.2</cell><cell>37.0</cell></row><row><cell>ER+RE [35]</cell><cell>ALB</cell><cell>-</cell><cell>-</cell><cell>47.6</cell></row><row><cell>BiTT-Med (Ours)</cell><cell>Bb</cell><cell>55.6</cell><cell>45.5</cell><cell>50.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>presents the comparison of BiTT-Med with previous methods on two medical datasets. Our BiTT-Med achieves solid F1 scores on ADE and CMeIE, which are 82.1% and 50.1%. On ADE, our model outperforms Table-Sequence by 2.0% in F1, and outperforms Rel-Metric by 5.7% in Prec and 4.0% in Rec. On CMeIE, our model outperforms ER+RE by 2.5% in F1, and outperforms CasRel by 2.1% in Prec and 17.3% in Rec. This indicates the effectiveness of our model for jointly extracting medical entities and relations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III THE</head><label>III</label><figDesc>PERFORMANCE COMPARISON OF DIFFERENT METHODS ON NYT, WEBNLG AND DUIE. ? INDICATES THE METRICS FOR THE MODELS FOLLOW PARTIAL MATCHING. ? INDICATES THAT THE RESULTS ON NYT AND DUIE IS UPGRADED FROM ORIGINAL ACHIEVEMENTS BY THE BERT-BASE ENCODER. THE MAIN ENCODERS APPLIED IN DIFFERENT MODELS: L = LSTM, L+G = LSTM + GCN, BB = BERT-BASE, BB+G = BERT-BASE + GCN. BiTT-BERT does not perform as well as the best method on NYT and WebNLG. It probably results from the fact that the proportion of ELS sentences in the samples with overlapping triples of NYT and WebNLG are 42.2% and 73.1%, which are not as large as in DuIE. This demonstrates the advantages of our BiTT scheme for handling ELS sentences.</figDesc><table><row><cell>Model</cell><cell>Encoder</cell><cell>Prec</cell><cell>Rec</cell><cell>F1</cell></row><row><cell>NYT</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NovelTagging [40] ?</cell><cell>L</cell><cell>62.4</cell><cell>31.7</cell><cell>42.0</cell></row><row><cell>CopyRE-Mul [37] ?</cell><cell>L</cell><cell>61.0</cell><cell>56.6</cell><cell>58.7</cell></row><row><cell>GraphRel-2p [5] ?</cell><cell>L+G</cell><cell>63.9</cell><cell>60.0</cell><cell>61.9</cell></row><row><cell>PA [3]</cell><cell>L</cell><cell>49.4</cell><cell>59.1</cell><cell>53.8</cell></row><row><cell>CopyMTL-Mul [38]</cell><cell>L</cell><cell>75.7</cell><cell>68.7</cell><cell>72.0</cell></row><row><cell>NovelTagging [40] ?</cell><cell>Bb</cell><cell>89.0</cell><cell>55.6</cell><cell>69.3</cell></row><row><cell>CopyRE-Mul [37] ?</cell><cell>Bb</cell><cell>39.1</cell><cell>36.5</cell><cell>37.8</cell></row><row><cell>GraphRel-2p [5] ?</cell><cell>Bb+G</cell><cell>82.5</cell><cell>57.9</cell><cell>68.1</cell></row><row><cell>CasRel [30] ?</cell><cell>Bb</cell><cell>89.7</cell><cell>89.5</cell><cell>89.6</cell></row><row><cell>BiTT-LSTM (Ours)</cell><cell>L</cell><cell>66.5</cell><cell>76.3</cell><cell>71.1</cell></row><row><cell>BiTT-BERT (Ours)</cell><cell>Bb</cell><cell>89.7</cell><cell>88.0</cell><cell>88.9</cell></row><row><cell>WebNLG</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NovelTagging [40] ?</cell><cell>L</cell><cell>52.5</cell><cell>19.3</cell><cell>28.3</cell></row><row><cell>CopyRE-Mul [37] ?</cell><cell>L</cell><cell>37.7</cell><cell>36.4</cell><cell>37.1</cell></row><row><cell>GraphRel-2p [5] ?</cell><cell>L+G</cell><cell>44.7</cell><cell>41.1</cell><cell>42.9</cell></row><row><cell>CopyMTL-Mul [38]</cell><cell>L</cell><cell>58.0</cell><cell>54.9</cell><cell>56.4</cell></row><row><cell>TPLinker [32]</cell><cell>Bb</cell><cell>88.9</cell><cell>84.5</cell><cell>86.7</cell></row><row><cell>BiTT-LSTM (Ours)</cell><cell>L</cell><cell>83.8</cell><cell>66.0</cell><cell>73.8</cell></row><row><cell>BiTT-BERT (Ours)</cell><cell>Bb</cell><cell>89.1</cell><cell>83.0</cell><cell>86.2</cell></row><row><cell>DuIE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NovelTagging [40] ?</cell><cell>Bb</cell><cell>75.0</cell><cell>38.0</cell><cell>50.4</cell></row><row><cell>GraphRel-1p [5] ?</cell><cell>Bb+G</cell><cell>52.2</cell><cell>23.9</cell><cell>32.8</cell></row><row><cell>GraphRel-2p [5] ?</cell><cell>Bb+G</cell><cell>41.1</cell><cell>25.8</cell><cell>31.8</cell></row><row><cell>CaseRel [30] ?</cell><cell>Bb</cell><cell>75.7</cell><cell>80.0</cell><cell>77.8</cell></row><row><cell>BiTT-BERT (Ours)</cell><cell>Bb</cell><cell>75.7</cell><cell>80.6</cell><cell>78.0</cell></row><row><cell cols="5">the superiority of the sequential tagging based methods for</cell></row><row><cell cols="2">conservative prediction. However,</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV THE</head><label>IV</label><figDesc>RESULTS OF THE ABLATION STUDY ON BITT-BERT MODEL AND THE NYT DATASET. F1-EPO, F1-ELS, F1-ILS ARE THE F1 SCORES ON THE EPO, ELS AND ILS SENTENCES. F1-ALL IS THE F1 SCORE ON ALL SENTENCES. THE SCALE OF DECODER PARAMETERS IS MILLION (M), AND THE SCALE OF TRAINING TIME IS SECOND PER EPOCH (S/EPOCH).</figDesc><table><row><cell>Metrics</cell><cell>w/o Group</cell><cell>w/o Bidirectional</cell><cell>w/o Multi-head</cell><cell>BiTT-BERT</cell></row><row><cell>F1-EPO</cell><cell>74.0</cell><cell>90.2</cell><cell>90.2</cell><cell>91.2</cell></row><row><cell>F1-ELS</cell><cell>76.2</cell><cell>84.5</cell><cell>85.5</cell><cell>87.3</cell></row><row><cell>F1-ILS</cell><cell>68.3</cell><cell>74.7</cell><cell>81.8</cell><cell>85.5</cell></row><row><cell>F1-All</cell><cell>81.9</cell><cell>88.2</cell><cell>87.7</cell><cell>88.9</cell></row><row><cell>Decoder Params</cell><cell>71.3</cell><cell>48.0</cell><cell>68.5</cell><cell>48.9</cell></row><row><cell>Training Time</cell><cell>-</cell><cell>-</cell><cell>2125.0</cell><cell>1739.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? Part 1 (P 1 ) indicates the position of a word in the entity node e with the "BIES" signs, i.e., P 1 ? {B(begin), I(in), E(end), S(single)}. ? Part 2 (P 2 ) indicates the information on the edge between</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Joint entity recognition and relation extraction as a multi-head selection problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bekoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Develder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="page" from="34" to="45" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adversarial training for multi-context joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bekoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Develder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2830" to="2836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and overlapping relations using position-attentive sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6300" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">GraphRel: modeling text as relational graphs for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1409" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Development of a benchmark corpus to support the automatic extraction of drug-related adverse effects from medical case reports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gurulingappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mateen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Angus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fluck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hofmann-Apitius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toldo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Informatics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="885" to="892" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Creating training corpora for NLG micro-planners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">CMeIE: construction and evaluation of Chinese medical information extraction dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of NLPCC</title>
		<meeting>NLPCC</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An improved non-monotonic transition system for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1373" to="1378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Investigating LSTMs for joint extraction of opinion entities and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="919" to="929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DuIE: a large-scale chinese dataset for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NLPCC</title>
		<meeting>NLPCC</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="791" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A bi-lstm-rnn model for relation classification using low-cost sequence features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ji</surname></persName>
		</author>
		<idno>abs/1608.07720</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A neural joint model for entity and relation extraction from biomedical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinform</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="198" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint models for extracting adverse drug events from biomedical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2838" to="2844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL and AFNLP</title>
		<meeting>ACL and AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using LSTMs on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling joint entity and relation extraction with table representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1858" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Effective modeling of encoder-decoder architecture for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI, 2020</title>
		<meeting>AAAI, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="8528" to="8535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Glove: global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">KeMRE: knowledge-enhanced medical relation extraction for chinese medicine instructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Informatics</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page">103834</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECML-PKDD</title>
		<meeting>ECML-PKDD</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural metric learning for fast end-to-end relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kavuluru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Combining recurrent and convolutional neural networks for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schutze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="534" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Two are better than one: Joint entity and relation extraction with table-sequence encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP, 2020</title>
		<meeting>EMNLP, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="1706" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A novel cascade binary tagging framework for relational triple extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL, 2020</title>
		<meeting>ACL, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="1476" to="1488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system: bridging the gap between</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<idno>abs/1609.08144</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">TPLinker: single-stage joint extraction of entities and relations through token pair linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1572" to="1582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Classifying relations via long short term memory networks along shortest dependency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1785" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Partition Filter Network for Joint Entity and Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP, 2021</title>
		<meeting>EMNLP, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="185" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">CBLUE: a chinese biomedical language understanding evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL, 2022</title>
		<meeting>ACL, 2022</meeting>
		<imprint>
			<biblScope unit="page" from="7888" to="7915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Extracting relational facts by an end-to-end neural model with copy mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="506" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">CopyMTL: copy mechanism for joint extraction of entities and relations with multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI, 2020</title>
		<meeting>AAAI, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="9507" to="9514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">UER: an open-source toolkit for pre-training models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-IJCNLP</title>
		<meeting>EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="241" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and relations based on a novel tagging scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
