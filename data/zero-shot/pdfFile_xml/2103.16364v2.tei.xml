<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ICE: Inter-instance Contrastive Encoding for Unsupervised Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universit? C?te d&apos;Azur</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">European Systems Integration</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Lagadec</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">European Systems Integration</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Bremond</surname></persName>
							<email>francois.bremond@inria.frbenoit.lagadec@esifrance.net</email>
							<affiliation key="aff0">
								<orgName type="department">Inria</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universit? C?te d&apos;Azur</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ICE: Inter-instance Contrastive Encoding for Unsupervised Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised person re-identification (ReID) aims at learning discriminative identity features without annotations. Recently, self-supervised contrastive learning has gained increasing attention for its effectiveness in unsupervised representation learning. The main idea of instance contrastive learning is to match a same instance in different augmented views. However, the relationship between different instances has not been fully explored in previous contrastive methods, especially for instance-level contrastive loss. To address this issue, we propose Interinstance Contrastive Encoding (ICE) that leverages interinstance pairwise similarity scores to boost previous classlevel contrastive ReID methods. We first use pairwise similarity ranking as one-hot hard pseudo labels for hard instance contrast, which aims at reducing intra-class variance. Then, we use similarity scores as soft pseudo labels to enhance the consistency between augmented and original views, which makes our model more robust to augmentation perturbations. Experiments on several largescale person ReID datasets validate the effectiveness of our proposed unsupervised method ICE, which is competitive with even supervised methods. Code is made available at https://github.com/chenhao2345/ICE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification (ReID) targets at retrieving an person of interest across non-overlapping cameras by comparing the similarity of appearance representations. Supervised ReID methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b22">23]</ref> use human-annotated labels to build discriminative appearance representations which are robust to pose, camera property and view-point variation. However, annotating cross-camera identity labels is a cumbersome task, which makes supervised methods less scalable in real-world deployments. Unsupervised methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33]</ref> directly train a model on unlabeled data and thus have a better scalability.</p><p>Most of previous unsupervised ReID methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b41">42]</ref> are based on unsupervised domain adaptation (UDA).</p><p>UDA methods adjust a model from a labeled source domain to an unlabeled target domain. The source domain provides a good starting point that facilitates target domain adaptation. With the help of a large-scale source dataset, state-ofthe-art UDA methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b41">42]</ref> significantly enhance the performance of unsupervised ReID. However, the performance of UDA methods is strongly influenced by source dataset's scale and quality. Moreover, a large-scale labeled dataset is not always available in the real world. In this case, fully unsupervised methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> own more flexibility, as they do not require any identity annotation and directly learn from unlabeled data in a target domain.</p><p>Recently, contrastive learning has shown excellent performance in unsupervised representation learning. State-ofthe-art contrastive methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14]</ref> consider each image instance as a class and learns representations by matching augmented views of a same instance. As a class is usually composed of multiple positive instances, it hurts the performance of fine-grained ReID tasks when different images of a same identity are considered as different classes. Self-paced Contrastive Learning (SpCL) <ref type="bibr" target="#b12">[13]</ref> alleviates this problem by matching an instance with the centroid of the multiple positives, where each positive converges to its centroid at a uniform pace. Although SpCL has achieved impressive performance, this method does not consider interinstance affinities, which can be leveraged to reduce intraclass variance and make clusters more compact. In supervised ReID, state-of-the-art methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23]</ref> usually adopt a hard triplet loss <ref type="bibr" target="#b15">[16]</ref> to lay more emphasis on hard samples inside a class, so that hard samples can get closer to normal samples. In this paper, we introduce Inter-instance Contrastive Encoding (ICE), in which we match an instance with its hardest positive in a mini-batch to make clusters more compact and improve pseudo label quality. Matching the hardest positive refers to using one-hot "hard" pseudo labels.</p><p>Since no ground truth is available, mining hardest positives within clusters is likely to introduce false positives into the training process. In addition, the one-hot label does not take the complex inter-instance relationship into consideration when multiple pseudo positives and negatives exist in a mini-batch. Contrastive methods usually use data augmentation to mimic real-world distortions, e.g., occlusion, view-point and resolution variance. After data augmentation operations, certain pseudo positives may become less similar to an anchor, while certain pseudo negatives may become more similar. As a robust model should be invariant to distortions from data augmentation, we propose to use the inter-instance pairwise similarity as "soft" pseudo labels to enhance the consistency before and after augmentation.</p><p>Our proposed ICE incorporates class-level label (centroid contrast), instance pairwise hard label (hardest positive contrast) and instance pairwise soft label (augmentation consistency) into one fully unsupervised person ReID framework. Without any identity annotation, ICE significantly outperforms state-of-the-art UDA and fully unsupervised methods on main-stream person ReID datasets.</p><p>To summarize, our contributions are: (1) We propose to use pairwise similarity ranking to mine hardest samples as one-hot hard pseudo labels for hard instance contrast, which reduces intra-class variance. (2) We propose to use pairwise similarity scores as soft pseudo labels to enhance the consistency between augmented and original instances, which alleviates label noise and makes our model more robust to augmentation perturbation. (3) Extensive experiments highlight the importance of inter-instance pairwise similarity in contrastive learning. Our proposed method ICE outperforms state-of-the-art methods by a considerable margin, significantly pushing unsupervised ReID to real-world deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Unsupervised person ReID. Recent unsupervised person ReID methods can be roughly categorized into unsupervised domain adaptation (UDA) and fully unsupervised methods. Among UDA-based methods, several works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b19">20]</ref> leverage semantic attributes to reduce the domain gap between source and target domains. Several works <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b3">4]</ref> use generative networks to transfer labeled source domain images into the style of target domain. Another possibility is to assign pseudo labels to unlabeled images, where pseudo labels are obtained from clustering <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b2">3]</ref> or reference data <ref type="bibr" target="#b39">[40]</ref>. Pseudo label noise can be reduced by selecting credible samples <ref type="bibr" target="#b0">[1]</ref> or using a teacher network to assign soft labels <ref type="bibr" target="#b10">[11]</ref>. All these UDA-based methods require a labeled source dataset. Fully unsupervised methods have a better flexibility for deployment. BUC <ref type="bibr" target="#b20">[21]</ref> first treats each image as a cluster and progressively merge clusters. Lin et al. <ref type="bibr" target="#b21">[22]</ref> replace clustering-based pseudo labels with similarity-based softened labels. Hierarchical Clustering is proposed in <ref type="bibr" target="#b40">[41]</ref> to improve the quality of pseudo labels. Since each identity usually has multiple positive instances, MMCL <ref type="bibr" target="#b32">[33]</ref> introduces a memory-based multi-label classification loss into unsupervised ReID. JVTC <ref type="bibr" target="#b18">[19]</ref> and CycAs <ref type="bibr" target="#b35">[36]</ref> explore temporal information to refine visual similarity. SpCL <ref type="bibr" target="#b12">[13]</ref> considers each cluster and outlier as a single class and then conduct instance-to-centroid contrastive learning. CAP <ref type="bibr" target="#b34">[35]</ref> calculates identity centroids for each camera and conducts intra-and inter-camera centroid contrastive learning. Both SpCL and CAP focus on instance-to-centroid contrast, but neglect inter-instance affinities.</p><p>Contrastive Learning. Recent contrastive learning methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b4">5]</ref> consider unsupervised representation learning as a dictionary look-up problem. Wu et al. <ref type="bibr" target="#b38">[39]</ref> retrieve a target representation from a memory bank that stores representations of all the images in a dataset. MoCo <ref type="bibr" target="#b13">[14]</ref> introduces a momentum encoder and a queue-like memory bank to dynamically update negatives for contrastive learning. In SimCLR <ref type="bibr" target="#b4">[5]</ref>, authors directly retrieve representations within a large batch. However, all these methods consider different instances of a same class as different classes, which is not suitable in a fine-grained ReID task. These methods learn invariance from augmented views, which can be regarded as a form of consistency regularization.</p><p>Consistency regularization. Consistency regularization refers to an assumption that model predictions should be consistent when fed perturbed versions of the same image, which is widely considered in recent semi-supervised learning <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b5">6]</ref>. The perturbation can come from data augmentation <ref type="bibr" target="#b26">[27]</ref>, temporal ensembling <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b11">12]</ref> and shallow-deep features <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b5">6]</ref>. Artificial perturbations are applied in contrastive learning as strong augmentation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">37]</ref> and momentum encoder <ref type="bibr" target="#b13">[14]</ref> to make a model robust to data variance. Based on temporal ensembling, Ge et al. <ref type="bibr" target="#b11">[12]</ref> use inter-instance similarity to mitigate pseudo label noise between different training epochs for image localization. Wei et al. <ref type="bibr" target="#b36">[37]</ref> propose to regularize inter-instance consistency between two sets of augmented views, which neglects intra-class variance problem. We simultaneously reduce intra-class variance and regularize consistency between augmented and original views, which is more suitable for fine-grained ReID tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Given a person ReID dataset X = {x 1 , x 2 , ..., x N }, our objective is to train a robust model on X without annotation. For inference, representations of a same person are supposed to be as close as possible. State-of-the-art contrastive methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b4">5]</ref> consider each image as an individual class and maximize similarities between augmented views of a same instance with InfoNCE loss <ref type="bibr" target="#b30">[31]</ref>:</p><formula xml:id="formula_0">L Inf oN CE = E[? log exp (q ? k + /? ) K i=0 exp (q ? k i /? ) ]<label>(1)</label></formula><p>where q and k + are two augmented views of a same instance in a set of candidates k i . ? is a temperature hyper-parameter <ref type="figure">Figure 1</ref>: General architecture of ICE. We maximize the similarity between anchor and pseudo positives in both inter-class (proxy agreement between an instance representation f1 and its cluster proxy p1) and intra-class (instance agreement between f1 and its pseudo positive m2) manners.</p><p>that controls the scale of similarities. Following MoCo <ref type="bibr" target="#b13">[14]</ref>, we design our proposed ICE with an online encoder and a momentum encoder as shown in <ref type="figure">Fig. 1</ref>. The online encoder is a regular network, e.g., ResNet50 <ref type="bibr" target="#b14">[15]</ref>, which is updated by back-propagation. The momentum encoder (weights noted as ? m ) has the same structure as the online encoder, but updated by accumulated weights of the online encoder (weights noted as ? o ):</p><formula xml:id="formula_1">? t m = ?? t?1 m + (1 ? ?)? t o<label>(2)</label></formula><p>where ? is a momentum coefficient that controls the update speed of the momentum encoder. t and t ? 1 refer respectively to the current and last iteration. The momentum encoder builds momentum representations with the moving averaged weights, which are more stable to label noise. At the beginning of each training epoch, we use the momentum encoder to extract appearance representations M = {m 1 , m 2 , ..., m N } of all the samples in the training set X . We use a clustering algorithm DBSCAN [9] on these appearance representations to generate pseudo identity labels Y = {y 1 , y 2 , ..., y N }. We only consider clustered inliers for contrastive learning, while un-clustered outliers are discarded. We calculate proxy centroids p 1 , p 2 , ... and store them in a memory for a proxy contrastive loss L proxy (see Sec. 3.2). Note that this proxy memory can be cameraagnostic <ref type="bibr" target="#b12">[13]</ref> or camera-aware <ref type="bibr" target="#b34">[35]</ref>.</p><p>Then, we use a random identity sampler to split the training set into mini-batches where each mini-batch contains N P pseudo identities and each identity has N K instances. We train the whole network by combining the L proxy (with class-level labels), a hard instance contrastive loss L h ins (with hard instance pairwise labels, see Sec. 3.3) and a soft instance consistency loss L s ins (with soft instance pairwise labels, see Sec. 3.4):</p><formula xml:id="formula_2">L total = L proxy + ? h L h ins + ? s L s ins (3)</formula><p>To increase the consistency before and after data augmentation, we use different augmentation settings for prediction and target representations in the three losses (see Tab. 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss</head><p>Predictions (augmentation) Targets (augmentation) </p><formula xml:id="formula_3">L proxy f (Strong) p (None) L h ins f (Strong) m (Strong) L s ins P (Strong) Q (None)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Proxy Centroid Contrastive Baseline</head><p>For a camera-agnostic memory, the proxy of cluster a is defined as the averaged momentum representations of all the instances belonging to this cluster:</p><formula xml:id="formula_4">p a = 1 N a mi?ya m i (4)</formula><p>where N a is the number of instances belonging to the cluster a.</p><p>We apply a set of data augmentation on X and feed them to the online encoder. For an online representation f a belonging to the cluster a, the camera-agnostic proxy contrastive loss is a softmax log loss with one positive proxy p a and all the negatives in the memory:</p><formula xml:id="formula_5">L agnostic = E[? log exp (f a ? p a /? a ) |p| i=1 exp (f a ? p i /? a ) ]<label>(5)</label></formula><p>where |p| is the number of clusters in a training epoch and ? a is a temperature hyper-parameter. Different from unified contrastive loss <ref type="bibr" target="#b10">[11]</ref>, outliers are not considered as single instance clusters. In such way, outliers are not pushed away from clustered instances, which allows us to mine more hard samples for our proposed hard instance contrast. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, all the clustered instances converge to a common cluster proxy centroid. However, images inside a cluster are prone to be affected by camera styles, leading to high intra-class variance. This problem can be alleviated by adding a cross-camera proxy contrastive loss <ref type="bibr" target="#b34">[35]</ref>.</p><p>For a camera-aware memory, if we have C = {c 1 , c 2 , ...} cameras, a camera proxy p ab is defined as the averaged momentum representations of all the instances belonging to the cluster a in camera c b :</p><formula xml:id="formula_6">p ab = 1 N ab mi?ya?mi?c b m i<label>(6)</label></formula><p>where N ab is the number of instances belonging to the cluster a captured by camera c b . Given an online representation f ab , the cross-camera proxy contrastive loss is a softmax log loss with one positive cross-camera proxy p ai and N neg nearest negative proxies in the memory:</p><formula xml:id="formula_7">Lcross = E[? 1 |P| i =b?i?C log exp (&lt; f ab ? pai &gt; /?c) Nneg +1 j=1 exp (&lt; f ab ? pj &gt; /?c) ]<label>(7)</label></formula><p>where &lt; ? &gt; denotes cosine similarity and ? c is a crosscamera temperature hyper-parameter. |P| is the number of cross-camera positive proxies. Thanks to this cross-camera proxy contrastive loss, instances from one camera are pulled closer to proxies of other cameras, which reduces intra-class camera style variance. We define a proxy contrastive loss by combining cluster and camera proxies with a weighting coefficient 0.5 from <ref type="bibr" target="#b34">[35]</ref>:</p><formula xml:id="formula_8">L proxy = L agnostic + 0.5L cross<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Hard Instance Contrastive Loss</head><p>Although intra-class variance can be alleviated by crosscamera contrastive loss, it has two drawbacks: 1) more memory space is needed to store camera-aware proxies, 2) impossible to use when camera ids are unavailable. We propose a camera-agnostic alternative by exploring interinstance relationship instead of using camera labels. Along with training, the encoders become more and more strong, which helps outliers progressively enter clusters and become hard inliers. Pulling hard inliers closer to normal inliers effectively increases the compactness of clusters.</p><p>A mini-batch is composed of N P identities, where each identity has N K positive instances. Given an anchor instance f i belonging to the ith class, we sample the hardest positive momentum representation m i k that has the lowest cosine similarity with f i , see <ref type="figure" target="#fig_2">Fig. 4</ref>. For the same anchor, we have J = (N P ? 1) ? N K negative instances that do not belong to the ith class. The hard instance contrastive loss for f i is a softmax log loss of J + 1 (1 positive and J negative) pairs, which is defined as:</p><formula xml:id="formula_9">L h ins = E[? log exp (&lt; f i ? m i k &gt; /? h ins ) J+1 j=1 exp (&lt; f i ? m j &gt; /? h ins ) ]<label>(9)</label></formula><p>where k = arg min k=1,..,N K (&lt; f i ? m i k &gt;) and ? h ins is the hard instance temperature hyper-parameter. By minimizing the distance between the anchor and the hardest positive and maximizing the distance between the anchor and all negatives, L h ins increases intra-class compactness and inter-class separability.</p><p>Relation with triplet loss. Both L h ins and triplet loss <ref type="bibr" target="#b15">[16]</ref> pull an anchor closer to positive instances and away from negative instances. As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, the traditional triplet loss pushes away a negative pair from a positive pair by a margin. Differently, the proposed L h ins pushes away all the negative instances as far as it could with a softmax. If we select one negative instance, the L h ins can be transformed into the triplet loss. If we calculate pairwise distance within a mini-batch to select the hardest positive and the hardest negative instances, the L h ins is equivalent to the batch-hard triplet loss <ref type="bibr" target="#b15">[16]</ref>. We compare hard triplet loss (hardest negative) with the proposed L h ins (all negatives  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Soft Instance Consistency Loss</head><p>Both proxy and hard instance contrastive losses are trained with one-hot hard pseudo labels, which can not capture the complex inter-instance similarity relationship between multiple pseudo positives and negatives. Especially, inter-instance similarity may change after data augmentation. As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, the anchor A becomes less similar to pseudo positives (P 1 , P 2 , P 3 ), because of the visual distortions. Meanwhile, the anchor A becomes more similar to pseudo negatives (N 1 , N 2 ), since both of them have red shirts. By maintaining the consistency before and after augmentation, a model is supposed to be more invariant to augmentation perturbations. We use the inter-instance similarity scores without augmentation as soft labels to rectify those with augmentation.</p><p>For a batch of images after data augmentation, we measure the inter-instance similarity between an anchor f A with all the mini-batch N K ? N P instances, as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. Then, the inter-instance similarity is turned into a prediction distribution P by a softmax:</p><formula xml:id="formula_10">P = exp (&lt; f A ? m &gt; /? s ins ) N P ?N K j=1 exp (&lt; f A ? m j &gt; /? s ins )<label>(10)</label></formula><p>where ? s ins is the soft instance temperature hyperparameter. f A is an online representation of the anchor, while m is momentum representation of each instance in a mini-batch. For the same batch without data augmentation, we measure the inter-instance similarity between momentum representations of the same anchor with all the mini-batch N K ? N P instances, because the momentum encoder is more stable. We get a target distribution Q:</p><formula xml:id="formula_11">Q = exp (&lt; m A ? m &gt; /? s ins ) N P ?N K j=1 exp (&lt; m A ? m j &gt; /? s ins )<label>(11)</label></formula><p>The soft instance consistency loss is Kullback-Leibler Divergence between two distributions:</p><formula xml:id="formula_12">L s ins = D KL (P ||Q)<label>(12)</label></formula><p>In previous methods, consistency is regularized between weakly augmented and strongly augmented images <ref type="bibr" target="#b26">[27]</ref> or two sets of differently strong augmented images <ref type="bibr" target="#b36">[37]</ref>. Some methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30]</ref> also adopted mean square error (MSE) as their consistency loss function. We compare our setting with other possible settings in Tab. 3.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>General training settings. To conduct a fair comparison with state-of-the-art methods, we use an ImageNet <ref type="bibr" target="#b25">[26]</ref> pretrained ResNet50 <ref type="bibr" target="#b14">[15]</ref> as our backbone network. We report results of IBN-ResNet50 <ref type="bibr" target="#b23">[24]</ref> in Appendix B. An Adam optimizer with a weight decay rate of 0.0005 is used to optimize our networks. The learning rate is set to 0.00035 with a warm-up scheme in the first 10 epochs. No learning rate decay is used in the training. The momentum encoder is up- dated with a momentum coefficient ? = 0.999. We renew pseudo labels every 400 iterations and repeat this process for 40 epochs. We use a batchsize of 32 where N P = 8 and N K = 4. We set ? a = 0.5, ? c = 0.07 and N neg = 50 in the proxy contrastive baseline. Our network is trained on 4 Nvidia 1080 GPUs under Pytorch framework. The total training time is around 2 hours on Market-1501. After training, only the momentum encoder is used for the inference.</p><p>Clustering settings. We calculate k-reciprocal Jaccard distance <ref type="bibr" target="#b46">[47]</ref> for clustering, where k is set to 30. We set a minimum cluster samples to 4 and a distance threshold to 0.55 for DBSCAN. We also report results of a smaller threshold 0.5 (more appropriate for the smaller dataset Mar-ket1501) and a larger threshold 0.6 (more appropriate for the larger dataset MSMT17) in Appendix C.</p><p>Data augmentation. All images are resized to 256?128. The strong data augmentation refers to random horizontal flipping, cropping, Gaussian blurring and erasing <ref type="bibr" target="#b47">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Parameter analysis</head><p>Compared to the proxy contrastive baseline, ICE brings in four more hyper-parameters, including ? h ins , ? h ins for hard instance contrastive loss and ? s ins , ? s ins for soft instance consistency loss. We analyze the sensitivity of each hyper-parameter on the Market-1501 dataset. The mAP results are illustrated in <ref type="figure" target="#fig_3">Fig. 5</ref>. As hardest positives are likely to be false positives, an overlarge ? h ins or undersized ? h ins introduce more noise. ? h ins and ? s ins balance the weight of each loss in Eq. (3). Given the results, we set ? h ins = 1 and ? s ins = 10. ? h ins and ? s ins control the similarity scale in hard instance contrastive loss and soft instance consistency loss. We finally set ? h ins = 0.1 and ? s ins = 0.4. Our hyper-parameters are tuned on Market-1501 and kept same for DukeMTMC-reID and MSMT17. Achieving state-of-the-art results simultaneously on the three datasets can validate the generalizability of these hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation study</head><p>The performance boost of ICE in unsupervised ReID mainly comes from the proposed hard instance contrastive loss and soft instance consistency loss. We conduct ablation experiments to validate the effectiveness of each loss, which is reported in Tab. 4. We illustrate the number of clusters  during the training in <ref type="figure" target="#fig_4">Fig. 6</ref> and t-SNE <ref type="bibr" target="#b31">[32]</ref> after training in <ref type="figure" target="#fig_6">Fig. 8</ref> to evaluate the compactness of clusters. We also illustrate the dynamic KL divergence of Eq. (12) to measure representation sensitivity to augmentation perturbation in <ref type="figure" target="#fig_5">Fig. 7</ref> .</p><p>Hard instance contrastive loss. Our proposed L h ins reduces the intra-class variance in a camera-agnostic manner, which increases the quality of pseudo labels. By reducing intra-class variance, a cluster is supposed to be more compact. With a same clustering algorithm, we expect to have less clusters when clusters are more compact. As shown in <ref type="figure" target="#fig_4">Fig. 6</ref>, DBSCAN generated more clusters during the training without our proposed L h ins . The full ICE framework has less clusters, which are closer to the real number of identities in the training set. On the other hand, as shown in <ref type="figure" target="#fig_6">Fig. 8</ref>, the full ICE framework has a better intra-class compactness and inter-class separability than the camera-aware baseline in the test set. The compactness contributes to bet- ter unsupervised ReID performance in Tab. 4.</p><p>Soft instance consistency loss. Hard instance contrastive loss reduces the intra-class variance between naturally captured views, while soft instance consistency loss mainly reduces the variance from artificially augmented perturbation. If we compare the blue (ICE full) and yellow (w/o soft) curves in <ref type="figure" target="#fig_5">Fig. 7</ref>, we can find that the model trained without L s ins is less robust to augmentation perturbation. The quantitative results in Tab. 4 confirms that the L s ins improves the performance of baseline. The best performance can be obtained by applying L h ins and L s ins on the camera-aware baseline.</p><p>Camera-agnostic scenario. Above results are obtained with a camera-aware memory, which strongly relies on ground truth camera ids. We further validate the effectiveness of the two proposed losses with a camera-agnostic memory, whose results are also reported in Tab. 4. Our proposed L h ins significantly improves the performance from the camera-agnostic baseline. However, L s ins should be used under low intra-class variance, which can be achieved by the variance constraints on camera styles L cross and hard samples L h ins . L h ins reduces intraclass variance, so that AA ? AP 1 ? AP 2 ? AP 3 ? 1 before augmentation in <ref type="figure" target="#fig_2">Fig. 4</ref>. L s ins permits that we still have AA ? AP 1 ? AP 2 ? AP 3 ? 1 after augmentation. However, when strong variance exists, e.g., AA ? AP 1 ? AP 2 ? AP 3 ? 1, maintaining this relationship equals maintaining intra-class variance, which decreases the ReID performance. On medium datasets (e.g., Market1501 and DukeMTMC-reID) without strong camera variance, our proposed camera-agnostic intra-class variance constraint L h ins is enough to make L s ins beneficial to ReID. On large datasets (e.g., 15 cameras in MSMT17) with strong camera variance, only camera-agnostic variance constraint L h ins is not enough. We provide the dynamic cluster numbers of camera-agnostic ICE in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with state-of-the-art methods</head><p>We compare ICE with state-of-the-art ReID methods in Tab. 5. Comparison with unsupervised method. Previous unsupervised methods can be categorized into unsupervised domain adaptation (UDA) and fully unsupervised methods. We first list state-of-the-art UDA methods, including MMCL <ref type="bibr" target="#b32">[33]</ref>, JVTC <ref type="bibr" target="#b18">[19]</ref>, DG-Net++ <ref type="bibr" target="#b51">[52]</ref>, ECN+ <ref type="bibr" target="#b50">[51]</ref>, MMT <ref type="bibr" target="#b10">[11]</ref>, DCML <ref type="bibr" target="#b0">[1]</ref>, MEB <ref type="bibr" target="#b41">[42]</ref>, SpCL <ref type="bibr" target="#b12">[13]</ref> and ABMT <ref type="bibr" target="#b2">[3]</ref>. UDA methods usually rely on source domain annotation to reduce the pseudo label noise. Without any identity annotation, our proposed ICE outperforms all of them on the three datasets.</p><p>Under the fully unsupervised setting, ICE also achieves better performance than state-of-the-art methods, including BUC <ref type="bibr" target="#b20">[21]</ref>, SSL <ref type="bibr" target="#b21">[22]</ref>, MMCL <ref type="bibr" target="#b32">[33]</ref>, JVTC <ref type="bibr" target="#b18">[19]</ref>, HCT <ref type="bibr" target="#b40">[41]</ref>, CycAs <ref type="bibr" target="#b35">[36]</ref>, GCL <ref type="bibr" target="#b3">[4]</ref>, SpCL <ref type="bibr" target="#b12">[13]</ref> and CAP <ref type="bibr" target="#b34">[35]</ref>. CycAs leveraged temporal information to assist visual matching, while our method only considers visual similarity. SpCL and CAP are based on proxy contrastive learning, which are considered respectively as camera-agnostic and cameraaware baselines in our method. With a camera-agnostic memory, the performance of ICE(agnostic) remarkably surpasses the camera-agnostic baseline SpCL, especially on Market1501 and MSMT17 datasets. With a camera-aware memory, ICE(aware) outperforms the camera-aware baseline CAP on all the three datasets. By mining hard positives to reduce intra-class variance, ICE is more robust to hard samples. We illustrate some hard examples in <ref type="figure">Fig. 9</ref>, where ICE succeeds to notice important visual clues, e.g., characters in the shirt (1st row), blonde hair (2nd row), brown shoulder bag (3rd row) and badge (4th row).  Comparison with supervised method. We further provide two well-known supervised methods for reference, including the Part-based Convolutional Baseline (PCB) <ref type="bibr" target="#b28">[29]</ref> and the joint Discriminative and Generative Network (DG-Net) <ref type="bibr" target="#b44">[45]</ref>. Unsupervised ICE achieves competitive performance with PCB. If we replace the clustering generated pseudo labels with ground truth, our ICE can be transformed into a supervised method. The supervised ICE is competitive with state-of-the-art supervised ReID methods (e.g., DG-Net), which shows that the supervised contrastive learning has a potential to be considered into future supervised ReID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel inter-instance contrastive encoding method ICE to address unsupervised ReID. Deviated from previous proxy based contrastive ReID methods, we focus on inter-instance affinities to make a model more robust to data variance. We first mine the hardest positive with mini-batch instance pairwise similarity ranking to form a hard instance contrastive loss, which effectively reduces intra-class variance. Smaller intra-class variance contributes to the compactness of clusters. Then, we use mini-batch instance pairwise similarity scores as soft labels to enhance the consistency before and after data augmentation, which makes a model robust to artificial augmentation variance. By combining the proposed hard instance contrastive loss and soft instance consistency loss,  trade-off number for Market1501, DukeMTMC-reID and MSMT17 datasets. To get a better understanding of how ICE is sensitive to the distance threshold, we vary the threshold from 0.45 to 0.6. As shown in Tab. 7, a smaller threshold 0.5 is more appreciate for the relatively smaller dataset Market1501, while a larger threshold 0.6 is more appreciate for the relatively larger dataset MSMT17. Stateof-the-art unsupervised ReID methods SpCL <ref type="bibr" target="#b12">[13]</ref> and CAP <ref type="bibr" target="#b34">[35]</ref> respectively used 0.6 and 0.5 as their distance threshold. Our proposed ICE can always outperform SpCL and CAP on the three datasets with a threshold between 0.5 and 0.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. Camera-agnostic scenario</head><p>As mentioned in the main paper, we provide the dynamic cluster numbers of camera-agnostic ICE during the training in <ref type="figure" target="#fig_7">Fig. 10</ref>. The red curve is trained without the hard instance contrastive loss L h ins as intra-class variance constraint. In this case, the soft instance consistency loss L s ins maintains high intra-class variance, e.g., AA ? AP 1 ? AP 2 ? AP 3 ? 1, which leads to less compact clusters. The orange curve is trained without L s ins , which has less clusters at the beginning but more clusters at last epochs than the blue curve. The blue curve is trained with both L h ins and L s ins , whose cluster number is most accurate among the three curves at last epochs. <ref type="figure" target="#fig_7">Fig. 10</ref> confirms that combining L h ins and L s ins reduces naturally captured and artificially augmented view variance at the same time, which gives optimal ReID performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E. Future work</head><p>Our proposed method is designed for traditional shortterm person ReID, in which persons do not change their clothes. For long-term person ReID, when persons take off or change their clothes, our method is prone to generate less robust pseudo labels, which relies on visual similarity (mainly based on cloth color). For future work, an interesting direction is to consider how to generate robust pseudo labels to tackle the cloth changing problem for long-term person ReID.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Proxy contrastive loss. Inside a cluster, an instance is pulled to a cluster centroid by Lagnostic and to cross-camera centroids by Lcross.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Comparison between triplet and hard instance contrastive loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Based on inter-instance similarity ranking between anchor (A), pseudo positives (P) and pseudo negatives (N), Hard Instance Contrastive Loss matches an anchor with its hardest positive in a mini-batch. Soft Instance Consistency Loss regularizes the interinstance similarity before and after data augmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Parameter analysis on Market-1501 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Dynamic cluster numbers during 40 training epochs on DukeMTMC-reID. "hard" and "soft" respectively denote L h ins and Ls ins. A lower number denotes that clusters are more compact.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Dynamic KL divergence during 40 training epochs on DukeMTMC-reID. Lower KL divergence denotes that a model is more robust to augmentation perturbation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>T-SNE visualization of 10 random classes in DukeMTMC-reID test set between camera-aware baseline (Left) and ICE (Right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Dynamic cluster numbers of ICE(agnostic) during 40 training epochs on DukeMTMC-reID. A lower number denotes that clusters are more compact (less intra-cluster variance).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Augmentation settings for 3 losses.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison between using the hardest negative and all negatives in the denominator of L h ins .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>4. Experiments</cell></row><row><cell>4.1. Datasets and Evaluation Protocols</cell></row><row><cell>Market-1501 [44], DukeMTMC-reID[25] and MSMT17</cell></row><row><cell>[38] datasets are used to evaluate our proposed method.</cell></row><row><cell>Market-1501 dataset is collected in front of a supermarket</cell></row><row><cell>in Tsinghua University from 6 cameras. It contains 12,936</cell></row><row><cell>images of 751 identities for training and 19,732 images of</cell></row><row><cell>750 identities for test. DukeMTMC-reID is a subset of the</cell></row><row><cell>DukeMTMC dataset. It contains 16,522 images of 702 per-</cell></row><row><cell>sons for training, 2,228 query images and 17,661 gallery</cell></row><row><cell>images of 702 persons for test from 8 cameras. MSMT17</cell></row><row><cell>is a large-scale Re-ID dataset, which contains 32,621 train-</cell></row><row><cell>ing images of 1,041 identities and 93,820 testing images</cell></row><row><cell>of 3,060 identities collected from 15 cameras. Both Cu-</cell></row><row><cell>mulative Matching Characteristics (CMC) Rank1, Rank5,</cell></row><row><cell>Rank10 accuracies and mean Average Precision (mAP) are</cell></row><row><cell>used in our experiments.</cell></row></table><note>Comparison of consistency loss. Ours refers to KL diver- gence between images with and without data augmentation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>proxy 79.3 91.5 96.8 97.6 67.3 81.4 90.8 92.9 36.4 67.8 78.7 82.5 +L h ins 80.5 92.6 97.3 98.4 68.8 82.4 90.4 93.6 38.0 69.1 79.9 83.4 +L s ins 81.1 93.2 97.5 98.5 68.4 82.0 91.0 93.2 38.1 68.7 79.8 83.7 +L h ins + L s ins 82.3 93.8 97.6 98.4 69.9 83.3 91.5 94.1 38.9 70.2 80.5 84.4</figDesc><table><row><cell>Camera-aware memory</cell><cell>Market1501 mAP R1 R5</cell><cell>DukeMTMC-reID R10 mAP R1 R5</cell><cell>MSMT17 R10 mAP R1 R5</cell><cell>R10</cell></row><row><cell>Baseline L Camera-agnostic memory</cell><cell>Market1501 mAP R1 R5</cell><cell>DukeMTMC-reID R10 mAP R1 R5</cell><cell>MSMT17 R10 mAP R1 R5</cell><cell>R10</cell></row><row><cell>Baseline L agnostic</cell><cell cols="4">65.8 85.3 95.1 96.6 50.9 67.9 81.6 86.6 24.1 52.3 66.2 71.6</cell></row><row><cell>+L h ins</cell><cell cols="4">78.2 91.3 96.9 98.0 65.4 79.6 88.9 91.9 30.3 60.8 72.9 77.6</cell></row><row><cell>+L s ins</cell><cell cols="4">47.2 66.7 86.0 91.6 36.2 50.4 70.3 76.3 17.8 38.8 54.2 60.9</cell></row><row><cell>+L h ins + L s ins</cell><cell cols="4">79.5 92.0 97.0 98.1 67.2 81.3 90.1 93.0 29.8 59.0 71.7 77.0</cell></row><row><cell cols="5">Table 4: Comparison of different losses. Camera-aware memory occupies up to 6, 8 and 15 times memory space than camera-agnostic</cell></row><row><cell cols="3">memory on Market1501, DukeMTMC-reID and MSMT17 datasets.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparison of ReID methods on Market1501, DukeMTMC-reID and MSMT17 datasets. The best and second best unsupervised results are marked in red and blue.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Figure 9 :Table 6 :Table 7 :</head><label>967</label><figDesc>Comparison of top 5 retrieved images on Market1501 between CAP [35] and ICE. Green boxes denote correct results, while red boxes denote false results. Important visual clues are marked with red dashes. 93.8 97.6 98.4 69.9 83.3 91.5 94.1 38.9 70.2 80.5 84.4 IBN-ResNet50 82.5 94.2 97.6 98.5 70.7 83.6 91.9 93.9 40.6 70.7 81.0 84.6 Comparison of ResNet50 and IBN-ResNet50 backbones on Market1501, DukeMTMC-reID and MSMT17 datasets. 93.4 97.5 98.3 68.0 82.8 91.5 93.4 36.6 69.2 79.3 82.7 0.5 83.0 94.1 97.7 98.3 69.2 82.9 91.2 93.2 38.4 69.9 80.2 83.8 0.55 82.3 93.8 97.6 98.4 69.9 83.3 91.5 94.1 38.9 70.2 80.5 84.4 0.6 81.2 93.0 97.3 98.5 69.4 83.5 91.4 94.0 39.4 70.9 81.0 84.5 Comparison of different distance thresholds on Market1501, DukeMTMC-reID and MSMT17 datasets.</figDesc><table><row><cell>Backbone</cell><cell>Market1501 mAP R1 R5</cell><cell cols="2">DukeMTMC-reID R10 mAP R1 R5</cell><cell>MSMT17 R10 mAP R1 R5</cell><cell>R10</cell></row><row><cell cols="2">ResNet50 82.3 Threshold mAP R1 Market1501 R5</cell><cell>DukeMTMC-reID R10 mAP R1 R5</cell><cell cols="2">MSMT17 R10 mAP R1 R5</cell><cell>R10</cell></row><row><cell>0.45</cell><cell>82.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">ICE significantly outperforms previous unsupervised ReID</cell></row><row><cell></cell><cell></cell><cell cols="4">methods on Market1501, DukeMTMC-reID and MSMT17</cell></row><row><cell></cell><cell></cell><cell>datasets.</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work has been supported by the French government, through the 3IA C?te d'Azur Investments in the Future project managed by the National Research Agency (ANR) with the reference number ANR-19-P3IA-0002. The authors are grateful to the OPAL infrastructure from Universit? C?te d'Azur for providing resources and support.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices Appendix A. Algorithm Details</head><p>The ICE algorithm details are provided in Algorithm 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Backbone Network</head><p>Instance-batch normalization (IBN) <ref type="bibr" target="#b23">[24]</ref> has shown better performance than regular batch normalization in unsupervised domain adaptation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b12">13]</ref> and domain generalization <ref type="bibr" target="#b16">[17]</ref>. We compare the performance of ICE with ResNet50 and IBN-ResNet50 backbones in Tab. 6. The performance of our proposed ICE can be further improved with an IBN-ResNet50 backbone network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Threshold in clustering</head><p>In DBSCAN <ref type="bibr" target="#b8">[9]</ref>, the distance threshold is the maximum distance between two samples for one to be considered as in the neighborhood of the other. A smaller distance threshold is likely to make DBSCAN mark more hard positives as different classes. On the contrary, a larger distance threshold makes DBSCAN mark more hard negatives as same class.</p><p>In the main paper, the distance threshold for DBSCAN between same cluster neighbors is set to 0.55, which is a</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep credible metric learning for unsupervised domain adaptation person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning discriminative and generalizable representations by spatial-channel partition for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Lagadec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Bremond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enhancing diversity in teacher-student networks via asymmetric branches for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Lagadec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Bremond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV, 2021</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Joint generative and contrastive learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohui</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antitza</forename><surname>Lagadec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Dantcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bremond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Instanceguided context rendering for cross-domain person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-similarity grouping: A simple unsupervised cross domain adaptation approach for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mutual meanteaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-supervising fine-grained region similarities for large-scale image localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-paced contrastive learning with hybrid memory for domain adaptive object re-id</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Frustratingly easy person re-identification: Generalizing person reid in practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieru</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint visual and temporal consistency for unsupervised domain adaptive person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-task mid-level feature alignment network for unsupervised cross-dataset person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Tsun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Chichung</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In BMVC</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A bottom-up clustering approach to unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification via softened similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bag of tricks and a strong baseline for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Hao Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Two at once: Enhancing learning and generalization capacities via ibn-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fixmatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptive re-identification: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangchen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lefei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>PR</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1807.03748</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Visualizing data using t-SNE. JMLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised person reidentification via multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongkai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Transferable joint attribute-identity deep learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Xiaojin Gong, and Xian-Sheng Hua. Camera-aware proxies for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baisheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Cycas: Selfsupervised cycle association for learning re-identifiable descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Co2: Consistent contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification by soft multilabel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hierarchical clustering with hard-batch triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munan</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multiple expert brainstorming for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxi</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Self-training with progressive augmentation for unsupervised cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiewei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Joint discriminative and generative learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Reranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Generalizing a person retrieval model hetero-and homogeneously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Invariance matters: Exemplar memory for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning to adapt invariance in memory for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Joint disentangling and adaptation for cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V K</forename><surname>Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
