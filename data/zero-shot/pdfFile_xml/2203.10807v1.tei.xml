<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ViM: Out-Of-Distribution with Virtual-logit Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-21">21 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Wang</surname></persName>
							<email>wanghaoqi@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="institution">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Li</surname></persName>
							<email>lizz@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="institution">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Litong</forename><surname>Feng</surname></persName>
							<email>fenglitong@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="institution">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
							<email>wayne.zhang@sensetime.com</email>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Qing Yuan Research Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ViM: Out-Of-Distribution with Virtual-logit Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-21">21 Mar 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most of the existing Out-Of-Distribution (OOD) detection algorithms depend on single input source: the feature, the logit, or the softmax probability. However, the immense diversity of the OOD examples makes such methods fragile. There are OOD samples that are easy to identify in the feature space while hard to distinguish in the logit space and vice versa. Motivated by this observation, we propose a novel OOD scoring method named Virtual-logit Matching (ViM), which combines the class-agnostic score from feature space and the In-Distribution (ID) class-dependent logits. Specifically, an additional logit representing the virtual OOD class is generated from the residual of the feature against the principal space, and then matched with the original logits by a constant scaling. The probability of this virtual logit after softmax is the indicator of OOD-ness. To facilitate the evaluation of large-scale OOD detection in academia, we create a new OOD dataset for ImageNet-1K, which is human-annotated and is 8.8? the size of existing datasets. We conducted extensive experiments, including CNNs and vision transformers, to demonstrate the effectiveness of the proposed ViM score. In particular, using the BiT-S model, our method gets an average AUROC 90.91% on four difficult OOD benchmarks, which is 4% ahead of the best baseline. Code and dataset are available at https://github.com/haoqiwang/vim.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Considering most deep image classification models are trained in the closed-world setting, the out-of-distribution (OOD) issue arises and deteriorates customer experience when the models are deployed in production, facing inputs coming from the open world <ref type="bibr" target="#b8">[9]</ref>. For instance, a model may wrongly but confidently classify an image of crab into the clapping class, even though no crab-related concepts appear in the training set. OOD detection is to decide whether an input belongs to the training distribution. OOD detection complements classification and finds its application in fields such as autonomous driving <ref type="bibr" target="#b19">[19]</ref>, medical analysis <ref type="bibr" target="#b30">[30]</ref> and industrial inspection <ref type="bibr" target="#b0">[1]</ref>. A comprehensive review of OOD and related topics including open set recognition, novelty detection and anomaly detection can be found in <ref type="bibr" target="#b38">[38]</ref>.</p><p>The core of an OOD detector is a scoring function ? that maps an input feature x to a scalar in R, indicating to what extent the sample is likely to be OOD. In testing, a threshold ? is decided, ensuring that the validation set retains at least a given true-positive rate (TPR), e.g. the typical value of 0.95. The input example is regarded as OOD if ?(x) &gt; ? and as ID (i.e., in-distribution) otherwise. In cases where a score indicating the ID-ness is convenient, we can mentally use the negative of OOD score as the ID score.</p><p>Researchers have designed quite a few scoring functions by seeking properties that are naturally held by ID examples and easily violated by OOD examples, or vice versa. Scores are mainly derived from three sources: <ref type="bibr" target="#b0">(1)</ref> the probability, such as the maximum softmax probabilities <ref type="bibr" target="#b13">[13]</ref>, the minimum KL-divergence between the softmax and the mean class-conditional distributions <ref type="bibr" target="#b11">[12]</ref>; <ref type="bibr" target="#b1">(2)</ref> the logit, such as the maximum logits <ref type="bibr" target="#b11">[12]</ref>, the logsumexp function over logits <ref type="bibr" target="#b25">[25]</ref>; and (3) the feature, such as the norm of the residual between feature and the pre-image of its low-dimensional embedding <ref type="bibr" target="#b27">[27]</ref>, the minimum Mahalanobis distance between the feature and the class centroids <ref type="bibr" target="#b23">[23]</ref>, etc. In these methods, OOD scores can be directly computed from existing models without re-training, making the deployment effortless. However, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, their performances are limited by the singleness of their information source: using features exclusively disregards the classification weights with class-dependent information; using the logit or the softmax solely misses feature variations in the null space <ref type="bibr" target="#b2">[3]</ref>, which carries class-agnostic information; and the softmax further discards the norm of logits. To cope with the immense diversity that manifests in OOD samples, we ask the question, is it helpful to design an OOD score that utilizes multiple sources?</p><p>Built upon the success of prior arts, we design a novel scoring function termed the Virtual-logit Matching (ViM) score, which is the softmax score of a constructed virtual OOD class whose logit is jointly determined by the feature and the existing logits. To be specific, the scoring function first extracts the residual of the feature against a principal subspace, and then converts it to a valid logit by matching its mean over training samples to the average maximum logits. Finally, the softmax probability of the devised OOD class is the OOD score. From the construction of ViM, we can see intuitively that the smaller the original logits and the greater the residual, the more likely it is to be OOD.</p><p>Different from the aforementioned methods, another line of works tailors the features learned by the network to better identify ID and OOD by imposing dedicated regularization losses <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b40">40]</ref> or by exposing generated or real collected OOD samples <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b37">37]</ref>. As they all require the retraining of the network, we briefly mention them here and will not delve into the details.</p><p>Recently, OOD detection in large-scale semantic space has attracted increasing attention <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b29">29]</ref>, advancing OOD detection methods toward real-world applications. However, the current shortage of clean and realistic OOD datasets for large-scale ID datasets becomes an impediment to the field. Previous OOD datasets were curated from public datasets which were collected with a predefined tag list, such as iNaturalist, Texture, and ImageNet-21k (Tab. 1). This may lead to a biased performance comparison, specifically, the hackability of small coverage as described in Sec. 5. To avoid this risk, we build a new OOD benchmark for ImageNet-1K <ref type="bibr" target="#b3">[4]</ref> models, OpenImage-O, from OpenImage dataset <ref type="bibr" target="#b21">[21]</ref> with natural class distribution. It contains 17,632 manually filtered images, and is 7.8? larger than the recent ImageNet-O <ref type="bibr" target="#b15">[15]</ref> dataset.</p><p>We extensively evaluate our method on various models using ImageNet-1K as the ID dataset. The model archi-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Image Distribution #Image Labeling Method</head><p>OpenImage-O natural class statistics 17, 632 image-level manual Texture <ref type="bibr" target="#b1">[2]</ref> predefined tag list 5, 160 tag-level manual iNaturalist <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b34">34]</ref>  tectures range from the classical ResNet-50 <ref type="bibr" target="#b10">[11]</ref>, to the recent BiT <ref type="bibr" target="#b20">[20]</ref>, and to the latest ViT-B16 <ref type="bibr" target="#b7">[8]</ref>, RepVGG <ref type="bibr" target="#b6">[7]</ref>, DeiT <ref type="bibr" target="#b33">[33]</ref> and Swin Transformer <ref type="bibr" target="#b26">[26]</ref>. From the results on four OOD datasets, including OpenImage-O, ImageNet-O, Texture, and iNaturalist, we found that model selection affected the performance of many baseline methods, while our method performs stably well. Specially, our method achieved an average AUROC of 90.91% using the BiT model, which greatly surpasses the best baseline whose average AUROC is 86.62%.</p><p>Our contributions are threefold. (1) We proposed a novel OOD detection method ViM, that works well for a large range of models and datasets, owing to the effective fusion of information from both features and logits. The method is lightweight and fast, requiring neither extra OOD data nor re-training. (2) We conducted comprehensive experiments and ablation studies on the ImageNet-1K dataset, including CNNs and vision transformers. (3) We curated a new OOD dataset for ImageNet-1K called OpenImage-O, which is very diverse and contains complex scenes. We believe it will facilitate research on large-scale OOD detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>OOD/ID Score Design Hendrycks et al. <ref type="bibr" target="#b13">[13]</ref> presented a baseline method using the maximum predicted softmax probability (MSP) as the ID score. ODIN <ref type="bibr" target="#b24">[24]</ref> enhances MSP by perturbing the inputs and rescaling the logits. Hendrycks et al. <ref type="bibr" target="#b11">[12]</ref> also experimented with the MaxLogit and the KL matching method on the ImageNet dataset. The energy score <ref type="bibr" target="#b25">[25]</ref> computes the logsumexp on logits, and ReAct <ref type="bibr" target="#b32">[32]</ref> strengthens the energy score by feature clipping. In <ref type="bibr" target="#b27">[27]</ref> the norm of the difference between the feature and the pre-image of its low-dimensional manifold embedding is used. Lee et al. <ref type="bibr" target="#b23">[23]</ref> computes the minimum Mahalanobis distance between the feature and the class-wise centroids. NuSA <ref type="bibr" target="#b2">[3]</ref> uses the ratio of the norm of feature projected onto the column space of the classification weight matrix to the original norm as the ID score. The gradients are also used as evidence for ID and OOD distinction in <ref type="bibr" target="#b17">[17]</ref>. For methods using logits/probabilities, feature variations on the null space of the weight matrix are completely ignored; while for methods that operate on the features space, the class-dependent information on weight matrix is dropped. Our method combines the strengths of feature-based scores and logit-based scores by the novel mechanism of virtual logit, and gets substantial improvements.</p><p>Network/Loss Design Many works redesign the training loss to be OOD-aware <ref type="bibr" target="#b4">[5]</ref> or add regularization terms <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b40">40]</ref> to push part ID/OOD features. DeVries et al. <ref type="bibr" target="#b4">[5]</ref> augment the network with a confidence estimation branch that uses misclassified in-distribution examples as a proxy for out-of-distribution examples. MOS <ref type="bibr" target="#b18">[18]</ref> modifies the loss to use the pre-defined group structure so that the minimum group-wise "else" class probability can indicate the OODness. Zaeemzadeh et al. <ref type="bibr" target="#b40">[40]</ref> forces the ID samples to embed into a union of 1-dimensional subspaces during training and computes the minimum angular distance from the feature to the class-wise subspaces. Generalized ODIN <ref type="bibr" target="#b16">[16]</ref> uses a dividend/divisor structure to encode the prior knowledge of decomposing the confidence of class probability. Different from these methods, our method does not require model retraining, thus not only is it easier to apply, but the ID classification accuracy is also preserved.</p><p>OOD Data Exposure Outlier Exposure <ref type="bibr" target="#b14">[14]</ref> utilizes an auxiliary OOD dataset to improve OOD detection. Dhamija et al. <ref type="bibr" target="#b5">[6]</ref> regularize samples from extra background classes to have uniform logits and to have small feature norms. Lee et al. <ref type="bibr" target="#b22">[22]</ref> use GAN to generate OOD samples that lie near the ID samples and push the prediction of OOD samples to the uniform distribution. Several methods, including MCD <ref type="bibr" target="#b39">[39]</ref>, NGC <ref type="bibr" target="#b36">[36]</ref> and UDG <ref type="bibr" target="#b37">[37]</ref>, can utilize external unlabeled noisy data to enhance the OOD detection performances. Different from these methods, our method does not require additional OOD data and thus avoids biases towards the introduced OOD samples [31].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Motivation: The Missing Info in Logits</head><p>For a series of OOD detection methods that are based on logits or softmax probabilities, we find that their performances are limited. In <ref type="figure" target="#fig_0">Fig. 1</ref>, feature-based OOD scores such as Mahalanobis and Residual are good at detecting OOD in ImageNet-O, while all methods that are based on logit/probability lag behind. This is not an accident, as is again shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. The AUROC of the state-of-the-art probability-based method KL Matching is still lower than straightforwardly designed OOD scores in feature space on Texture dataset. This motivates us to study the influence of the lost information going from features to logits.</p><p>Consider a C-class classification model whose logit l ? R C is transformed from the feature x ? R N by a fully connected layer with weight W ? R N ?C and bias b ? R C , i.e. l = W T x + b. The predicted probability is p(x) = softmax(l). For convenience, we set the point o := ?(W T ) + b, where (?) + is the Moore-Penrose inverse, as the origin of a new coordinate system of feature space,</p><formula xml:id="formula_0">l = W T x ? = W T (x ? o), ?x.<label>(1)</label></formula><p>Geometrically, each logit l i is the inner product between the feature x ? and the class vector w i (the i-th column of W ). Later when generalizing logits to virtual logits, we will replace w i with a subspace, and replace the inner product with a projection. The bias term is safely omitted in the new coordinate system. In the remaining part of the paper, we assume the feature space uses the new coordinate system. Logits contain class-dependent information, yet there is class-agnostic information in feature space that is not recoverable from logits. We study two cases (null space and principal space) and discuss the two OOD scores (NuSA and Residual) that rely on them, respectively.</p><p>OOD Score Based on Null Space A feature x can be de- </p><formula xml:id="formula_1">composed into x = x W ? + x W ,</formula><formula xml:id="formula_2">W T x W ? = 0.</formula><p>The component x W ? does not affect classification, but it influences OOD detection. It is demonstrated in <ref type="bibr" target="#b2">[3]</ref> that one can perturb an image intensely yet constrain the difference between the features in W ? . The resulting outlier images are not like any of the ID images but retains high confidence in classification. Taking advantage of this, they define an ID score NuSA (null space analysis) as</p><formula xml:id="formula_3">NuSA(x) = x 2 ? x W ? 2 x .<label>(2)</label></formula><p>Intuitively, NuSA uses the angle (= arccos(NuSA(x))) between x and W to indicate the OOD-ness. From <ref type="figure" target="#fig_1">Fig. 2</ref> we can see that the simple angle information clearly distinguishes OOD examples in Texture with an AUROC 95.50%, surpassing methods based on logits and the competitive method KL Matching based on softmax probability.</p><p>OOD Score Based on Principal Space It is generally assumed that features lie in low-dimensional manifolds <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b40">40]</ref>. For simplicity, we use linear subspace (in the new coordinate system) passing through the origin o as the model. We define the principal space as the D-dimensional subspace P spanned by eigenvectors of the largest D eigenvalues of the matrix X T X, where X is the ID data matrix.</p><p>Features that deviate from the principal space are likely to be OOD examples. We can define</p><formula xml:id="formula_4">Residual(x) = x P ? ,<label>(3)</label></formula><p>to capture the deviation of features from the principal space. Here x = x P + x P ? and x P ? is the projection of x to P ? . The residual score is similar to the reconstruction error in <ref type="bibr" target="#b27">[27]</ref> except that they employ nonlinear manifold learning for dimension reduction. Note that after the projection onto logits, this deviation is corrupted since the matrix W T projects to a lower dimensional space than the feature space. <ref type="figure" target="#fig_1">Fig. 2</ref> shows that Residual score improves over the NuSA score on both datasets, making the performance contrast between feature-based methods with logit/probability-based methods more striking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fusing Class-dependent and Class-agnostic Information</head><p>In contrast to methods on logit/probability, both the NuSA and the Residual do not consider information that is specific to individual ID classes, namely they are class-agnostic. As a consequence, these scores ignore the feature similarity to each ID class, and are ignorant about which class the input resembles most. This gives an explanation of their worse performance on the iNaturalist OOD benchmark, as iNaturalist samples need to distinguish subtle differences between fine-grained classes. We hypothesize that unifying the information from feature space and the logits could improve the detection performance on a broader type of OOD samples. Such a solution is presented in Sec. 4 using the concept of virtual logit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Virtual-logit Matching</head><p>To unify the class-agnostic and class-dependent information for OOD detection, we propose an OOD score by Virtual-logit Matching, abbreviated as ViM. The pipeline is illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>, where there are three steps, operating at the feature, the logit, and the probability, respectively. To be specific, for feature x, (1) extract the residual x P ? of x against the principal subspace P ; (2) convert the norm x P ? to a virtual logit by rescaling; and (3) output the softmax probability of the virtual logit as the ViM score. Below we give more details. Recall the notations: C is the number of classes, N is the feature dimension, and W and b are the classification weight and bias, respectively.</p><p>Principal Subspace and Residual Firstly we offset the feature space by a vector o = ?(W T ) + b so that it is biasfree in the computation of logits as Eq. (1). The principal subspace P is defined by the training set X, where rows are features in the new coordinate system with origin o. Suppose the eigendecomposition on the matrix X T X is</p><formula xml:id="formula_5">X T X = Q?Q ?1 ,<label>(4)</label></formula><p>where eigenvalues in ? are sorted decreasingly, then the span of the first D columns is the D-dimensional principal subspace P . The residual x P ? is the projection of x onto P ? , Let the (D + 1)-th column to the last column of Q in Eq.</p><formula xml:id="formula_6">(4) be a new matrix R ? R N ?(N ?D) , then x P ? = RR T x.</formula><p>The residual x P ? is sent to the next step.</p><p>Virtual-logit Matching The virtual logit</p><formula xml:id="formula_7">l 0 := ? x P ? = ? ? x T RR T x<label>(5)</label></formula><p>is the norm of the residual rescaled by a per-model constant ?. The norm x P ? cannot be used as a new logit directly since the latter softmax will normalize over the exponential of logits and thus is very sensitive to the scale of logits. If the residual is very small compared to the largest logit, then after the softmax the residual will be buried in the noise of logits. To match the scales of the virtual logit, we compute the average norm of the virtual logit on the training set and also the mean of the maximum logit on the training set, then</p><formula xml:id="formula_8">? := K i=1 max j=1,...,C {l i j } K i=1 x P ? i ,<label>(6)</label></formula><p>where x 1 , x 2 , . . . , x K are uniformly sampled K training examples, and l i j is the j-th logit of x i . In this way, on average, the scale of the virtual logit is the same as the maximum of the original logits.  <ref type="formula" target="#formula_8">6)</ref>. In inference, feature x is computed by the network, and the virtual logit ? x P ? is computed by projection and scaling. After softmax, the probability corresponding to the virtual logit is the OOD score. It is OOD if the score is larger than threshold ? .</p><p>The ViM Score We append the virtual logit to the original logits and compute the softmax. The probability corresponding to the virtual logit is defined as ViM. Mathematically, let the i-th logit of x be l i , and then the score is</p><formula xml:id="formula_9">ViM(x) = e ? ? x T RR T x C i=1 e li + e ? ? x T RR T x .<label>(7)</label></formula><p>This equation reveals that two factors affect the ViM score: if its original logits are larger, then it is less of an OOD example; while if the norm of residual is larger, it is more likely to be OOD. The computational overhead is comparable to the last fully-connected layer (mapping from feature to logit) in the classification network, which is small.</p><p>Connection to Existing Methods Note that applying a strictly increasing function to the scores does not affect the OOD evaluation. Apply the function t(x) = ? ln 1 x ? 1 to the ViM score, then we have an equivalent expression</p><formula xml:id="formula_10">? x P ? ? ln C i=1 e li .<label>(8)</label></formula><p>The first term is the virtual logit in Eq. (5) while the second term is the energy score <ref type="bibr" target="#b25">[25]</ref>. ViM completes the energy method by feeding extra residual information from features. The performance is much superior to energy and residual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">OpenImage-O Dataset</head><p>We build a new OOD dataset called OpenImage-O for the ID dataset ImageNet-1K. It is manually annotated, comes with a naturally diverse distribution, and has a large scale with 17,632 images. It is built to overcome several shortcomings of existing OOD benchmarks. OpenImage-O is selected image-by-image from the test set of OpenImage-V3, including 125,436 images collected from Flickr without a predefined list of class names or tags, leading to natural class statistics and avoiding an initial design bias.</p><p>Necessity for Image-Level Annotation Some previous works on large-scale OOD detection select a portion of other datasets solely based on class labels. While classlevel annotation costs less, the resulting dataset might be much noisier than expected. For example, the Places and the SUN dataset selected by <ref type="bibr" target="#b18">[18]</ref> have a large portion of images that are indistinguishable from ID samples. Another example is the Texture <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">18]</ref>, in which the bubbly texture overlaps with the bubble class in ImageNet. Thus creating OOD datasets by querying tags is not reliable and per-image human inspection is needed for the confirmation of validity.</p><p>Hackability of Small Coverage If the OOD dataset has a central topic such as the Texture, featuring a less diverse distribution, then it might be easy to be "hacked". In Tab. 2, the gap between the highest and the average AUROC over nine methods for BiT are: OpenImage-O 5.61, iNaturalist 6.06, Texture 10.52, and ImageNet-O 14.39. Having larger gaps implies that the dataset is easier to improve.</p><p>Construction Process of OpenImage-O We construct the OpenImage-O based on the OpenImage-v3 dataset <ref type="bibr" target="#b21">[21]</ref>. For every image in its testing set, we let human labelers to determine whether it is an OOD sample. To assist labeling, we simplified the task as distinguishing the image from the top-10 categories predicted by an ImageNet-1K classification model, i.e., the image is OOD if it does not belong to any of the 10 categories. Category labels as well as the most similar image to the test image in each category, measured by cosine similarity in the feature space, were presented for visualization. To further improve the annotation quality, we design several schemes: (1) Labelers can choose "Difficult", if they cannot decide whether the image belongs to any of the 10 categories; (2) Each image was labeled by at least two labelers independently, and we took the set of OOD images having consensus from the two; (3) Random inspection was performed to guarantee the quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiment</head><p>In this section, we compare our algorithm with state-ofthe-art OOD detection algorithms. Following the prior work on large-scale OOD detection, we choose ImageNet-1K as the ID dataset. We benchmark the algorithms using both the CNN-based and the transformer-based models. Detailed experimental settings are as follows.</p><p>OOD Datasets Four OOD datasets (Tab. 1) are used to comprehensively benchmark the algorithms. OpenImage-O is our newly collected large-scale OOD dataset. Texture <ref type="bibr" target="#b1">[2]</ref> consists of natural textural images and we removed four categories (bubbly, honeycombed, cobwebbed, spiralled) that overlapped with ImageNet. iNaturalist <ref type="bibr" target="#b34">[34]</ref> is a fine-grained species classification dataset. We use the subset from <ref type="bibr" target="#b18">[18]</ref>. Images in ImageNet-O <ref type="bibr" target="#b15">[15]</ref> are adversarially filtered so that they can fool OOD detectors.</p><p>Evaluation Metrics Two commonly used metrics are reported. The AUROC is a threshold-free metric that computes the area under the receiver operating characteristic curve. Higher value indicates better detection performance. FPR95 is short for FPR@TPR95, which is the false positive rate when the true positive rate is 95%. The smaller FPR95 the better. We report both their numbers in percentage. <ref type="bibr" target="#b20">[20]</ref> is a variant of ResNet-v2, which employs group normalization and weight standardization. The BiT-S model series is pretrained on ImageNet-1K, and we take the officially released checkpoint of BiT-S-R101?1 for experiments. ViT (Vision Transformer) <ref type="bibr" target="#b7">[8]</ref> is a transformer-based image classification model which treats images as sequences of patches. We use the officially released ViT-B/16 model, which is pretrained on ImageNet-21K and fine-tuned on ImageNet-1K. Since the compared algorithms do not require re-training, the ID accuracies are not affected. Results on more model architectures, including CNN-based RepVGG <ref type="bibr" target="#b6">[7]</ref>, ResNet-50d <ref type="bibr" target="#b10">[11]</ref>, and transformer based Swin <ref type="bibr" target="#b26">[26]</ref> and DeiT <ref type="bibr" target="#b33">[33]</ref>, are listed in Sec. 6.3. Their pre-trained weights are obtained from the timm repo <ref type="bibr" target="#b35">[35]</ref>. When estimating the principal space, K = 200, 000 images are randomly sampled from the training set. For features spaces with dimension N &gt; 1500, we set the dimension of principal space to D = 1000, and set D = 512 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Settings BiT (Big Transfer)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Methods</head><p>We compare ViM with eight baselines that do not require fine-tuning. They are MSP <ref type="bibr" target="#b13">[13]</ref>, Energy <ref type="bibr" target="#b25">[25]</ref>, ODIN <ref type="bibr" target="#b24">[24]</ref>, MaxLogit <ref type="bibr" target="#b11">[12]</ref>, KL Matching <ref type="bibr" target="#b11">[12]</ref>, Residual, ReAct <ref type="bibr" target="#b32">[32]</ref> and Mahalanobis <ref type="bibr" target="#b23">[23]</ref>. For Mahalanobis, we followed the setting in <ref type="bibr" target="#b9">[10]</ref>, which uses only the final feature instead of an ensemble of multiple layers <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b23">23]</ref>. For ReAct, we use the Energy+ReAct setting with rectification percentile p = 99. The Residual is defined in Eq. (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Results on BiT</head><p>We present the results of the BiT model at the first half of Tab. 2. The best AUROC is shown in bold and the second and third place ones are shown with underlines.</p><p>ViM vs. Baselines On three datasets, including OpenImage-O, Texture, and ImageNet-O, ViM achieves the largest AUROC and the smallest FPR95. On average ViM has 90.91% AUROC, which surpasses the second place by 4.29%. The average FPR95 is also the lowest among them. In particular, regarding Eq. <ref type="formula" target="#formula_10">(8)</ref>, an interpretation of ViM in terms of the Residual score and the Energy score, the results show that ViM is significantly better than the two methods on all datasets. This indicates that ViM non-trivially combined the OOD information in Residual and in Energy. However, on iNaturalist, ViM is only on the third place. We hypothesize that its moderate performance on iNaturalist relates to how much information is contained in the residual, because iNaturalist has the smallest average residual norm among four OOD datasets (iNaturalist 4.65, OpenImage-O 5.04, ImageNet-O 5.16, and Texture 8.16).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Information Source</head><p>For OOD detection performances on BiT model, Tab. 2 shows an interesting pattern regarding the information source. If feature variations in the null space are absent, such as in methods that rely on logits and softmax, performances on Texture and ImageNet-O are restricted. For example, on the Texture dataset, the best performing method that relies on logit and softmax is KL Matching, which has 86.92% AUROC and is far behind ViM, Mahalanobis, and Residual, which operate on the feature space. In contrast, if the class-dependent information is dropped, such as in the Residual method, performances in iNaturalist and OpenImage-O are also limited. The proposed ViM score, however, is competent regardless of dataset types. <ref type="bibr" target="#b9">[10]</ref> has discussed the benefit of large-scale pre-trained transformers on OOD tasks. However, their experiments are conducted on CIFAR100/10 and only two baseline methods are compared. We provide a comprehensive OOD evaluation on ImageNet-1K over a wide range of methods in the second half of Tab. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Results on ViT</head><p>ViM vs. Baselines The two best-performing methods for the ViT model are ViM and Mahalanobis. Their AU-  <ref type="table">Table 3</ref>. Results on RepVGG, ResNet50-d, Swin and DeiT. Due to space limitation, only their average AUROC (A?) and average FPR95 (F?) are reported. The numbers are in percentage. All models are using pre-trained weights taken from timm <ref type="bibr" target="#b35">[35]</ref>.</p><p>ROCs are close on all four datasets. However, Mahalanobis needs to compute the class-wise Mahalanobis distance, which makes its computation costly. In contrast, our method is lightweight and fast. Four methods, ReAct, Energy, MaxLogit, and ODIN, are the second best ones, and the remaining three methods have relatively low AUROCs.</p><p>Difference between ViT and BiT Since the ViT model is pre-trained on the ImageNet-21K dataset, the semantics it has seen is much larger than the BiT model. The OOD performance is relatively saturated. Although on most OOD datasets ViT is significantly better than BiT, we observe that ViT performs less competitively on the Texture dataset. We hypothesize that it is related to the observation in <ref type="bibr" target="#b28">[28]</ref> that higher layers of ViT maintain spatial location information more faithfully than ResNets. ViT has high responses for local patches. However, textural images with similar local patches but not revealing the whole object are regarded as OOD of ImageNet (see example images in <ref type="figure" target="#fig_1">Fig. 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Results on More Model Architectures</head><p>We show more results on a variety of model architectures. In particular, we choose two CNN-based models RepVGG <ref type="bibr" target="#b6">[7]</ref> and ResNet-50d <ref type="bibr" target="#b10">[11]</ref> and two transformerbased models Swin Transformer <ref type="bibr" target="#b26">[26]</ref> and DeiT <ref type="bibr" target="#b33">[33]</ref>. Their average AUROCs and average FPR95s over the four OOD datasets are listed in Tab. 3. It is shown that ViM is robust to model architecture changes. The detailed experiment setting and results are in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">The Effect of Hyperparameter</head><p>The Dimension D of Principal Space In <ref type="bibr" target="#b40">[40]</ref> the feature of each class is represented by a 1-dimensional subspace, so a natural choice for the dimension D of principal space  is the number of classes C. For models like ViT whose feature dimension N may be less than the number of classes C, we empirically suggest taking a number in the range [N/3, 2N/3]. We show in <ref type="figure" target="#fig_3">Fig. 4</ref> that our method is robust to the selection of dimensions. However, if the application permits, one can adjust this parameter according to a holdout OOD dataset. In our experiments, we set D = 1000 for BiT and D = 512 for ViT.</p><p>The Matching Parameter ? The matching parameter controls the relative importance of the trade-off between different OOD features. Since OOD distribution is unknown, we suggest keeping them to be of equal importance. This is how ? is defined in Eq. <ref type="bibr" target="#b5">(6)</ref>. It is easy to tune the parameter to fit some types of OOD datasets, but it is hard to improve all datasets at the same time. We show the result of perturbing the matching parameter by multiplying a factor in <ref type="figure" target="#fig_4">Fig. 5</ref>. If the multiple is larger, then information from the feature space is given more weight. Otherwise, information from logits is given more importance. Overall the best choice is no perturbation, suggesting that the defined ? is a good choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">The Effect of Grouping</head><p>In addition, we also compare with MOS <ref type="bibr" target="#b18">[18]</ref>, which exploits grouping structure in large-scale semantic spaces. Two methods are added to the comparison. (1) MaxGroup is the group version of MSP, which first obtains the groupwise probability by summing over the constituent classes, and then takes the maximum group probability as the ID  <ref type="table">Table 4</ref>. AUROC of methods with grouping information. A? is AUROC and F? is FPR95. All numbers are in percentage. BiT is used and the grouping is defined in <ref type="bibr" target="#b18">[18]</ref> based on taxonomy. * MOS needs fine-tuning while others do not.</p><p>score.</p><p>(2) ViM+Group also takes the maximum group probability as the ID score, except that the probabilities are taken from the (C + 1) dimensional vector, with an extra ViM virtual class participating in the softmax normalization.</p><p>MaxGroup and ViM+Group are evaluated on the pretrained weights of BiT, while MOS needs to fine-tune the model using group-based learning. Results are shown in Tab. 4. We observe that (1) the average AUROC of Max-Group improves over the vanilla MSP from 77.25% to 79.23%, showing the usefulness of group information; and (2) both our original ViM and the group version of ViM are better than MOS on three of four datasets by large margins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.">Limitation of ViM</head><p>As we have noticed in Sec. 6.1, ViM shows less performance gains on OOD datasets that have small residuals, such as iNaturalist. Besides, the property that ViM does not need training is a double-edged sword. It means that ViM is limited by the feature quality of the original network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we present a novel OOD detection method: the Virtual-logit Matching (ViM) score. It combines the information from both the feature space and the logits, which provides the class-agnostic information and the classdependent information, respectively. Extensive experiments on the large-scale OOD benchmarks show the effectiveness and robustness of the method. Especially, we tested ViM on both CNN-based models and transformer-based models, showing its robustness across model architectures. To facilitate the evaluation of large-scale OOD detection, we create the OpenImage-O dataset for ImageNet-1K, which is of high-quality and large-scale. <ref type="figure">Figure 6</ref>. A demonstrative UI for the labelers. The image on the left-top corner is the candidate OOD image to be labeled. The two rows of images below are from the 10 most similar ID classes. Labelers choose from yes/difficult/no according to these information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Detailed Information of Models (Sec. 6)</head><p>In the experiment, we benchmarked a collection of deep classification models. Their detailed information, including the specification, the architecture, the pre-train information, and the top-1 accuracy, is listed in Tab. <ref type="bibr" target="#b4">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Details on OpenImage-O (Sec. 5)</head><p>An illustrative software interface for labelers is shown in <ref type="figure">Fig. 6</ref>. For each candidate OOD image to be labeled, we find the top 10 classes in ImageNet-1K predicted by a classification model. Then we gather the most similar images in those top 10 classes by cosine similarity in the feature space. Next, we patch them as well as their labels with the corresponding OpenImage samples, and let the labelers distinguish whether the OpenImage sample belongs to any of the top 10 categories. We also set a choice called difficult, so that labelers can put the undistinguishable hard samples into the difficult category. To reduce annotation noises, each image is labeled twice from different group of labelers. Then we take the set of OOD images having consensus from the two groups, resulting in an OOD dataset with 17,632 unique images. In the end, a random inspection process is performed to guarantee the quality of the OOD dataset.</p><p>The OpenImage-O follows a natural image distribution as both the source dataset and the labeling process do not involve any filtration based on pre-defined list of labels. To get a sense of its distribution, we use the BiT model to find the most similar ID class in ImageNet for each OOD image. Then the histogram is illustrated in <ref type="figure">Fig. 7</ref>. It shows that the coverage of OpenImage-O is broader compared to the other three OOD datasets. D. Details on Grouping (Sec. 6.5) MOS <ref type="bibr" target="#b18">[18]</ref> is trained using the officially released code and its default parameter setting. For all experiments in Sec. 6.5, the grouping strategy follows the taxonomy grouping defined in <ref type="bibr" target="#b18">[18]</ref>.</p><p>Grouping Results on ViT The grouping strategy is less effective for the ViT model, as seen from results in Tab. 7. Comparing MSP with its group version, MaxGroup, we can see that the improvement on AUROC is very small, while FPRs become even worse. Examining ViM with its group variant ViM+Group, we can see that their difference is very small, and the original version of ViM is slightly better than ViM+Group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Details on Baselines (Sec. 6)</head><p>Mahalanobis On the BiT model, when including lower level features, the performance of Mahalanobis degrades a lot. The average AUROC on the four OOD datasets is 56%, which is much worse than the baseline MSP. Similar results is also found in [18, <ref type="table" target="#tab_0">Table 1</ref>]. In this paper, we implement the Mahalanobis score using the feature vector before the final classification fc layer, as in <ref type="bibr" target="#b9">[10]</ref>. The precision matrix and the class-wise average vector are estimated using 200,000 random training samples. The ground-truth class label is used during computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KL Matching</head><p>We estimate the class-wise average probability using 200,000 random training samples. Following the practice of <ref type="bibr" target="#b11">[12]</ref>, the predicted class is used instead of ground-truth labels. We would like to note that the hyperparameter selection for OOD methods should not base on the ID set that is used for computing FPR95 and AUROC (in our case, its the validation set of ImageNet), because once the OOD method overfits the validation set, the evaluation result can be higher than the actual performance.</p><p>ReAct For ReAct, we use the Energy+ReAct setting, which is the most effective settings in <ref type="bibr" target="#b32">[32]</ref>. In the original paper, they recommended the 90-th percentile of activations estimated on the ID data for the clipping threshold. However, for BiT and ViT, we found that the rectification</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Specification</head><p>Architecture Pre-Trained Dataset Top1 (%)</p><p>BiT <ref type="bibr" target="#b20">[20]</ref> BiT-S-R101x1 CNN -81.30 ViT <ref type="bibr" target="#b7">[8]</ref> ViT-B/16 Transformer ImageNet-21K 85.43 RepVGG <ref type="bibr" target="#b6">[7]</ref> RepVGG-b3 CNN -80.52 Res50d <ref type="bibr" target="#b10">[11]</ref> ResNet-50d CNN -80.52 Swin <ref type="bibr" target="#b26">[26]</ref> Swin-base-patch4-window7-224 Transformer ImageNet-21K 85.27 DeiT <ref type="bibr" target="#b33">[33]</ref> DeiT-base-patch16-224 Transformer -81.98 <ref type="table">Table 5</ref>. Detailed information on the used models. The detailed specification and the top-1 accuracy of the model are provided. Three of them are CNN-based, and the other three are transformer-based. Both ViT and Swin Transformer are pre-trained on ImageNet-21K before training on ImageNet-1K, so their general OOD performances are much better than alternatives. <ref type="figure">Figure 7</ref>. The diversity of four OOD datasets shown by how they look similar to the ImageNet-1K classes. We use the BiT model to predict which ID class the image most resembles, and count the number of such OOD images for each class. Results are shown above. Due to space limitation, the y-axis is clipped at 155. Our newly created OpenImage-O has a wider coverage on ImageNet ID classes.  <ref type="table">Table 8</ref>. Score computation time for four methods on four OOD datasets. We assume that the features have been extracted, so the network forward time is not included. The implementation uses numpy and runs on Intel Xeon (Skylake) 23.20GHz CPU.</p><p>percentile p = 99 works much better than 90. So we report results using p = 99. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The AUROC (in percentage) of nine OOD detection algorithms applied to a BiT model trained on ImageNet-1K. The OOD datasets are ImageNet-O (x-axis) and OpenImage-O (yaxis). Methods marked with box use the feature space; methods with triangle ? use the logit; and methods with diamond ? use the softmax probability. The proposed method ViM (marked with *) uses information from both features and logits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Comparison of AUROC for OOD detection algorithms that are based on probability (marked with diamond ?), logit (?), and feature ( ) of 9 OOD detection algorithms applied to a BiT model trained on ImageNet-1K. The OOD datasets are Texture (xaxis) and iNaturalist (y-axis). Example images for the ID dataset ImageNet-1K and the two OOD datasets are illustrated at the top.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The pipeline of ViM. The principal space P and the matching constant ? are determined by the training set beforehand using Eq. (4) and Eq. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Robustness against principal space dimension. Left is BiT and right is ViT. The performance changes are small when D varies in a wide range of values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Perturbation of ? by multiplying a factor. Left is BiT, and right is ViT. For both models, the proposed matching parameter fits well for the trends.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FFigure 8 .</head><label>8</label><figDesc>. OOD Examples Detected by KL Matching and Residual (Sec. 3) OOD examples detected by Residual and KL Matching. There are three rows for each OOD dataset. The first row shows images from the top 5% OODs detected by Residual, with overlapping images in the top 50% list of KL Matching removed. The second row displays images from the intersection of the top 5% OODs detected by Residual and the top 5% OODs detected by KL Matching. The third row shows images from the top 5% OODs detected by KL Matching, with overlapping ones in the top 50% list of Residual removed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>predefined tag list 10, 000 tag-level manual ImageNet-O<ref type="bibr" target="#b18">[18]</ref> hard adversarial OOD 2, 000 image-level manual OpenImage-O follows natural class statistics, while ImageNet-O is adversarially built to be hard. Both datasets have image-level OOD annotation. Texture and iNaturalist are selected by tags, and their OOD labels are annotated in tag-level.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Energy 76.38 78.99 71.08 78.39 87.77 35.08 72.80 70.14 ODIN 77.72 72.68 75.27 68.56 88.00 36.58 77.13 63.92 MaxLogit 77.56 73.50 75.39 69.34 88.40 35.28 76.79 64.49 KL Matching 81.35 61.65 82.72 64.41 88.87 46.99 83.49 64.80 Residual 84.19 59.00 87.01 58.55 92.88 37.38 84.15 74.13 ReAct 49.14 98.96 82.93 58.63 90.17 31.36 77.37 67.00 Mahalanobis 86.07 59.39 88.33 55.70 92.16 40.39 85.03 73.18 ViM (Ours) 87.81 50.50 89.22 52.61 94.11 31.04 85.25 69.95</figDesc><table><row><cell cols="3">Model Method</cell><cell>Source</cell><cell cols="4">OpenImage-O AUROC?FPR95? AUROC?FPR95? AUROC?FPR95? AUROC?FPR95? AUROC?FPR95? Texture iNaturalist ImageNet-O Average</cell></row><row><cell></cell><cell cols="2">MSP [13]</cell><cell>prob</cell><cell cols="2">84.16 73.72</cell><cell>79.80 76.65</cell><cell>87.92 64.09</cell><cell>57.12 96.85</cell><cell>77.25 77.83</cell></row><row><cell></cell><cell cols="2">Energy [25]</cell><cell>logit</cell><cell cols="2">84.77 73.42</cell><cell>81.09 73.91</cell><cell>84.47 74.98</cell><cell>63.59 96.40</cell><cell>78.48 79.68</cell></row><row><cell></cell><cell cols="2">ODIN [24]</cell><cell cols="3">prob+grad 85.64 72.83</cell><cell>81.60 74.07</cell><cell>86.73 70.75</cell><cell>63.00 96.85</cell><cell>79.24 78.63</cell></row><row><cell></cell><cell cols="2">MaxLogit [12]</cell><cell>logit</cell><cell cols="2">85.67 72.68</cell><cell>81.66 73.72</cell><cell>86.76 70.59</cell><cell>63.01 96.85</cell><cell>79.27 78.46</cell></row><row><cell>BiT</cell><cell cols="3">KL Matching [12] prob</cell><cell cols="2">88.96 51.51</cell><cell>86.92 51.05</cell><cell>92.95 33.28 65.68 86.65</cell><cell>83.63 55.62</cell></row><row><cell></cell><cell cols="2">Residual  ?</cell><cell>feat</cell><cell cols="2">80.58 67.85</cell><cell>97.66 11.16</cell><cell>76.76 80.41</cell><cell>81.57 65.50</cell><cell>84.14 56.23</cell></row><row><cell></cell><cell cols="2">ReAct [32]</cell><cell cols="3">feat+logit 88.94 54.97</cell><cell>90.64 50.25</cell><cell>91.45 48.60</cell><cell>67.07 91.70</cell><cell>84.53 61.38</cell></row><row><cell></cell><cell cols="5">Mahalanobis [23] feat+label 83.10 64.32</cell><cell>97.33 14.05</cell><cell>85.70 64.95</cell><cell>80.37 70.05</cell><cell>86.62 53.34</cell></row><row><cell></cell><cell cols="2">ViM (Ours)</cell><cell cols="5">feat+logit 91.54 43.96 98.92 4.69 89.30 55.71</cell><cell>83.87 61.50 90.91 41.46</cell></row><row><cell></cell><cell cols="2">MSP [13]</cell><cell>prob</cell><cell cols="2">92.53 34.18</cell><cell>87.10 48.55</cell><cell>96.11 19.04</cell><cell>81.86 64.85</cell><cell>89.40 41.65</cell></row><row><cell></cell><cell cols="2">Energy [25]</cell><cell>logit</cell><cell cols="2">97.11 14.04</cell><cell>93.39 28.22</cell><cell>98.66 6.16</cell><cell>90.46 41.30</cell><cell>94.90 22.43</cell></row><row><cell></cell><cell cols="2">ODIN [24]</cell><cell cols="3">prob+grad 96.86 15.68</cell><cell>93.01 30.60</cell><cell>98.57 6.58</cell><cell>89.85 44.15</cell><cell>94.57 24.25</cell></row><row><cell></cell><cell cols="2">MaxLogit [12]</cell><cell>logit</cell><cell cols="2">96.87 15.68</cell><cell>93.01 30.60</cell><cell>98.57 6.58</cell><cell>89.85 44.15</cell><cell>94.57 24.25</cell></row><row><cell>ViT</cell><cell cols="3">KL Matching [12] prob</cell><cell cols="2">93.80 28.49</cell><cell>88.76 44.09</cell><cell>96.88 14.79</cell><cell>84.12 55.70</cell><cell>90.89 35.77</cell></row><row><cell></cell><cell cols="2">Residual  ?</cell><cell>feat</cell><cell cols="2">92.72 32.63</cell><cell>92.21 33.80</cell><cell>98.57 6.63</cell><cell>88.23 47.85</cell><cell>92.93 30.23</cell></row><row><cell></cell><cell cols="2">ReAct [32]</cell><cell cols="3">feat+logit 97.38 13.50</cell><cell>93.34 28.49</cell><cell>99.00 4.31</cell><cell>90.71 42.60</cell><cell>95.11 22.22</cell></row><row><cell></cell><cell cols="5">Mahalanobis [23] feat+label 97.48 13.54</cell><cell>94.24 25.17</cell><cell>99.54 2.12 92.81 36.95</cell><cell>96.02 19.45</cell></row><row><cell></cell><cell cols="2">ViM (Ours)</cell><cell cols="5">feat+logit 97.61 12.61 95.34 20.31 99.41 2.60</cell><cell>92.55 36.75 96.23 18.07</cell></row><row><cell cols="2">Method</cell><cell cols="2">RepVGG [7] Res50d [11] A? F? A? F?</cell><cell>Swin [26] A? F?</cell><cell cols="2">DeiT [33] A? F?</cell></row><row><cell>MSP</cell><cell></cell><cell cols="5">78.10 70.55 77.99 67.96 87.57 43.44 79.48 66.43</cell></row></table><note>Table 2. OOD detection for ViM and baseline methods. The ID dataset is ImageNet-1K, and OOD datasets are OpenImage-O, Texture, iNaturalist and ImageNet-O. Both metrics AUROC and FPR95 are in percentage. A pre-trained BiT-S-R101?1 model and a pre-trained ViT-B/16 model is tested. The best method is emphasized in bold, and the 2nd and 3rd ones are underlined. ODIN needs backpropagation for producing input perturbations, so it is prob+grad. ReAct clips feature and uses Energy subsequently, so it is feat+logit. Mahalanobis need gt labels to compute the class-wise mean feature, so it is feat+label.? : Residual is defined in Eq. (3).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>89.14 41.97 82.35 59.30 98.15 9.28 60.62 86.65 MaxGroup 84.75 71.22 80.42 77.87 89.50 57.18 63.93 92.45 ViM+Group 91.92 42.26 98.91 4.69 90.16 52.74 83.43 62.00</figDesc><table><row><cell>Method</cell><cell>OpenImage-O A? F?</cell><cell>Texture A? F?</cell><cell>iNaturalist A? F?</cell><cell>ImageNet-O A? F?</cell></row><row><cell>MOS* [18]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>. To summarize, half of them are CNN-based, and half are transformerbased. Vision Transformer and Swin Transformer are pretrained on ImageNet-21K before training on the ImageNet-1K. B. Detailed Results of Four Models (Sec. 6.3)In Sec. 6.3 we gave the average AUROC and FPR95 for RepVGG, ResNet-50d, Swin Transformer and DeiT. We provide the detailed AUROC and FPR95 on OpenImage-O, Texture, iNaturalist, and ImageNet-O in Tab. 6.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Comparison of effect of grouping on ViT. All numbers are in percentage. The grouping is defined in<ref type="bibr" target="#b18">[18]</ref> based on taxonomy. MaxGroup is the group version of MSP and ViM+Group is the group version of ViM.</figDesc><table><row><cell>Model</cell><cell>Method</cell><cell>Source</cell><cell>OpenImage-O</cell><cell>Texture</cell><cell>iNaturalist</cell><cell>ImageNet-O</cell><cell>Average</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported in part by Innovation and Technology Commission of the Hong Kong Special Administrative Region, China (Enterprise Support Scheme under the Innovation and Technology Fund B/E030/18). Haoqi Wang was also supported by the Technology Leaders of Tomorrow (TLT) Programme of HKSTP InnoAcademy.</p><p>In Sec. 3, we showed that feature-based OOD scores (e.g. Residual) and logit/softmax-based OOD scores (e.g. KL Matching) have different performances on the Texture OOD dataset. Here we visualize the OOD examples found by the two methods in <ref type="figure">Fig. 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Running Time of Four Methods (Sec. 6.2)</head><p>From Tab. 2 and Tab. 6, it is clear that the four most competitive methods are ViM, Mahalanobis, KL Matching, and Residual. Our ViM is the fastest among all four methods. We show their inference time on the four datasets in Tab. 8.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>AUROC?FPR95? AUROC?FPR95? AUROC?FPR95? AUROC?FPR95? AUROC?FPR95? RepVGG <ref type="bibr" target="#b6">[7]</ref> MSP <ref type="bibr" target="#b13">[13]</ref> prob 85.06 63. <ref type="bibr" target="#b36">36</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MVTec AD-a comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9592" to="9600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sammy</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Outlier detection through null space analysis of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Zare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Gader</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01263</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning confidence for out-of-distribution detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04865</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reducing network agnostophobia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Akshay Raj Dhamija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>G?nther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">RepVGG: Making VGG-style ConvNets great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The open world assumption. In eSI Workshop: The Closed World of Databases meets the Open World of the Semantic Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Shearer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploring the limits of out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Mohammadreza Mostajabi</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<imprint>
			<publisher>Jacob Steinhardt, and Dawn Song</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Scaling out-of-distribution detection for real-world settings</title>
		<idno type="arXiv">arXiv:1911.11132</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with outlier exposure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Natural adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generalized ODIN: Detecting out-of-distribution image without learning from out-of-distribution data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chang</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the importance of gradients for detecting distributional shifts in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">MOS: Towards scaling out-ofdistribution detection for large semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual odometry based on stereo image sequences with ransacbased outlier rejection scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Kitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Lategahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Big transfer (BiT): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">OpenImages: A public dataset for large-scale multi-label and multi-class image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<ptr target="https://github.com/openimages" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Training confidence-calibrated classifiers for detecting outof-distribution samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A simple unified framework for detecting out-of-distribution samples and adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Enhancing the reliability of out-of-distribution image detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Energy-based out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weitang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Out-of-distribution detection with subspace techniques and probabilistic modeling of features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahima</forename><surname>Ndiour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilesh</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omesh</forename><surname>Tickoo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04250</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Do vision transformers see like convolutional neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Are open set classification methods effective on large-scale datasets?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryne</forename><surname>Roady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayesha</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gonzales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Processing in Medical Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A less biased evaluation of out-of-distribution sample detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Shafaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th British Machine Vision Conference 2019, BMVC 2019</title>
		<meeting><address><addrLine>Cardiff, UK</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ReAct: Out-ofdistribution detection with rectified activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The iNaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">PyTorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">NGC: A unified framework for learning with open-world noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Fan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaojie</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="62" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Semantically coherent out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Litong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huabin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Generalized out-of-distribution detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.11334</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised out-ofdistribution detection by maximum classifier discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9518" to="9526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Outof-distribution detection using union of 1-dimensional subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zaeemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niccol?</forename><surname>Bisagno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeno</forename><surname>Sambugaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Conci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Rahnavard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="9452" to="9461" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
