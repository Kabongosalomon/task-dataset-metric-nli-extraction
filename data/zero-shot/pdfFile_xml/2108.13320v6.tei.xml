<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NEURAL HMMS ARE ALL YOU NEED (FOR HIGH-QUALITY ATTENTION-FREE TTS)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivam</forename><surname>Mehta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Division of Speech, Music and Hearing</orgName>
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
								<address>
									<settlement>Stockholm</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?va</forename><surname>Sz?kely</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Division of Speech, Music and Hearing</orgName>
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
								<address>
									<settlement>Stockholm</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Beskow</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Division of Speech, Music and Hearing</orgName>
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
								<address>
									<settlement>Stockholm</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><forename type="middle">Eje</forename><surname>Henter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Division of Speech, Music and Hearing</orgName>
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
								<address>
									<settlement>Stockholm</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NEURAL HMMS ARE ALL YOU NEED (FOR HIGH-QUALITY ATTENTION-FREE TTS)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-seq2seq</term>
					<term>attention</term>
					<term>HMMs</term>
					<term>duration modelling</term>
					<term>acoustic modelling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural sequence-to-sequence TTS has achieved significantly better output quality than statistical speech synthesis using HMMs. However, neural TTS is generally not probabilistic and uses nonmonotonic attention. Attention failures increase training time and can make synthesis babble incoherently. This paper describes how the old and new paradigms can be combined to obtain the advantages of both worlds, by replacing attention in neural TTS with an autoregressive left-right no-skip hidden Markov model defined by a neural network. Based on this proposal, we modify Tacotron 2 to obtain an HMM-based neural TTS model with monotonic alignment, trained to maximise the full sequence likelihood without approximation. We also describe how to combine ideas from classical and contemporary TTS for best results. The resulting example system is smaller and simpler than Tacotron 2, and learns to speak with fewer iterations and less data, whilst achieving comparable naturalness prior to the post-net. Our approach also allows easy control over speaking rate.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Text-to-speech (TTS) technology has advanced tremendously in the last decade, and output speech quality has seen a number of step changes as the field evolved. Statistical parametric speech synthesis (SPSS) based on hidden Markov models (HMMs) <ref type="bibr" target="#b0">[1]</ref>, has now largely been supplanted by neural TTS <ref type="bibr" target="#b1">[2]</ref>. Waveform-level deep learning greatly improved segmental quality over signal-processing based vocoders, while sequence-to-sequence models with attention, e.g., <ref type="bibr" target="#b2">[3]</ref>, demonstrated greatly improved prosody. Combined, as in Tacotron 2 <ref type="bibr" target="#b3">[4]</ref>, these innovations produce synthetic speech whose naturalness sometimes rivals that of recorded speech.</p><p>However, not all aspects of TTS systems have improved along the way. The integration of deep learning with positional features into HMM-based TTS increased naturalness <ref type="bibr" target="#b4">[5]</ref>, but sacrificed the ability to learn to speak and align simultaneously, instead requiring an external forced aligner. Attention-based neural TTS systems <ref type="bibr" target="#b2">[3]</ref> reintroduced the ability to learn to align, but are not grounded in probability and require more data and time to start speaking. Furthermore, their non-monotonic attention mechanisms do not enforce a consistent ordering of speech sounds. As a result, synthesis is susceptible to skipping and stuttering artefacts (as seen in <ref type="bibr" target="#b5">[6]</ref>), and may break down catastrophically, resulting in unintelligible gibberish.</p><p>In this article, we 1) make the case that HMM-based and neural TTS approaches can be combined to gain the benefits of both worlds. We 2) support this claim by describing a neural TTS architecture based on Tacotron 2, but with the attention mechanism replaced by This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. a Markovian hidden state, to obtain a fully probabilistic, joint model of durations and acoustics. The model development leverages design principles from both HMM-based and sequence-to-sequence TTS. Experiments show that the model gives a speech quality on par with that of a comparable Tacotron 2 model, and produces intelligible speech already after 1k updates, a 15-fold improvement on Tacotron 2. Unlike standard Tacotron 2, it also allows control over speaking rate. For audio examples and code, please see our demo webpage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND</head><p>The starting point of this work is <ref type="bibr" target="#b5">[6]</ref>, which identified four key differences between HMM-based SPSS and sequence-to-sequence attention-based TTS that had a notable impact on output quality:</p><p>1. Neural vocoder with mel-spectrogram inputs 2. Learned front-end (the encoder) 3. Acoustic feedback (autroregression) 4. Attention instead of HMM-based alignment Among these, items 1-3 led to improved speech quality, whereas attention sometimes made the output significantly worse. This paper incorporates aspects 1-3 into a TTS system that leverages neural HMMs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> rather than attention for sequence-to-sequence modelling. Sec. 2.1, below, describes how to add aspects 1-3 to HMMs based on prior work, with attention (aspect 4) discussed in Sec. 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Adding neural TTS aspects to HMM-based TTS</head><p>For aspect 1, high-quality neural vocoders are now available off the shelf. Furthermore, most of these use spectral features as input. This helps avoid flat intonation caused by explicit averaging over pitch contours, commonly seen in systems that use a separate f 0 feature to parameterise speech <ref type="bibr" target="#b5">[6]</ref>. However, nothing prevents HMM-based TTS from using mel-spectrogram features and neural vocoders: this is just a straightforward change of acoustic features, and the HMMbased approach described in this paper uses this setup.</p><p>Another factor in the improved prosody is item 2, the learned front-end (i.e., the encoder). Again, there is nothing that prevents using this idea in a system that leverages HMMs. The HMM-based systems we introduce all use the same encoder architecture as Tacotron 2 <ref type="bibr" target="#b3">[4]</ref> with no additional linguistic features added.</p><p>The situation for item 3, autoregression (AR), is again similar, in that AR and HMMs are not mutually exclusive. Acoustic models in HMM-based TTS systems benefit from using positional and durational information <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b4">5]</ref>, that increases granularity by enabling the statistics of each generated frame to be different, together with dynamic features <ref type="bibr" target="#b9">[10]</ref> to promote continuity across time. However, positional and durational features violate the Markov assumption (e.g., they depend on the time spent in the current state), preventing realignment during TTS training. In a model like Tacotron, positional information is instead mediated and continuity enforced by autoregression. Since this only involves dependencies on observed vari-ables, it is possible to devise autoregressive models that do not violate the Markov assumption, and linear autoregressive HMMs (AR-HMMs) <ref type="bibr" target="#b10">[11]</ref> have previously been explored in HMM-based SPSS <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. In this paper, we describe HMMs that, like Tacotron, use stronger, nonlinear AR models defined by a neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Attention in TTS</head><p>In a typical sequence-to-sequence based TTS system, the attention mechanism is responsible for duration modelling and for learning to align input symbols with output frames during training. Watts et al. <ref type="bibr" target="#b5">[6]</ref> found that the use of neural attention did not necessarily benefit TTS, and more suitable TTS attention mechanisms have recently been a focus of intense research. Only some of the relevant work can be surveyed here; please see <ref type="bibr" target="#b1">[2]</ref> for additional references. He et al. <ref type="bibr" target="#b14">[15]</ref> emphasised that TTS alignments should be local (each output frame is associated with a single input symbol), monotonic (never move backwards), and complete (not skip any speech sounds). HMMs are local by design, while the two other concepts map directly onto the classes of left-right and no-skip HMMs. Most neural TTS attention mechanisms do not satisfy these requirements <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>Many systems that do satisfy all three criteria rely on external tools for input-output alignment to obtain duration data (see <ref type="bibr" target="#b1">[2]</ref> for a list), and do not jointly learn to speak and align, unlike regular HMMs or Tacotron 1/2. However, some proposals do learn to speak and align without external tools, mostly (e.g., <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>) by introducing duration models into neural TTS, which will be our focus here. Many of these models only optimise a lower bound on the sequence likelihood, either due to the use of variational methods (e.g., Non-Attentive Tacotron <ref type="bibr" target="#b18">[19]</ref> and the VQ-VAEs in <ref type="bibr" target="#b19">[20]</ref>) or by not marginalising over all possible alignments (Glow-TTS <ref type="bibr" target="#b17">[18]</ref>). By using a mean squared error (MSE) duration loss, Glow-TTS also implicitly treats the positive, integer-valued durations (frame counts) as outcomes from a Gaussian distribution on the real line, which violates probabilistic assumptions. Our proposal avoids these issues.</p><p>AlignTTS <ref type="bibr" target="#b16">[17]</ref> is more similar to an HMM and uses a variant of the HMM forward recursions <ref type="bibr" target="#b10">[11]</ref>, but requires a complex, four-stage training procedure that culminates in training a separate, non-probabilistic duration predictor that is used at synthesis time. AlignTTS is also parallel, while our proposal is autoregressive.</p><p>The constant-per-state transition probability of regular HMMs implicitly describes a geometric duration distribution, which is a poor fit for natural speech <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. A solution to this in SPSS was to introduce explicit duration modelling through hidden semi-Markov models (HSMMs) <ref type="bibr" target="#b22">[23]</ref>. These sacrifice the Markovian property to describe more general duration distributions, by letting transition probabilities depend on the time spent in the current state. Independent work <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> concurrent to ours proposes to integrate HSMMs into neural TTS, obtaining better results than Tacotron 2, but uses a variational approximation and again assumes a Gaussian distribution for the positive-integer frame durations. In contrast, <ref type="bibr" target="#b23">[24]</ref> described how arbitrary discrete duration distributions can be parameterised implicitly via frame-dependent transition probabilities, and then predicted jointly with output frames in a single, joint model of durations and acoustics. This paper combines this idea with autoregression acting as an indirect, "acoustic memory" of the time spent in a state, to obtain a fully probabilistic model with general discrete durations, that can be trained efficiently on the exact log-likelihood.</p><p>The most similar work to ours is SSNT-TTS <ref type="bibr" target="#b15">[16]</ref>, which essentially describes a neural HMM for TTS, albeit under another name. We differ in applying an HMM perspective to the approach, in integrating more SPSS ideas to improve our system, in using a different duration-generation method, in demonstrating control over speaking rate, and in reporting better TTS quality, on par with Tacotron 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHOD</head><p>We now (in Sec. 3.1 and <ref type="figure" target="#fig_0">Fig. 1</ref>) describe the key modifications used to put HMMs into neural TTS such as Tacotron 2. Sec. 3.2 then describes how ideas and implementation aspects from classic HMMbased TTS can be adapted to further improve neural HMM TTS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Replacing attention with neural HMMs</head><p>The location-sensitive attention <ref type="bibr" target="#b24">[25]</ref> used by Tacotron 2 is a function that uses information from previously generated acoustic frames x1:t?1 to select which encoder output vector(s) hn to send to the decoder, to generate the next frame xt. (We use bold font for vectorvalued quantities and index input-sequence symbols by n and output frames by t.) The attention also has an internal state, in the form of previous attention weights ?1:t?1,n. <ref type="figure" target="#fig_0">Fig. 1a</ref> shows the procedure to generate one frame t of output using Tacotron 2. It can be written as</p><formula xml:id="formula_0">at = LSTM(PreNet(xt?1), gt?1, at?1) (1) et,n = ? tanh W at + V hn + U (F * t &lt;t ? t ,n ) + b (2) ?t,n = exp(et,n) / n exp(e t,n ) (3) gt = n ?t,nhn (xt, ?t) = OutputNet(gt, at).<label>(4)</label></formula><p>Here, at?1 represents the hidden and cell state variables of the first decoder LSTM, OutputNet is the upper part of the decoder in <ref type="figure" target="#fig_0">Fig.  1a</ref> (which contains a second LSTM), while ?t ? [0, 1] is the stop token. The latter is an estimate of the probability that the current frame is the last in the utterance, terminating synthesis if ?t &gt; 0.5.</p><p>To swap in neural HMMs, we remove the dependence on gt?1 from Eq. (1), and replace attention by a probabilistic OutputNet that uses at and the HMM state st ? {1, . . . , N } to estimate the distribution of frame xt, by outputting the parameters ?t of an HMM emission distribution o(?). The stop token becomes a transition probability ?t ? [0, 1] for st, with s1 = 1. Eqs. (2)-(4) then become</p><formula xml:id="formula_1">gt = hs t (?t, ?t) = OutputNet(gt, at) (5) xt ? o(?t) st+1 = st + Bernoulli(?t),<label>(6)</label></formula><p>where Bernoulli(p) is a binary random variable on {0, 1} that equals 1 with probability p. The attention state variables ?t,n of Tacotron 2 have thus been replaced by a single, integer state variable st that evolves stochastically based on ?t. This transition probability depends on the h-vector of the current state st (through gt) and on the entire previous acoustics x1:t?1 (through at), so it can be different for every frame t even for the same state. This can model arbitrary duration distributions <ref type="bibr" target="#b23">[24]</ref>. st &gt; N terminates synthesis. The end result is a left-right no-skip neural HMM, an AR-HMM parameterised by the decoder network in <ref type="figure" target="#fig_0">Fig. 1b</ref>. The encoder turns each input sequence into a unique HMM, where each vector hn represents a state. Feeding this state vector and the AR input x1:t?1 into the decoder yields the HMM emission distribution o(?t) and nextstate transition probability ?t of state n at time t. Neural HMMs were first described concurrently by <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b7">[8]</ref>, the latter under the name segment-to-segment neural transduction (SSNT).</p><p>For the model to be a proper HMM satisfying the Markov property, (?t, ?t) must not depend on anything other than the current state st (through the state vector gt) and the past observations x1:t?1. This necessitates an additional change to the Tacotron 2 architecture, namely removing the recurrence inside OutputNet by changing its LSTM layer to a feedforward layer, since an LSTM would propagate a dependence on past hidden states. This change also substantially reduces the number of parameters in the model. Finally, the full Tacotron 2 architecture contains a non-causal convolutional post-net that enhances the initial AR-generated melspectrogram in a residual setup. This resembles post-filtering and global variance compensation <ref type="bibr" target="#b25">[26]</ref> in classic SPSS. Tacotron 2 training minimises the sum of the MSEs before and after the post-net. However, the non-invertibility of the Tacotron post-net makes it incompatible with likelihood-based models like ours. A post-net can be added, but must either be trained separately, or be invertible like in <ref type="bibr" target="#b17">[18]</ref>. We leave this as future work, and instead evaluate our proposal against Tacotron 2 output from both before and after the post-net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Practical considerations</head><p>Numerical stability: When working with HMMs, it is crucial for numerical precision to perform all computations in the logarithmic domain using the "log-sum-exp trick". Since zeroes in these computations map to ln 0 = ?? in the log domain, care must be taken to avoid NaN gradients in deep-learning frameworks like PyTorch.</p><p>Like classic HMM-based TTS <ref type="bibr" target="#b0">[1]</ref>, we chose to use diagonalcovariance Gaussian emission distributions o(?, ?) in this work. We also used softplus (not exponential) nonlinearities for ?, with a non-zero minimum value ("variance flooring"), here clamped at 0.001, since this has been important in other generative models.</p><p>Architecture enhancements: Tacotron 2 can represent intermediate states using soft attention, since the ?t,n-values have many degrees of freedom. Major HMM-based synthesisers instead use 5 sub-states per input phone and run at 200 fps <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9]</ref>. Tacotron 2 runs at 80 fps, i.e., 40% the framerate, hence we use 2 states per phone to get the same time resolution as these HMMs. This is implemented by doubling the size of the decoder output layer and interpreting its output as two concatenated state vectors h for each phone.</p><p>Classic HMM-based TTS includes a model of the dependencies between several adjacent frames to promote temporally smooth output <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b8">9]</ref>. Although Tacotron 2 and the neural HMMs in this article only take the latest frame xt?1 as AR input, the LSTM in Eq.</p><p>(1) means they can remember information arbitrarily far back, which is beneficial for modelling utterance-level prosody. We also treat x0, the initial AR context (the "go token"), as a learnable parameter.</p><p>Initialisation: HMMs are often initialised using a flat start, in which all states have the same statistics <ref type="bibr" target="#b26">[27]</ref>. By zeroing out all weights in the decoder output layer but initialising other layers as normal, all states will have the same output (zero), but different and nonzero gradients, thus enabling learning <ref type="bibr" target="#b27">[28]</ref>. The last-layer bias values were chosen so that ? = 0 and ? = 1 for every state at the start of training, to match the global statistics of our normalised data.</p><p>Training: Neural HMM training <ref type="bibr" target="#b6">[7]</ref> is a hybrid of old and new: We use the classic (scaled) forward algorithm <ref type="bibr" target="#b10">[11]</ref> to compute the exact sequence log-likelihood, but then leverage backpropagation and automatic differentiation to optimise it using Adam. These parts correspond to the E step and the M step of the (generalised) EM algorithm <ref type="bibr" target="#b28">[29]</ref>, respectively. Computations during training parallelise over the states but, like Tacotron 2, are sequential across time due to the temporal recurrences.</p><p>Maximum-likelihood estimation of linear AR-HMMs can lead to unstable models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12]</ref>. A similar problem exists for nonlinear, autoregressive neural TTS <ref type="bibr" target="#b1">[2]</ref>. Tacotron 2 works around this by adding dropout to the pre-net, and we retain that solution here.</p><p>Synthesis: We can iteratively use the equations in Sec. 3.1 and randomly sample new frames xt ? o(?t). However, HMM-based TTS generally benefits from deterministically generating typical output rather than random sampling <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>. For acoustics, this is done by generating the most probable output sequence <ref type="bibr" target="#b9">[10]</ref>, which is the same as the mean ?t when o(?t) is Gaussian. By iteratively taking xt = ?t (red arrow in <ref type="figure" target="#fig_0">Fig. 1b)</ref>, we obtain a greedy approximation of <ref type="bibr" target="#b9">[10]</ref>. This is closely related to Tacotron 2 output generation, since it is trained using the MSE, which is minimised by the mean E[Xt].</p><p>SSNT-TTS found that randomly sampling transitions led to poor pause durations when synthesising <ref type="bibr" target="#b15">[16]</ref>, and classic HMM-based systems typically base the time in each state on the mean duration of the state <ref type="bibr" target="#b22">[23]</ref>. This mean is difficult to compute with duration distributions implicitly defined through transition probabilities ?t, as here. We instead use the simple algorithm from <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b31">32]</ref> for deterministic duration generation based on duration quantiles (e.g., the median rather than the mean). A quantile threshold controls speaking rate, which can be adjusted on a per-state basis, unlike <ref type="bibr" target="#b32">[33]</ref>. For the models evaluated in this paper, informal listening showed that deterministic generation of acoustics and durations both led to clear quality improvements; examples are provided on the webpage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>To validate our proposal and show that neural HMMs provide notable advantages over attention in neural TTS, we performed a number of experiments (including a subjective listening test) comparing TTS using neural HMMs to a maximally similar <ref type="table">Tacotron</ref>  be found at https://shivammehta007.github.io/Neural-HMM/.</p><p>We based our systems on the widely used PyTorch open-source Nvidia implementation 1 of Tacotron 2. The systems were trained on the LJ Speech dataset 2 , which contains utterances (normalised text and matching audio) adapted from free audiobooks read by a female speaker of US English. We used the default train/val/test split in the repository, which designates about 23 h of audio for training. We likewise used the default text-processing, including the pronouncing dictionary (CMUdict), since this generally benefits neural TTS <ref type="bibr" target="#b33">[34]</ref>. Output features were normalised to zero mean and unit variance over the training data, and waveforms were generated using the default, pre-trained v5 "universal" WaveGlow <ref type="bibr" target="#b34">[35]</ref> vocoder. <ref type="bibr" target="#b2">3</ref> We trained three systems: one Tacotron 2 baseline (T2) and two neural HMM systems, with either two (NH2) or one (NH1) state per phone. We expect NH2 to perform the best, with NH1 functioning as an ablation. All systems used the same architecture and hyperparameters (layer widths, learning rates, etc.) as the repository defaults, except that the size of the decoder output vectors was doubled to 1024 in the two-state system, since the decoder output now represents two concatenated state vectors. From the single Tacotron 2 baseline system, we synthesised two outputs: T2+P, using the full mel-spectrogram output after the post-net, and T2?P, using the initial mel-spectrogram prior to post-net enhancement, which is directly comparable to our neural HMMs. Model sizes for the different setups are listed in <ref type="table" target="#tab_0">Table 1</ref>. We see that both neural HMMs are significantly smaller than Tacotron 2, even if the post-net is removed.</p><p>Each system was trained for 30k mixed-precision updates on 7 GPUs using a batch size of 6. It took approximately 14.5k updates for T2 to learn to speak coherently, whereas NH2 was intelligible after 1k updates. <ref type="figure" target="#fig_1">Fig. 2</ref> graphs how the Google ASR word error rate (WER) of synthesising the 100 validation utterances evolves during training, including results from training on a small subset (500 utterances) of the data. Audio of speech synthesised during training is also provided on our demo webpage. We see that NH2 rapidly learns to speak intelligibly in both cases, much faster than Tacotron 2, which does not learn to speak at all on the smaller dataset. Even after the WER stabilised, we could consistently reproduce the effect where Tacotron 2 (including the best pre-trained system made available by Nvidia) degenerates into unintelligible babbling on long and short sentences, with examples provided on our webpage. Tacotron 2 applies pre-net dropout both during training and synthesis <ref type="bibr" target="#b3">[4]</ref>, otherwise attention breaks down. Our neural HMMs retained this dropout, since it improved the speech quality in informal listening. Audio synthesised without it is provided on our webpage.</p><p>The distribution of phone durations in natural speech is skewed to the right. The median of a skewed distribution lies between the mode and the mean, and median-based duration generation therefore often gives a faster-than-average speaking rate; cf. <ref type="bibr" target="#b30">[31]</ref>. Following the proposal in <ref type="bibr" target="#b31">[32]</ref>, the transition threshold of the deterministic duration-generation procedure was manually tuned to make the speaking rate of the NH systems match T2. The resulting thresholdquantile values were 0.57 for NH2 and 0.45 for NH1. Our webpage provides examples of speech generated with different threshold quantiles, to demonstrate speaking-rate control at synthesis time.</p><p>We conducted a subjective listening test to evaluate speech naturalness for the four conditions in <ref type="table" target="#tab_0">Table 1</ref>. In the test, participants were presented with four parallel stimuli at a time, one from each condition (unlabelled and in random order), all speaking the same sentence. Participants were asked to rate the naturalness of each stimulus on an integer scale from 1 (worst) to 5 (best), anchored using the classic MOS labels "Bad" through "Excellent". Stimuli were drawn from a pool of 9 sets of Harvard sentences <ref type="bibr" target="#b35">[36]</ref>, which are sets of 10 sentences each, designed so that each set is approximately phonetically balanced. All stimuli were loudness normalised to ?20 dB LUFS following EBU R128 <ref type="bibr" target="#b36">[37]</ref>. We manually verified that no T2 stimuli exhibited babbling due to failed attention.</p><p>We used Prolific to recruit 30 test participants ages 21 through 70, all self-reported headphone-wearing native English speakers from UK, Ireland, USA, Canada, Australia, and New Zealand. Each participant rated 3 randomly selected sets of 10 Harvard sentences, giving a grand total of 3600 ratings, 900 per condition. A completed test took on average 17 minutes and was rewarded with 3.50 GBP.</p><p>The mean opinion scores (MOS) from the test are reported in <ref type="table" target="#tab_0">Table 1</ref>, together with 95% confidence intervals based on a Gaussian approximation. Pairwise t-tests find all conditions to be significantly different (with p&lt;10 ?3 ) except NH2 and T2?P (p&gt;0.98), whose respective mean opinion scores differ by less than 0.002 before rounding. We can conclude that the proposed neural HMM TTS (NH2), despite being simpler and lighter, achieved a naturalness on par with the most comparable Tacotron 2 condition (T2?P). This was not achieved by SSNT-TTS <ref type="bibr" target="#b15">[16]</ref>. Neural HMMs were found to benefit from using two states per phone (NH2 vs. NH1), whilst Tacotron 2 improved from the use of a post-net (T2+P vs. T2?P).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION AND FUTURE WORK</head><p>We have described how classical and contemporary TTS paradigms can be combined to obtain fully probabilistic, attention-free sequenceto-sequence TTS based on neural HMMs. Our example system is smaller than Tacotron 2, yet achieves comparable naturalness, learns to speak and align faster, needs less data, and does not babble. To our knowledge, this is the first time an HMM-based system demonstrates a speech quality matching prior neural TTS. The neural HMMs also permit easy control over the speaking rate of the synthetic speech.</p><p>Future work includes stronger network architectures, e.g., based on transformers and with a separately trained post-net. It also seems compelling to combine neural HMMs with powerful distribution families such as normalising flows, either replacing the Gaussian assumption (as done for non-neural HMMs in <ref type="bibr" target="#b37">[38]</ref>) or as a probabilistic post-net like in <ref type="bibr" target="#b17">[18]</ref>. This may allow the naturalness of sampled speech to surpass that of deterministic output generation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Tacotron 2 architecture modified to use a neural HMM Synthesis-time architecture diagrams. Recurrences, delays, and the cumulative attention in Eq. (2) are drawn as grey arrows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>2 [4] system. Synthetic speech examples from the different experiments can Average utterance ASR WER of validation-set resynthesis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>41?0.01 3.25?0.01 3.24?0.01 2.68?0.01 Models from the experiments, with number of parameters and mean opinion scores (with 95% confidence intervals) for each.</figDesc><table><row><cell>1 https://github.com/NVIDIA/tacotron2/</cell></row><row><cell>2 https://keithito.com/LJ-Speech-Dataset/</cell></row><row><cell>3 https://github.com/NVIDIA/waveglow/</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Statistical parametric speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tokuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A survey on neural speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Soong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.15561</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tacotron: Towards end-to-end speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural TTS synthesis by conditioning WaveNet on mel spectrogram predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">From HMMs to DNNs: where do the improvements come from?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Henter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Merritt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Where do the improvements come from in sequence-tosequence neural TTS?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Henter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Valentini-Botinhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SSW</title>
		<meeting>SSW</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised neural hidden Markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop on Structured Prediction for NLP</title>
		<meeting>Workshop on Structured Prediction for NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Online segment to segment neural transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Merlin: An open source neural network speech synthesis system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SSW</title>
		<meeting>SSW</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Speech parameter generation algorithms for HMMbased speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tokuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Masuko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kitamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A tutorial on hidden Markov models and selected applications in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">77</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Autoregressive models for statistical parametric speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shannon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Audio Speech</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Autoregressive HMM speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Quillen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An autoregressive recurrent mixture density network for parametric speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust sequence-to-sequence acoustic modeling with stepwise monotonic attention for neural TTS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Initial investigation of encoder-decoder end-to-end TTS using marginalization of monotonic hard alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yasuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SSW</title>
		<meeting>SSW</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">AlignTTS: Efficient feed-forward text-to-speech system without explicit alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Glow-TTS: A generative flow for text-to-speech via monotonic alignment search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Elias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04301</idno>
		<title level="m">Non-Attentive Tacotron: Robust and controllable neural TTS synthesis including unsupervised duration modeling</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end textto-speech using latent duration based on VQ-VAE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yasuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Neural sequence-tosequence speech synthesis using a hidden semi-Markov model based structured attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nankaku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sumiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.13985</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Autoregressive variational autoencoder with a hidden semi-Markov model-based structured attention for speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nankaku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tokuda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hidden semi-Markov model based speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tokuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Masuko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kitamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLP</title>
		<meeting>SLP</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Medianbased generation of synthetic speech durations using a nonparametric approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ronanki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Henter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT</title>
		<meeting>SLT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A speech parameter generation algorithm considering global variance for HMM-based speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tokuda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE T. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The HTK Book (for HTK Version 3.2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Evermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kershaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Odell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ollason</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fixup initialization: Residual learning without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Stat. Soc. B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Measuring the perceptual effects of modelling assumptions in speech synthesis using stimuli constructed from repeated natural speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Henter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Merritt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shannon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robust TTS duration modelling using DNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Henter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ronanki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Nonparametric duration modelling for speech synthesis with a joint model of acoustics and duration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Henter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ronanki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">414</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Speaking speed control of end-to-end speech synthesis using sentence-level conditioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A comparison between letters and phones as input to sequence-to-sequence models for speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Richmond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SSW</title>
		<meeting>SSW</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">WaveGlow: A flow-based generative network for speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">IEEE recommended practice for speech quality measurements</title>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Acoust. Speech</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1969" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Loudness normalisation and permitted maximum level of audio signals</title>
	</analytic>
	<monogr>
		<title level="m">EBU Recommendation EBU R 128v4</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
		<respStmt>
			<orgName>EBU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Normalizing flow based hidden Markov models for classification of speech phones with explainability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Honor?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Henter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chatterjee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00730</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
