<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2018 DEEP COMPLEX NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiheb</forename><surname>Trabelsi</surname></persName>
							<email>chiheb.trabelsi@polymtl.ca</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Montreal Institute for Learning Algorithms (MILA)</orgName>
								<orgName type="institution" key="instit2">Montreal ? Ecole Polytechnique</orgName>
								<orgName type="institution" key="instit3">Montreal ? Microsoft Research</orgName>
								<orgName type="institution" key="instit4">Montreal ? Element AI</orgName>
								<address>
									<settlement>Montreal</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">?</forename><surname>Olexa Bilaniuk</surname></persName>
							<email>olexa.bilaniuk@umontreal.ca</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Montreal Institute for Learning Algorithms (MILA)</orgName>
								<orgName type="institution" key="instit2">Montreal ? Ecole Polytechnique</orgName>
								<orgName type="institution" key="instit3">Montreal ? Microsoft Research</orgName>
								<orgName type="institution" key="instit4">Montreal ? Element AI</orgName>
								<address>
									<settlement>Montreal</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Montreal Institute for Learning Algorithms (MILA)</orgName>
								<orgName type="institution" key="instit2">Montreal ? Ecole Polytechnique</orgName>
								<orgName type="institution" key="instit3">Montreal ? Microsoft Research</orgName>
								<orgName type="institution" key="instit4">Montreal ? Element AI</orgName>
								<address>
									<settlement>Montreal</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
							<email>serdyuk@iro</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Montreal Institute for Learning Algorithms (MILA)</orgName>
								<orgName type="institution" key="instit2">Montreal ? Ecole Polytechnique</orgName>
								<orgName type="institution" key="instit3">Montreal ? Microsoft Research</orgName>
								<orgName type="institution" key="instit4">Montreal ? Element AI</orgName>
								<address>
									<settlement>Montreal</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
							<email>sandeep.subramanian.1@umontreal.ca</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Montreal Institute for Learning Algorithms (MILA)</orgName>
								<orgName type="institution" key="instit2">Montreal ? Ecole Polytechnique</orgName>
								<orgName type="institution" key="instit3">Montreal ? Microsoft Research</orgName>
								<orgName type="institution" key="instit4">Montreal ? Element AI</orgName>
								<address>
									<settlement>Montreal</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Jo?o</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Santos</surname></persName>
							<email>jfsantos@emt.inrs.ca</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Montreal Institute for Learning Algorithms (MILA)</orgName>
								<orgName type="institution" key="instit2">Montreal ? Ecole Polytechnique</orgName>
								<orgName type="institution" key="instit3">Montreal ? Microsoft Research</orgName>
								<orgName type="institution" key="instit4">Montreal ? Element AI</orgName>
								<address>
									<settlement>Montreal</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroush</forename><surname>Mehri</surname></persName>
							<email>soroush.mehri@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Negar</forename><surname>Rostamzadeh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Montreal Institute for Learning Algorithms (MILA)</orgName>
								<orgName type="institution" key="instit2">Montreal ? Ecole Polytechnique</orgName>
								<orgName type="institution" key="instit3">Montreal ? Microsoft Research</orgName>
								<orgName type="institution" key="instit4">Montreal ? Element AI</orgName>
								<address>
									<settlement>Montreal</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">CIFAR Senior Fellow</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
							<email>christopher.pal@polymtl.ca</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Montreal Institute for Learning Algorithms (MILA)</orgName>
								<orgName type="institution" key="instit2">Montreal ? Ecole Polytechnique</orgName>
								<orgName type="institution" key="instit3">Montreal ? Microsoft Research</orgName>
								<orgName type="institution" key="instit4">Montreal ? Element AI</orgName>
								<address>
									<settlement>Montreal</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2018 DEEP COMPLEX NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>At present, the vast majority of building blocks, techniques, and architectures for deep learning are based on real-valued operations and representations. However, recent work on recurrent neural networks and older fundamental theoretical analysis suggests that complex numbers could have a richer representational capacity and could also facilitate noise-robust memory retrieval mechanisms. Despite their attractive properties and potential for opening up entirely new neural architectures, complex-valued deep neural networks have been marginalized due to the absence of the building blocks required to design such models. In this work, we provide the key atomic components for complex-valued deep neural networks and apply them to convolutional feed-forward networks and convolutional LSTMs. More precisely, we rely on complex convolutions and present algorithms for complex batch-normalization, complex weight initialization strategies for complex-valued neural nets and we use them in experiments with end-to-end training schemes. We demonstrate that such complex-valued models are competitive with their realvalued counterparts. We test deep complex models on several computer vision tasks, on music transcription using the MusicNet dataset and on Speech Spectrum Prediction using the TIMIT dataset. We achieve state-of-the-art performance on these audio-related tasks.</p><p>Published as a conference paper at ICLR 2018 show that learning explicit residuals of layers helps in avoiding the vanishing gradient problem and provides the network with an easier optimization problem. Batch normalization <ref type="bibr" target="#b0">(Ioffe and Szegedy, 2015)</ref> demonstrates that standardizing the activations of intermediate layers in a network across a minibatch acts as a powerful regularizer as well as providing faster training and better convergence properties. Further, such techniques that standardize layer outputs become critical in deep architectures due to the vanishing and exploding gradient problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent research advances have made significant progress in addressing the difficulties involved in learning deep neural network architectures. Key innovations include normalization techniques <ref type="bibr" target="#b0">(Ioffe and Szegedy, 2015;</ref><ref type="bibr" target="#b1">Salimans and Kingma, 2016)</ref> and the emergence of gating-based feed-forward neural networks like Highway Networks <ref type="bibr" target="#b2">(Srivastava et al., 2015)</ref>. Residual networks <ref type="bibr" target="#b3">(He et al., 2015a;</ref> have emerged as one of the most popular and effective strategies for training very deep convolutional neural networks (CNNs). Both highway networks and residual networks facilitate the training of deep networks by providing shortcut paths for easy gradient flow to lower network layers thereby diminishing the effects of vanishing gradients <ref type="bibr" target="#b5">(Hochreiter, 1991)</ref>. <ref type="bibr" target="#b4">He et al. (2016)</ref> The role of representations based on complex numbers has started to receive increased attention, due to their potential to enable easier optimization <ref type="bibr" target="#b6">(Nitta, 2002)</ref>, better generalization characteristics <ref type="bibr" target="#b7">(Hirose and Yoshida, 2012)</ref>, faster learning <ref type="bibr" target="#b8">(Arjovsky et al., 2015;</ref><ref type="bibr" target="#b9">Danihelka et al., 2016;</ref><ref type="bibr" target="#b10">Wisdom et al., 2016)</ref> and to allow for noise-robust memory mechanisms <ref type="bibr" target="#b9">(Danihelka et al., 2016)</ref>. <ref type="bibr" target="#b10">Wisdom et al. (2016)</ref> and <ref type="bibr" target="#b8">Arjovsky et al. (2015)</ref> show that using complex numbers in recurrent neural networks (RNNs) allows the network to have a richer representational capacity. <ref type="bibr" target="#b9">Danihelka et al. (2016)</ref> present an LSTM <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997)</ref> architecture augmented with associative memory with complex-valued internal representations. Their work highlights the advantages of using complex-valued representations with respect to retrieval and insertion into an associative memory. In residual networks, the output of each block is added to the output history accumulated by summation until that point. An efficient retrieval mechanism could help to extract useful information and process it within the block.</p><p>In order to exploit the advantages offered by complex representations, we present a general formulation for the building components of complex-valued deep neural networks and apply it to the context of feed-forward convolutional networks and convolutional LSTMs. Our contributions in this paper are as follows:</p><p>1. A formulation of complex batch normalization, which is described in Section 3.5; 2. Complex weight initialization, which is presented in Section 3.6; 3. A comparison of different complex-valued ReLU-based activation functions presented in Section 4.1; 4. A state of the art result on the MusicNet multi-instrument music transcription dataset, presented in Section 4.2; 5. A state of the art result in the Speech Spectrum Prediction task on the TIMIT dataset, presented in Section 4.3.</p><p>We perform a sanity check of our deep complex network and demonstrate its effectiveness on standard image classification benchmarks, specifically, CIFAR-10, CIFAR-100. We also use a reducedtraining set of SVHN that we call SVHN * . For audio-related tasks, we perform a music transcription task on the MusicNet dataset and a Speech Spectrum prediction task on TIMIT. The results obtained for vision classification tasks show that learning complex-valued representations results in performance that is competitive with the respective real-valued architectures. Our promising results in music transcription and speech spectrum prediction underscore the potential of deep complexvalued neural networks applied to acoustic related tasks 1 -We continue this paper with discussion of motivation for using complex operations and related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MOTIVATION AND RELATED WORK</head><p>Using complex parameters has numerous advantages from computational, biological, and signal processing perspectives. From a computational point of view, <ref type="bibr" target="#b9">Danihelka et al. (2016)</ref> has shown that Holographic Reduced Representations <ref type="bibr" target="#b12">(Plate, 2003)</ref>, which use complex numbers, are numerically efficient and stable in the context of information retrieval from an associative memory. <ref type="bibr" target="#b9">Danihelka et al. (2016)</ref> insert key-value pairs in the associative memory by addition into a memory trace.</p><p>Although not typically viewed as such, residual networks <ref type="bibr" target="#b3">(He et al., 2015a;</ref> and Highway Networks <ref type="bibr" target="#b2">(Srivastava et al., 2015)</ref> have a similar architecture to associative memories: each ResNet residual path computes a residual that is then inserted -by summing into the "memory" provided by the identity connection. Given residual networks' resounding success on several benchmarks and their functional similarity to associative memories, it seems interesting to marry both together. This motivates us to incorporate complex weights and activations in residual networks. Together, they offer a mechanism by which useful information may be retrieved, processed and inserted in each residual block.</p><p>Orthogonal weight matrices provide a novel angle of attack on the well-known vanishing and exploding gradient problems in RNNs. Unitary RNNs <ref type="bibr" target="#b8">(Arjovsky et al., 2015)</ref> are based on unitary weight matrices, which are a complex generalization of orthogonal weight matrices. Compared to their orthogonal counterparts, unitary matrices provide a richer representation, for instance being capable of implementing the discrete Fourier transform, and thus of discovering spectral representations. <ref type="bibr" target="#b8">Arjovsky et al. (2015)</ref> show the potential of this type of recurrent neural networks on toy tasks. <ref type="bibr" target="#b10">Wisdom et al. (2016)</ref> provided a more general framework for learning unitary matrices and they applied their method on toy tasks and on a real-world speech task.</p><p>Using complex weights in neural networks also has biological motivation. <ref type="bibr" target="#b13">Reichert and Serre (2013)</ref> have proposed a biologically plausible deep network that allows one to construct richer and more versatile representations using complex-valued neuronal units. The complex-valued formulation allows one to express the neuron's output in terms of its firing rate and the relative timing of its activity. The amplitude of the complex neuron represents the former and its phase the latter. Input neurons that have similar phases are called synchronous as they add constructively, whereas asynchronous neurons add destructively and thus interfere with each other. This is related to the gating mechanism used in both deep feed-forward neural networks <ref type="bibr" target="#b2">(Srivastava et al., 2015;</ref><ref type="bibr" target="#b14">van den Oord et al., 2016a;</ref><ref type="bibr">b)</ref> and recurrent neural networks <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b16">Cho et al., 2014;</ref><ref type="bibr" target="#b17">Zilly et al., 2016)</ref> as this mechanism learns to synchronize inputs that the network propagates at a given feed-forward layer or time step. In the context of deep gating-based networks, synchronization means the propagation of inputs whose controlling gates simultaneously hold high values. These controlling gates are usually the activations of a sigmoid function. This ability to take into account phase information might explain the effectiveness of incorporating complex-valued representations in the context of recurrent neural networks.</p><p>The phase component is not only important from a biological point of view but also from a signal processing perspective. It has been shown that the phase information in speech signals affects their intelligibility <ref type="bibr" target="#b18">(Shi et al., 2006)</ref>. Also <ref type="bibr" target="#b19">Oppenheim and Lim (1981)</ref> show that the amount of information present in the phase of an image is sufficient to recover the majority of the information encoded in its magnitude. In fact, phase provides a detailed description of objects as it encodes shapes, edges, and orientations.</p><p>Recently, <ref type="bibr" target="#b20">Rippel et al. (2015)</ref> leveraged the Fourier spectral representation for convolutional neural networks, providing a technique for parameterizing convolution kernel weights in the spectral domain, and performing pooling on the spectral representation of the signal. However, the authors avoid performing complex-valued convolutions, instead building from real-valued kernels in the spatial domain. In order to ensure that a complex parametrization in the spectral domain maps onto real-valued kernels, the authors impose a conjugate symmetry constraint on the spectral-domain weights, such that when the inverse Fourier transform is applied to them, it only yields real-valued kernels.</p><p>As pointed out in Reichert and Serre (2013), the use of complex-valued neural networks <ref type="bibr" target="#b21">(Georgiou and Koutsougeras, 1992;</ref><ref type="bibr" target="#b22">Zemel et al., 1995;</ref><ref type="bibr" target="#b23">Kim and Adal?, 2003;</ref><ref type="bibr" target="#b24">Hirose, 2003;</ref><ref type="bibr" target="#b25">Nitta, 2004)</ref> has been investigated long before the earliest deep learning breakthroughs <ref type="bibr" target="#b26">(Hinton et al., 2006;</ref><ref type="bibr" target="#b27">Bengio et al., 2007;</ref><ref type="bibr" target="#b28">Poultney et al., 2007)</ref>. Recently Reichert and Serre (2013); <ref type="bibr" target="#b29">Bruna et al. (2015)</ref>; <ref type="bibr" target="#b8">Arjovsky et al. (2015)</ref>; <ref type="bibr" target="#b9">Danihelka et al. (2016)</ref>; <ref type="bibr" target="#b10">Wisdom et al. (2016)</ref> have tried to bring more attention to the usefulness of deep complex neural networks by providing theoretical and mathematical motivation for using complex-valued deep networks. However, to the best of our knowledge, most of the recent works using complex valued networks have been applied on toy tasks, with the exception of some attempts. In fact, <ref type="bibr" target="#b30">(Oyallon and Mallat, 2015;</ref><ref type="bibr" target="#b32">Worrall et al., 2016)</ref> have used complex representation in vision tasks. <ref type="bibr" target="#b10">Wisdom et al. (2016)</ref> have also performed a real-world speech task consisting of predicting the log magnitude of the future short time Fourier transform frames. In Natural Language Processing, <ref type="bibr" target="#b33">(Trouillon et al., 2016;</ref><ref type="bibr" target="#b34">Trouillon and Nickel, 2017)</ref> have used complex-valued embeddings. Much remains to be done to develop proper tools and a general framework for training deep neural networks with complex-valued parameters.</p><p>Given the compelling reasons for using complex-valued representations, the absence of such frameworks represents a gap in machine learning tooling, which we fill by providing a set of building blocks for deep complex-valued neural networks that enable them to achieve competitive results with their real-valued counterparts on real-world tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">COMPLEX BUILDING BLOCKS</head><p>In this section, we present the core of our work, laying down the mathematical framework for implementing complex-valued building blocks of a deep neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">REPRESENTATION OF COMPLEX NUMBERS</head><p>We start by outlining the way in which complex numbers are represented in our framework. A complex number z = a + ib has a real component a and an imaginary component b. We represent the real part a and the imaginary part b of a complex number as logically distinct real valued entities and simulate complex arithmetic using real-valued arithmetic internally. Consider a typical realvalued 2D convolution layer that has N feature maps such that N is divisible by 2; to represent these as complex numbers, we allocate the first N/2 feature maps to represent the real components and the remaining N/2 to represent the imaginary ones. Thus, for a four dimensional weight tensor W that links N in input feature maps to N out output feature maps and whose kernel size is m ? m we would have a weight tensor of size (N out ? N in ? m ? m) /2 complex weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">COMPLEX CONVOLUTION</head><p>In order to perform the equivalent of a traditional real-valued 2D convolution in the complex domain, we convolve a complex filter matrix W = A + iB by a complex vector h = x + iy where A and B are real matrices and x and y are real vectors since we are simulating complex arithmetic using real-valued entities. As the convolution operator is distributive, convolving the vector h by the filter W we obtain:</p><formula xml:id="formula_0">W * h = (A * x ? B * y) + i (B * x + A * y).<label>(1)</label></formula><p>As illustrated in <ref type="figure" target="#fig_2">Figure 1a</ref>, if we use matrix notation to represent real and imaginary parts of the convolution operation we have:</p><formula xml:id="formula_1">(W * h) (W * h) = A ?B B A * x y .<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">COMPLEX DIFFERENTIABILITY</head><p>In order to perform backpropagation in a complex-valued neural network, a sufficient condition is to have a cost function and activations that are differentiable with respect to the real and imaginary parts of each complex parameter in the network. See Section 6.3 in the Appendix for the complex chain rule.</p><p>By constraining activation functions to be complex differentiable or holomorphic, we restrict the use of possible activation functions for a complex valued neural networks (For further details about holomorphism please refer to Section 6.2 in the appendix). <ref type="bibr" target="#b7">Hirose and Yoshida (2012)</ref> shows that it is unnecessarily restrictive to limit oneself only to holomorphic activation functions; Those functions that are differentiable with respect to the real part and the imaginary part of each parameter are also compatible with backpropagation. <ref type="bibr" target="#b8">(Arjovsky et al., 2015;</ref><ref type="bibr" target="#b10">Wisdom et al., 2016;</ref><ref type="bibr" target="#b9">Danihelka et al., 2016)</ref> have used non-holomorphic activation functions and optimized the network using regular, real-valued backpropagation to compute partial derivatives of the cost with respect to the real and imaginary parts.</p><p>Even though their use greatly restricts the set of potential activations, it is worth mentioning that holomorphic functions can be leveraged for computational efficiency purposes. As pointed out in Sarroff et al. <ref type="formula" target="#formula_0">(2015)</ref>, using holomorphic functions allows one to share gradient values (because the activation satisfies the Cauchy-Riemann equations 11 and 12 in the appendix). So, instead of computing and backpropagating 4 different gradients, only 2 are required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">COMPLEX-VALUED ACTIVATIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">MODRELU</head><p>Numerous activation functions have been proposed in the literature in order to deal with complexvalued representations. <ref type="bibr" target="#b8">(Arjovsky et al., 2015)</ref> have proposed modReLU, which is defined as follows:</p><formula xml:id="formula_2">modReLU(z) = ReLU(|z| + b) e i?z = (|z| + b) z |z| if |z| + b ? 0, 0 otherwise,<label>(3)</label></formula><p>where z ? C, ? z is the phase of z, and b ? R is a learnable parameter. As |z| is always positive, a bias b is introduced in order to create a "dead zone" of radius b around the origin 0 where the neuron is inactive, and outside of which it is active. The authors have used modReLU in the context of unitary RNNs. Their design of modReLU is motivated by the fact that applying separate ReLUs on both real and imaginary parts of a neuron performs poorly on toy tasks. The intuition behind the design of modReLU is to preserve the pre-activated phase ? z , as altering it with an activation function severely impacts the complex-valued representation. modReLU does not satisfy the Cauchy-Riemann equations, and thus is not holomorphic. We have tested modReLU in deep feed-forward complex networks and the results are given in <ref type="table" target="#tab_3">Table 6</ref>.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">CRELU AND zRELU</head><p>We call Complex ReLU (or CReLU) the complex activation that applies separate ReLUs on both of the real and the imaginary part of a neuron, i.e:</p><formula xml:id="formula_3">CReLU(z) = ReLU( (z)) + i ReLU( (z)).<label>(4)</label></formula><p>CReLU satisfies the Cauchy-Riemann equations when both the real and imaginary parts are at the same time either strictly positive or strictly negative. This means that CReLU satisfies the Cauchy-Riemann equations when ? z ? ]0, ?/2[ or ? z ? ]?, 3?/2[. We have tested CReLU in deep feedforward neural networks and the results are given in <ref type="table" target="#tab_3">Table 6</ref>.4.</p><p>It is also worthwhile to mention the work done by <ref type="bibr" target="#b36">Guberman (2016)</ref> where a ReLU-based complex activation which satisfies the Cauchy-Riemann equations everywhere except for the set of points { (z) &gt; 0, (z) = 0} ? { (z) = 0, (z) &gt; 0} ias used. The activation function has similarities to CReLU. We call Guberman (2016) activation as zReLU and is defined as follows:</p><formula xml:id="formula_4">zReLU(z) = z if ? z ? [0, ?/2], 0 otherwise,<label>(5)</label></formula><p>We have tested zReLU in deep feed-forward complex networks and the results are given in <ref type="table" target="#tab_3">Table  6</ref>.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">COMPLEX BATCH NORMALIZATION</head><p>Deep networks generally rely upon Batch Normalization <ref type="bibr" target="#b0">(Ioffe and Szegedy, 2015)</ref> to accelerate learning. In some cases batch normalization is essential to optimize the model. The standard formulation of Batch Normalization applies only to real values. In this section, we propose a batch normalization formulation that can be applied for complex values.</p><p>To standardize an array of complex numbers to the standard normal complex distribution, it is not sufficient to translate and scale them such that their mean is 0 and their variance 1. This type of normalization does not ensure equal variance in both the real and imaginary components, and the resulting distribution is not guaranteed to be circular; It will be elliptical, potentially with high eccentricity.</p><p>We instead choose to treat this problem as one of whitening 2D vectors, which implies scaling the data by the square root of their variances along each of the two principal components. This can be done by multiplying the 0-centered data (x ? E[x]) by the inverse square root of the 2?2 covariance matrix V :</p><formula xml:id="formula_5">x = (V ) ? 1 2 (x ? E[x]) , where the covariance matrix V is V = V rr V ri V ir V ii = Cov( {x}, {x}) Cov( {x}, {x}) Cov( {x}, {x}) Cov( {x}, {x}) .</formula><p>The square root and inverse of 2?2 matrices has an inexpensive, analytical solution, and its existence is guaranteed by the positive (semi-)definiteness of V . Positive definiteness of V is ensured by the addition of I to V (Tikhonov regularization). The mean subtraction and multiplication by the inverse square root of the variance ensures thatx has standard complex distribution with mean ? = 0, covariance ? = 1 and pseudo-covariance (also called relation) C = 0. The mean, the covariance and the pseudo-covariance are given by:</p><formula xml:id="formula_6">? = E [x] ? = E [(x ? ?) (x ? ?) * ] = V rr + V ii + i (V ir ? V ri ) C = E [(x ? ?) (x ? ?)] = V rr ? V ii + i (V ir + V ri ).<label>(6)</label></formula><p>The normalization procedure allows one to decorrelate the imaginary and real parts of a unit. This has the advantage of avoiding co-adaptation between the two components which reduces the risk of overfitting <ref type="bibr" target="#b37">(Cogswell et al., 2015;</ref><ref type="bibr" target="#b38">Srivastava et al., 2014)</ref>.</p><p>Analogously to the real-valued batch normalization algorithm, we use two parameters, ? and ?. The shift parameter ? is a complex parameter with two learnable components (the real and imaginary means). The scaling parameter ? is a 2 ? 2 positive semi-definite matrix with only three degrees of freedom, and thus only three learnable components. In much the same way that the matrix (V ) ? 1 2 normalized the variance of the input to 1 along both of its original principal components, so does ? scale the input along desired new principal components to achieve a desired variance. The scaling parameter ? is given by:</p><formula xml:id="formula_7">? = ? rr ? ri ? ri ? ii .</formula><p>As the normalized inputx has real and imaginary variance 1, we initialize both ? rr and ? ii to 1/ ? 2 in order to obtain a modulus of 1 for the variance of the normalized value. ? ri , {?} and {?} are initialized to 0. The complex batch normalization is defined as:</p><formula xml:id="formula_8">BN (x) = ?x + ?.<label>(7)</label></formula><p>We use running averages with momentum to maintain an estimate of the complex batch normalization statistics during training and testing. The moving averages of V ri and ? are initialized to 0. The moving averages of V rr and V ii are initialized to 1/ ? 2. The momentum for the moving averages is set to 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">COMPLEX WEIGHT INITIALIZATION</head><p>In a general case, particularly when batch normalization is not performed, proper initialization is critical in reducing the risks of vanishing or exploding gradients. To do this, we follow the same steps as in <ref type="bibr" target="#b39">Glorot and Bengio (2010)</ref> and <ref type="bibr" target="#b40">He et al. (2015b)</ref> to derive the variance of the complex weight parameters.</p><p>A complex weight has a polar form as well as a rectangular form</p><formula xml:id="formula_9">W = |W |e i? = {W } + i {W },<label>(8)</label></formula><p>where ? and |W | are respectively the argument (phase) and magnitude of W .</p><p>Variance is the difference between the expectation of the squared magnitude and the square of the expectation:</p><formula xml:id="formula_10">Var(W ) = E [W W * ] ? (E [W ]) 2 = E |W | 2 ? (E [W ]) 2 ,</formula><p>which reduces, in the case of W symmetrically distributed around 0, to E |W | 2 . We do not know yet the value of Var(W ) = E |W | 2 . However, we do know a related quantity, Var(|W |), because the magnitude of complex normal values, |W |, follows the Rayleigh distribution (Chi-distributed with two degrees of freedom (DOFs)). This quantity is</p><formula xml:id="formula_11">Var(|W |) = E [|W ||W | * ] ? (E [|W |]) 2 = E |W | 2 ? (E [|W |]) 2 .<label>(9)</label></formula><p>Putting them together:</p><formula xml:id="formula_12">Var(|W |) = Var(W ) ? (E [|W |]) 2 ,</formula><p>and Var(W ) = Var(|W |) + (E [|W |]) 2 . We now have a formulation for the variance of W in terms of the variance and expectation of its magnitude, both properties analytically computable from the Rayleigh distribution's single parameter, ?, indicating the mode. These are:</p><formula xml:id="formula_13">E [|W |] = ? ? 2 , Var(|W |) = 4 ? ? 2 ? 2 .</formula><p>The variance of W can thus be expressed in terms of its generating Rayleigh distribution's single parameter, ?, thus:</p><formula xml:id="formula_14">Var(W ) = 4 ? ? 2 ? 2 + ? ? 2 2 = 2? 2 .<label>(10)</label></formula><p>If we want to respect the <ref type="bibr" target="#b39">Glorot and Bengio (2010)</ref> criterion which ensures that the variances of the input, the output and their gradients are the same, then we would have Var(W ) = 2/(n in + n out ), where n in and n out are the number of input and output units respectively. In such case, ? = 1/ ? n in + n out . If we want to respect the <ref type="bibr" target="#b40">He et al. (2015b)</ref> initialization that presents an initialization criterion that is specific to ReLUs, then Var(W ) = 2/n in which ? = 1/ ? n in .</p><p>The magnitude of the complex parameter W is then initialized using the Rayleigh distribution with the appropriate mode ?. We can see from equation 10, that the variance of W depends on on its magnitude and not on its phase. We then initialize the phase using the uniform distribution between ?? and ?. By performing the multiplication of the magnitude by the phasor as is detailed in equation 8, we perform the complete initialization of the complex parameter.</p><p>In all the experiments that we report, we use variant of this initialization which leverages the independence property of unitary matrices. As it is stated in <ref type="bibr" target="#b37">Cogswell et al. (2015)</ref>, <ref type="bibr" target="#b38">Srivastava et al. (2014)</ref>, and <ref type="bibr" target="#b41">Tompson et al. (2015)</ref>, learning decorrelated features is beneficial for learning as it allows to perform better generalization and faster learning. This motivates us to achieve initialization by considering a (semi-)unitary matrix which is reshaped to the size of the weight tensor. Once this is done, the weight tensor is mutiplied by He var /Var(W ) or Glorot var /Var(W ) where Glorot var and He var are respectively equal to 2/(n in + n out ) and 2/n in . In such a way we allow kernels to be independent from each other as much as possible while respecting the desired criterion. Note that we perform the analogous initialization for real-valued models by leveraging the independence property of orthogonal matrices in order to build kernels that are as much independent from each other as possible while respecting a given criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">COMPLEX CONVOLUTIONAL RESIDUAL NETWORK</head><p>A deep convolutional residual network of the nature presented in <ref type="bibr" target="#b3">He et al. (2015a;</ref> consists of 3 stages within which feature maps maintain the same shape. At the end of a stage, the feature maps are downsampled by a factor of 2 and the number of convolution filters are doubled. The sizes of the convolution kernels are always set to 3 x 3. Within a stage, there are several residual blocks which comprise 2 convolution layers each. The contents of one such residual block in the real and complex setting is illustrated in Appendix <ref type="figure" target="#fig_2">Figure 1b</ref>.</p><p>In the complex valued setting, the majority of the architecture remains identical to the one presented in <ref type="bibr" target="#b4">He et al. (2016)</ref> with a few subtle differences. Since all datasets that we work with have realvalued inputs, we present a way to learn their imaginary components to let the rest of the network operate in the complex plane. We learn the initial imaginary component of our input by performing the operations present within a single real-valued residual block</p><formula xml:id="formula_15">BN ? ReLU ? Conv ? BN ? ReLU ? Conv</formula><p>Using this learning block yielded better emprical results than assuming that the input image has a null imaginary part. The parameters of this real-valued residual block are trained by backpropagating errors from the task specific loss function. Secondly, we perform a Conv ? BN ? Activation operation on the obtained complex input before feeding it to the first residual block. We also perform the same operation on the real-valued network input instead of Conv ? M axpooling as in <ref type="bibr" target="#b4">He et al. (2016)</ref>. Inside, residual blocks, we subtly alter the way in which we perform a projection at <ref type="table">Table 1</ref>: Classification error on CIFAR-10, CIFAR-100 and SVHN * using different complex activations functions (zReLU, modReLU and CReLU). WS, DN and IB stand for the wide and shallow, deep and narrow and in-between models respectively. The prefixes R &amp; C refer to the real and complex valued networks respectively. Performance differences between the real network and the complex network using CReLU are reported between their respective best models. All models are constructed to have roughly 1.7M parameters except the modReLU models which have roughly 2.5M parameters. modReLU and zReLU were largely outperformed by CReLU in the reported experiments. Due to limited resources, we haven't performed all possible experiments as the conducted ones are already conclusive. A "-" is filled in front of an unperformed experiment.    the end of a stage in our network. We concatenate the output of the last residual block with the output of a 1x1 convolution applied on it with the same number of filters used throughout the stage and subsample by a factor of 2. In contrast, <ref type="bibr" target="#b4">He et al. (2016)</ref> perform a similar 1x1 convolution with twice the number of feature filters in the current stage to both downsample the feature maps spatially and double them in number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>In this section, we present empirical results from using our model to perform image, music classification and spectrum prediction. First, we present our model's architecture followed by the results we obtained on CIFAR-10, CIFAR-100, and SVHN * as well as the results on automatic music transcription on the MusicNet benchmark and speech spectrum prediction on TIMIT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">IMAGE RECOGNITION</head><p>We adopt an architecture inspired by <ref type="bibr" target="#b4">He et al. (2016)</ref>. The latter will also serve as a baseline to compare against. We train comparable real-valued Neural Networks using the standard ReLU activation function. We have tested our complex models with the CReLU, zReLU and modRelu activation functions. We use a cross entropy loss for both real and complex models. A global average pooling layer followed by a single fully connected layer with a softmax function is used to classify the input as belonging to one of 10 classes in the CIFAR-10 and SVHN datasets and 100 classes for CIFAR-100.</p><p>We consider architectures that trade-off model depth (number of residual blocks per stage) and width (number of convolutional filters in each layer) given a fixed parameter budget. Specifically, we build three different models -wide and shallow (WS), deep and narrow (DN) and in-between (IB). In a model that has roughly 1.7 million parameters, our WS architecture for a complex network starts with 12 complex filters (24 real filters) per convolution layer in the initial stage and 16 residual blocks per stage. The DN architecture starts with 10 complex filters and 23 blocks per stage while the IB variant starts with 11 complex filters and 19 blocks per stage. The real-valued counterpart has also 1.7 million parameters. Its WS architecture starts with 18 real filters per convolutional layer and 14 blocks per stage. The DN architecture starts with 14 real filters and 23 blocks per stage and the IB architecture starts with 16 real filters and 18 blocks per stage.</p><p>All models (real and complex) were trained using the backpropagation algorithm with Stochastic Gradient Descent with Nesterov momentum <ref type="bibr" target="#b42">(Nesterov, 1983</ref>) set at 0.9. We also clip the norm of our gradients to 1. We tweaked the learning rate schedule used in <ref type="bibr" target="#b4">He et al. (2016)</ref> in both the real and complex residual networks to extract small performance improvements in both. We start our learning rate at 0.01 for the first 10 epochs to warm up the training and then set it at 0.1 from epoch 10-100 and then anneal the learning rates by a factor of 10 at epochs 120 and 150. We end the training at epoch 200.  <ref type="figure">SVHN)</ref>. We still test on all 26,032 images. For all the tasks and for both the real-and complex-valued models, The WS architecture has yielded the best performances. This is in concordance with <ref type="bibr" target="#b43">Zagoruyko and Komodakis (2016)</ref> who observed that wider and shallower residual networks perform better than their deeper and narrower counterpart. On CIFAR-10 and SVHN * , the real-valued representation performs slightly better than its complex counterpart. On CIFAR-100, the complex representation outperforms the real one. In general, the obtained results for both representation are quite comparable. To understand the effect of using either real or complex representation for a given task, we designed hybrid models that combine both. <ref type="table">Table 2</ref> contains the results for hybrid models. We can observe in the <ref type="table">Table 2</ref> that in cases where complex representation outperformed the real one (wide and shallow on CIFAR-100), combining a real-valued convolutional filter with a complex batch normalization improves the accuracy of the real-valued convolutional model. However, the complex-valued one is still outperforming it. In cases, where real-valued representation outperformed the complex one (wide and shallow on CIFAR-10 and SVHN * ), replacing a complex batch normalization by a regular one increased the accuracy of the complex convolutional model. Despite that replacement, the real-valued model performs better in terms of accuracy for such tasks. In general, these experiments show that the difference in efficiency between the real and complex models varies according to the dataset, to the task and to the architecture.</p><p>Ablation studies were performed in order to investigate the importance of the 2D whitening operation that occurs in the complex batch normalization. We replaced the complex batch normalization layers with a naive variant (NCBN) which, instead of left multiplying the centred unit by the inverse square root of its covariance matrix, just divides it by its complex variance. Here, this naive variant of CBN is Mimicking the regular BN by not taking into account correlation between the elements in the complex unit. The Naive variant of the Complex Batch Normalization performed very poorly;</p><p>In 5 out of 6 experiments, training failed with the appearance of NaNs (See Section 6.6 for the explanation). By way of contrast, all 6 complex-valued Batch Normalization experiments converged. Results are given in <ref type="table">Table 2</ref>.</p><p>Another ablation study was undertaken to compare CReLU, modReLU and zRELU. Again the differences were stark: All CReLU experiments converged and outperformed both modReLU and zReLU, both which variously failed to converge or fared substantially worse. We think that modRelu didn't perform as well as CReLU due to the fact that consecutive layers in a feed-forward net do not represent time-sequential patterns, and so, they might need to drop some phase information. Results are reported in <ref type="table" target="#tab_3">Table 6</ref>.4. More discussion about phase information encoding is presented in section 6.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">AUTOMATIC MUSIC TRANSCRIPTION</head><p>In this section we present results for the automatic music transcription (AMT) task. The nature of an audio signal allows one to exploit complex operations as presented earlier in the paper. The experiments were performed on the MusicNet dataset <ref type="bibr" target="#b44">(Thickstun et al., 2016)</ref>. For computational efficiency we resampled the original input from 44.1kHz to 11kHz using the algorithm described in <ref type="bibr" target="#b45">Smith (2002)</ref>. This sampling rate is sufficient to recognize frequencies presented in the dataset while reducing computational cost dramatically. We modeled each of the 84 notes that are present in the dataset with independent sigmoids (due to the fact that notes can fire simultaneously). We initialized the bias of the last layer to the value of -5 to reflect the distribution of silent/non-silent notes. As in the baseline, we performed experiments on the raw signal and the frequency spectrum. For complex experiments with the raw signal, we considered its imaginary part equal to zero. When using the spectrum input we used its complex representation (instead of only the magnitudes, as usual for AMT) for both real and complex models. For the real model, we considered the real and imaginary components of the spectrum as separate channels. The model we used for raw signals is a shallow convolutional network similar to the model used in the baseline, with the size reduced by a factor of 4 (corresponding to the reduction of the sampling rate). The filter size was 512 samples (about 12ms) with a stride of 16. The model for the spectral input is similar to the VGG model <ref type="bibr" target="#b46">(Simonyan and Zisserman, 2015)</ref>. The first layer has filter with size of 7 and is followed by 5 convolutional layers with filters of size 3. The final convolution block is followed by a fully connected layer with 2048 units. The latter is followed, in its turn, by another fully connected layer with 84 sigmoidal units. In all of our experiments we use an input window of 4096 samples or its corresponding FFT (which corresponds to the 16,384 window used in the baseline) and predicted notes in the center of the window. All networks were optimized with Adam. We start our learning rate at 10 ?3 for the first 10 epochs and then anneal it by a factor of 10 at each of the epochs 100, 120 and 150. We end the training at epoch 200. For the real-valued models, we have used ReLU as activation. CReLU has been used as activation for the complex-valued models.</p><p>The complex network was initialized using the unitary initialization scheme respecting the He criterion as described in Section 3.6. For the real-valued network, we have used the analogue initialization of the weight tensor. It consists of performing an orthogonal initialization with a gain of ? 2. The complex batch normalization was applied according to Section 3.5. Following <ref type="bibr" target="#b44">Thickstun et al. (2016)</ref> we used recordings with ids '2303', '2382', '1819' as the test subset and additionally we created a validation subset using recording ids <ref type="bibr">'2131', '2384', '1792', '2514', '2567', '1876'</ref> (randomly chosen from the training set). The validation subset was used for model selection and early stopping. The remaining 321 files were used for training. The results are summarized on Table 3. We achieve a performance comparable to the baseline with the shallow convolutional network. our VGG-based deep real-valued model reaches 69.6% average precision on the downsampled data. With significantly fewer parameters than its real counterpart, the VGG-based deep complex model, achieves 72.9% average precision which is the state of the art to the best of our knowledge. See <ref type="figure">Figures 2 and 3</ref> in the Appendix for precision-recall curves and a sample of the output of the model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">SPEECH SPECTRUM PREDICTION</head><p>We apply both a real <ref type="bibr">Convolutional LSTM Xingjian et al. (2015)</ref> and a complex Convolutional LSTM on speech spectrum prediction task (See section 6.5 in the Appendix for the details of the real and complex Convolutional LSTMs). In this task, the model predicts the magnitude spectrum. It implicitly infers the real and imaginary components of the spectrum at time t + 1, given all the spectrum (imaginary part and real components) up to time t. This is slightly different from <ref type="bibr" target="#b10">(Wisdom et al., 2016)</ref>. The real and imaginary components are considered as separate channels in both model. We evaluate the model with mean-square-error (MSE) on log-magnitude to compare with the others <ref type="bibr" target="#b10">Wisdom et al. (2016)</ref>. The experiments are conducted on a downsampled (8kHz) version of the TIMIT dataset. By following the steps in <ref type="bibr" target="#b10">Wisdom et al. (2016)</ref>, raw audio waves are transformed into frequency domain via short-time Fourier transform (STFT) with a Hann analysis window of 256 samples and a window hop of 128 samples (50% overlap). We use a training set with 3690 utterances, a validation set with 400 utterances and a standard test set with 192 utterance.</p><p>To match the number of parameters for both model, the Convolutional LSTM has 84 feature maps while the complex model has 60 complex feature maps (120 feature maps in total). Adam <ref type="bibr" target="#b48">Kingma and Ba (2014)</ref> with a fixed learning rate of 1e-4 is used in both experiments. We initialize the complex model with the unitary initialization scheme and the real model with orthogonal initialization respecting the Glorot criterion. The result is shown in <ref type="table" target="#tab_5">Table 4</ref> and the learning curve is shown in <ref type="figure">Figure 4</ref>. Our baseline model has achieved the state of the art and the complex convolutional LSTM model performs better over the baseline in terms of MSE and convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>We have presented key building blocks required to train complex valued neural networks, such as complex batch normalization and complex weight initialization. We have also explored a wide variety of complex convolutional network architectures, including some yielding competitive results for image classification and state of the art results for a music transcription task and speech spectrum prediction. We hope that our work will stimulate further investigation of complex valued networks for deep learning models and their application to more challenging tasks such as generative models for audio and images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">APPENDIX</head><p>In practice, the complex convolution operation is implemented as illustrated in <ref type="figure" target="#fig_2">Fig.1a</ref> where MI , MR refer to imaginary and real feature maps and KI and KR refer to imaginary and real kernels. MI KI refers to result of a real-valued convolution between the imaginary kernels KI and the imaginary feature maps MI .</p><p>(a) An illustration of the complex convolution operator.  . u and v are real-valued functions. One possible way of expressing ?z is to have ?z = ?x + i ?y. ?z can approach 0 from multiple directions (along the real axis, imaginary axis or in-between). However, in order to be complex differentiable, f (z0) must be the same complex quantity regardless of direction of approach. When ?z approaches 0 along the real axis, f (z0) could be written as:</p><formula xml:id="formula_16">f (z0) ? lim ?z?0 (f (z0) + ?z) ? f (z0) ?z = lim ?x?0 lim ?y?0 ?u(x0, y0) + i ?v(x0, y0) ?x + i ?y = lim ?x?0 ?u(x0, y0) + i ?v(x0, y0) ?x + i 0 .<label>(11)</label></formula><p>When ?z approaches 0 along the imaginary axis, f (z0) could be written as:</p><formula xml:id="formula_17">= lim ?y?0 lim ?x?0 ?u(x0, y0) + i ?v(x0, y0) ?x + i ?y = lim ?y?0 ?u(x0, y0) + i ?v(x0, y0) 0 + i ?y<label>(12)</label></formula><p>Satisfying equations 11 and 12 is equivalent of having ?f ?z = ?u ?x + i ?v ?x = ?i ?u ?y + ?v ?y . So, in order to be complex differentiable, f should satisfy ?u ?x = ?v ?y and ?u ?y = ? ?v ?x . These are called the Cauchy-Riemann equations and they give a necessary condition for f to be complex differentiable or "holomorphic". Given that u and v have continuous first partial derivatives, the Cauchy-Riemann equations become a sufficient condition for f to be holomorphic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">THE GENRALIZED COMPLEX CHAIN RULE FOR A REAL-VALUED LOSS FUNCTION</head><p>If L is a real-valued loss function and z is a complex variable such that z = x + i y where x, y ? R, then:</p><formula xml:id="formula_18">?L(z) = ?L ?z = ?L ?x + i ?L ?y = ?L ? (z) + i ?L ? (z) = (?L(z)) + i (?L(z)).<label>(13)</label></formula><p>Now if we have another complex variable t = r + i s where z could be expressed in terms of t and r, s ? R, we would then have: (14)</p><formula xml:id="formula_19">?L(t) =<label>?L</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">COMPUTATIONAL COMPLEXITY AND FLOPS</head><p>In terms of computational complexity, the convolutional operation and the complex batchnorm are of the same order as their real counterparts. However, as a complex multiplication is 4 times more expensive than its real counterpart, all complex convolutions are 4 times more expensive as well.</p><p>Additionally, the complex BatchNorm is not implemented in cuDNN and therefore had to be simulated with a sizeable sequence of elementwise operations. This leads to a ballooning of the number of nodes in the compute graph and to inefficiencies due to lack of effective operation fusion. A dedicated cuDNN kernel will, however, reduce the cost to little more than that of the real-valued BatchNorm.</p><p>Ignoring elementwise operations, which constitute a negligible fraction of the floating-point operations in the neural network, we find that for all architectures in and for all of CIFAR10, CIFAR100 or SVHN, the inference cost in real FLOPS per example is roughly identical. It is ? 265 MFLOPS for the R-valued variant and ? 1030 MFLOPS for the C-valued variant of the architecture, approximately quadruple.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">CONVOLUTIONAL LSTM</head><p>A Convolutional LSTM is similar to a fully connected LSTM. The only difference is that, instead of using matrix multiplications to perform computation, we use convolutional operations. The computation in a realvalued Convolutional LSTM is defined as follows: </p><p>Where ? denotes the sigmoidal activation function, ? the elementwise multiplication and * the real-valued convolution. it, f t, ot represent the vector notation of the input, forget and output gates respectively. ct and ht represent the vector notation of the cell and hidden states respectively. the gates and states in a ConvLSTM are tensors whose last two dimensions are spatial dimensions. For each of the gates, Wxgate and W hgate are respectively the input and hidden kernels.</p><p>For the Complex Convolutional LSTM, we just replace the real-valued convolutional operation by its complex counterpart. We maintain the real-valued elementwise multiplication. The sigmoid and tanh are both performed separately on the real and the imaginary parts.</p><p>Published as a conference paper at ICLR 2018 Deeper representations correspond to greater z (blue axis). The gray ellipse encloses the input scalars within 1 standard deviation of the mean. Red ellipses enclose all scalars within 1 standard deviation of the mean after "standardization". Blue ellipses enclose all scalars within 1 standard deviation of the mean after left-multiplying all the scalars by a random 2 ? 2 linear transformation matrix. With the naive standardization, the distribution becomes progressively more elliptical with every layer, eventually collapsing to a line. This ill-conditioning manifests itself as NaNs in the forward pass or backward pass. With the complex standardization, the points' distribution is always successfully re-circularized. The bottom figure corresponds to the case where b &lt; 0 for modReLU. The radius of the white circle is equal to |b|. In case where b ? 0, the whole complex plane would be preserving both phase and magnitude information and the whole plane would have been colored with orange. Different colors represents different encoding of the complex information in the plane. We can see the for both zReLU and modReLU, the complex representation is discriminated into two regions, i.e, the one that preserves the whole complex information (colored in orange) and the one that cancels it (colored in white). However, CReLU discriminates the complex information into 4 regions where in two of which, phase information is projected and not canceled. This allows CReLU to discriminate information easier with respect to phase information than the other activation functions. For both zReLU and modReLU, we can see that phase information may be preserved explicitly through a number of layers when these activation functions are operating in their linear regime, prior to a layer further up in a network where the phase of an input lies in a zero region. CReLU has more flexibility manipulating phase as it can either set it to zero or ?/2, or even delete the phase information (when both real and imaginary parts are canceled) at a given level of depth in the network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>2: Classification error on CIFAR-10, CIFAR-100 and SVHN * using different normalization strategies. NCBN, CBN and BN stand for a Naive variant of the complex batch-normalization, complex batch-normalization and regular batch normalization respectively. (R) &amp; (C) refer to the use of the real-and complex-valued convolution respectively. The complex models use CReLU as activation. All models are constructed to have roughly 1.7M parameters. 5 out of 6 experiments using the naive variant of the complex batch normalization failed with the apparition of NaNs during training. As these experiments are already conclusive and due to limited resources, we haven't conducted other experiments for the NCBN model. A "-" is filled in front of an unperformed experiment. CBN(R) BN(C) NCBN(C) CBN(R) BN(C) NCBN(C) CBN(R) BN(C)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(b) A complex convolutional residual network (left) and an equivalent real-valued residual network (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Complex convolution and residual network implementation details. 6.1 MUSICNET ILLUSTRATIONS Figure 2: Precision-recall curveFigure 3: Predictions (Top) vs. ground truth (Bottom) for a music segment from the test set. 6.2 HOLOMORPHISM AND CAUCHY-RIEMANN EQUATIONS Holomorphism, also called analyticity, ensures that a complex-valued function is complex differentiable in the neighborhood of every point in its domain. This means that the derivative, f (z0) ? lim?z?0[ (f (z 0 )+?z)?f (z 0 ) ?z ] of f , exists at every point z0 in the domain of f where f is a complex-valued function of a complex variable z = x + i y such that f (z) = u(x, y) + i v(x, y)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Learning curve for speech spectrum prediction from dev set. 6.6 COMPLEX STANDARDIZATION AND INTERNAL COVARIATE SHIFT Depiction of Complex Standardization in Deep Complex Networks. At left, Naive Complex Standardization (division by complex standard deviation); At right, Complex Standardization (left-multiplication by inverse square root of covariance matrix between and ). The 250 input complex scalars are at the bottom, with (v) plotted on x (red axis) and (v) plotted on y (green axis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Phase information encoding for each of the activation functions tested for the Deep Complex Network. The x-axis represents the real part and the y-axis axis represents the imaginary part;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table /><note>4 presents our results on performing image classification on CIFAR-10, CIFAR-100. In addi- tion, we also consider a truncated version of the Street View House Numbers (SVHN) dataset which we call SVHN* . For computational reasons, we use the required 73,257 training images of Street View House Numbers (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>MusicNet experiments. FS is the sampling rate. Params is the total number of parameters. We report the average precision (AP) metric that is the area under the precision-recall curve.</figDesc><table><row><cell>ARCHITECTURE</cell><cell>FS</cell><cell cols="2">PARAMS AP, %</cell></row><row><cell>SHALLOW, REAL</cell><cell>11KHZ</cell><cell></cell><cell>66.1</cell></row><row><cell>SHALLOW, COMPLEX</cell><cell>11KHZ</cell><cell></cell><cell>66.0</cell></row><row><cell cols="2">SHALLOW, THICKSTUN ET AL. (2016) 44.1KHZ</cell><cell>-</cell><cell>67.8</cell></row><row><cell>DEEP, REAL</cell><cell>11KHZ</cell><cell>10.0M</cell><cell>69.6</cell></row><row><cell>DEEP, COMPLEX</cell><cell>11KHZ</cell><cell>8.8M</cell><cell>72.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Speech Spectrum Prediction on TIMIT test set. CConv-LSTM denotes the Complex Convolutional LSTM.</figDesc><table><row><cell>MODEL</cell><cell cols="3">#PARAMS MSE(VALIDATION) MSE(TEST)</cell></row><row><cell>LSTM WISDOM ET AL. (2016)</cell><cell>? 135K</cell><cell>16.59</cell><cell>16.98</cell></row><row><cell>FULL-CAPACITY URNN WISDOM ET AL. (2016)</cell><cell>? 135K</cell><cell>14.56</cell><cell>14.66</cell></row><row><cell>CONV-LSTM (OUR BASELINE)</cell><cell>? 88K</cell><cell>11.10</cell><cell>12.18</cell></row><row><cell>CCONV-LSTM (OURS)</cell><cell>? 88K</cell><cell>10.78</cell><cell>11.90</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>it = ?(Wxi * xt + W hi * Wt?1 + bi) f t = ?(W xf * xt + W hf * ht?1 + b f ) ct = f t ? ct?1 + it ? tanh(Wxc * xt + W hc * ht?1 + bc) ot = ?(Wxo * xt + W ho * ht?1 + bo) ht = ot ? tanh(ct)</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The source code is located at http://github.com/ChihebTrabelsi/deep_complex_ networks</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We are grateful to Roderick Murray-Smith, J?rn-Henrik Jacobsen, Jesse Engel and all the students at MILA, especially Jason Jo, Anna Huang and Akram Erraqabi for helpful feedback and discussions. We also thank the developers of Theano (Theano Development Team, 2016) and Keras <ref type="bibr" target="#b50">(Chollet et al., 2015)</ref>. We are grateful to Samsung and the Fonds de Recherche du Qu?bec -Nature et Technologie for their financial support. We would also like to acknowledge NVIDIA for donating a DGX-1 computer used in this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07868</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2377" to="2385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<title level="m">Identity mappings in deep residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Untersuchungen zu dynamischen neuronalen Netzen. PhD thesis, diploma thesis, institut f?r informatik, lehrstuhl prof. brauer, technische universit?t m?nchen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the critical points of the complex-valued neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nitta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing, 2002. ICONIP&apos;02. Proceedings of the 9th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1099" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generalization characteristics of complex-valued feedforward neural networks in relation to signal coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Hirose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shotaro</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unitary evolution recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06464</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Associative long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benigno</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.03032</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Full-capacity unitary recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Les</forename><surname>Atlas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4880" to="4888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Holographic reduced representation: Distributed representation for cognitive structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Plate</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Reichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Serre</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6115</idno>
		<title level="m">Neuronal synchrony in complex-valued deep networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno>abs/1609.03499</idno>
	</analytic>
	<monogr>
		<title level="m">Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw</title>
		<meeting><address><addrLine>Alex Graves</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Oriol Vinyals</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Encoder-decoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Georg Zilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutn?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.03474</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Recurrent highway networks</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the importance of phase in human speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangji</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><forename type="middle">Modir</forename><surname>Shanechi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parham</forename><surname>Aarabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on audio, speech, and language processing</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The importance of phase in signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Alan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">S</forename><surname>Oppenheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1981" />
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="529" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spectral representations for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2449" to="2457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Complex domain backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cris</forename><surname>Georgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koutsougeras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on Circuits and systems II: analog and digital signal processing</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="330" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lending direction to neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mozer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="503" to="512" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Approximation by fully complex multilayer perceptrons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taehwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T?lay</forename><surname>Adal?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1641" to="1666" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Complex-valued neural networks: theories and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Hirose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Scientific</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Orthogonality of decision boundaries in complex-valued neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tohru</forename><surname>Nitta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="97" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee-Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">153</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient learning of sparse representations with an energy-based model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Poultney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yann L Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1137" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A mathematical motivation for complex-valued convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serkan</forename><surname>Piantino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Tygert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03438</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep roto-translation scattering for object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Oyallon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2865" to="2873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Tygert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.08230</idno>
		<title level="m">Scale-invariant learning and convolutional networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">J</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniyar</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brostow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04642</idno>
		<title level="m">Deep translation and rotation equivariance</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th?o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Complex and holographic embeddings of knowledge graphs: a comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th?o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01475</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sarroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael A</forename><surname>Shepardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Casey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06351</idno>
		<title level="m">Learning representations using complex-valued nets</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitzan</forename><surname>Guberman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.09046</idno>
		<title level="m">On complex valued convolutional neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06068</idno>
		<title level="m">Reducing overfitting in deep networks by decorrelating representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Aistats</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A method of solving a convex programming problem with convergence rate o (1/k2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurii</forename><surname>Nesterov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning features of music from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Thickstun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Julius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<ptr target="http://www-ccrma.stanford.edu/?jos/resample" />
		<title level="m">Digital audio resampling</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Shi Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theano</forename><surname>Development Team</surname></persName>
		</author>
		<idno>abs/1605.02688</idno>
		<ptr target="http://arxiv.org/abs/1605.02688" />
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Keras: Deep learning library for theano and tensorflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io/k" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
