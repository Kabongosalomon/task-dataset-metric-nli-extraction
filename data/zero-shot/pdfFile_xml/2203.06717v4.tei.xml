<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="department" key="dep2">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhuang</forename><surname>Zhou</surname></persName>
							<email>zhouyizhuang@megvii.com</email>
							<affiliation key="aff1">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
							<email>jungonghan77@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Aberystwyth University</orgName>
								<address>
									<postCode>SY23 3FL</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
							<email>dinggg@tsinghua.edu.cnsunjian@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="department" key="dep2">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We revisit large kernel design in modern convolutional neural networks (CNNs). Inspired by recent advances in vision transformers (ViTs), in this paper, we demonstrate that using a few large convolutional kernels instead of a stack of small kernels could be a more powerful paradigm. We suggested five guidelines, e.g., applying re-parameterized large depth-wise convolutions, to design efficient highperformance large-kernel CNNs. Following the guidelines, we propose RepLKNet, a pure CNN architecture whose kernel size is as large as 31?31, in contrast to commonly used 3?3. RepLKNet greatly closes the performance gap between CNNs and ViTs, e.g., achieving comparable or superior results than Swin Transformer on ImageNet and a few typical downstream tasks, with lower latency. RepLKNet also shows nice scalability to big data and large models, obtaining 87.8% top-1 accuracy on ImageNet and 56.0% mIoU on ADE20K, which is very competitive among the state-of-the-arts with similar model sizes. Our study further reveals that, in contrast to small-kernel CNNs, largekernel CNNs have much larger effective receptive fields and higher shape bias rather than texture bias. Code &amp; models at https://github.com/megvii-research/ RepLKNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional neural networks (CNNs) <ref type="bibr" target="#b21">[42,</ref><ref type="bibr" target="#b35">55]</ref> used to be a common choice of visual encoders in modern computer vision systems. However, recently, CNNs <ref type="bibr" target="#b21">[42,</ref><ref type="bibr" target="#b35">55]</ref> have been greatly challenged by Vision Transformers (ViTs) <ref type="bibr" target="#b14">[35,</ref><ref type="bibr" target="#b41">61,</ref><ref type="bibr" target="#b70">89,</ref><ref type="bibr" target="#b79">98]</ref>, which have shown leading performances on many visual tasks -not only image classification <ref type="bibr" target="#b14">[35,</ref><ref type="bibr" target="#b90">109]</ref> and representation learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr">10,</ref><ref type="bibr">17,</ref><ref type="bibr" target="#b86">105]</ref>, but also many downstream tasks such as object detection <ref type="bibr">[25,</ref><ref type="bibr" target="#b41">61]</ref>, semantic segmentation <ref type="bibr" target="#b79">[98,</ref><ref type="bibr" target="#b84">103]</ref> and image restoration <ref type="bibr">[11,</ref><ref type="bibr" target="#b36">56]</ref>. Why are ViTs super powerful? Some works believed that multi-head self-attention (MHSA) mechanism in ViTs plays a key role. They provided empirical results to demonstrate that, MHSA is more flexible <ref type="bibr" target="#b32">[52]</ref>, capable (less inductive bias) <ref type="bibr">[21]</ref>, more robust to distortions <ref type="bibr" target="#b48">[68,</ref><ref type="bibr" target="#b84">103]</ref>, or able to model long-range dependencies <ref type="bibr" target="#b51">[71,</ref><ref type="bibr" target="#b74">93]</ref>. But some works challenge the necessity of MHSA <ref type="bibr" target="#b102">[121]</ref>, attributing the high performance of ViTs to the proper building blocks <ref type="bibr" target="#b13">[34]</ref>, and/or dynamic sparse weights <ref type="bibr" target="#b19">[40,</ref><ref type="bibr" target="#b97">116]</ref>. More works <ref type="bibr">[21,</ref><ref type="bibr" target="#b19">40,</ref><ref type="bibr" target="#b23">44,</ref><ref type="bibr" target="#b81">100,</ref><ref type="bibr" target="#b102">121]</ref> explained the superiority of ViTs from different point of views.</p><p>In this work, we focus on one view: the way of building up large receptive fields. In ViTs, MHSA is usually designed to be either global <ref type="bibr" target="#b14">[35,</ref><ref type="bibr" target="#b60">80,</ref><ref type="bibr" target="#b79">98]</ref> or local but with large kernels <ref type="bibr" target="#b41">[61,</ref><ref type="bibr" target="#b52">72,</ref><ref type="bibr" target="#b73">92]</ref>, thus each output from a single MHSA layer is able to gather information from a large region. However, large kernels are not popularly employed in CNNs (except for the first layer <ref type="bibr" target="#b21">[42]</ref>). Instead, a typical fashion is to use a stack of many small spatial convolutions <ref type="bibr" target="#b0">1</ref>  <ref type="bibr" target="#b21">[42,</ref><ref type="bibr" target="#b25">46,</ref><ref type="bibr" target="#b28">49,</ref><ref type="bibr" target="#b50">70,</ref><ref type="bibr" target="#b59">79,</ref><ref type="bibr" target="#b64">84,</ref><ref type="bibr" target="#b95">114]</ref> (e.g., 3?3) to enlarge the receptive fields in state-of-the-art CNNs. Only some old-fashioned networks such as AlexNet <ref type="bibr" target="#b35">[55]</ref>, Inceptions <ref type="bibr" target="#b61">[81]</ref><ref type="bibr" target="#b62">[82]</ref><ref type="bibr" target="#b63">[83]</ref> and a few architectures derived from neural architecture search <ref type="bibr" target="#b18">[39,</ref><ref type="bibr" target="#b24">45,</ref><ref type="bibr" target="#b38">58,</ref><ref type="bibr" target="#b103">122]</ref> adopt large spatial convolutions (whose size is greater than 5) as the main part. The above view naturally lead to a question: what if we use a few large instead of many small kernels to conventional CNNs? Is large kernel or the way of building large receptive fields the key to close the performance gap between CNNs and ViTs?</p><p>To answer this question, we systematically explore the large kernel design of CNNs. We follow a very simple "philosophy": just introducing large depth-wise convolutions into conventional networks, whose sizes range from 3?3 to 31?31, although there exist other alternatives to introduce large receptive fields via a single or a few layers, e.g. feature pyramids <ref type="bibr" target="#b77">[96]</ref>, dilated convolutions [14, <ref type="bibr" target="#b87">106,</ref><ref type="bibr" target="#b88">107]</ref> and deformable convolutions <ref type="bibr">[24]</ref>. Through a series of experiments, we summarize five empirical guidelines to effectively employ large convolutions: 1) very large kernels can still be efficient in practice; 2) identity shortcut is vital especially for networks with very large kernels; 3) reparameterizing <ref type="bibr" target="#b10">[31]</ref> with small kernels helps to make up the optimization issue; 4) large convolutions boost downstream tasks much more than ImageNet; 5) large kernel is useful even on small feature maps.</p><p>Based on the above guidelines, we propose a new architecture named RepLKNet, a pure 2 CNN where reparameterized large convolutions are employed to build up large receptive fields. Our network in general follows the macro architecture of Swin Transformer <ref type="bibr" target="#b41">[61]</ref> with a few modifications, while replacing the multi-head selfattentions with large depth-wise convolutions. We mainly benchmark middle-size and large-size models, since ViTs used to be believed to surpass CNNs on large data and models. On ImageNet classification, our baseline (similar model size with Swin-B), whose kernel size is as large as 31?31, achieves 84.8% top-1 accuracy trained only on ImageNet-1K dataset, which is 0.3% better than Swin-B but much more efficient in latency.</p><p>More importantly, we find that the large kernel design is particularly powerful on downstream tasks. For example, our networks outperform ResNeXt-101 <ref type="bibr" target="#b85">[104]</ref> or ResNet-101 <ref type="bibr" target="#b21">[42]</ref> backbones by 4.4% on COCO detection <ref type="bibr" target="#b37">[57]</ref> and 6.1% on ADE20K segmentation <ref type="bibr" target="#b101">[120]</ref> under the similar complexity and parameter budget, which is also on par with or even better than the counterpart Swin Transformers but with higher inference speed. Given more pretraining data (e.g., 73M images) and more computational budget, our best model obtains very competitive results among the stateof-the-arts with similar model sizes, e.g. 87.8% top-1 accuracy on ImageNet and 56.0% on ADE20K, which shows excellent scalability towards large-scale applications.</p><p>We believe the high performance of RepLKNet is mainly because of the large effective receptive fields (ERFs) <ref type="bibr" target="#b45">[65]</ref> built via large kernels, as compared in <ref type="figure" target="#fig_0">Fig. 1</ref>. Moreover, Re-pLKNet is shown to leverage more shape information than conventional CNNs, which partially agrees with human's cognition. We hope our findings can help to understand the intrinsic mechanism of both CNNs and ViTs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Models with Large Kernels</head><p>As mentioned in the introduction, apart from a few old-fashioned models like Inceptions <ref type="bibr" target="#b61">[81]</ref><ref type="bibr" target="#b62">[82]</ref><ref type="bibr" target="#b63">[83]</ref>, largekernel models became not popular after VGG-Net <ref type="bibr" target="#b59">[79]</ref>. One representative work is Global Convolution Networks (GCNs) <ref type="bibr" target="#b49">[69]</ref>, which uses very large convolutions of 1?K followed by K?1 to improve semantic segmentation task. However, large kernels are reported to harm the performance on ImageNet. Local Relation Networks (LR-Net) <ref type="bibr" target="#b26">[47]</ref> proposes a spatial aggregation operator (LR-Layer) to replace standard convolutions, which can be viewed as a dynamic convolution. LR-Net could benefit from a kernel size of 7?7, but the performance decreases with 9?9. With a kernel size as large as the feature map, the top-1 accuracy significantly reduced from 75.7% to 68.4%.</p><p>Recently, Swin Transformers <ref type="bibr" target="#b41">[61]</ref> propose to capture the spatial patterns with shifted window attention, whose window sizes range from 7 to 12, which can also be viewed as a variant of large kernel. The follow-ups <ref type="bibr" target="#b12">[33,</ref><ref type="bibr" target="#b40">60]</ref> employ even larger window sizes. Inspired by the success of those local transformers, a recent work <ref type="bibr" target="#b19">[40]</ref> replaces MHSA layers with static or dynamic 7?7 depth-wise convolutions in <ref type="bibr" target="#b41">[61]</ref> while still maintains comparable results. Though the network proposed by <ref type="bibr" target="#b19">[40]</ref> shares similar design pattern with ours, the motivations are different: <ref type="bibr" target="#b19">[40]</ref> does not investigate the relationship between ERFs, large kernels and performances; instead, it attributes the superior performances of vision transformers to sparse connections, shared parameters and dynamic mechanisms. Another three representative works are Global Filter Networks (GFNets) <ref type="bibr" target="#b54">[74]</ref>, CK-Conv <ref type="bibr" target="#b56">[76]</ref> and FlexConv <ref type="bibr" target="#b55">[75]</ref>. GFNet optimizes the spatial connection weights in the Fourier domain, which is equivalent to circular global convolutions in the spatial domain. CKConv formulates kernels as continuous functions to process sequential data, which can construct arbitrarily large kernels. FlexConv learns different kernel sizes for different <ref type="table">Table 1</ref>. Inference speed of a stack of 24-layer depth-wise convolutions with various kernel sizes and resolutions on a single GTX 2080Ti GPU. The input shape is <ref type="bibr">(64, 384, R, R)</ref>. Baselines are evaluated with Pytorch 1.9.0 + cuDNN 7.6.5, in FP32 precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Resolution R Impl</head><p>Latency <ref type="table" target="#tab_1">(ms) @ Kernel size  3  5  7  9  13  17  21  27  29</ref>  layers, which can be as large as the feature maps. Although they use very large kernels, they do not intend to answer the key questions we desire: why do traditional CNNs underperform ViTs, and how to apply large kernels in common CNNs. Besides, both <ref type="bibr" target="#b19">[40]</ref> and <ref type="bibr" target="#b54">[74]</ref> do not evaluate their models on strong baselines, e.g., models larger than Swin-L. Hence it is still unclear whether large-kernel CNNs can scale up well as transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concurrent works.</head><p>ConvMixer <ref type="bibr" target="#b71">[90]</ref> uses up to 9?9 convolutions to replace the "mixer" component of ViTs <ref type="bibr" target="#b14">[35]</ref> or MLPs <ref type="bibr" target="#b68">[87,</ref><ref type="bibr" target="#b69">88]</ref>. MetaFormer <ref type="bibr" target="#b89">[108]</ref> suggests pooling layer is an alternate to self-attention. ConvNeXt <ref type="bibr" target="#b42">[62]</ref> employs 7?7 depth-wise convolutions to design strong architectures, pushing the limit of CNN performances. Although those works show excellent performances, they do not show benefits from much larger convolutions (e.g., 31?31).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Model Scaling Techniques</head><p>Given a small model, it is a common practice to scale it up for better performance, thus scaling strategy plays a vital role in the resultant accuracy-efficiency trade-offs. For CNNs, existing scaling approaches usually focus on model depth, width, input resolution <ref type="bibr" target="#b11">[32,</ref><ref type="bibr" target="#b50">70,</ref><ref type="bibr" target="#b64">84]</ref>, bottleneck ratio and group width <ref type="bibr" target="#b11">[32,</ref><ref type="bibr" target="#b50">70]</ref>. Kernel size, however, is often neglected. In Sec. 3, we will show that the kernel size is also an important scaling dimension in CNNs, especially for downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Structural Re-parameterization</head><p>Structural Re-parameterization <ref type="bibr" target="#b6">[27]</ref><ref type="bibr" target="#b7">[28]</ref><ref type="bibr" target="#b8">[29]</ref><ref type="bibr" target="#b9">[30]</ref><ref type="bibr" target="#b10">[31]</ref> is a methodology of equivalently converting model structures via transforming the parameters. For example, RepVGG targeted at a deep inference-time VGG-like (e.g., branch-free) model, and constructed extra ResNet-style shortcuts parallel to the 3?3 layers during training. In contrast to a real VGG-like model that is difficult to train <ref type="bibr" target="#b21">[42]</ref>, such shortcuts helped the model reach a satisfactory performance. After training, the shortcuts are absorbed into the parallel 3?3 kernels via a series of linear transformations, so that the resultant model becomes a VGG-like model. In this paper, we use this methodology to add a relatively small (e.g., 3?3 or 5?5) kernel into a very large kernel. In this way, we make the very large kernel capable of capturing small-scale patterns, hence improve the performance of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Guidelines of Applying Large Convolutions</head><p>Trivially applying large convolutions to CNNs usually leads to inferior performance and speed. In this section, we summarize 5 guidelines for effectively using large kernels.</p><p>Guideline 1: large depth-wise convolutions can be efficient in practice. It is believed that large-kernel convolutions are computationally expensive because the kernel size quadratically increases the number of parameters and FLOPs. The drawback can be greatly overcome by applying depth-wise (DW) convolutions <ref type="bibr">[18,</ref><ref type="bibr" target="#b25">46]</ref>. For example, in our proposed RepLKNet (see <ref type="table" target="#tab_5">Table 5</ref> for details), increasing the kernel sizes in different stages from <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b2">3]</ref> to <ref type="bibr" target="#b10">[31,</ref><ref type="bibr" target="#b8">29,</ref><ref type="bibr" target="#b6">27,</ref><ref type="bibr">13]</ref> only increases the FLOPs and number of parameters by 18.6% and 10.4% respectively, which is acceptable. The remaining 1?1 convolutions actually dominate most of the complexity.</p><p>One may concern that DW convolutions could be very inefficient on modern parallel computing devices like GPUs. It is true for conventional DW 3?3 kernels <ref type="bibr" target="#b25">[46,</ref><ref type="bibr" target="#b57">77,</ref><ref type="bibr" target="#b95">114]</ref>, because DW operations introduce low ratio of computation vs. memory access cost <ref type="bibr" target="#b46">[66]</ref>, which is not friendly to modern computing architecture. However, we find when kernel size becomes large, the computational density increases: for example, in a DW 11?11 kernel, each time we load a value from the feature map, it can attend at most 121 multiplications, while in a 3?3 kernel the number is only 9. Therefore, according to the roofline model, the actual latency should not increase as much as the increasing of FLOPs when kernel size becomes larger. Remark 1. Unfortunately, we find off-the-shelf deep learning tools (such as Pytorch) support large DW convolutions poorly, as shown in <ref type="table">Table 1</ref>. Hence we try several approaches to optimize the CUDA kernels. FFT-based approach <ref type="bibr" target="#b47">[67]</ref> appears reasonable to implement large convolutions. However, in practice we find block-wise (inverse) implicit gemm algorithm is a better choice. The implementation has been integrated into the open-sourced framework MegEngine <ref type="bibr" target="#b0">[1]</ref> and we omit the details here. We have also released an efficient implementation <ref type="bibr" target="#b1">[2]</ref> for PyTorch. Ta- ble 1 shows that our implementation is far more efficient, compared with the Pytorch baseline. With our optimization, the latency contribution of DW convolutions in RepLKNet reduces from 49.5% to 12.3%, which is roughly in proportion to the FLOPs occupation.</p><p>Guideline 2: identity shortcut is vital especially for networks with very large kernels. To demonstrate this, we use MobileNet V2 <ref type="bibr" target="#b57">[77]</ref> to benchmark, since it heavily uses DW layers and has two published variants (with or without shortcuts). For the large-kernel counterparts, we simply replace all the DW 3?3 layers with 13?13. All the models are trained on ImageNet with the identical training configurations for 100 epochs (see Appendix A for details). <ref type="table" target="#tab_1">Table 2</ref> shows large kernels improve the accuracy of MobileNet V2 with shortcuts by 0.77%. However, without shortcuts, large kernels reduce the accuracy to only 53.98%. Remark 2. The guideline also works for ViTs. A recent work <ref type="bibr" target="#b13">[34]</ref> finds that without identity shortcut, attention loses rank doubly exponentially with depth, leading to oversmoothing issue. Although large-kernel CNNs may degenerate in a different mechanism from ViT's, we also observed without shortcut, it is difficult for the network to capture local details. From a similar perspective as <ref type="bibr" target="#b75">[94]</ref>, shortcuts make the model an implicit ensemble composed of numerous models with different receptive fields (RFs), so it can benefit from a much larger maximum RF while not losing the ability to capture small-scale patterns.</p><p>Guideline 3: re-parameterizing <ref type="bibr" target="#b10">[31]</ref> with small kernels helps to make up the optimization issue. We replace the 3?3 layers of MobileNet V2 by 9?9 and 13?13 respectively, and optionally adopt Structural Reparameterization <ref type="bibr" target="#b6">[27,</ref><ref type="bibr" target="#b7">28,</ref><ref type="bibr" target="#b10">31]</ref> methodology. Specifically, we construct a 3?3 layer parallel to the large one, then add up their outputs after Batch normalization (BN) <ref type="bibr" target="#b31">[51]</ref> layers ( <ref type="figure">Fig. 2)</ref>. After training, we merge the small kernel as well as BN parameters into the large kernel, so the resultant model is equivalent to the model for training but no longer has small kernels. <ref type="table" target="#tab_2">Table 3</ref> shows directly increasing the kernel size from 9 to 13 reduces the accuracy, while re-parameterization addresses the issue. We then transfer the ImageNet-trained models to semantic segmentation with DeepLabv3+ [16] on Cityscapes <ref type="bibr">[22]</ref>. We only replace the backbone and keep all the default training settings provided by MMSegmentation <ref type="bibr">[20]</ref>. The observation is similar to that on ImageNet: 3?3 re-param im- proves the mIoU of the 9?9 model by 0.19 and the 13?13 model by 0.93. With such simple re-parameterization, increasing kernel size from 9 to 13 no longer degrades the performance on both ImageNet and Cityscapes. Remark 3. It is known that ViTs have optimization problem especially on small datasets <ref type="bibr" target="#b14">[35,</ref><ref type="bibr" target="#b39">59]</ref>. A common workaround is to introduce convolutional prior, e.g., add a DW 3?3 convolution to each self-attention block <ref type="bibr">[19,</ref><ref type="bibr" target="#b82">101]</ref>, which is analogous to ours. Those strategies introduce additional translational equivariance and locality prior to the network, making it easier to optimize on small dataset without loss of generality. Similar to what ViT behaves <ref type="bibr" target="#b14">[35]</ref>, we also find when the pretraining dataset increases to 73 million images (refer to RepLKNet-XL in the next section), re-parameterization can be omitted without degradation.</p><p>Guideline 4: large convolutions boost downstream tasks much more than ImageNet classification.  <ref type="figure">Figure 2</ref>. An example of re-parameterizing a small kernel (e.g., 3?3) into a large one (e.g., 7?7). See <ref type="bibr" target="#b7">[28,</ref><ref type="bibr" target="#b10">31]</ref> for details.    <ref type="bibr" target="#b15">36]</ref>. Fortunately, we find simply enlarging the kernel size in CNNs can effectively improve the shape bias. Please refer to Appendix C for details.</p><p>Guideline 5: large kernel (e.g., 13?13) is useful even on small feature maps (e.g., 7?7). To validate it, We enlarge the DW convolutions in the last stage of MobileNet V2 to 7?7 or 13?13, hence the kernel size is on par with or even larger than feature map size (7?7 by default). We apply re-parameterization to the large kernels as suggested by Guideline 3. <ref type="table" target="#tab_4">Table 4</ref> shows although convolutions in the last stage already involve very large receptive field, further increasing the kernel sizes still leads to performance improvements, especially on downstream tasks such as Cityscapes. Remark 5. When kernel size becomes large, notice that translational equivariance of CNNs does not strictly hold.</p><p>As illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>, two outputs at adjacent spatial locations share only a fraction of the kernel weights, i.e., are transformed by different mappings. The property also agrees with the "philosophy" of ViTs -relaxing the symmetric prior to obtain more capacity. Interestingly, we find 2D Relative Position Embedding (RPE) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b58">78]</ref>, which is widely used in the transformer community, can also be viewed as a large depth-wise kernel of size (2H ?1)?(2W ?1), where H and W are feature map height and width respectively. Large kernels not only help to learn the relative positions between concepts, but also encode the absolute position information due to padding effect <ref type="bibr" target="#b33">[53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RepLKNet: a Large-Kernel Architecture</head><p>Following the above guidelines, in this section we propose RepLKNet, a pure CNN architecture with large kernel design. To our knowledge, up to now CNNs still dominate small models <ref type="bibr" target="#b94">[113,</ref><ref type="bibr" target="#b96">115]</ref>, while vision transformers are believed to be better than CNNs under more complexity budget. Therefore, in the paper we mainly focus on relatively large models (whose complexity is on par with or larger than ResNet-152 <ref type="bibr" target="#b21">[42]</ref> or Swin-B <ref type="bibr" target="#b41">[61]</ref>), in order to verify whether large kernel design could eliminate the performance gap between CNNs and ViTs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Architecture Specification</head><p>We sketch the architecture of RepLKNet in <ref type="figure">Fig. 4</ref>: Stem refers to the beginning layers. Since we target at high performance on downstream dense-prediction tasks, we desire to capture more details by several conv layers at the beginning. After the first 3?3 with 2? downsampling, we arrange a DW 3?3 layer to capture low-level patterns, a 1?1 conv, and another DW 3?3 layer for downsampling.</p><p>Stages 1-4 each contains several RepLK Blocks, which use shortcuts (Guideline 2) and DW large kernels (Guideline 1). We use 1?1 conv before and after DW conv as a common practice. Note that each DW large conv uses a 5?5 kernel for re-parameterization (Guideline 3), which is not shown in <ref type="figure">Fig. 4</ref>. Except for the large conv layers which provide sufficient receptive field and the ability to aggregate spatial information, the model's representational capacity is also closely related to the depth. To provide more nonlinearities and information communications across channels, we desire to use 1?1 layers to increase the depth. Inspired by the Feed-Forward Network (FFN) which has been widely used in transformers <ref type="bibr" target="#b14">[35,</ref><ref type="bibr" target="#b41">61]</ref> and MLPs <ref type="bibr" target="#b6">[27,</ref><ref type="bibr" target="#b68">87,</ref><ref type="bibr" target="#b69">88]</ref>, we use a similar CNN-style block composed of shortcut, BN, two 1?1 layers and GELU <ref type="bibr" target="#b22">[43]</ref>, so it is referred to as ConvFFN Block. Compared to the classic FFN which uses Layer Normalization <ref type="bibr" target="#b2">[3]</ref> before the fully-connected layers, BN has an advantage that it can be fused into conv for efficient inference. As a common practice, the number of internal channels of the ConvFFN Block is 4? as the input. Simply following ViT and Swin, which interleave attention and FFN blocks, we place a ConvFFN after each RepLK Block.</p><p>Transition Blocks are placed between stages, which first increase the channel dimension via 1?1 conv and then con-  <ref type="figure">Figure 4</ref>. RepLKNet comprises Stem, Stages and Transitions. Except for depth-wise (DW) large kernel, the other components include DW 3?3, dense 1?1 conv, and batch normalization <ref type="bibr" target="#b31">[51]</ref> (BN). Note that every conv layer has a following BN, which are not depicted. Such conv-BN sequences use ReLU as the activation function, except those before the shortcut-addition (as a common practice <ref type="bibr" target="#b21">[42,</ref><ref type="bibr" target="#b57">77]</ref>) and those preceding GELU <ref type="bibr" target="#b22">[43]</ref>. In summary, each stage has three architectural hyper-parameters: the number of RepLK Blocks B, the channel dimension C, and the kernel size K. So that a RepLKNet architecture is defined by  <ref type="bibr" target="#b10">[31,</ref><ref type="bibr" target="#b8">29,</ref><ref type="bibr" target="#b6">27,</ref><ref type="bibr">13]</ref>, respectively, and refer to the models as RepLKNet-13/25/31. We also construct two small-kernel baselines where the kernel sizes are all 3 or 7 (RepLKNet-3/7).</p><formula xml:id="formula_0">[B 1 , B 2 , B 3 , B 4 ],[C 1 , C 2 , C 3 , C 4 ],[K 1 , K 2 , K 3 , K 4 ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Making Large Kernels Even Larger</head><p>On ImageNet, we train for 120 epochs with AdamW [64] optimizer, RandAugment [23], mixup <ref type="bibr" target="#b92">[111]</ref>, CutMix <ref type="bibr" target="#b91">[110]</ref>, For semantic segmentation, we use ADE20K <ref type="bibr" target="#b101">[120]</ref>, which is a widely-used large-scale semantic segmentation dataset containing 20K images of 150 categories for training and 2K for validation. We use the ImageNet-trained models as backbones and adopt UperNet <ref type="bibr" target="#b83">[102]</ref> implemented by MMSegmentation [20] with the 80K-iteration training setting and test the single-scale mIoU. <ref type="table" target="#tab_5">Table 5</ref> shows our results with different kernel sizes. On ImageNet, though increasing the kernel sizes from 3 to 13 improves the accuracy, making them even larger brings no further improvements. However, on ADE20K, scaling up the kernels from [13, 13, 13, 13] to <ref type="bibr" target="#b10">[31,</ref><ref type="bibr" target="#b8">29,</ref><ref type="bibr" target="#b6">27</ref>, 13] brings 0.82 higher mIoU with only 5.3% more parameters and 3.5% higher FLOPs, which highlights the significance of large kernels for downstream tasks.</p><p>In the following subsections, we use RepLKNet-31 with stronger training configurations to compare with the state-of-the-arts on ImageNet classification, Cityscapes/ADE20K semantic segmentation and COCO <ref type="bibr" target="#b37">[57]</ref> object detection. We refer to the aforementioned model as RepLKNet-31B (B for Base) and a wider model with C = [192, 384, 768, 1536] as RepLKNet-31L (Large). We construct another RepLKNet-XL with C = [256, 512, 1024, 2048] and 1.5? inverted bottleneck design in the RepLK Blocks (i.e., the channels of the DW large conv layers are 1.5? as the inputs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">ImageNet Classification</head><p>Since the overall architecture of RepLKNet is akin to Swin, we desire to make a comparison at first. For RepLKNet-31B on ImageNet-1K, we extend the aforementioned training schedule to 300 epochs for a fair comparison. Then we finetune for 30 epochs with input resolution of 384?384, so that the total training cost is much lower We also present the throughput tested with a batch size of 64 on the same 2080Ti GPU. The training configurations are presented in the Appendix. <ref type="table" target="#tab_7">Table 6</ref> shows that though very large kernels are not intended for ImageNet classification, our RepLKNet models show a a favorable trade-off between accuracy and efficiency. Notably, with only ImageNet-1K training, RepLKNet-31B reaches 84.8% accuracy, which is 0.3% higher than Swin-B, and runs 43% faster. And even though RepLKNet-XL has higher FLOPs than Swin-L, it runs faster, which highlights the efficiency of very large kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Semantic Segmentation</head><p>We then use the pretrained models as the backbones on Cityscapes <ref type="table" target="#tab_8">(Table 7)</ref> and ADE20K <ref type="table">(Table 8)</ref>. Specifically, we use the UperNet <ref type="bibr" target="#b83">[102]</ref> implemented by MMSegmentation [20] with the 80K-iteration training schedule for Cityscapes and 160K for ADE20K. Since we desire to evaluate the backbone only, we do not use any advanced techniques, tricks, nor custom algorithms.</p><p>On Cityscapes, ImageNet-1K-pretrained RepLKNet-31B outperforms Swin-B by a significant margin (singlescale mIoU of 2.7), and even outperforms the ImageNet-22K-pretrained Swin-L. Even equipped with DiversePatch <ref type="bibr" target="#b17">[38]</ref>, a technique customized for vision transformers, the single-scale mIoU of the 22K-pretrained Swin-L is still lower than our 1K-pretrained RepLKNet-31B, though the former has 2? parameters.</p><p>On ADE20K, RepLKNet-31B outperforms Swin-B with both 1K and 22K pretraining, and the margins of singlescale mIoU are particularly significant. Pretrained with our semi-supervised dataset MegData73M, RepLKNet-XL achieves an mIoU of 56.0, which shows feasible scalability towards large-scale vision applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Object Detection</head><p>For object detection, we use RepLKNets as the backbone of FCOS <ref type="bibr" target="#b66">[86]</ref> and Cascade Mask R-CNN <ref type="bibr">[9,</ref><ref type="bibr" target="#b20">41]</ref>, which are representatives of one-stage and two-stage detection methods, and the default configurations in MMDetection <ref type="bibr">[13]</ref>. The FCOS model is trained with the 2x (24-epoch) training schedule for a fair comparison with the X101 (short for ResNeXt-101 <ref type="bibr" target="#b85">[104]</ref>) baseline from the same code base <ref type="bibr">[20]</ref>, and the other results with Cascade Mask R-CNN all use 3x <ref type="bibr">(36-epoch)</ref>. Again, we simply replace the backbone and do not use any advanced techniques. <ref type="table">Table 9</ref> shows RepLKNets outperform ResNeXt-101-64x4d by up to 4.4 mAP while have fewer parameters and lower FLOPs. Note that the results may be further improved with the advanced techniques like HTC [12], HTC++ <ref type="bibr" target="#b41">[61]</ref>, Soft-NMS [7] or a 6x (72-epoch) schedule. Compared to Swin, RepLKNets achieve higher or comparable mAP with fewer parameters and lower FLOPs. Notably, RepLKNet-XL achieves an mAP of 55.5, which demonstrates the scalability again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Large-Kernel CNNs have Larger ERF than Deep Small-Kernel Models</head><p>We have demonstrated large kernel design can significantly boost CNNs (especially on downstream tasks). However, it is worth noting that large kernel can be expressed <ref type="table">Table 9</ref>. Object detection on COCO. The FLOPs is computed with 1280?800 inputs. The results of ResNeXt-101-64x4d + Cas Mask are reported by <ref type="bibr" target="#b41">[61]</ref>. The results of 22K-pretrained Swin (without HTC++ <ref type="bibr" target="#b41">[61]</ref>) are reported by <ref type="bibr" target="#b42">[62]</ref>. by a series of small convolutions <ref type="bibr" target="#b59">[79]</ref>, e.g., a 7?7 convolution can be decomposed into a stack of three 3?3 kernels without information loss (more channels are required after the decomposition to maintain the degree of freedom). Given that fact, a question naturally comes up: why do conventional CNNs, which may contain tens or hundreds of small convolutions (e.g., ResNets <ref type="bibr" target="#b21">[42]</ref>), still behave inferior to large-kernel networks?</p><p>We argue that in terms of obtaining large receptive field, a single large kernel is much more effective than many small kernels. First, according to the theory of Effective Receptive Field (ERF) <ref type="bibr" target="#b45">[65]</ref>, ERF is proportion to O(K ? L), where K is the kernel size and L is the depth, i.e., number of layers. In other words, ERF grows linearly with the kernel size while sub-linearly with the depth. Second, the increasing depth introduces optimization difficulty <ref type="bibr" target="#b21">[42]</ref>. Although ResNets seem to overcome the dilemma, managing to train a network with hundreds of layers, some works <ref type="bibr" target="#b5">[26,</ref><ref type="bibr" target="#b75">94]</ref> indicate ResNets might not be as deep as they appear to be. For example, <ref type="bibr" target="#b75">[94]</ref> suggests ResNets behave like ensembles of shallow networks, which implies the ERFs of ResNets could still be very limited even if the depth dramatically increases. Such phenomenon is also empirically observed in previous works <ref type="bibr" target="#b34">[54]</ref>. To summarize, large kernels design requires fewer layers to obtain large ERFs and avoids the optimization issue brought by the increasing depth.</p><p>To support our viewpoint, we choose ResNet-101/152 and the aforementioned RepLKNet-13/31 as the representatives of small-kernel and large-kernel models, which are all well-trained on ImageNet, and test with 50 images from the ImageNet validation set resized to 1024?1024. To visualize the ERF, we use a simple yet effective method (code released at <ref type="bibr" target="#b1">[2]</ref>) as introduced in Appendix B, following <ref type="bibr" target="#b34">[54]</ref>. Briefly, we produce an aggregated contribution score matrix A (1024?1024), where each entry a (0 ? a ? 1) measures the contribution of the corresponding pixel on the in- <ref type="table">Table 10</ref>. Quantitative analysis on the ERF with the highcontribution area ratio r. A larger r suggests a smoother distribution of high-contribution pixels, hence larger ERF. put image to the central point of the feature map produced by the last layer. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the high-contribution pixels of ResNet-101 gather around the central point, but the outer points have very low contributions, indicating a limited ERF. ResNet-152 shows a similar pattern, suggesting the more 3?3 layers do not significantly increase the ERF. On the other hand, the high-contribution pixels in <ref type="figure" target="#fig_0">Fig. 1 (C)</ref> are more evenly distributed, suggesting RepLKNet-13 attends to more outer pixels. With larger kernels, RepLKNet-31 makes the high-contribution pixels spread more uniformly, indicating an even larger ERF. <ref type="table">Table 10</ref> presents a quantitative analysis, where we report the high-contribution area ratio r of a minimum rectangle that covers the contribution scores over a given threshold t. For examples, 20% of the pixel contributions (A values) of ResNet-101 reside within a 103?103 area at the center, so that the area ratio is (103/1024) 2 = 1.0% with t = 20%. We make several intriguing observations. 1) While being significantly deeper, ResNets have much smaller ERFs than RepLKNets. For example, over 99% of the contribution scores of ResNet-101 reside within a small area which takes up only 23.4% of the total area, while such area ratio of RepLKNet-31 is 98.6%, which means most of pixels considerably contribute to the final predictions. 2) Adding more layers to ResNet-101 does not effectively enlarge the ERF, while scaling up the kernels improves the ERF with marginal computational costs.</p><formula xml:id="formula_1">t = 20% t = 30% t = 50% t =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Large-Kernel Models are More Similar to Human in Shape Bias</head><p>We have found out that RepLKNet-31B has much higher shape bias than Swin Transformer and small-kernel CNNs.</p><p>A recent work <ref type="bibr" target="#b72">[91]</ref> reported that vision transformers are more similar to the human vision systems in that they make predictions more based on the overall shapes of objects, while CNNs focus more on the local textures. We follow its methodology and use its toolbox [6] to obtain the shape bias (e.g., the fraction of predictions made based on the shapes, rather than the textures) of RepLKNet-31B and Swin-B pretrained on ImageNet-1K or 22K, together with two small-kernel baselines, RepLKNet-3 and ResNet-152. <ref type="figure" target="#fig_4">Fig. 5</ref> shows that RepLKNet has higher shape bias than Swin. Considering RepLKNet and Swin have similar overall architectures, we reckon shape bias is closely related to the Effective Receptive Field rather than the concrete formulation of self-attention (i.e., the query-key-value design). This also explains 1) the high shape bias of ViTs <ref type="bibr" target="#b14">[35]</ref> reported by <ref type="bibr" target="#b72">[91]</ref> (since ViTs employ global attention), 2) the low shape bias of 1K-pretrained Swin (attention within local windows), and 3) the shape bias of the small-kernel baseline RepLKNet-3, which is very close to ResNet-152 (both models are composed of 3 ? 3 convolutions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Large Kernel Design is a Generic Design Element that Works with ConvNeXt</head><p>Replacing the 7?7 convolutions in ConvNeXt <ref type="bibr" target="#b42">[62]</ref> by kernels as large as 31?31 brings significant improvements, e.g., ConNeXt-Tiny + large kernel &gt;ConNeXt-Small , and ConNeXt-Small + large kernel &gt;ConNeXt-Base.</p><p>We use the recently proposed ConvNeXt <ref type="bibr" target="#b42">[62]</ref> as the benchmark architecture to evaluate large kernel as a generic design element. We simply replace the 7?7 convolutions in ConvNeXt <ref type="bibr" target="#b42">[62]</ref> by kernels as large as 31?31. The training configurations on ImageNet (120 epochs) and ADE20K (80K iterations) are identical to the results shown in Sec. 4.2. <ref type="table" target="#tab_11">Table 11</ref> shows that though the original kernels are already 7?7, further increasing the kernel sizes still brings significant improvements, especially on the downstream task: with kernels as large as 31?31, ConvNeXt-Tiny outperforms the original ConvNeXt-Small, and the large-kernel ConvNeXt-Small outperforms the original ConvNeXt-Base. Again, such phenomena demonstrate that kernel size is an important scaling dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Large Kernels Outperform Small Kernels with High Dilation Rates</head><p>Please refer to Appendix C for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Limitations</head><p>Although large kernel design greatly improves CNNs on both ImageNet and downstream tasks, however, according to <ref type="table" target="#tab_7">Table 6</ref>, as the scale of data and model increases, RepLKNets start to fall behind Swin Transformers, e.g., the ImageNet top-1 accuracy of RepLKNet-31L is 0.7% lower than Swin-L with ImageNet-22K pretraining (while the downstream scores are still comparable). It is not clear whether the gap is resulted from suboptimal hyperparameter tuning or some other fundamental drawback of CNNs which emerges when data/model scales up. We are working in progress on the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>This paper revisits large convolutional kernels, which have long been neglected in designing CNN architectures. We demonstrate that using a few large kernels instead of many small kernels results in larger effective receptive field more efficiently, boosting CNN's performances especially on downstream tasks by a large margin, and greatly closing the performance gap between CNNs and ViTs when data and models scale up. We hope our work could advance both studies of CNNs and ViTs. On one hand, for CNN community, our findings suggest that we should pay special attention to ERFs, which may be the key to high performances.</p><p>On the other hand, for ViT community, since large convolutions act as an alternative to multi-head self-attentions with similar behaviors, it may help to understand the intrinsic mechanism of self-attentions. As another alternative to implement large convolutions, dilated convolution [14,106] is a common component to increase the receptive field (RF). However, <ref type="table" target="#tab_1">Table 12</ref> shows though a depth-wise dilated convolution may have the same maximum RF as a depth-wise dense convolution, its representational capacity is much lower, which is expected because it is mathematically equivalent to a sparse large convolution. Literature (e.g., <ref type="bibr" target="#b78">[97,</ref><ref type="bibr" target="#b84">103]</ref>) further suggests that dilated convolutions may suffer from gridding problem. We reckon the drawbacks of dilated convolutions could be overcome by mixture of convolutions with different dilations, which will be investigated in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D: Visualizing the Kernel Weights with Small-Kernel Re-parameterization</head><p>We visualize the weights of the re-parameterized 13?13 kernels. Specifically, we investigate into the MobileNet V2 models both with and without 3?3 re-parameterization. As Shown in Sec. 3 (Guideline 3) , the ImageNet scores are 73.24% and 72.53%, respectively. We use the first stride-1 13?13 conv in the last stage (i.e., the stage with input resolution of 7?7) as the representative, and aggregate (take the absolute value and sum up across channels) the resultant kernel into a 13?13 matrix, and respectively rescale to [0, 1] for the comparability. For the model with 3?3 re-param, we show both the original 13?13 kernel (only after BN fusion) and the result after re-param (i.e., adding the 3?3 kernel onto the central part of 13?13). For the model without reparam, we also fuse the BN for the fair comparison.</p><p>We observe that every aggregated kernel shows a similar pattern: the central point has the largest magnitude; generally, points closer to the center have larger values; and the "skeleton" parameters (the 13?1 and 1?13 criss-cross parts) are relatively larger, which is consistent with the discovery reported by ACNet <ref type="bibr" target="#b7">[28]</ref>. But the kernel with 3?3 re-param differs in that the central 3?3 part of the resultant kernel is further enhanced, which is found to improve the performance.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The Effective Receptive Field (ERF) of ResNet-101/152 and RepLKNet-13/31 respectively. A more widely distributed dark area indicates a larger ERF. More layers (e.g., from ResNet-101 to ResNet-152) help little in enlarging ERFs. Instead, our large kernel model RepLKNet effectively obtains large ERFs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Illustration to convolution with small feature map and large kernel. Two outputs at adjacent locations only share a part of kernel weights. Translational equivariance does not strictly hold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Shape bias of RepLKNet, Swin, and ResNet-152 pretrained on ImageNet-1K or 22K. The scatters represent the shape bias of 16 categories, and the vertical lines are the averages across categories (note RepLKNet-3 and ResNet-152 are very close).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Before re-param.(b) After re-param.(c) Without re-param.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Parameters of 13?13 kernels in MobileNet V2 aggregated into 13?13 matrices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results of different kernel sizes in normal/shortcut-free MobileNet V2.</figDesc><table><row><cell cols="2">Shortcut Kernel size ImageNet top-1 accuracy (%)</cell></row><row><cell>3?3</cell><cell>71.76</cell></row><row><cell>13?13</cell><cell>72.53</cell></row><row><cell>3?3</cell><cell>68.67</cell></row><row><cell>13?13</cell><cell>53.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results of 3?3 re-parameterization on MobileNet V2 with various kernel sizes.</figDesc><table><row><cell cols="2">Kernel 3?3 re-param</cell><cell>ImageNet top-1 acc (%)</cell><cell>Cityscapes val mIoU (%)</cell></row><row><cell>3?3</cell><cell>N/A</cell><cell>71.76</cell><cell>72.31</cell></row><row><cell>9?9</cell><cell></cell><cell>72.67</cell><cell>76.11</cell></row><row><cell>9?9</cell><cell></cell><cell>73.09</cell><cell>76.30</cell></row><row><cell>13?13</cell><cell></cell><cell>72.53</cell><cell>75.67</cell></row><row><cell>13?13</cell><cell></cell><cell>73.24</cell><cell>76.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Cityscapes mIoU by 3.99%.Table 5shows a similar trend: as the kernel sizes increase from<ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b2">3]</ref> to<ref type="bibr" target="#b10">[31,</ref><ref type="bibr" target="#b8">29,</ref><ref type="bibr" target="#b6">27,</ref> 13], the ImageNet accuracy improves by only 0.96%, while the mIoU on ADE20K [120] improves by 3.12%. Such phenomenon indicates that models of similar ImageNet scores could have very different capability in downstream tasks (just as the bottom 3 models inTable 5). Remark 4. What causes the phenomenon? First, large kernel design significantly increases the Effective Receptive Fields (ERFs)<ref type="bibr" target="#b45">[65]</ref>. Numerous works have demonstrated "contextual" information, which implies large ERFs, is crucial in many downstream tasks like object detection and semantic segmentation<ref type="bibr" target="#b43">[63,</ref><ref type="bibr" target="#b49">69,</ref><ref type="bibr" target="#b77">96,</ref><ref type="bibr" target="#b87">106,</ref><ref type="bibr" target="#b88">107]</ref>. We will discuss the topic in Sec. 5. Second, We deem another reason might be that large kernel design contributes more shape biases to the network. Briefly speaking, ImageNet pictures can be correctly classified according to either texture or shape, as proposed in[8,<ref type="bibr" target="#b15">36]</ref>. However, humans recognize objects mainly based on shape cue rather than texture, therefore a model with stronger shape bias may transfer better to downstream tasks. A recent study<ref type="bibr" target="#b72">[91]</ref> points out ViTs are strong in shape bias, which partially explains why ViTs are super powerful in transfer tasks. In contrast, conventional CNNs trained on ImageNet tend to bias towards tex-</figDesc><table><row><cell>input</cell><cell></cell></row><row><cell>7?7</cell><cell>3?3</cell></row><row><cell>BN</cell><cell>BN</cell></row><row><cell>+</cell><cell></cell></row></table><note>(after re-param) shows increasing the kernel size of MobileNet V2 from 3?3 to 9?9 improves the ImageNet accuracy by 1.33% but the</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Results of various kernel sizes in the last stage of Mo-bileNet V2. Kernel sizes in previous stages remain to be 3 ? 3.</figDesc><table><row><cell cols="3">Kernel size ImageNet acc (%) Cityscapes mIoU (%)</cell></row><row><cell>3?3</cell><cell>71.76</cell><cell>72.31</cell></row><row><cell>7?7</cell><cell>72.00</cell><cell>74.30</cell></row><row><cell>13?13</cell><cell>71.97</cell><cell>74.62</cell></row><row><cell>ture [8,</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>RepLKNet with different kernel sizes. The models are pretrained on ImageNet-1K in 120 epochs with 224?224 input and finetuned on ADE20K with UperNet in 80K iterations. On ADE20K, we test the single-scale mIoU, and compute the FLOPs with input of 2048?512, following Swin. 2M 13.1G 48.05 104.6M 1123G 13-13-13-13 83.02 73.7M 13.4G 48.35 106.0M 1130G 25-25-25-13 83.00 78.2M 14.8G 48.68 110.6M 1159G 31-29-27-13 83.07 79.3M 15.3G 49.17 111.7M 1170G duct 2? downsampling with DW 3?3 conv.</figDesc><table><row><cell></cell><cell>ImageNet</cell><cell>ADE20K</cell></row><row><cell cols="3">Kernel size Top-1 Params FLOPs mIoU Params FLOPs</cell></row><row><cell>3-3-3-3</cell><cell cols="2">82.11 71.8M 12.9G 46.05 104.1M 1119G</cell></row><row><cell>7-7-7-7</cell><cell>82.73 72.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>We continue to evaluate large kernels on RepLKNet via fixing B=[2, 2, 18, 2], C=[128, 256, 512, 1024], varying K and observing the performance of both classification and semantic segmentation. Without careful tuning of the hyper-parameters, we casually set the kernel sizes as [13, 13, 13, 13], [25, 25, 25, 13],</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>ImageNet results. The throughput is tested with FP32 and a batch size of 64 on 2080Ti. ? indicates ImageNet-22K pretraining. indicates pretrained with extra data.</figDesc><table><row><cell>Model</cell><cell>Input resolution</cell><cell>Top-1 acc</cell><cell>Params (M)</cell><cell>FLOPs (G)</cell><cell>Throughput examples/s</cell></row><row><cell cols="3">RepLKNet-31B 224?224 83.5</cell><cell>79</cell><cell>15.3</cell><cell>295.5</cell></row><row><cell>Swin-B</cell><cell cols="2">224?224 83.5</cell><cell>88</cell><cell>15.4</cell><cell>226.2</cell></row><row><cell cols="3">RepLKNet-31B 384?384 84.8</cell><cell>79</cell><cell>45.1</cell><cell>97.0</cell></row><row><cell>Swin-B</cell><cell cols="2">384?384 84.5</cell><cell>88</cell><cell>47.0</cell><cell>67.9</cell></row><row><cell cols="3">RepLKNet-31B  ? 224?224 85.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Swin-B  ?</cell><cell cols="2">224?224 85.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">RepLKNet-31B  ? 384?384 86.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Swin-B  ?</cell><cell cols="2">384?384 86.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">RepLKNet-31L  ? 384?384 86.6 172</cell><cell>96.0</cell><cell>50.2</cell></row><row><cell>Swin-L  ?</cell><cell cols="4">384?384 87.3 197 103.9</cell><cell>36.2</cell></row><row><cell cols="5">RepLKNet-XL 320?320 87.8 335 128.7</cell><cell>39.1</cell></row></table><note>Rand Erasing [118] and Stochastic Depth [50], following the recent works [4, 61, 62, 89]. The detailed training con- figurations are presented in Appendix A.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Cityscapes results. The FLOPs is computed with 1024?2048 inputs. The mIoU is tested with single-scale (ss) and multi-scale (ms). The results with Swin are implemented by<ref type="bibr" target="#b17">[38]</ref>. ? indicates ImageNet-22K pretraining.</figDesc><table><row><cell>Backbone</cell><cell>Method</cell><cell>mIoU (ss)</cell><cell>mIoU (ms)</cell><cell>Param (M)</cell><cell>FLOPs (G)</cell></row><row><cell>RepLKNet-31B</cell><cell>UperNet [102]</cell><cell cols="4">83.1 83.5 110 2315</cell></row><row><cell cols="3">ResNeSt-200 [112] DeepLabv3 [15] -</cell><cell>82.7</cell><cell>-</cell><cell>-</cell></row><row><cell>Axial-Res-XL</cell><cell>Axial-DL [95]</cell><cell cols="4">80.6 81.1 173 2446</cell></row><row><cell>Swin-B</cell><cell>UperNet</cell><cell cols="4">80.4 81.5 121 2613</cell></row><row><cell>Swin-B</cell><cell cols="4">UperNet + [38] 80.8 81.8 121</cell><cell>-</cell></row><row><cell>ViT-L  ?</cell><cell cols="4">SETR-PUP [117] 79.3 82.1 318</cell><cell>-</cell></row><row><cell>ViT-L  ?</cell><cell>SETR-MLA</cell><cell>77.2</cell><cell>-</cell><cell>310</cell><cell>-</cell></row><row><cell>Swin-L  ?</cell><cell>UperNet</cell><cell cols="4">82.3 83.1 234 3771</cell></row><row><cell>Swin-L  ?</cell><cell cols="4">UperNet + [38] 82.7 83.6 234</cell><cell>-</cell></row><row><cell cols="6">Table 8. ADE20K results. The mIoU is tested with single-scale</cell></row><row><cell cols="6">(ss) and multi-scale (ms). The results with 1K-pretrained Swin are</cell></row><row><cell cols="6">cited from the official GitHub repository.  ? indicates ImageNet-</cell></row><row><cell cols="6">22K pretraining and 640?640 finetuning on ADE20K. indi-</cell></row><row><cell cols="6">cates pretrained with extra data. The FLOPs is computed with</cell></row><row><cell cols="6">2048?512 for the ImageNet-1K pretrained models and 2560?640</cell></row><row><cell cols="4">for the ImageNet-22K and larger, following Swin.</cell><cell></cell><cell></cell></row><row><cell>Backbone</cell><cell>Method</cell><cell>mIoU (ss)</cell><cell>mIoU (ms)</cell><cell>Param (M)</cell><cell>FLOPs (G)</cell></row><row><cell>RepLKNet-31B</cell><cell>UperNet</cell><cell cols="4">49.9 50.6 112 1170</cell></row><row><cell>ResNet-101</cell><cell>UperNet [102]</cell><cell cols="2">43.8 44.9</cell><cell>86</cell><cell>1029</cell></row><row><cell cols="3">ResNeSt-200 [112] DeepLabv3 [15] -</cell><cell cols="3">48.4 113 1752</cell></row><row><cell>Swin-B</cell><cell>UperNet</cell><cell cols="4">48.1 49.7 121 1188</cell></row><row><cell>Swin-B</cell><cell>UperNet + [38]</cell><cell cols="3">48.4 50.1 121</cell><cell>-</cell></row><row><cell>ViT-Hybrid</cell><cell cols="2">DPT-Hybrid [73] -</cell><cell>49.0</cell><cell>90</cell><cell>-</cell></row><row><cell>ViT-L</cell><cell>DPT-Large</cell><cell>-</cell><cell cols="2">47.6 307</cell><cell>-</cell></row><row><cell>ViT-B</cell><cell cols="3">SETR-PUP [117] 46.3 47.3</cell><cell>97</cell><cell>-</cell></row><row><cell>ViT-B</cell><cell cols="3">SETR-MLA [117] 46.2 47.7</cell><cell>92</cell><cell>-</cell></row><row><cell cols="2">RepLKNet-31B  ? UperNet</cell><cell cols="4">51.5 52.3 112 1829</cell></row><row><cell>Swin-B  ?</cell><cell>UperNet</cell><cell cols="4">50.0 51.6 121 1841</cell></row><row><cell cols="2">RepLKNet-31L  ? UperNet</cell><cell cols="4">52.4 52.7 207 2404</cell></row><row><cell>Swin-L  ?</cell><cell>UperNet</cell><cell cols="4">52.1 53.5 234 2468</cell></row><row><cell>ViT-L  ?</cell><cell>SETR-PUP</cell><cell cols="3">48.6 50.1 318</cell><cell>-</cell></row><row><cell>ViT-L  ?</cell><cell>SETR-MLA</cell><cell cols="3">48.6 50.3 310</cell><cell>-</cell></row><row><cell>RepLKNet-XL</cell><cell>UperNet</cell><cell cols="4">55.2 56.0 374 3431</cell></row><row><cell cols="6">than the Swin-B model, which was trained with 384?384</cell></row><row><cell cols="6">from scratch. Then we pretrain RepLKNet-B/L models on</cell></row><row><cell cols="6">ImageNet-22K and finetune on ImageNet-1K. RepLKNet-</cell></row><row><cell cols="6">XL is pretrained on our private semi-supervised dataset</cell></row><row><cell cols="6">named MegData73M, which is introduced in the Appendix.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>? indicates ImageNet-22K pretraining. indicates pretrained with extra data.</figDesc><table><row><cell>Backbone</cell><cell>Method</cell><cell cols="3">AP box AP mask Param (M)</cell><cell>FLOPs (G)</cell></row><row><cell cols="2">RepLKNet-31B FCOS</cell><cell>47.0</cell><cell>-</cell><cell>87</cell><cell>437</cell></row><row><cell>X101-64x4d</cell><cell>FCOS</cell><cell>42.6</cell><cell>-</cell><cell>90</cell><cell>439</cell></row><row><cell cols="2">RepLKNet-31B Cas Mask</cell><cell cols="3">52.2 45.2 137</cell><cell>965</cell></row><row><cell>X101-64x4d</cell><cell>Cas Mask</cell><cell cols="3">48.3 41.7 140</cell><cell>972</cell></row><row><cell>ResNeSt-200</cell><cell cols="2">Cas R-CNN [9] 49.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Swin-B</cell><cell>Cas Mask</cell><cell cols="3">51.9 45.0 145</cell><cell>982</cell></row><row><cell cols="2">RepLKNet-31B  ? Cas Mask</cell><cell cols="3">53.0 46.0 137</cell><cell>965</cell></row><row><cell>Swin-B  ?</cell><cell>Cas Mask</cell><cell cols="3">53.0 45.8 145</cell><cell>982</cell></row><row><cell cols="2">RepLKNet-31L  ? Cas Mask</cell><cell cols="4">53.9 46.5 229 1321</cell></row><row><cell>Swin-L  ?</cell><cell>Cas Mask</cell><cell cols="4">53.9 46.7 254 1382</cell></row><row><cell cols="2">RepLKNet-XL Cas Mask</cell><cell cols="4">55.5 48.0 392 1958</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 .</head><label>11</label><figDesc>ConvNeXt with different kernel sizes. The models are pretrained on ImageNet-1K in 120 epochs with 224?224 input and finetuned on ADE20K with UperNet in 80K iterations. On ADE20K, we test the single-scale mIoU, and compute the FLOPs with input of 2048?512, following Swin. Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-attention and convolutional layers. arXiv preprint arXiv:1911.03584, 2019. 1 [22] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 3213-3223. IEEE Computer Society, 2016. 4 [23] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 702-703, 2020. 6, 15 [24] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 764-773, 2017. 2 [25] Xiyang Dai, Yinpeng Chen, Bin Xiao, Dongdong Chen, Mengchen Liu, Lu Yuan, and Lei Zhang. Dynamic head: Unifying object detection heads with attentions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7373-7382, 2021. 1 Table 12. MobileNet V2 with all regular DW 3?3 layers replaced by 3?3 dilated layers.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ImageNet</cell><cell>ADE20K</cell></row><row><cell></cell><cell cols="3">Kernel size Architecture</cell><cell></cell><cell>Top-1 Params FLOPs mIoU Params FLOPs</cell></row><row><cell></cell><cell cols="2">7-7-7-7</cell><cell cols="2">ConvNeXt-Tiny</cell><cell>81.0 29M</cell><cell>4.5G 44.6 60M 939G</cell></row><row><cell></cell><cell cols="2">7-7-7-7</cell><cell cols="3">ConvNeXt-Small 82.1 50M</cell><cell>8.7G 45.9 82M 1027G</cell></row><row><cell></cell><cell cols="2">7-7-7-7</cell><cell cols="3">ConvNeXt-Base 82.8 89M 15.4G 47.2 122M 1170G</cell></row><row><cell></cell><cell cols="4">31-29-27-13 ConvNeXt-Tiny</cell><cell>81.6 32M</cell><cell>6.1G 46.2 64M 973G</cell></row><row><cell></cell><cell cols="5">31-29-27-13 ConvNeXt-Small 82.5 58M 11.3G 48.2 90M 1081G</cell></row><row><cell>[6] bethgelab.</cell><cell cols="3">Toolbox of model-vs-human.</cell><cell cols="2">https :</cell><cell>for semantic image segmentation.</cell><cell>arXiv preprint</cell></row><row><cell cols="6">//github.com/bethgelab/model-vs-human,</cell><cell>arXiv:1706.05587, 2017. 7</cell></row><row><cell>2022. 8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[16] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Flo-</cell></row><row><cell cols="6">[7] Navaneeth Bodla, Bharat Singh, Rama Chellappa, and</cell><cell>rian Schroff, and Hartwig Adam. Encoder-decoder with</cell></row><row><cell cols="6">Larry S Davis. Soft-nms-improving object detection with</cell><cell>atrous separable convolution for semantic image segmenta-</cell></row><row><cell cols="6">one line of code. In Proceedings of the IEEE international</cell><cell>tion. In Proceedings of the European conference on com-</cell></row><row><cell cols="6">conference on computer vision, pages 5561-5569, 2017. 7</cell><cell>puter vision (ECCV), pages 801-818, 2018. 4</cell></row><row><cell cols="6">[8] Wieland Brendel and Matthias Bethge. Approximating</cell><cell>[17] Xinlei Chen, Saining Xie, and Kaiming He. An empirical</cell></row><row><cell cols="6">cnns with bag-of-local-features models works surprisingly</cell><cell>study of training self-supervised visual transformers. arXiv</cell></row><row><cell cols="6">well on imagenet. arXiv preprint arXiv:1904.00760, 2019.</cell><cell>e-prints, pages arXiv-2104, 2021. 1</cell></row><row><cell>4, 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[18] Fran?ois Chollet. Xception: Deep learning with depthwise</cell></row><row><cell cols="6">[9] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: High</cell><cell>separable convolutions. In Proceedings of the IEEE con-</cell></row><row><cell cols="6">quality object detection and instance segmentation. IEEE</cell><cell>ference on computer vision and pattern recognition, pages</cell></row><row><cell cols="6">Transactions on Pattern Analysis and Machine Intelligence,</cell><cell>1251-1258, 2017. 3</cell></row><row><cell>2019. 7, 8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[19] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xi-</cell></row><row><cell cols="6">[10] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv? J?gou,</cell><cell>aolin Wei, Huaxia Xia, and Chunhua Shen. Conditional po-</cell></row><row><cell cols="6">Julien Mairal, Piotr Bojanowski, and Armand Joulin.</cell><cell>sitional encodings for vision transformers. arXiv preprint</cell></row><row><cell cols="6">Emerging properties in self-supervised vision transformers.</cell><cell>arXiv:2102.10882, 2021. 4</cell></row><row><cell cols="4">arXiv preprint arXiv:2104.14294, 2021. 1</cell><cell></cell><cell>[20] MMSegmentation Contributors.</cell><cell>MMSegmentation:</cell></row><row><cell cols="6">[11] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yip-</cell><cell>Openmmlab semantic segmentation toolbox and</cell></row><row><cell cols="6">ing Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu,</cell><cell>benchmark.</cell><cell>https : / / github . com / open -</cell></row><row><cell cols="6">and Wen Gao. Pre-trained image processing transformer.</cell><cell>mmlab/mmsegmentation, 2020. 4, 6, 7</cell></row><row><cell cols="6">In Proceedings of the IEEE/CVF Conference on Computer</cell><cell>[21]</cell></row><row><cell cols="6">Vision and Pattern Recognition, pages 12299-12310, 2021.</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">[12] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaox-</cell></row><row><cell cols="6">iao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping</cell></row><row><cell cols="6">Shi, Wanli Ouyang, et al. Hybrid task cascade for instance</cell></row><row><cell cols="6">segmentation. In Proceedings of the IEEE/CVF Conference</cell></row><row><cell cols="6">on Computer Vision and Pattern Recognition, pages 4974-</cell></row><row><cell>4983, 2019. 7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">[13] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu</cell></row><row><cell cols="6">Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei</cell></row><row><cell cols="6">Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu,</cell></row><row><cell cols="6">Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu,</cell></row><row><cell cols="6">Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli</cell></row><row><cell cols="6">Ouyang, Chen Change Loy, and Dahua Lin. MMDetec-</cell></row><row><cell cols="6">tion: Open mmlab detection toolbox and benchmark. arXiv</cell></row><row><cell cols="4">preprint arXiv:1906.07155, 2019. 7</cell><cell></cell></row><row><cell cols="6">[14] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,</cell></row><row><cell cols="6">Kevin Murphy, and Alan L Yuille. Deeplab: Semantic im-</cell></row><row><cell cols="6">age segmentation with deep convolutional nets, atrous con-</cell></row><row><cell cols="6">volution, and fully connected crfs. IEEE transactions on</cell></row><row><cell cols="6">pattern analysis and machine intelligence, 40(4):834-848,</cell></row><row><cell>2017. 2, 16</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">[15] Liang-Chieh Chen, George Papandreou, Florian Schroff,</cell></row><row><cell cols="2">and Hartwig Adam.</cell><cell cols="4">Rethinking atrous convolution</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Convolutional kernels (including the variants such as depthwise/group convolutions) whose spatial size is larger than 1?1.<ref type="bibr" target="#b1">2</ref> Namely CNNs free of any attention or dynamic mechanism, e.g., squeeze-and-excitation<ref type="bibr" target="#b27">[48]</ref>, multi-head self-attention, dynamic weights<ref type="bibr" target="#b19">[40,</ref><ref type="bibr" target="#b81">100]</ref>, and etc.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B: Visualizing the ERF</head><p>Formally, let I(n ? 3 ? h ? w) be the input image, M(n ? c ? h ? w ) be the final output feature map, we desire to measure the contributions of every pixel on I to the central points of every channel on M, i.e., M :,:,h /2,w /2 , which can be simply implemented via taking the derivatives of M :,:,h /2,w /2 to I with the auto-grad mechanism. Concretely, we sum up the central points, take the derivatives to the input as the pixel-wise contribution scores and remove the negative parts (denoted by P). Then we aggregate the entries across all the examples and the three input channels, and take the logarithm for better visualization. Formally, the aggregated contribution score matrix A(h ? w) is given by</p><p>A = log 10 ( n i 3 j P i,j,:,: + 1) .</p><p>Then we respectively rescale A of each model to [0, 1] via dividing the maximum entry for the comparability across models.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Megengine:a fast, scalable and easy-to-use deep learning framework</title>
		<idno>2020. 3</idno>
		<ptr target="https://github.com/MegEngine/MegEngine" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Official pytorch implementation of replknet</title>
		<ptr target="https://github.com/DingXiaoH/RepLKNet-pytorch" />
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m">Beit: Bert pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3286" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Batch normalization biases residual blocks towards the identity function in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10444</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Repmlpnet: Hierarchical vision mlp with re-parameterized locality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11081</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Acnet: Strengthening the kernel skeletons for powerful cnn via asymmetric convolution blocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1911" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Resrep: Lossless cnn pruning via decoupling remembering and forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxiang</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Diverse branch block: Building a convolution as an inception-like unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10886" to="10895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Repvgg: Making vggstyle convnets great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="13733" to="13742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast and accurate model scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="924" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Cswin transformer: A general vision transformer backbone with cross-shaped windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00652</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Attention is not all you need: Pure attention loses rank doubly exponentially with depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihe</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03404</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net, 2021. 1, 3, 4, 5, 9</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Felix A Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brendel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12231</idno>
		<title level="m">Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-task self-training for learning general representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improve vision transformers training by suppressing over-smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.12753</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Single path oneshot neural architecture search with uniform sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="544" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Demystifying local vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejia</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04263</idno>
	</analytic>
	<monogr>
		<title level="m">Sparse connectivity, weight sharing, and dynamic weight</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">How to represent part-whole hierarchies in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12627</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-11-02" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3464" to="3473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<imprint/>
	</monogr>
	<note>14th European</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Proceedings, Part IV</title>
		<meeting>Part IV<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9908</biblScope>
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
	<note>Conference</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhsuan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05918</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On translation invariance in cnns: Convolutional layers can exploit absolute spatial location</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan C Van</forename><surname>Osman Semih Kayhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14274" to="14285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeyeon</forename><surname>Bum Jun Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonah</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonseok</forename><surname>Gu Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><forename type="middle">Woo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.13576</idno>
		<title level="m">Dead pixel test using effective receptive field</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Swinir: Image restoration using swin transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1833" to="1844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Understanding the difficulty of training transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08249,2020.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09883</idno>
		<title level="m">Swin transformer v2: Scaling up capacity and resolution</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.03545</idno>
	</analytic>
	<monogr>
		<title level="m">Trevor Darrell, and Saining Xie. A convnet for the 2020s</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<editor>Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett</editor>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4898" to="4906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fast training of convolutional networks through ffts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-14" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayak</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07581</idno>
		<title level="m">Vision transformers are robust learners</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Large kernel matters-improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Do vision transformers see like convolutional neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.08810</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Standalone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05909</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Vision transformers for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12179" to="12188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00645</idno>
		<title level="m">Global filter networks for image classification</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert-Jan</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruintjes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoogendoorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flexconv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08059</idno>
		<title level="m">Continuous kernel convolutions with differentiable kernel sizes</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>David W Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Kuzina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoogendoorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ckconv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.02611</idno>
		<title level="m">Continuous kernel convolution for sequential data</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02155</idno>
		<title level="m">Selfattention with relative position representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16519" to="16529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-first AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Yfcc100m: The new data in multimedia research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">FCOS: fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<title level="m">IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-11-02" />
			<biblScope unit="page" from="9626" to="9635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03404</idno>
		<title level="m">Feedforward networks for image classification with data-efficient training</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>PMLR, 2021. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asher</forename><surname>Trockman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.09792,2022.3</idno>
		<title level="m">Patches are all you need? arXiv preprint</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Are convolutional neural networks or transformers more like human vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Tuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishita</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas L</forename><surname>Griffiths</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07197</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12894" to="12904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Residual networks behave like ensembles of relatively shallow networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="550" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Axial-deeplab: Standalone axial-attention for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="108" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panqu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrison</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE winter conference on applications of computer vision (WACV)</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Timm implementation of randaugment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models/blob/master/timm/data/auto_augment.py,2022.15" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10430</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.15203</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Self-supervised learning with swin transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04553</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Metaformer is actually what you need for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11418</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13112</idno>
		<title level="m">Vision outlooker for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manmatha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Basisnet: Two-stage model synthesis for efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Te</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Hwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Kovashka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3081" to="3090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03815</idno>
		<title level="m">Collaboration of experts: Achieving 80% top-1 accuracy on imagenet with 100m flops</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">A battle of network structures: An empirical study of cnn, transformer, and mlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanxin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.13002</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Semantic understanding of scenes through the ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">An empirical study of spatial attention mechanisms in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6688" to="6697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
				<title level="m">Appendix A: Training Configurations ImageNet-1K For training MobileNet V2 models</title>
		<meeting><address><addrLine>Sec</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">batch size of 32 per GPU, input resolution of 224?224, weight decay of 4 ? 10 ?5 , learning rate schedule with 5-epoch warmup, initial value of 0.1 and cosine annealing for 100 epochs. For the data augmentation, we only use random cropping and left-right flipping, as a common practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gpus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SGD optimizer with momentum of 0.9</title>
		<imprint>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
	<note>For training RepLKNet models (Sec. 4.2</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">The learning rate setting includes an initial value of 4?10 ?3 , cosine annealing and 10-epoch warm-up. For the data augmentation and regularization, we use RandAugment [23] (&quot;rand-m9-mstd0.5-inc1&quot; as implemented by timm [99]), label smoothing coefficient of 0.1, mixup [111] with ? = 0.8, CutMix with ? = 1.0, Rand Erasing [118] with probability of 25% and Stochastic Depth with a drop-path rate of 30%, following the recent works</title>
	</analytic>
	<monogr>
		<title level="m">GPUs and a batch size of 64 per GPU to train for 120 epochs. The optimizer is AdamW [64] with momentum of 0.9 and weight decay of 0.05</title>
		<imprint/>
	</monogr>
	<note>with the same configurations except the epoch number of 300 and drop-path rate of 50%. For finetuning the 224?224-trained RepLKNet-31B with 384?384, we use 32 GPUs, a batch size of 32 per GPU, initial learning rate of 4 ? 10 ?4 , cosine annealing, 1-epoch warm-up, 30 epochs. model EMA (Exponential Moving Average) with momentum of 10 ?4 , the same Ran-dAugment as above but no CutMix nor mixup</note>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">GPUs and a batch size of 32 per GPU to train for 90 epochs with a drop-path rate of 10%. The other configurations are the same as the aforementioned ImageNet-1K pretraining. Then for finetuning RepLKNet-31B with 224?224, we use 16 GPUs, a batch size of 32 per GPU, drop-path rate of 20%, initial learning rate of 4 ? 10 ?4 , cosine annealing, model EMA with momentum of 10 ?4 to finetune for 30 epochs. Note again that we use the same RandAugment as above but no CutMix nor mixup</title>
	</analytic>
	<monogr>
		<title level="m">ImageNet-22K Pretraining and 1K Finetuning For pretraining RepLKNet-31B/L on ImageNet-22K, we use 128</title>
		<imprint/>
	</monogr>
	<note>For finetuning RepLKNet-31B/L with 384?384, we use 32 GPUs and a batch size of 16 per GPU, and the drop-path rate is raised to 30%</note>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Unlabeled images are selected from YFCC100M [85]. We design a multi-task label system according to [37], and utilize soft pseudo labels which are offline generated by multiple task-specific ViT-Ls wherever human annotations are unavailable. We pretrain our model for up to 15 epochs with similar configurations as ImageNet-1K pretraining. We do not use Cut-Mix or mixup, decrease drop-path rate to 20%, and use a lower initial learning rate of 1.5 ? 10 ?3 and a total batch size of 2048. Structural Re-parameterization is omitted because it only brings less than 0.1% performance gain on such a large-scale dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Xl</forename><surname>Replknet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semi</forename></persName>
		</author>
		<imprint>
			<biblScope unit="volume">119</biblScope>
		</imprint>
	</monogr>
	<note>and introduce inverted bottleneck with expansion ratio of 1.5 to each RepLK Block. During pretraining, we use a private semi-supervised dataset named Meg-Data73M, which contains 38 million labeled images and 35 million unlabeled ones. Labeled images come from public and private classification datasets such as ImageNet-1K, ImageNet-22K and Places365. In other words, we observe that the inductive bias (re-parameterization with small kernels) becomes less important as the data become bigger, which is similar to the discoveries reported by ViT [35</note>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">We finetune on ImageNet-1K with input resolution of 320?320 for 30 epochs following BeiT [4], except for a higher learning rate of 10 ?4 and stage-wise learning rate decay of 0.4. Finetuning with a higher resolution of 384?384 brings no further improvements. For downstream tasks, we use the default training setting except for a drop-path rate of 50% and stage-wise learning rate decay</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
