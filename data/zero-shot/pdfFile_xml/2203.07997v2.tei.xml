<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Inverted Pyramid Multi-task Transformer for Dense Scene Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrong</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">HKUST Clear Water Bay</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
							<email>danxu@cse.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">HKUST Clear Water Bay</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Inverted Pyramid Multi-task Transformer for Dense Scene Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-task dense scene understanding is a thriving research domain that requires simultaneous perception and reasoning on a series of correlated tasks with pixel-wise prediction. Most existing works encounter a severe limitation of modeling in the locality due to heavy utilization of convolution operations, while learning interactions and inference in a global spatial-position and multi-task context is critical for this problem. In this paper, we propose a novel end-to-end Inverted Pyramid multi-task Transformer (InvPT) to perform simultaneous modeling of spatial positions and multiple tasks in a unified framework. To the best of our knowledge, this is the first work that explores designing a transformer structure for multi-task dense prediction for scene understanding. Besides, it is widely demonstrated that a higher spatial resolution is remarkably beneficial for dense predictions, while it is very challenging for existing transformers to go deeper with higher resolutions due to huge complexity to large spatial size. InvPT presents an efficient UP-Transformer block to learn multi-task feature interaction at gradually increased resolutions, which also incorporates effective self-attention message passing and multi-scale feature aggregation to produce taskspecific prediction at a high resolution. Our method achieves superior multi-task performance on NYUD-v2 and PASCAL-Context datasets respectively, and significantly outperforms previous state-of-the-arts. The code is available at https://github.com/prismformore/InvPT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-task visual scene understanding typically requires joint learning and reasoning on a bunch of correlated tasks <ref type="bibr" target="#b40">[41]</ref>, which is highly important in computer vision and has a wide range of application scenarios such as autonomous driving, robotics, and augmented or virtual reality (AR/VR). Many of visual scene understanding tasks produce pixel-wise predictions for dense understanding of the scene <ref type="bibr" target="#b18">[19]</ref> such as semantic segmentation, monocular depth estimation, and human parsing. These dense prediction tasks essentially have abundant explicit and implicit correlation at the pixel level <ref type="bibr" target="#b47">[48]</ref>, which is very beneficial and can be fully utilized to improve the overall performance of multi-task models. However, how to effectively learn and exploit the cross-task correlation (e.g. complementarity and consistency) in a single model remains a challenging open issue.  <ref type="figure">Fig. 1</ref>: Joint learning and inference of global spatial interaction and simultaneous all-task interaction is critically important for multi-task dense prediction.</p><p>To advance the multi-task dense scene understanding, existing works mostly rely on the powerful Convolutional Neural Networks (CNN), and significant effort has been made by developing multi-task optimization losses <ref type="bibr" target="#b18">[19]</ref> and designing <ref type="bibr" target="#b47">[48]</ref> or searching <ref type="bibr" target="#b15">[16]</ref> multi-task information sharing strategies and network structures. Although promising performance has been achieved, these works are still limited by the nature of convolution kernels that are heavily used in their deep learning frameworks, which model critical spatial and task related contexts in relatively local perceptive fields (i.e. locality discussed by previous works <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b2">3]</ref>). Although the recently proposed attention based methods address this issue <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b4">5]</ref>, the scope of their cross-task interaction is still highly limited. However, for multi-task dense scene understanding, the capability of capturing long-range dependency and simultaneously modeling global relationships of all tasks is crucially important for this pixel-wise multi-task problem (see <ref type="figure">Fig. 1</ref>).</p><p>On the other hand, recently the transformer models have been introduced to model the long-range spatial relationship for dense prediction problems <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b49">50]</ref> but they only target the setting of single-task learning, while the joint modeling of multiple dense prediction tasks with transformer is rarely explored in the literature, and it is not a trivial problem to globally model both the spatial and the cross-task correlations in a unified transformer framework. Besides, the performance of dense prediction tasks is greatly affected by the resolution of the final feature maps produced from the model, while it is very difficult for existing transformers to go deeper with higher resolution because of huge complexity brought by large spatial size, and typically many transformers downsample the spatial resolution dramatically to reduce the computation overhead <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>To tackle the above-mentioned issues, in this work we propose a novel endto-end Inverted Pyramid Multi-task Transformer (InvPT) framework, which can jointly model the long-range dependency within spatial and all-task contexts, and also efficiently learn fine-grained dense prediction maps at a higher resolution, for multi-task dense scene understanding. Specifically, it consists of three core designs: (i) an InvPT transformer encoder to learn generic image representations for input images, (ii) an InvPT Transformer decoder built by consecutively stacking a proposed efficient UP-Transformer block, to model implicit correlations among all the dense prediction tasks and produce multi-task features with gradually increased resolutions, and (iii) two effective multi-scale strategies, i.e. cross-scale Attention Message Passing (AMP) that aggregates self-attention maps across different transformer blocks, and multi-scale Encoder Feature Aggregation (EFA) that enhances the decoding features with multi-scale information from the InvPT transformer encoder.</p><p>The proposed method yields strong multi-task performance measured by the relative improvement metric <ref type="bibr" target="#b24">[25]</ref>, which is 2.59% and 1.76% on NYUD-v2 and PASCAL-Context datasets respectively. The proposed method also largely outperforms other state-of-the-art methods on all 9 evaluation metrics of these two benchmarks. Notably on NYUD-v2, it surpasses the best competitor by 7.23 points (mIoU) on semantic segmentation, while on PASCAL-Context it outperforms the previous best result by 11.36 and 4.68 points (mIoU) on semantic segmentation and human parsing, respectively.</p><p>In summary, our main contribution is three-fold: -We propose a novel end-to-end Inverted Pyramid Multi-task Transformer (InvPT) framework for jointly learning multiple dense prediction tasks, which can effectively model long-range interaction in both spatial and all-task contexts in a unified architecture. As far as we know, it is the first work to present a transformer structure for this problem. -We design an efficient UP-Transformer block, which allows for multi-task feature interaction and refinement at gradually increased resolutions, and can construct a multi-layer InvPT decoder by consecutively stacking multiple blocks to produce final feature maps with a high resolution to largely boost dense predictions. The UP-Transformer block can also flexibly embed multi-scale information through two designed strategies, i.e. cross-scale self-attention message passing and InvPT encoder feature aggregation. -The proposed framework obtains superior performance on multi-task dense prediction and remarkably outperforms the previous state-of-the-arts on two challenging benchmarks (i.e. Pascal-Context and NYUD-v2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We review the most related works in the literature from two angles, i.e. multitask deep learning for scene understanding and visual transformers.</p><p>Multi-task Deep Learning for Scene Understanding As an active research field <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b47">48]</ref>, multi-task deep learning can greatly help to improve the efficiency of training, as it only needs to optimize once for multiple tasks, and the overall performance of scene understanding when compared with performing several scene understanding tasks separately <ref type="bibr" target="#b41">[42]</ref>. Multi-task deep learning methods mainly focus on two directions <ref type="bibr" target="#b40">[41]</ref>, i.e. multi-task optimization and network architecture design. Previous works in the former direction typically investigate loss balancing techniques in the optimization process to address the problem of task competition <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19]</ref>. In the latter direction researchers design explicit or implicit mechanisms for modeling cross-task interaction and embed them into the whole deep model <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b16">17]</ref> or only the decoder stage <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b50">51]</ref>. Regarding the multi-task dense scene understanding where all the tasks require pixel-wise predictions, many pioneering research works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b47">48]</ref> have explored this field. Specifically, Xu et al. <ref type="bibr" target="#b47">[48]</ref> propose PAD-Net with an effective information distillation module with attention guided cross-task message <ref type="figure" target="#fig_3">Fig. 2</ref>: Framework overview of the proposed Inverted Pyramid Multi-task Transformer (InvPT) for dense scene understanding. The task-shared transformer encoder learns generic visual representations from the input image. Then for each task t ? {1 . . . T }, the preliminary decoder produces task-specific feature F d t and preliminary prediction P t , which are combined as F c t , serving as the input of the InvPT decoder to generate refined and resolution-enlarged task-specific features via globally modeling spatial and all-task interactions for the final prediction. c ?, p ? and L denote the channel-wise concatenation, linear projection layer and loss function, respectively. passing on multi-task predictions. MTI-Net <ref type="bibr" target="#b41">[42]</ref> designs a sophisticated multiscale and multi-task CNN architecture to distill information at multiple feature scales. However, most of these methods adopt CNN to learn multi-task representation and model it in a limited local context. To address this issue, there are also some exciting works developing attention-based mechanisms. For instance, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b54">[55]</ref> and <ref type="bibr" target="#b50">[51]</ref> design spatial global or local attention learning strategies within each task-specific branch, and propagate the attention to refine features among all the tasks. Concurrent to our work, <ref type="bibr" target="#b4">[5]</ref> proposes to build up decoder via searching for proper cross-task attention structures with neural architecture search (NAS). Despite their innovative designs, these works still fail to jointly model spatial and cross-task contexts in a global manner.</p><p>Different from these works, we propose a novel multi-task transformer framework to learn long-range interactions of spatial and cross-task relationships in global contexts, which is critical for this complex multi-task problem. Visual Transformer Research interest in visual transformers grows rapidly nowadays inspired by the recent success of transformers <ref type="bibr" target="#b42">[43]</ref> in multi-modal learning <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33]</ref>, 2D <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b7">8]</ref> and 3D computer vision tasks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b52">53]</ref>. The transformer models are originally designed for natural language processing tasks, and then show strong performance and generalization ability in solving vision problems <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b1">2]</ref>. Exciting results have been achieved from different aspects including: (i) Enhancing the self-attention mechanism to incorporate useful inductive bias <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b48">49]</ref>. e.g. Swin Transformer <ref type="bibr" target="#b21">[22]</ref> replaces the global attention with shifted window attention to improve efficiency; Focal Transformer <ref type="bibr" target="#b48">[49]</ref> combines coarse-granularity global attention with fine-grained local attention to balance model efficiency and effectiveness; (ii) Combining transformer with CNNs <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46]</ref>. e.g. BOTNet <ref type="bibr" target="#b37">[38]</ref> uses a specially designed multi-head selfattention head to replace final three bottleneck blocks of ResNet; PVT <ref type="bibr" target="#b43">[44]</ref> and CVT <ref type="bibr" target="#b45">[46]</ref> embed convolutional layers in a hierarchical transformer framework and demonstrate that they can help improve the performance; (iii) Designing special training techniques <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b39">40]</ref>. e.g. DEIT <ref type="bibr" target="#b38">[39]</ref> proposes a special transformer token for knowledge distillation from CNNs; DRLOC <ref type="bibr" target="#b20">[21]</ref> designs a selfsupervised auxiliary task to make transformer learn spatial relations. Regarding dense scene understanding tasks <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b46">47]</ref>, Ranftl et al. <ref type="bibr" target="#b33">[34]</ref> recently propose a visual transformer framework with transpose convolution for dealing with dense prediction, while HRFormer <ref type="bibr" target="#b49">[50]</ref> adopts local attention to keep multi-scale features efficiently in the network.</p><p>To the best of our knowledge, this is the first exploration of simultaneously modeling multiple dense prediction tasks in a unified transformer framework for scene understanding. The proposed framework jointly learns spatial and alltask interactions in a global context at gradually increased resolutions with a novel and efficient UP-Transformer block, which can produce spatially higherresolution feature maps to significantly boost dense predictions, and effectively incorporate multi-scale information via the proposed strategies of self-attention message passing and encoder feature aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">InvPT for Multi-Task Dense Prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Framework Overview</head><p>The overall framework of the proposed Inverted Pyramid Multi-task Transformer (InvPT) is depicted in <ref type="figure" target="#fig_3">Fig. 2</ref>. It mainly consists of three core parts, i.e. a task-shared InvPT transformer encoder, the task-specific preliminary decoders, and the InvPT transformer decoder. Specifically, the transformer encoder learns generic visual representations from the input images for all tasks. Then, the preliminary decoders produce task-specific features and preliminary predictions, which are supervised by the ground-truth labels. The task-specific feature and preliminary prediction of each task are combined and concatenated as a sequence serving as input of the InvPT Transformer decoder, to learn to produce refined task-specific representations within global spatial and task contexts, which are further used to produce the final predictions with task-specific linear projection layers. The details of these parts are introduced as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">InvPT Transformer Encoder</head><p>The transformer encoder is shared for different tasks to learn generic visual representations from the input image. The self-attention mechanism of the transformer can help to learn a global feature representation of the input via longrange modeling of the spatial dependency of image pixels or patches. In our implementation, we consider different alternatives for the encoder including ViT <ref type="bibr" target="#b13">[14]</ref> and Swin Transformer <ref type="bibr" target="#b21">[22]</ref>. We obtain from the encoder a feature sequence and reshape it as a spatial feature map with resolution H 0 ? W 0 where H 0 and W 0 denote its height and width respectively. The feature map is then input into T preliminary decoders to learn the T task-specific feature maps. </p><formula xml:id="formula_0">( ) T C H W ? ? ? ( ) H C W ? ? H C W ? ? 2 2 H C W ? ? (2 2 ) ' C H W T ? ? ? Output</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reshaping</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bilinear Interpolation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reshaping</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task-specific Splitting</head><p>Concatenation </p><formula xml:id="formula_1">H W ? C # 1 #T 2 2 H W ? C ? ? ? ' (2 ) 2 H C W ? ? T { T { Conv-BN-ReLU ? Conv-BN-ReLU</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Task-Specific Preliminary Decoders</head><p>To learn task-specific representations for different tasks, we construct a decoding block consisting of a 3 ? 3 convolutional layer, a batch normalization layer, and a ReLU activation function (i.e. "Conv-BN-ReLU"). The preliminary decoder uses two such blocks to produce task-specific feature for each task. Then suppose that we have T tasks in total, for the t-th task, the output task-specific feature F d t from the preliminary decoder is projected by a linear projection layer (i.e. 1 ? 1 convolution) to produce a preliminary task prediction P t , which is supervised by the ground-truth labels. Then, we concatenate F d t and P t along the channel dimension, and adjust the channel number to C 0 with a linear projection layer, to make the different task-specific features with the same channel dimension to facilitate the processing in the transformer decoder. The combined output is denoted as F c t . We flatten F c t spatially to a sequence, and concatenate all T sequences as F c with F c ? R T H0W0?C0 . F c serves as the input of the proposed InvPT transformer decoder, which enables global task interaction and progressively generates refined task-specific feature maps with a high-resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">InvPT Transformer Decoder via UP-Transformer Block</head><p>As global self-attention is prohibitively expensive when the spatial resolution is high, many visual transformer models typically downsample the feature maps dramatically <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b43">44]</ref> and output features with low spatial resolution. However, resolution of the feature map is a critical factor for dense prediction problems, as we need to predict task labels for each pixel, and the sizes of semantic objects in images vary tremendously <ref type="bibr" target="#b53">[54]</ref>. Another point is that different scales of the feature maps can model different levels of visual information <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b49">50]</ref>, and thus it is particularly beneficial to make different tasks learn from each other at multiple scales. With these motivations, we design a progressively resolutionenlarged transformer decoder termed as "Inverted Pyramid Transformer Decoder" (i.e. InvPT decoder), which consists of an efficient UP-Transformer block, cross-scale self-attention message passing, and multi-scale encoder feature aggregation, in a unified network module.</p><p>Main Structure As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, there are three stages in the InvPT decoder, and each stage is the designed UP-Transformer block computing selfattention and updating features at different spatial resolutions. The first stage (i.e. stage 0) of InvPT decoder learns cross-task self-attention at the output resolution of the InvPT encoder (i.e. H 0 ? W 0 ), while the following two stages enlarge the spatial resolution of the feature maps, and calculate cross-task selfattention at higher resolutions. The latter two stages (i.e. stage 1 and 2) use the proposed UP-Transformer blocks which refine features at higher resolution and enable cross-scale self-attention propagation as well as multi-scale transformer feature aggregation from the transformer encoder.</p><p>To simplify the description, for the s-th stage (e.g. s = 0, 1, 2), we denote its input feature as F s with F s ? R T HsWs?Cs , where H s and W s are the spatial height and width of the feature, and C s is the channel number. Thus, the input of stage 0 (i.e. F 0 ) is F c , which is a combination of a set of T task-specific features as introduced in Sec. 3.3. For the output of InvPT decoder, we add up the feature maps from different stages after the alignment of feature resolution and channel dimension with bilinear interpolation and a linear projection layer, and then pass it to a Conv-BN-ReLU block to produce T upsampled and refined task-specific features for the final prediction.</p><p>Task-Specific Reshaping and Upsampling The transformer computing block typically operates on 2D feature-token sequences with the spatial structure of the feature map broken down, while the spatial structure is critical for dense prediction tasks, and it is not straightforward to directly perform upsampling on the feature sequence with the consideration of spatial structure. Another issue is that the input features into the InvPT decoder consist of multiple different taskspecific features, we need to perform feature upsampling and refinement for each single task separately to avoid feature corruption by other tasks. To address these issues, we design a task-specific reshaping and upsampling computation block for the InvPT decoder, i.e. the Reshape &amp; UP module as illustrated in <ref type="figure" target="#fig_2">Fig. 3 (b)</ref> and <ref type="figure">Fig. 4</ref>. Reshape &amp; Up first splits the feature tokens of F s ? R T HsWs?Cs in the first dimension into T (i.e. the number of tasks) groups of features with tensor slicing, and reshapes each of them back to a spatial feature map with a shape of R Hs?Ws?Cs . Then a bilinear interpolation with a scaling factor of 2 is performed to enlarge the height and width by 2?. Each scaled task-specific feature map is further fed into a Conv-BN-Relu block to perform feature fusion <ref type="figure">Fig. 4</ref>: Illustration of the proposed UP-Transformer Block. Input: A s?1 is the attention score matrix of the (s ? 1)-th stage, F s is the input multi-task feature sequence of the s-th stage, and F e s is the feature passed from the transformer encoder. Output: A ? s is the enhanced attention score matrix passing to the next stage and F s+1 is the refined and upsampled feature sequence. and reduction of channel dimension if necessary. Finally, The T feature maps are reshaped back to token sequences and finally concatenated as an upsampled multi-task token sequence.</p><p>Multi-task UP-Transformer Block The multi-task UP-Transformer block (see <ref type="figure">Fig. 4</ref>) is employed in both stage 1 and 2 (i.e. s = 1, 2) and learns to gradually increase the spatial resolution of the multi-task feature and also perform feature interaction and refinement among all the tasks in a global manner. As illustrated in <ref type="figure">Fig. 4</ref>, after the Reshape &amp; Up module with a 3 ? 3 Conv-BN-ReLU computation, we reduce the feature channel dimension by 2? and produce a learned upsampled feature-token sequence F up s ? R 4T HsWs?(Cs/2) . F up s is first added by the feature sequence passed from the transformer encoder, which we introduce later in this section, and a layer normalization (LN) <ref type="bibr" target="#b0">[1]</ref> is performed on the combined features to produce F ? s ? R 4T HsWs?(Cs/2) , which serves as the input for the self-attention calculation.</p><p>To calculate a global self-attention from the upsampled high-resolution multitask feature sequence F ? s , the memory footprint is prohibitively large. Thus, we first reduce the size of query Q s , key K s , and value V s matrices for self-attention computation following works <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b45">46]</ref>. Specifically, we first split and reshape the feature-token sequence F ? s into T groups of spatial feature maps corresponding to the T tasks, each with a shape of R 2Hs?2Ws?(Cs/2) . To generate the query embedding from each feature map, we use a convolution with a fixed kernel size k c = 3 and a stride 2 for each group, denoted as Conv(?, k c ). To generate the key and value embeddings, we use an average pooling operation Pool(?, k s ) with a kernel size k s (k s = 2 s+1 for the s-th stage). By controlling the kernel parameters of the convolution and pooling operations, we can largely improve the memory and computation efficiency of the global self-attention calculation, which makes the utilization of multiple consecutive UP-Transformer blocks possible. Then, we define Flac(?) as a function that first flattens the T groups of spatial feature maps and then performs concatenation to produce a multi-task feature-token sequence. Then we can perform global interaction among all the tasks with a multi-task self-attention. Let W q s , W k s , and W v s be the parameter matrices of three linear projection layers, the calculation of Q s , K s , and V s can be formulated as follows:</p><formula xml:id="formula_2">Q s = W q s ? Flac Conv(F ? s , k c ) , Q s ? R T HsWs? Cs 2 , K s = W k s ? Flac Pool(F ? s , k s ) , K s ? R 4T HsWs (ks) 2 ? Cs 2 , V s = W v s ? Flac Pool(F ? s , k s ) , V s ? R 4T Hs Ws (ks ) 2 ? Cs 2 .<label>(1)</label></formula><p>With Q s and K s , the self-attention score matrix A s for the s-th stage can be calculated as:</p><formula xml:id="formula_3">A s = Q s K T s C ? s , A s ? R T HsWs? 4T HsWs (ks) 2 ,<label>(2)</label></formula><p>where C ? s with C ? s = Cs 2 is a scaling factor to address the magnitude problem <ref type="bibr" target="#b42">[43]</ref>. In vanilla transformers, the self-attention map is directly calculated with a softmax function on A s . We propose a mechanism of Attention Message Passing (AMP) to enhance the attention A s before the Softmax normalization, using multi-scale information from different transformer stages.</p><p>Cross-Scale Self-Attention Message Passing To enable the InvPT decoder to model the cross-task interaction at different scales more effectively, we pass attention message to the attention score matrix A s at the current s-th stage</p><formula xml:id="formula_4">from A s?1 ? R T Hs Ws 4 ? 4T HsWs (ks ) 2</formula><p>, which is computed at the (s ? 1)-th stage. It can be noted that the second dimension of A s at different stages maintain the same size due to the design of the kernel size k s . An illustration of AMP is shown in <ref type="figure">Fig. 4</ref>. Specifically, we perform 'Reshape &amp; Up' operation to first adjust the shape of A s?1 to R Hs 2 ? Ws 2 ? T HsWs which has the same dimension as A s . Finally, we perform a linear combination of the self-attention maps A s and M s?1 to pass attention message from the (s ? 1)-th stage to the s-th stage as:</p><formula xml:id="formula_5">A ? s = ? 1 s A s + ? 2 s M s?1 , A m s = Softmax(A ? s ),<label>(3)</label></formula><p>where ? 1 s and ? 2 s are learnable weights for A s and M s?1 , respectively; Softmax(?) is a row-wise softmax function. After multiplying with the new attention map A m s with the value matrix V s , we obtain the final multi-task feature F s+1 as output, which is refined and upsampled, and can be further fed into the next UP-Transformer block. The whole process can be formulated as follows:</p><formula xml:id="formula_6">F s+1 = Reshape Up(A m s ? V s ) + F ? s ,<label>(4)</label></formula><p>where the function Reshape Up(?) denotes the task-specific reshaping and upsampling. It enlarges the feature map by 2? to be the same spatial resolution as F ? s which is a residual feature map from the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depth Boundary Normal Semseg Input</head><p>InvPT GT ATRC <ref type="figure">Fig. 5</ref>: Qualitative comparison with the previous best method ATRC <ref type="bibr" target="#b4">[5]</ref> on NYUD-v2. Ours produces more accurate predictions on the different tasks. Efficient Multi-Scale Encoder Feature Aggregation For dense scene understanding, some basic tasks such as object boundary detection require lowerlevel visual representation. However, it is tricky to efficiently use the multi-scale features in transformer as standard transformer structures have a quadratic computational complexity regarding the image resolution, and typically only operate on small-resolution feature maps. To gradually increase the feature map size and also incorporate multi-scale features is very challenging to GPU memory. Therefore, we design an efficient yet effective multi-scale encoder feature aggregation (EFA) strategy. As shown in <ref type="figure">Fig. 4</ref>, in each upsampling stage s of the InvPT decoder, we obtain a corresponding-scale feature sequence F e s with channel number C e s from the transformer encoder. We reshape it to a spatial feature map and apply a 3 ? 3 convolution to produce a new feature map with channel dimension C s , and then reshape and expand it T times to align with the dimension of F up s and add to it. Benefiting from the proposed efficient UP-Transformer block, we can still maintain high efficiency even if at each consecutive stage of the decoder, the resolution is gradually enlarged by two times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We present extensive experiments to demonstrate the effectiveness of the proposed InvPT framework for multi-task dense prediction.</p><p>Datasets The experiments are conducted on two popular scene understanding datasets with multi-task labels, i.e. NYUD-v2 <ref type="bibr" target="#b36">[37]</ref> and PASCAL-Context <ref type="bibr" target="#b8">[9]</ref>. NYUD-v2 contains various indoor scenes such as offices and living rooms with 795 training and 654 testing images. It provides different dense labels, including semantic segmentation, monocular depth estimation, surface normal estimation and object boundary detection. PASCAL-Context is formed from PASCAL dataset <ref type="bibr" target="#b14">[15]</ref>. It has 4,998 images in the training split and 5,105 in the testing split, covering both indoor and outdoor scenes. This dataset provides pixel-wise labels for semantic segmentation, human parsing and object boundary detection. Additionally, <ref type="bibr" target="#b24">[25]</ref> generates surface normal and saliency labels for this dataset.</p><p>We perform experiments on all tasks in both datasets for evaluation. Evaluation Semantic segmentation (Semseg) and human parsing (Parsing) are evaluated with mean Intersection over Union (mIoU); monocular depth estima- <ref type="table">Table 1</ref>: Ablation study on the InvPT decoder. The proposed InvPT and its components yield consistent improvement on different datasets and achieve clear overall improvement on each single task and the multi-task (MT) performance ? m . The gain shows absolute performance point improvement. The different variants of the InvPT all use Swin-tiny as its encoder structure. '?' means lower better and '?' means higher better.  tion (Depth) is evaluated with Root Mean Square Error (RMSE); surface normal estimation (Normal) is evaluated by the mean error (mErr) of predicted angles; saliency detection (Saliency) is evaluated with maximal F-measure (maxF); object boundary detection (Boundary) is evaluated with the optimal-dataset-scale F-measure (odsF). To evaluate the average performance gain of multi-task models against single-task models, we adopt the "multi-task learning performance" (MT Performance) metric ? m introduced in <ref type="bibr" target="#b24">[25]</ref>, which is an important metric to reflect the performance of a multi-task model (the higher the better). Implementation Details For the ablation study, we adopt Swin-Tiny transformer <ref type="bibr" target="#b21">[22]</ref> pre-trained on ImageNet-22K <ref type="bibr" target="#b12">[13]</ref> as the transformer encoder. The models for different evaluation experiments are trained for 40,000 iterations on both datasets, with a batch size of 6. For the transformer models, Adam optimizer is adopted with a learning rate of 2 ? 10 ?5 , and a weight decay rate of 1?10 ?6 . Polynomial learning rate scheduler is used. The output channel number of preliminary decoder is 768. More details are in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Analysis</head><p>Baselines and Model Variants To have a deep analysis of the proposed In-vPT framework, we first define several model baselines and variants (see <ref type="table">Table 1</ref>): (i) "InvPT Baseline (MT)" denotes a strong multi-task baseline model of the proposed InvPT framework. It uses Swin-tiny encoder and two 3 ? 3 Conv-BN-ReLU blocks as decoder for each task, which is equivalent to the preliminary  decoder in InvPT. The encoder feature map is upsampled by 8? before the final prediction. It also combines multi-scale features from the encoder to help boost performance. This is a typical multi-task baseline structure as in previous works <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b4">5]</ref>. (ii) "InvPT Baseline (ST)" has a similar structure as "InvPT Baseline" but it is trained under single-task setting.  <ref type="table">Table 1</ref>. It can be observed that the UTB, AMP and EFA modules all achieve clear improvement. Specifically, as a core module of the proposed InvPT framework, UTB significantly improves the task Semseg by 2.12 (mIoU), Depth by 0.0707 (RMSE) and Normal by 0.42 (mErr) on NYUD-v2. Finally, the full model of InvPT achieves remarkable performance gain compared against both the Single-task (ST) baseline (see MT performance ? m ) and the multi-task (MT) baseline (see the Gain on each task), clearly verifying the effectivenss of the proposed InvPT decoder. For qualitative comparison, in <ref type="figure" target="#fig_4">Fig. 6</ref>, we show prediction examples generated by different model variants which add these modules one by one on PASCAL-Context. It is intuitively to observe that the proposed UTB, AMP and EFA all help produce visually more accurate predictions to against the baseline.</p><p>Multi-task Improvement against Single-task Setting To validate the effectiveness of the proposed multi-task model, we compare it with its single-task variant "InvPT Baseline (ST)" on both datasets in <ref type="table">Table 1</ref>. Our full model achieves strong performance improvement against the single-task model, yielding 2.59% multi-task performance on NYUD-v2 and 1.76% on PASCAL-Context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Different Transformer Encoders</head><p>We also compare the model performance using two families of transformer encoders, i.e. Swin Transformer (Swin-T, Swin-B, and Swin-L) <ref type="bibr" target="#b21">[22]</ref> and ViT (ViT-B and ViT-L) <ref type="bibr" target="#b13">[14]</ref>, used as our InvPT encoder. The results on PASCAL-Context are shown in <ref type="table" target="#tab_2">Table 2</ref>, and results on NYUD-v2 can be found in Appendix. We observe that the models with higher capacity in the same family can generally obtain consistent performance  gain on tasks including Semseg and Parsing, while on other lower-level tasks (e.g. Boundary) the improvement is however not clear. One possible reason for the difference of performance gain is the task competition problem in training as discussed in previous works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Effect of the Number of Stages Our UP-Transformer block typically consists of three stages. In <ref type="figure">Fig. 7</ref>, we show how the number of stages of InvPT decoder influences the performance of the different tasks on PASCAL-Context. It can be observed that using more stages can help the InvPT decoder learn better predictions for all tasks. Our efficient design makes the multi-task decoding feature maps with gradually increased resolutions possible.</p><p>Qualitative Study of learned Features with InvPT In <ref type="figure" target="#fig_7">Fig. 8</ref>, we show visualization comparison of the learned final features between the transformer baseline (i.e. InvPT Baseline (MT)) and our InvPT full model, to further demonstrate how the features are improved using our proposed InvPT model. The statistics of the learned feature points is visualized with t-SNE <ref type="bibr" target="#b23">[24]</ref> on all 20 semantic classes of Pascal-Context dataset. It is obvious that our model helps learn more discriminative features, thus resulting in higher quantitative results. The generated spatial feature maps for segmentation are also intuitively better. Generalization Performance To qualitatively study the generalization performance of the proposed multi-task transformer for dense scene understanding, we compare it with the best performing methods, including ATRC <ref type="bibr" target="#b4">[5]</ref> and PAD-Net <ref type="bibr" target="#b47">[48]</ref>, on the challenging DAVIS video segmentation Dataset <ref type="bibr" target="#b29">[30]</ref>. The results are shown in the video demo in the github page. All the models are trained on PASCAL-Context with all the five tasks. Then the models are directly tested on DAVIS to generate multi-task predictions on video sequences. One example frame is shown in <ref type="figure">Fig. 9</ref>, which clearly shows our advantage on this perspective. <ref type="figure">Fig. 9</ref>: Study on generalization performance. Models are all trained on PASCAL-Context <ref type="bibr" target="#b8">[9]</ref> and tested on DAVIS video dataset <ref type="bibr" target="#b29">[30]</ref>. The proposed method yields better generalization performance compared to PAD-Net <ref type="bibr" target="#b47">[48]</ref> and ATRC <ref type="bibr" target="#b4">[5]</ref>.  <ref type="table" target="#tab_3">Table 3</ref> shows a comparison of the proposed InvPT method against existing state-of-the-arts, including PAD-Net <ref type="bibr" target="#b47">[48]</ref>, MTI-Net <ref type="bibr" target="#b41">[42]</ref> and ATRC <ref type="bibr" target="#b4">[5]</ref>, on both NYUD-v2 and PASCAL-Context. On all the 9 metrics from these two benchmarks, the proposed InvPT achieves clearly superior performance, especially for higher-level scene understanding tasks such as Semseg and Parsing. Notably, on NYUD-v2, our InvPT surpasses the previous best performing method (i.e. ATRC) by +7.23 (mIoU) on Semseg, while on PASCAL-Context, InvPT outperforms ATRC by +11.36 (mIoU) and +4.68 (mIoU) on Semseg and Parsing, respectively. A qualitative comparison with ATRC is shown in <ref type="figure">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper presented a novel transformer framework, Inverted Pyramid Multitask Transformer (InvPT), for the multi-task dense prediction for visual scene understanding. InvPT is able to effectively learn the long-range interaction in both spatial and all-task contexts on the multi-task feature maps with gradually increased spatial resolution for dense prediction. Extensive experiments demonstrated the effectiveness of the proposed method, and also showed its significantly better performance on two popular benchmarks compared to the previous state-of-the-art methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 More Implementation Details</head><p>In this section, we provide more details about our model implementation in addition to those discussed in the paper.</p><p>Model Optimization. For evaluation on NYUD-v2 <ref type="bibr" target="#b36">[37]</ref> and PASCAL-Context <ref type="bibr" target="#b8">[9]</ref>, we totally consider six dense prediction tasks, including semantic segmentation (Semseg), monocular depth estimation (Depth), surface normal estimation (Normal), human parsing (Parsing), saliency detection (Saliency), and object boundary detection (Boundary). For the continuous regression tasks (i.e. Depth and Normal) a L1 Loss is employed. For the discrete classification tasks (i.e. Semseg, Parsing, Saliency, and Boundary), a cross-entropy loss is utilized. For the sake of simplicity, we use the same set of loss functions for both intermediate and final supervision. The whole model can be end-to-end optimized. Data Processing. For a fair comparison with ATRC <ref type="bibr" target="#b4">[5]</ref>, we follow its data processing pipeline. On PASCAL-Context, we pad the image to the size of 512 ? 512, while on NYUD-v2, we randomly crop the input image to the size of 448 ? 576 as Swin Transformer <ref type="bibr" target="#b21">[22]</ref> requires both the height and width to be even for conducting patch merging. We use typical data augmentation including random scaling, cropping, horizontal flipping and color jittering. Implementation Details of Encoder Feature Aggregation (EFA). For Swin Transformer encoders <ref type="bibr" target="#b21">[22]</ref>, we pass feature sequences from the first three stages to Inverted Pyramid Transformer Decoder (InvPT decoder). For ViT encoders <ref type="bibr" target="#b13">[14]</ref>, since they do not explicitly define the concept of stage, we evenly choose 3 layers based on the depth and unfold their output spatially, and then use transposed convolution to upsample the resolution of feature maps to match the spatial resolution in the corresponding decoder stage before further transformation. Specifically, for ViT-base encoder, we use the output token sequences of layer 3, 6, and 9, while for ViT-large encoder we use output token sequences of layer 6, 12, and 18. The kernel size and stride of the transposed convolution for the feature at the first scale are 4, and those at the second scale are 2. Details about Self-attention in InvPT Decoder. The specific shapes of the query, the key, and the value matrices (i.e. Q, K, V) in different UP-Transformer stages are shown in <ref type="table" target="#tab_5">Table 4</ref>. Please refer to Sec. 3.4 in paper for the detailed definitions of the notations in the table. </p><formula xml:id="formula_7">s = 0 s = 1 s = 2 Q T H 0 W 0 4 , C0 T H0W0, C 0 2 4T H0W0, C 0 4 K T H 0 W 0 4 , C0 T H 0 W 0 4 , C 0 2 T H 0 W 0 4 , C 0 4 V T H 0 W 0 4 , C0 T H 0 W 0 4 , C 0 2 T H 0 W 0 4</formula><p>, C 0 <ref type="figure">Fig. 10</ref>: An example frame of the demo video for the study of generalization performance. Models are all trained on PASCAL-Context and tested on DAVIS video dataset. Our method yields qualitatively better generalization performance compared to PAD-Net <ref type="bibr" target="#b47">[48]</ref> and ATRC <ref type="bibr" target="#b4">[5]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 More Experimental Results and Analysis</head><p>Video Demo for Generalization Performance Comparison on DAVIS Video Dataset. As introduced in the paper, to qualitatively study the generalization ability of the proposed multi-task transformer for dense scene understanding, we compare it with the best performing method, including ATRC <ref type="bibr" target="#b4">[5]</ref> and PAD-Net <ref type="bibr" target="#b47">[48]</ref>, on the challenging DAVIS video Dataset <ref type="bibr" target="#b29">[30]</ref>. The results are shown in the attached video demo. All the models are trained on Pascal-Context with 5 tasks, i.e. semantic segmentation, surface normal estimation, human parsing, saliency detection, and object boundary detection. Then the models are directly tested on DAVIS to generate multi-task predictions in the demo video. Significantly stronger generalization ability of our InvPT is observed and an example frame is shown in <ref type="figure">Fig. 10</ref>. Effect of Different Transformer Encoders. Similar to the results on PASCAL-Context in the paper, we compare two families of transformer encoders: Swin Transformer (Swin-T, Swin-B and Swin-L) <ref type="bibr" target="#b21">[22]</ref> and ViT (ViT-B and ViT-L) <ref type="bibr" target="#b13">[14]</ref> on NYUD-v2 <ref type="table" target="#tab_6">(Table 5</ref>). We observe that the bigger model of the same model family consistently brings performance gain on semantic segmentation and monocular depth estimation, while on other dense tasks (i.e. saliency and boundary detection), it does not necessarily yield significantly better performance despite with higher model capacity. This phenomenon may result from the distinct characteristics of different dense prediction tasks. Computation cost of InvPT We show the computation cost of the proposed InvPT model in <ref type="table" target="#tab_7">Table 6</ref>, including runtime per image, single-sample GPU-  memory consumption, and the model size (in terms of number of parameters) of InvPT with different transformer backbone architectures. We run on the test split (i.e. 5,105 images in total) of PASCAL-Context dataset and calculate the average inference runtime per sample using a NVIDIA RTX 3090 GPU.</p><p>Computation efficiency of UP-Transformer Block <ref type="table" target="#tab_8">Table 7</ref> compares between our UP-Transformer block and a vallina vision transformer-based <ref type="bibr" target="#b13">[14]</ref> upsampling block which upsamples the multi-task outputs with the same three stages. They use the same Vit-B encoder. It is clear that our design achieves significant improvement on efficiency compared to baseline (e.g. approximately 3? more efficient in terms of runtime, GPU memory, and number of parameters). More Qualitative Results. We show more prediction results by InvPT (ours) and the SOTA method ATRC <ref type="bibr" target="#b4">[5]</ref> on the challenging PASCAL-Context dataset in <ref type="figure">Fig. 11</ref> and <ref type="figure" target="#fig_3">Fig. 12</ref>. It is clear that our method produces significantly better results than ATRC, especially on semantic segmentation and human parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Comparison of the Preliminary and Final Predictions of</head><p>InvPT. <ref type="figure" target="#fig_2">Fig. 13</ref> shows the qualitative comparison of the preliminary predictions and the final predictions generated by InvPT on PASCAL-Context. We can observe that InvPT decoder successfully refines the preliminary predictions and generates remarkably better results on all these dense prediction tasks. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>arXiv:2203.07997v2 [cs.CV] 18 Jul 2022</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>(a) Illustration of InvPT Transformer decoder. Stage 0 uses a transformer block which keeps the feature resolution unchanged, while stage 1 and 2 employ the proposed UP-Transformer block which enlarges the spatial resolution by 2?. The Attention Message Passing (AMP) enables cross-scale self-attention interaction, and the Encoder Feature aggregation incorporates multi-scale information from the InvPT encoder. "?" and "Proj" denote the accumulation operation and the linear projection layer, respectively. (b) Pipeline of task-specific reshaping and upsampling block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(ks ) 2 ,</head><label>2</label><figDesc>and then perform 2? bilinear interpolation in the first two spatial dimensions, and finally flatten it to have the attention message matrix M s?1 ? R T HsWs? 4T Hs Ws (ks) 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Qualitative analysis of InvPT decoder on PASCAL-Context. Results of different model variants are shown by columns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(iii) "InvPT w/ UTB" indicates adding the proposed UP-Transformer block upon "InvPT Baseline (MT)"; Similarly, "InvPT w/ UTB + AMP" indicates further adding the cross-scale Attention Message Passing, and "InvPT w/ UTB + AMP + EFA" denotes the full model by further adding multi-scale Encoder Feature Aggregation. Effectiveness of InvPT Decoder In this part, we investigate the effectiveness of the proposed three modules to demonstrate the proposed InvPT decoder, i.e. UP-Transformer Block (UTB), cross-scale Attention Message Passing (AMP), and Encoder Feature Aggregation (EFA), on both datasets. The experimental results for this investigation are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Statistics of learned features with t-SNE [23] of all 20 classes on Pascal-Context. Visualization of examples of learned feature from the semantic segmentation task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Qualitative interpretation of learned features using the proposed InvPT method. From both (a) and (b), it can be observed that features learned by In-vPT is effectively improved and is more discriminative compared to the baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 :Fig. 12 :FinalFig. 13 :</head><label>111213</label><figDesc>Qualitative comparison with the best performing method ATRC<ref type="bibr" target="#b4">[5]</ref> on PASCAL-Context. Our method generates significantly better results especially on semantic segmentation and human parsing. Qualitative comparison with the best performing method ATRC<ref type="bibr" target="#b4">[5]</ref> on PASCAL-Context. Our method generates significantly better predictions especially on semantic segmentation and human parsing. Qualitative comparison of the predictions from the preliminary decoder and the final predictions of InvPT decoder on PASCAL-Context. The final predictions on all these tasks are significantly more accurate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Investigation of the number of stages in the InvPT decoder.</figDesc><table><row><cell>Semseg (mIoU) 71.72 73.51 73.93 Fig. 7: Encoder Parsing (mIoU) Saliency (maxF) Normal (mErr) Boundary (odsF) 0 20 40 60 80 Metric 59.28 82.79 15.52 52.70 61.75 84.01 14.51 68.10 62.73 84.24 14.15 72.60 1 stage 2 stages 3 stages Swin-T Swin-B Swin-L Vit-B Vit-L</cell><cell>Semseg mIoU ? 73.93 77.50 78.53 77.33 79.03</cell><cell>Parsing mIoU ? 62.73 66.83 68.58 66.62 67.61</cell><cell>Saliency maxF ? 84.24 83.65 83.71 85.14 84.81</cell><cell>Normal mErr ? 14.15 14.63 14.56 13.78 14.15</cell><cell>Boundary odsF ? 72.60 73.00 73.60 73.20 73.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison of using different transformer encoder structures in InvPT on PASCAL-Context.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="11">: State-of-the-art comparison on NYUD-v2 (left) and PASCAL-Context</cell></row><row><cell cols="11">(right). Our InvPT significantly outperforms the previous state-of-the-arts by a</cell></row><row><cell cols="9">large margin. '?' means lower better and '?' means higher better.</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="4">Semseg Depth Normal Boundary mIoU ? RMSE ? mErr ? odsF ?</cell><cell>Model</cell><cell cols="5">Semseg Parsing Saliency Normal Boundary mIoU ? mIoU ? maxF ? mErr ? odsF ?</cell></row><row><cell>Cross-Stitch [27]</cell><cell>36.34</cell><cell>0.6290</cell><cell>20.88</cell><cell>76.38</cell><cell>ASTMT [25]</cell><cell>68.00</cell><cell>61.10</cell><cell>65.70</cell><cell>14.70</cell><cell>72.40</cell></row><row><cell>PAP [52]</cell><cell>36.72</cell><cell>0.6178</cell><cell>20.82</cell><cell>76.42</cell><cell>PAD-Net [48]</cell><cell>53.60</cell><cell>59.60</cell><cell>65.80</cell><cell>15.30</cell><cell>72.50</cell></row><row><cell>PSD [55]</cell><cell>36.69</cell><cell>0.6246</cell><cell>20.87</cell><cell>76.42</cell><cell>MTI-Net [42]</cell><cell>61.70</cell><cell>60.18</cell><cell>84.78</cell><cell>14.23</cell><cell>70.80</cell></row><row><cell>PAD-Net [48]</cell><cell>36.61</cell><cell>0.6270</cell><cell>20.85</cell><cell>76.38</cell><cell>ATRC [5]</cell><cell>62.69</cell><cell>59.42</cell><cell>84.70</cell><cell>14.20</cell><cell>70.96</cell></row><row><cell>MTI-Net [42]</cell><cell>45.97</cell><cell>0.5365</cell><cell>20.27</cell><cell>77.86</cell><cell>ATRC-ASPP [5]</cell><cell>63.60</cell><cell>60.23</cell><cell>83.91</cell><cell>14.30</cell><cell>70.86</cell></row><row><cell>ATRC [5]</cell><cell>46.33</cell><cell>0.5363</cell><cell>20.18</cell><cell>77.94</cell><cell cols="2">ATRC-BMTAS [5] 67.67</cell><cell>62.93</cell><cell>82.29</cell><cell>14.24</cell><cell>72.42</cell></row><row><cell>InvPT (ours)</cell><cell>53.56</cell><cell>0.5183</cell><cell>19.04</cell><cell>78.10</cell><cell>InvPT (ours)</cell><cell>79.03</cell><cell>67.61</cell><cell>84.81</cell><cell>14.15</cell><cell>73.00</cell></row><row><cell cols="5">4.2 State-of-the-art Comparison</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Shapes of Q, K, V matrices in different upsampling stages. Please refer to Sec. 3.4 in paper for the detailed definitions of the notations in the table.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison of using different transformer encoder structures in InvPT on NYUD-v2.</figDesc><table><row><cell>Model</cell><cell cols="4">Semseg Depth Normal Boundary mIoU ? RMSE ? mErr ? odsF ?</cell></row><row><cell cols="2">Swin-T 44.27</cell><cell>0.5589</cell><cell>20.46</cell><cell>76.10</cell></row><row><cell cols="2">Swin-B 50.97</cell><cell cols="2">0.5071 19.39</cell><cell>77.30</cell></row><row><cell cols="4">Swin-L 51.76 0.5020 19.39</cell><cell>77.60</cell></row><row><cell>Vit-B</cell><cell>50.30</cell><cell cols="2">0.5367 19.00</cell><cell>77.60</cell></row><row><cell>Vit-L</cell><cell cols="3">53.56 0.5183 19.04</cell><cell>78.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Computation analysis of InvPT with different backbones.</figDesc><table><row><cell>InvPT</cell><cell cols="5">w/ Swin-T w/ Swin-B w/ Swin-L w/ Vit-B w/ Vit-L</cell></row><row><cell>Runtime (sec/img)</cell><cell>0.0270</cell><cell>0.0375</cell><cell>0.0462</cell><cell>0.0356</cell><cell>0.0633</cell></row><row><cell>GPU memory (MB)</cell><cell>2921</cell><cell>3238</cell><cell>4973</cell><cell>3175</cell><cell>4977</cell></row><row><cell>Number of parameters</cell><cell>78M</cell><cell>162M</cell><cell>364M</cell><cell>138M</cell><cell>380M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Computation efficiency of UP-Transformer Block.</figDesc><table><row><cell>Method</cell><cell cols="3">Runtime (sec/img) GPU memory (MB) Number of parameters</cell></row><row><cell>InvPT w/ vanilla ViT Upsampling</cell><cell>0.1194</cell><cell>11827</cell><cell>400M</cell></row><row><cell>InvPT w/ UP-Transformer (ours)</cell><cell>0.0356</cell><cell>3175</cell><cell>138M</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<title level="m">Are transformers more robust than cnns? In: NeurIPS (2021)</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Understanding robustness of transformers for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploring relational context for multi-task dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bruggemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kanakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Obukhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV (2021) 2, 4</title>
		<imprint>
			<date type="published" when="1920" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Pre-trained image processing transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pix2seq: A language modeling framework for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.10852</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Detect what you can: Detecting and representing objects using holistic models and body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Just pick a sign: Optimizing deep multitask models with gradient sign dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sagun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
	<note>In: ICLR (2021) 2, 4, 5, 6, 12</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mtl-nas: Task-agnostic neural architecture search towards general-purpose multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Nddr-cnn: Layerwise feature fusing in multi-task cnns by neural discriminative dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transformer in transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Multi-task multi-sensor fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Efficient training of visual transformers with small-size datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lepri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Nadai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03746</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV (2021)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Attentive single-tasking of multiple tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Voxel transformer for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Cross-stitch networks for multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">3d object detection with pointformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<title level="m">Learning transferable visual models from natural language supervision</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Vision transformers for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV (2021)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Latent multi-task architecture learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bingel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>S?gaard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-task learning as multi-objective optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Going deeper with image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-task learning for dense prediction tasks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mti-net: Multi-scale task interaction networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (2020) 3, 4</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In: ICCV (2021) 2, 4, 5, 6, 8</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<title level="m">Cvt: Introducing convolutions to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pad-net: Multi-tasks guided predictionand-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Focal self-attention for local-global interactions in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Hrformer: High-resolution transformer for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Transfer vision patterns for multi-task pixel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ACMMM</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pattern-affinitive propagation across depth, surface normal and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Point transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV (2021)</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Patternstructure diffusion for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>In: CVPR (2020) 2, 3, 4</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
