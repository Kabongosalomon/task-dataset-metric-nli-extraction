<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Good Practices for Deep 3D Hand Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-07-23">23 Jul 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengkai</forename><surname>Guo</surname></persName>
							<email>guohengkaighk@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guijin</forename><surname>Wang</surname></persName>
							<email>wangguijin@tsinghua.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
							<email>chenxinghao1010@163.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cairong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Good Practices for Deep 3D Hand Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-07-23">23 Jul 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D hand pose estimation from single depth image is an important and challenging problem for human-computer interaction. Recently deep convolutional networks (ConvNet) with sophisticated design have been employed to address it, but the improvement over traditional random forest based methods is not so apparent. To exploit the good practice and promote the performance for hand pose estimation, we propose a tree-structured Region Ensemble Network (REN) for directly 3D coordinate regression. It first partitions the last convolution outputs of ConvNet into several grid regions. The results from separate fully-connected (FC) regressors on each regions are then integrated by another FC layer to perform the estimation. By exploitation of several training strategies including data augmentation and smooth L 1 loss, proposed REN can significantly improve the performance of ConvNet to localize hand joints. The experimental results demonstrate that our approach achieves the best performance among state-of-the-art algorithms on three public hand pose datasets. We also experiment our methods on fingertip detection and human pose datasets and obtain stateof-the-art accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D hand pose estimation from depth imaging has drawn lots of attention from researchers <ref type="bibr" target="#b25">[26]</ref>  <ref type="bibr" target="#b37">[38]</ref>  <ref type="bibr" target="#b34">[35]</ref> due to its important role in applications of augmented reality (AR) and human-computer interface (HCI) <ref type="bibr" target="#b42">[43]</ref>. It aims to predict the 3D accurate positions for hand joints <ref type="bibr" target="#b29">[30]</ref> with monocular depth images, which is critical for gesture recognition <ref type="bibr" target="#b5">[6]</ref>. Though has been studied for several years <ref type="bibr" target="#b29">[30]</ref>, it is still challenging owing to high joint flexibility, large view variance, poor depth quality, severe self occlusion, and sim-ilar part confusion.</p><p>Recently, deep convolutional networks (ConvNets) have exhibited state-of-the-art performance across several computer vision tasks such as object classification <ref type="bibr" target="#b18">[19]</ref>, object detection <ref type="bibr" target="#b10">[11]</ref>, and image segmentation <ref type="bibr" target="#b4">[5]</ref>. ConvNets have also been employed to solve the problem of hand pose estimation, often with complicated structure design such as multi-branch inputs <ref type="bibr" target="#b31">[32]</ref> <ref type="bibr" target="#b21">[22]</ref> and multi-model regression <ref type="bibr" target="#b21">[22]</ref> [23] <ref type="bibr" target="#b8">[9]</ref>  <ref type="bibr" target="#b40">[41]</ref>. Thanks to the great modeling capacity and end-to-end feature learning, deep ConvNets have achieved competitive accuracy for hand pose estimation. However, ConvNets remain unable to obtain significant advantage over traditional random forest based methods <ref type="bibr" target="#b27">[28]</ref> [ <ref type="bibr" target="#b34">35]</ref>, which may result from the relatively shallow ConvNet structure (often 3 -5 convolution layers <ref type="bibr" target="#b31">[32]</ref> [23] <ref type="bibr" target="#b40">[41]</ref>) and high risk of overfitting with relative small datasets compared to image classification.</p><p>In this paper, we explore multiple good practices with hand pose estimation in single depth images. Most importantly, inspired by model ensemble and multi-view voting <ref type="bibr" target="#b18">[19]</ref>, we present a single deep ConvNet architecture named Region Ensemble Net (REN) <ref type="bibr" target="#b0">1</ref>  <ref type="figure" target="#fig_0">(Fig.1)</ref> to directly regress the 3D hand joint coordinates with end-to-end optimization and inference. We implement it by training individual fully-connected (FC) layers on multiple feature regions and combining them as ensembles. In addition, we adopt several approaches to enhance the performance including residual connection <ref type="bibr" target="#b15">[16]</ref>, data augmentation and smooth L 1 loss <ref type="bibr" target="#b9">[10]</ref>. As shown in our experiments, REN significantly promotes the performance of our ConvNet, which outperforms state-of-the-art methods on three challenging hand pose benchmarks <ref type="bibr" target="#b31">[32]</ref> [31] <ref type="bibr" target="#b27">[28]</ref>. Evaluated on fingertip <ref type="bibr" target="#b31">[32]</ref> and human pose benchmarks <ref type="bibr" target="#b13">[14]</ref>, our REN also achieves <ref type="bibr" target="#b0">1</ref> Codes and models are available at https://github.com/guohengkai/region-ensemble-network the best accuracy. This paper builds on our preliminary publication <ref type="bibr" target="#b12">[13]</ref>. Compared with it, this paper describes more techinical details and discusses several important factors for good practice, leading to slightly better results than <ref type="bibr" target="#b12">[13]</ref> with different region settings. In addition, we add results for one extra hand pose dataset <ref type="bibr" target="#b27">[28]</ref>, and further evaluate our REN for fingertip detection and human pose estimation with stateof-the-art performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>We briefly review relevant hand pose estimation methods with ConvNets for depth imaging, and examine methodologies related to the proposed algorithm, including ensemble methods and multi-view testing for ConvNets. Finally we also introduce works using ConvNets for RGB-D fingertip detection and human pose estimation, which will be compared in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">RGB-D Hand pose estimation with ConvNets</head><p>Recently deep ConvNets have been applied on hand pose estimation for depth imaging. Tompson et al. <ref type="bibr" target="#b31">[32]</ref> first use ConvNets to produce 2D heat maps with multi-scale inputs and infer the 3D hand pose with inverse kinematics. Oberweger et al. <ref type="bibr" target="#b21">[22]</ref> directly regress the 3D joint locations with multi-scale and multi-stage ConvNets using a linear layer as pose prior. In <ref type="bibr" target="#b22">[23]</ref>, a feedback loop is employed to iteratively correct the mistakes of inference, in which three ConvNets are used for pose initialization, image synthesis and pose updating. Ge et al. <ref type="bibr" target="#b8">[9]</ref> employ three ConvNets from orthogonal views to separately regress 2D heat maps for each views with depth projections and fuse them to produce 3D hand pose. In <ref type="bibr" target="#b41">[42]</ref>, physical joint constraints are incorporated into a forward kinematics based layer in Con-vNet. Similarly, Zhang et al. <ref type="bibr" target="#b40">[41]</ref> embed skeletal manifold into ConvNets and train the model end-to-end to render sequential prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-model ensemble methods for ConvNets</head><p>Traditional ensemble learning means that training multiple individual models and combining their outputs via averaging or weighted fusions, which is widely adopted in recognition competitions <ref type="bibr" target="#b18">[19]</ref>. In addition to bagging <ref type="bibr" target="#b18">[19]</ref> [29], boosting is also introduced for people counting <ref type="bibr" target="#b32">[33]</ref>. However, using multiple ConvNets for both training and testing requires huge cost of memory and time, which is not practical for applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Multi-branch ensemble methods for ConvNets</head><p>Single ConvNet with the fusion of multiple branches can also be regarded as a generalized type of ensemble. One popular strategy is to fuse different scaling inputs [32] <ref type="bibr" target="#b21">[22]</ref> or different image cues <ref type="bibr" target="#b11">[12]</ref> [21] <ref type="bibr" target="#b6">[7]</ref> with multi-input branches. Another approach is to employ multi-output branches with shared convolutional feature extractor, either training with different samples <ref type="bibr" target="#b19">[20]</ref> or learning to predict different categories <ref type="bibr" target="#b0">[1]</ref>. Compared with multi-input ensemble, multi-output methods cost less time because inference of FC layers is much faster than that of convolutional layers. Our method also falls into such category, but we apply ensemble on feature regions instead of inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Multi-view testing for ConvNets</head><p>Multi-view testing is widely adopted to improve accuracy for object classification <ref type="bibr" target="#b18">[19]</ref> [24] <ref type="bibr" target="#b14">[15]</ref>. In <ref type="bibr" target="#b18">[19]</ref>, predictions from 10-crop (four corners and one center with horizontal flip) are averaged on single ConvNet. In <ref type="bibr" target="#b23">[24]</ref> [15], fully-convolutional networks are employed in testing with multi-scale and multi-view inputs. Then spatially average pooling is applied on the class score map to obtain the final scores. To best of our knowledge, such strategy has not been applied on 3D pose regression yet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">RGB-D fingertip detection and human pose estimation for ConvNets</head><p>Fingertips play an important role in human-computer interaction among the hand joints. Wetzler et al. <ref type="bibr" target="#b36">[37]</ref> employ ConvNet for in-plane derotation of hand depth image and then use random forests or ConvNets for fingertip coordinate regression. Guo et al. <ref type="bibr" target="#b11">[12]</ref> introduce a two-stream ConvNet to detect the 3D fingertips, which makes use of both depth information and edge information with slow fusion strategy.</p><p>Human pose estimation is also important for HCI applications such as action recognition <ref type="bibr" target="#b39">[40]</ref>  <ref type="bibr" target="#b3">[4]</ref>. Though Con-vNets are widely used in human pose estimation for RGB images <ref type="bibr" target="#b2">[3]</ref>  <ref type="bibr" target="#b1">[2]</ref>, there are limited number of works using ConvNets for depth imaging <ref type="bibr" target="#b24">[25]</ref> [36] due to relatively small size of training datasets. Haque et al. <ref type="bibr" target="#b13">[14]</ref> introduce a viewpoint invariant model using ConvNets and recurrent networks (RNNs) for human pose estimation. Local regions from depth images are transformed into a learned feature space via ConvNets and then RNNs are leveraged to predict the offsets of pose sequentially with multi-task setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Region Ensemble Network</head><p>As in <ref type="figure" target="#fig_0">Fig.1</ref>, REN starts with a ConvNet for feature extraction. Then the features are divided into multiple grid regions. Each region is fed into FC layers and learnt to fuse for hand pose prediction. In this section we introduce the basic network architecture, region ensemble structure and implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network architecture with residual connection</head><p>The architecture of our ConvNet for feature extraction consists of six convolutional layers with 3 ? 3 kernels ( <ref type="figure">Fig.2</ref>) and three pooling layers with 2 ? 2 kernels. Each convolutional layer is followed by a Rectified Linear Unit (ReLU) activation. The ConvNet accepts inputs of a 96?96 depth image and outputs the feature maps with dimension of 12 ? 12 ? 64. To improve the learning ability, two residual connections are adopted between pooling layers with 1 ? 1 convolution filters for dimension increase as in <ref type="bibr" target="#b15">[16]</ref>. So there are totally eight convolutional layers in our model, which is deeper than ConvNets in <ref type="bibr" target="#b40">[41]</ref> with five layers.</p><p>For regression, we use two 2048 dimension FC layers with dropout rate <ref type="bibr" target="#b26">[27]</ref> of 0.5 for each regressor to avoid overfitting. The output of regressor is a 3 ? J vector representing the 3D world coordinates for hand joints, where J is the number of joints. <ref type="figure">Figure 2</ref>. Structure of basic ConvNet for feature extraction. The ConvNet consists of six convolutional layers and three pooling layers. The dotted arrows represent residual connections with dimension increase <ref type="bibr" target="#b15">[16]</ref>. The non-linear activation layers following each convolutional layers are not showed in the figure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Region ensemble structure</head><p>Multi-view testing averages predictions from different crops of original input image, which reduces the variance for image classification <ref type="bibr" target="#b18">[19]</ref>. Because image classification is invariant to translation and cropping, multi-view testing is easy to apply by directly cropping on the input image.</p><p>When it comes to pose regression, each cropped parts will correspond to different hand pose configurations. So we should adapt the 3D coordinates of hand pose to the cropped view. Meanwhile, using multiple inputs to feed the Con-vNet one-by-one is time-consuming.</p><p>Because each activation in the convolutional feature maps is contributed by a receptive field in the input image domain, we can project the multi-view inputs onto the regions of the feature maps. By using separate regions as features, we can train separate regressors instead of single regressor. So multi-view voting could be extended to regression task by utilizing each regions to separately predict the whole hand pose and then combining the results.</p><p>Based on this inspiration, we define a tree-structured network consisting of a single ConvNet trunk and several regression branches as shown in <ref type="figure" target="#fig_0">Fig.1</ref>. We first divide the feature maps of ConvNet into several regions. For each region, we feed it into the FC layers respectively as branches. There are several ways to combine different branches. A simple strategy is bagging, which averages all outputs of branches using average pooling. In order to boost the predictions from all the regions, we employ region ensemble strategy instead of bagging: features from the last FC layers of all regions are concatenated and used to infer the coordinates with an extra regression layer. The whole network can be trained end-to-end by minimizing the regression loss.</p><p>For region setting, we use nine regions with size of 6 ? 6 located at four conners (left part in <ref type="figure" target="#fig_1">Fig. 3</ref>, which is also the whole setting in <ref type="bibr" target="#b12">[13]</ref>), four centers near the edges (middle part in <ref type="figure" target="#fig_1">Fig. 3</ref>) and the center of the feature maps. The receptive fields of different regions within the 96 ? 96 image bounding are shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, which is similar to the corner and center crop in <ref type="bibr" target="#b18">[19]</ref>. We will discuss the effect of different region settings on accuracy in Section 4.3.2. There are three main differences between proposed methods and multi-view voting: 1) To our knowledge, all multi-view testing methods before are designed for image classification while our region ensemble can be applied on both classification and regression. By applying fusion FC layer in REN, different views of inputs are trained to simultaneously predict the same pose. 2) We adopt end-to-end training for region ensemble instead of testing only, making the ConvNet adjust the contributions from each views. 3) We replace the average pooling with one FC layer on concatenated features to learn the fusion parameters, which increases the learning ability of the network. We will perform the comparison in Section 4.3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation details</head><p>We implement our REN with Caffe <ref type="bibr" target="#b17">[18]</ref> written in C++. We use stochastic gradient descent (SGD) with a mini-batch size of 128. The learning rate starts from 0.005 and is divided by 10 every 20 epochs, and the model is trained for total 80 epochs. In the meanwhile, we use a weight decay of 0.0005 and a momentum of 0.9. Our model is trained from scratch with random initialization <ref type="bibr" target="#b16">[17]</ref>. Moreover, there are three important strategies for training: patch cropping, data augmentation, and smooth L 1 loss. The details are described below. And we will show the incremental contributions of these strategies in later section. Patch cropping For ConvNet inputs, we extract a cube with fixed size of 150mm from the depth image centered in the hand region. Then the cube is resized into a 96 ? 96 patch of depth values normalized to [?1, 1] as input for ConvNet. The 3D coordinates are also normalized to [?1, 1] according to the cube. To compute the center, we first segment the foreground with fixed thresholds and calculate the centroid of foreground. Data augmentation We apply data augmentation during training, including translation within [?10, 10] pixel, scaling within [0.9, 1.1] and rotation within [?180, 180] degree. Random augmentation effectively increases the size of training dataset, so it can improve the generalization performance.</p><p>Smooth L 1 loss To deal with noisy annotations, we adopt similar smooth L 1 loss in <ref type="bibr" target="#b9">[10]</ref>:</p><formula xml:id="formula_0">smooth L1 (x) = 0.5x 2 if|x| &lt; 0.01 0.01(|x| ? 0.005) otherwise</formula><p>Because it is less sensitive to outliers than the L 2 loss, it can benefit the training of ConvNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first introduce the evaluation datasets and metrics for our experiments. Then we compare our REN with several state-of-the-art methods on public hand pose datasets. Next we explore several good practices of training ConvNets for hand pose estimation, discuss different region settings and also compare with traditional ensembles and multi-view testing. Finally we apply our REN on fingertip detection and human pose estimation for public benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment setup 4.1.1 Datasets</head><p>We conduct our experiments on four publicly RGB-D datasets: ICVL hand pose dataset <ref type="bibr" target="#b30">[31]</ref>, NYU hand pose dataset <ref type="bibr" target="#b31">[32]</ref>, MSRA hand pose dataset <ref type="bibr" target="#b27">[28]</ref>, and ITOP human pose dataset <ref type="bibr" target="#b13">[14]</ref>. For self-comparison, ICVL dataset is mainly used. More details for datasets are as follows: ICVL dataset The training set of ICVL dataset contains 300K images with different rotations, and the testing set contains 1.6K images. All the depth images are captured by Intel RealSense. Totally 16 hand joints are initialized by the output of camera and manually refined. NYU dataset The NYU dataset has 72K images for training and 8K for testing with 36 3D annotated joints, collected from Microsoft Kinect camera. Following <ref type="bibr" target="#b31">[32]</ref>, 14 hand joints with front-view image are used in experiments. And this dataset is also used to evaluate fingertip detection on the 5 fingertip joints in <ref type="bibr" target="#b36">[37]</ref>  <ref type="bibr" target="#b11">[12]</ref>. MSRA dataset The MSRA dataset contains 9 subjects with 17 gestures for each subject. 76K depth images with 21 annotated joints are collected with Intel's Creative Interactive Camera. For evaluation, each subject is alternatively used as testing data when other 8 subjects are used for training. This is repeated 9 times and the average metrics are reported. ITOP dataset The ITOP dataset consists 18K training images and 5K testing images for front view and top view acquired by two Kinect cameras. Each depth image is labelled with fifteen 3D joint locations of human body.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation metrics</head><p>We employ different metrics for hand pose estimation and human pose estimation following the literatures <ref type="bibr" target="#b30">[31]</ref> [32] <ref type="bibr" target="#b13">[14]</ref>. For hand pose, the performance is evaluated by two metrics: 1) average 3D distance error is computed as the average Euclidean distance for each joint (in millimeters). 2) percentage of success frames is defined as the rate of frames in which all Euclidean errors of joints are below a variant threshold <ref type="bibr" target="#b21">[22]</ref>. In addition, mean precision (mP) with a threshold of 15mm as defined in <ref type="bibr" target="#b36">[37]</ref> is calculated for fingertip detection.</p><p>For human pose, we compute the mean average precision (mAP) <ref type="bibr" target="#b13">[14]</ref>, which is defined as the average detected rate for all human body joints. A joint is counted as detected when the Euclidean distance between predicted position and ground truth is below 10cm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with the state of the art</head><p>We compare our methods against several state-of-the-art approaches on ICVL dataset <ref type="bibr">[</ref>  <ref type="bibr" target="#b33">[34]</ref>. Overall, <ref type="figure" target="#fig_3">Fig.5 -7</ref> show that proposed REN obtains the best accuracy among all the algorithms for hand pose estimation.</p><p>In details, on ICVL dataset our method surpasses other methods with a large margin. And the mean error 7.31mm obtains a 0.80mm decrease compared with LSN <ref type="bibr" target="#b34">[35]</ref>, which is a 9.87% relative improvement. Similarly on NYU dataset, our results are more accurate (12.69mm) than other approaches, and reduce the error of <ref type="bibr" target="#b40">[41]</ref> by 10.3%. For MSRA dataset, our algorithm also significantly outperform all state-of-the-art methods for nearly all thresholds, with an average error of 9.79mm. Surprisely, it reduces the mean error of <ref type="bibr" target="#b8">[9]</ref> by 25.7%. Note that either LSN or multi-view ConvNets <ref type="bibr" target="#b8">[9]</ref> employ multiple models with complicated design, while our REN only uses single model without multistage regression, which indicates the power for proposed region ensemble strategy. <ref type="figure">Fig. 8</ref> shows some good cases and bad cases for all datasets. We can find that the failure cases are often caused by severe occlusion and bad depth images.</p><p>For MSRA dataset we also report the average joint errors distributed over all yaw and pitch viewpoint angles as in <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b8">[9]</ref>, shown in <ref type="figure" target="#fig_6">Fig. 9</ref>. On all angles our method achieve the best accuracy with the smallest deviation, which indicates the robustness for viewpoint variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Self-comparison</head><p>We perform self comparison experiments for different strategies and setting of region ensemble network on ICVL dataset <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Exploration study</head><p>In this section, we focus on the investigation of good practices. Specifically, we incrementally introduce five strategies on a basic shallow network in <ref type="figure" target="#fig_0">Fig. 10: 1)</ref> adding one convolution layer after each convolution layer to increase the depth of ConvNet. 2) adding residual connection edges across pooling layers as described in Section 3.1. 3) using smooth L 1 loss <ref type="bibr" target="#b9">[10]</ref> instead of Euclidean L 2 loss for regression optimization. 4) augmenting the input patches as described in Section 3.3. 5) proposed region ensemble.</p><p>The experimental results are summarized in <ref type="table" target="#tab_1">Table 1</ref>. Combining all the strategies reduces the errors by 3.17mm (relative 30.2%), which is a significant improvement of accuracy. Among them, L 1 loss and region ensemble are two most important factors for performance boosting, because L 1 loss is more suitable for labels with relative large noise and region ensemble can help improve the generalization for model.</p><p>Qualitative comparison on ICVL dataset are shown in <ref type="figure" target="#fig_0">Fig.11</ref> for region ensemble (second row, corresponding to the sixth row in <ref type="table" target="#tab_1">Table 1</ref>) and basic network (third row, corresponding to the fifth row in <ref type="table" target="#tab_1">Table 1</ref>). The estimations are more accurate for region ensemble especially for fingers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Region setting</head><p>According to the analysis in Section 3.2, different region partitions are equal to different patterns of multi-view inputs. Here we explore the effect of different settings of regions, including: 1) multi-scale regions with three regions of size 12 ? 12, 8 ? 8 and 4 ? 4, which is similar to multiscale inputs as in <ref type="bibr" target="#b31">[32]</ref>  <ref type="bibr" target="#b21">[22]</ref>. 2) four regions of size 6?6 (left parts in <ref type="figure" target="#fig_1">Fig. 3)</ref>, which is the setting in <ref type="bibr" target="#b12">[13]</ref>. 3) nine regions of size 6 ? 6 (four as left parts, four as middle parts in <ref type="figure" target="#fig_1">Fig.  3</ref> and one in the center), which is the setting in this paper. 4) nine regions of size 4 ? 4 with similar positions as (3). 5) nine regions of size 8 ? 8 with similar positions as (3). From <ref type="figure" target="#fig_0">Fig. 12</ref>, regions with same size are significantly more accurate than multi-scale regions due to the balance parameter number for FC layers of different regions. And more regions with moderate size (i.e. 9 ? 6 ? 6) obtain slightly better performance. Too large or too small receptive field hurts the accuracy of hand pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Comparison with ensembles and multi-view testing</head><p>We compare with traditional ensembles and multi-view testing in this section. In details, we implement three baselines: 1) Basic network has the same convolution structure in <ref type="figure">Fig.2 and single</ref>      with two FC layers of 2048 dimensions. 2) Basic Bagging network has nine basic networks as (1) that trained independently on the same data with different random order and augmentation. The average predictions of all the networks form the final prediction. 3) Multi-view Testing trains single basic network as (1) but averages the predicted 3D hand poses with nine multi-view inputs. The inputs are cropped as in Section 3.3, but on different centers with bias of ?d/0/dmm on their x and y coordinates relative to the centroid. We use d = 26.5625 to approximately match the nine region positions in REN. <ref type="figure" target="#fig_0">Fig.13</ref> shows that ensemble based methods (both basic bagging and region ensemble) are significantly more effective that baseline network. And the performance of our region ensemble is much better than traditional bagging. Because REN only employs multiple FC layers instead of multiple complete ConvNets, it also costs less time and memory than traditional bagging. Meanwhile, the improvement from multi-view testing is limited for hand pose estimation, because the model is more sensitive to translation in regression tasks than that in classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation on other RGB-D tasks</head><p>Here we also test our REN on challenging benchmarks for fingertip detection and human pose estimation and compare with state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Fingertip detection</head><p>We compare the fingertip detection results to several stateof-the-art algorithms <ref type="bibr" target="#b36">[37]</ref> [12] on NYU dataset without retraining our REN model. <ref type="table" target="#tab_6">Table 3</ref> illustrates that our REN achieves the best performance among all the methods, with an average error of 15.6mm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Human pose estimation</head><p>The results for human pose estimation are reported in <ref type="table">Ta</ref>  <ref type="figure">Figure 7</ref>. Comparison with state-of-the-arts on MSRA <ref type="bibr" target="#b27">[28]</ref> datasets: distance error (left) and percentage of success frames (right). <ref type="figure">Figure 8</ref>. Example results on ICVL <ref type="bibr" target="#b30">[31]</ref>, NYU <ref type="bibr" target="#b31">[32]</ref> and MSRA <ref type="bibr" target="#b27">[28]</ref> datasets: ground truth (first row) and region ensemble network (second row) for each datasets.</p><p>ConvNets with 16 convolution layers in their models. See <ref type="figure" target="#fig_0">Fig. 14 for some visualization results</ref>. Implementation details For human pose, a small Con-vNet is trained to predict the torso position as center. The size of cube is 800 ? 1200 ? 800mm 3 for front-view and 600 ? 600 ? 1000mm 3 for top-view. For data augmentation, random flip of image with probability of 0.5 is also used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>To boost the performance of single ConvNet for 3D hand pose estimation, we exploit several good practices and present a simple but powerful region ensemble structure by   <ref type="table">Table 2</ref>. Mean precision (mP) and average 3D distance error (mm) for fingertips of different methods on NYU dataset <ref type="bibr" target="#b31">[32]</ref>. Higher is better for mP and lower is better for error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>mP Error(mm) CNN-DeROT <ref type="bibr" target="#b36">[37]</ref> 0.63 -DeepPrior <ref type="bibr" target="#b21">[22]</ref> 0.43 <ref type="bibr" target="#b25">26</ref> dividing the feature maps into different regions and jointly training multiple regressors on all regions with fusion. Such strategies significantly improve the accuracy of ConvNet.</p><p>The experimental results demonstrate that our method outperforms all the state-of-the-arts on three hand pose datasets and one human pose dataset. Since region ensemble is easy to be introduced into ConvNets, we believe that proposed structure could be applied on more computer vision tasks and achieve more promising results. <ref type="figure" target="#fig_0">Figure 11</ref>. Example results on ICVL <ref type="bibr" target="#b30">[31]</ref> dataset: ground truth (first row), basic network (second row, corresponding to the fifth row in <ref type="table" target="#tab_1">Table 1</ref>), and region ensemble network (third row, corresponding to the seventh row in <ref type="table" target="#tab_1">Table 1</ref>).    <ref type="figure" target="#fig_0">Figure 13</ref>. Comparison of ensembles and mutli-view testing for percentage of success frames on ICVL dataset <ref type="bibr" target="#b30">[31]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Region ensemble network (REN) with four regions: First deep ConvNet is used to extract features of depth image. The feature maps from ConvNet are then divided into regions. Each region is finally fed into fully-connected (FC) layers and then fused to predict the hand pose. The green rectangles represent the receptive field of the top-left region on the feature maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Different region setting for feature maps: four conners [13] (left), four centers in each edges (middle), and multi-scale regions with the same center (right). Proposed REN adopts nine regions with size of 6 ? 6 including the center of feature maps and all the eight regions in left and middle figures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Receptive fields for different region positions: (a) 62?62 for conners, (b) 62 ? 76 or 76 ? 62 for centers in each edges, and (c) 76 ? 76 for the center of feature maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Comparison with state-of-the-arts on ICVL<ref type="bibr" target="#b30">[31]</ref> dataset: distance error (left) and percentage of success frames (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Comparison with state-of-the-arts on NYU<ref type="bibr" target="#b31">[32]</ref> datasets: distance error (left) and percentage of success frames (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>The average joint errors distributed over all yaw/pitch viewpoint angles on MSRA<ref type="bibr" target="#b27">[28]</ref> dataset. The standard deviations of the error distributions are shown in the legend titles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>Structure of basic shallow ConvNet with three convolution layers and three pooling layers. The non-linear activation layers following each convolution layers are not showed in the figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 .</head><label>12</label><figDesc>Comparison of different region settings (region number ? region width ? region height) for percentage of success frames on ICVL dataset<ref type="bibr" target="#b30">[31]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Average 3D distance error (mm) of incremental strategies on ICVL dataset<ref type="bibr" target="#b30">[31]</ref>. Lower is better.</figDesc><table><row><cell>Strategy</cell><cell>Error(mm)</cell></row><row><cell>Shallow</cell><cell>10.48</cell></row><row><cell>+Deeper</cell><cell>10.02</cell></row><row><cell>+Residual Edge</cell><cell>9.73</cell></row><row><cell>+Smooth L 1 Loss</cell><cell>8.59</cell></row><row><cell>+Augmentation</cell><cell>8.36</cell></row><row><cell>+Region Ensemble</cell><cell>7.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>ble 4, where we compare our method with [39] [3] using mAP metric on ITOP dataet. For frontal view, proposed REN with 84.9 mAP significantly outperforms RTW and REF. And the accuracy for lower body is much higher. For top-down view, our method is better than RTW and shows comparable performance with REF, which contains deeper</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>18 16</cell><cell></cell><cell>Multi?view Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell>Cascaded CF Multi?view LSN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell cols="2">LSN (Classify)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean Error Distance (mm)</cell><cell>4 6 8 10 12 14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fraction of frames within distance</cell><cell>0.3 0.4 0.5 0.6 0.7</cell><cell cols="2">LSN (Joint) CrossingNets Ours</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>Wrist</cell><cell>Index.R</cell><cell>Index.T</cell><cell>Mid.R</cell><cell>Mid.T</cell><cell>Ring.R</cell><cell>Ring.T</cell><cell>Pinky.R</cell><cell>Pinky.T</cell><cell>Thumb.R</cell><cell>Thumb.T</cell><cell>Mean</cell><cell>0.1 0 0</cell><cell>10</cell><cell>20</cell><cell>30 Distance Threshold (mm) 40 50</cell><cell>60</cell><cell>70</cell><cell>80</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Mean average precision (mAP, unit: %) of different methods on frontal view and top view of ITOP dataset<ref type="bibr" target="#b13">[14]</ref> using a 10cm threshold. Higher is better.</figDesc><table><row><cell></cell><cell cols="2">mAP (front-view)</cell><cell></cell><cell cols="2">mAP (top-view)</cell><cell></cell></row><row><cell cols="7">Body Part RTW [39] REF [3] REN RTW [39] REF [3] REN</cell></row><row><cell>Head</cell><cell>97.8</cell><cell>98.1</cell><cell>98.7</cell><cell>98.3</cell><cell>98.1</cell><cell>98.2</cell></row><row><cell>Neck</cell><cell>95.8</cell><cell>97.5</cell><cell>99.4</cell><cell>82.2</cell><cell>97.6</cell><cell>98.9</cell></row><row><cell>Shoulders</cell><cell>94.1</cell><cell>96.6</cell><cell>96.1</cell><cell>91.8</cell><cell>96.1</cell><cell>96.6</cell></row><row><cell>Elbows</cell><cell>77.9</cell><cell>73.3</cell><cell>74.7</cell><cell>80.1</cell><cell>86.2</cell><cell>74.4</cell></row><row><cell>Hands</cell><cell>70.5</cell><cell>68.6</cell><cell>55.2</cell><cell>76.9</cell><cell>85.5</cell><cell>50.7</cell></row><row><cell>Torso</cell><cell>93.8</cell><cell>85.6</cell><cell>98.7</cell><cell>68.1</cell><cell>72.9</cell><cell>98.1</cell></row><row><cell>Hips</cell><cell>80.3</cell><cell>72.0</cell><cell>91.8</cell><cell>55.7</cell><cell>61.1</cell><cell>85.5</cell></row><row><cell>Knees</cell><cell>68.8</cell><cell>69.0</cell><cell>89.0</cell><cell>53.9</cell><cell>51.6</cell><cell>70.0</cell></row><row><cell>Feet</cell><cell>68.4</cell><cell>60.8</cell><cell>81.1</cell><cell>28.6</cell><cell>51.5</cell><cell>41.6</cell></row><row><cell>Mean</cell><cell>80.5</cell><cell>77.2</cell><cell>84.9</cell><cell>68.5</cell><cell>75.5</cell><cell>75.5</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Network of experts for large-scale image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Baig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06119</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="717" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4733" to="4742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A novel hierarchical framework for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="148" to="159" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Static hand gesture recognition based on finger root-center-angle and length weighted mahalanobis distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Photonics Europe, pages 98970U-98970U. International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accurate fingertip detection from binocular mask images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Communications and Image Processing (VCIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A collaborative filtering approach to real-time hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Hee</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2336" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust 3d hand pose estimation in single depth images: from single-view cnn to multi-view cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Regionbased convolutional networks for accurate object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="142" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Two-stream convolutional neural network for accurate rgb-d fingertip detection using depth and edge information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07978</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02447</idno>
		<title level="m">Region ensemble network: Improving convolutional network for hand pose estimation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards viewpoint invariant 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="160" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Convolutional neural net bagging for online visual tracking. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="120" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deeptrack: Learning discriminative feature representations online for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1834" to="1848" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Hands deep in deep learning for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Computer Vision Winter Workshop</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Training a feedback loop for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">High-accuracy stereo matching based on adaptive ground control points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1412" to="1423" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deephand: robust hand pose estimation by completing a matrix imputed with deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cascaded hand pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="824" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Depth-based hand pose estimation: methods, data, and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Supancic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Latent regression forest: Structured estimation of 3d articulated hand posture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3786" to="3793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Real-time continuous pose recovery of human hands using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Perlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">169</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to count with cnn boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Walach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="660" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Crossing nets: Dual generative models with a shared latent space for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03431</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hand pose estimation from local surface normals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Depth estimation for speckle projection system using progressive reliable points growing matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied optics</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="516" to="524" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Rule of thumb: Deep derotation for improved fingertip detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wetzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Slossberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.05726</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Figure 14. Example results on ITOP [3] dataset</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spatial attention deep net with partial pso for hierarchical hybrid hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Random tree walk toward instantaneous 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Seok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I. Dong</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2467" to="2474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rgb-d-based action recognition datasets: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="86" to="105" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning to search on manifolds for 3d pose estimation of articulated objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00596</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Modelbased deep hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A novel finger and hand pose estimation technique for real-time hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="102" to="114" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
