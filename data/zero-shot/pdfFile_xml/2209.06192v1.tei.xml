<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">StoryDALL-E: Adapting Pretrained Text-to-Image Transformers for Story Continuation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-09-13">13 Sep 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adyasha</forename><surname>Maharana</surname></persName>
							<email>adyasha@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel Hill</orgName>
								<address>
									<postCode>27514</postCode>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darryl</forename><surname>Hannan</surname></persName>
							<email>dhannan@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel Hill</orgName>
								<address>
									<postCode>27514</postCode>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
							<email>mbansal@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel Hill</orgName>
								<address>
									<postCode>27514</postCode>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">StoryDALL-E: Adapting Pretrained Text-to-Image Transformers for Story Continuation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-09-13">13 Sep 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in text-to-image synthesis have led to large pretrained transformers with excellent capabilities to generate visualizations from a given text. However, these models are ill-suited for specialized tasks like story visualization, which requires an agent to produce a sequence of images given a corresponding sequence of captions, forming a narrative. Moreover, we find that the story visualization task fails to accommodate generalization to unseen plots and characters in new narratives. Hence, we first propose the task of story continuation, where the generated visual story is conditioned on a source image, allowing for better generalization to narratives with new characters. It is difficult to collect large-scale datasets to train large models for this task from scratch due to the need for continuity and an explicit narrative among the images in a story. Therefore, we propose to leverage the pretrained knowledge of text-to-image synthesis models to overcome the low-resource scenario and improve generation for story continuation. To that end, we enhance or 'retro-fit' the pretrained text-to-image synthesis models with taskspecific modules for (a) sequential image generation and (b) copying relevant elements from an initial frame. Then, we explore full-model finetuning, as well as prompt-based tuning for parameter-efficient adaptation, of the pre-trained model. We evaluate our approach StoryDALL-E on two existing datasets, PororoSV and FlintstonesSV, and introduce a new dataset DiDeMoSV collected from a video-captioning dataset. We also develop a model StoryGANc based on Generative Adversarial Networks (GAN) for story continuation, and compare it with the StoryDALL-E model to demonstrate the advantages of our approach. We show that our retro-fitting approach outperforms GAN-based models for story continuation and facilitates copying of visual elements from the source image, thereby improving continuity in the generated visual story. Finally, our analysis suggests that pretrained transformers struggle to comprehend narratives containing several characters and translating them into appropriate imagery. Overall, our work demonstrates that pretrained text-to-image synthesis models can be adapted for complex and low-resource tasks like story continuation. Our results encourage future research into story continuation as well as exploration of the latest, larger models for the task. Code, data, demo and model card available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Recent advances in text-to-image synthesis have led to large pretrained transformers with excellent capabilities to generate visualizations from a given text. However, these models are ill-suited for specialized tasks like story visualization, which requires an agent to produce a sequence of images given a corresponding sequence of captions, forming a narrative. Moreover, we find that the story visualization task fails to accommodate generalization to unseen plots and characters in new narratives. Hence, we first propose the task of story continuation, where the generated visual story is conditioned on a source image, allowing for better generalization to narratives with new characters. It is difficult to collect large-scale datasets to train large models for this task from scratch due to the need for continuity and an explicit narrative among the images in a story. Therefore, we propose to leverage the pretrained knowledge of text-to-image synthesis models to overcome the low-resource scenario and improve generation for story continuation. To that end, we enhance or 'retro-fit' the pretrained text-to-image synthesis models with taskspecific modules for (a) sequential image generation and (b) copying relevant elements from an initial frame. Then, we explore full-model finetuning, as well as prompt-based tuning for parameter-efficient adaptation, of the pre-trained model. We evaluate our approach StoryDALL-E on two existing datasets, PororoSV and FlintstonesSV, and introduce a new dataset DiDeMoSV collected from a video-captioning dataset. We also develop a model StoryGANc based on Generative Adversarial Networks (GAN) for story continuation, and compare it with the StoryDALL-E model to demonstrate the advantages of our approach. We show that our retro-fitting approach outperforms GAN-based models for story continuation and facilitates copying of visual elements from the source image, thereby improving continuity in the generated visual story. Finally, our analysis suggests that pretrained transformers struggle to comprehend narratives containing several characters and translating them into appropriate imagery. Overall, our work demonstrates that pretrained text-to-image synthesis models can be adapted for complex and low-resource tasks like story continuation. Our results encourage future research into story continuation as well as exploration of the latest, larger models for the task. Code, data, demo and model card available at https://github.com/adymaharana/storydalle.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pretrained text-to-image synthesis models like DALL-E <ref type="bibr" target="#b21">[38]</ref> have shown unprecedented ability to convert an input caption into a coherent visualization. Several subsequent approaches have also leveraged powerful multimodal models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">37]</ref> for creating artistic renditions of input captions <ref type="bibr" target="#b6">[7]</ref>, demonstrating their potential for democratizing art. However, these models are designed to process only a single, short caption as input. In contrast, many use cases of text-to-image synthesis require models to process long narratives and metaphorical expressions, condition on existing visuals and generate more than one image to capture the meaning of the input text. In the past, multiple works have developed specialized Generative Adversarial Networks (GAN) models such as image-to-image translation <ref type="bibr">[17]</ref>, style transfer [20] etc. For instance, story visualization models <ref type="bibr" target="#b10">[27]</ref> convert a sequence of captions into a sequence of images that illustrate the story. However, the recent advent of transformer-based large pretrained models opens up possibilities for leveraging latent knowledge from large-scale pretrained datasets for performing these specialized tasks more effectively, in a paradigm that is similar to finetuning of pretrained language models for performing downstream tasks based on language understanding. Hence, in this paper, we explore methods to adapt a pretrained text-to-image synthesis model for complex downstream tasks, with a focus on story visualization. Story visualization is a challenging task that lies at the intersection of image generation and narrative understanding. Given a series of captions, which compose a story, an agent must generate a corresponding sequence of images that depicts the contents of these captions. While prior work in story visualization has discussed potential applications of the task <ref type="bibr" target="#b10">[27,</ref><ref type="bibr" target="#b14">31,</ref><ref type="bibr" target="#b15">32,</ref><ref type="bibr">44]</ref>, the task itself presents some difficulties when being applied to real world settings. The model is limited to the fixed set of characters, settings, and events on which it is trained and has no way of knowing how to depict a new character that appears in a caption during test time; captions do not contain enough information to fully describe the character's appearance. Therefore, in order to generalize to new story elements, the model must have a mechanism for obtaining additional information about how these elements should be visually represented. First, we make story visualization more conducive to these use cases by presenting a new task called 'story continuation'. In this task, we provide an initial scene that can be obtained in real world use cases. By including this scene, the model can then copy and adapt elements from it as it generates subsequent images (see <ref type="figure" target="#fig_1">Fig. 1</ref>). This has the additional benefit of shifting the focus from text-to-image generation, which is already a task attracting plenty of research, and instead focuses on the narrative structure of a sequence of images, e.g., how an image should change over time to reflect new narrative information in the captions. We introduce a new dataset, DiDeMoSV <ref type="bibr">[13]</ref>, and also convert two existing visualization datasets PororoSV <ref type="bibr" target="#b10">[27]</ref> and FlintstonesSV [10] to the story continuation setting.</p><p>Next, in order to adapt a text-to-image synthesis model to this story continuation task, we need to finetune the pretrained model (such as DALL-E <ref type="bibr" target="#b21">[38]</ref>) on a sequential text-to-image generation task, with the additional flexibility to copy from a prior input. To do so, we first 'retro-fit' the model with additional layers to copy relevant output from the initial scene. Then, we introduce a self-attention block for generating story embeddings that provide global semantic context of the story during generation of each frame. The model is finetuned on the story continuation task, where these additional modules are trained from scratch. We name this approach StoryDALL-E and also compare with a GAN-based model StoryGANc for story continuation. We also explore the parameter-efficient framework of prompt-tuning and introduce a prompt consisting of task-specific embeddings to coax the pretrained model into generating visualizations for the target domain. During training of this prompt-tuning version of the model, the pretrained weights are frozen and the new parameters are learned from scratch, which is time as well as memory-efficient.</p><p>Results show that our retro-fitting approach in StoryDALL-E is useful for leveraging the latent pretrained knowledge of DALL-E for the story continuation task, and outperforms the GAN-based model on several metrics. Further, we find that the copying mechanism allows for improved generation in low-resource scenarios and of unseen characters during inference. In summary, -We introduce the task of story continuation, that is more closely aligned with real-world downstream applications for story visualization, and provide the community with a new story continuation dataset. -We introduce StoryDALL-E, an adaptation of pretrained transformers for story continuation, using retro-fitting. We also develop StoryGANc as a strong GAN baseline for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-We perform comparative experiments and ablations to show that finetuned</head><p>StoryDALL-E outperforms StoryGANc on three story continuation datasets along several metrics.</p><p>-Our analysis shows that the copying mechanism improves correlation of the generated images with the source image, leading to better continuity in the visual story and generation of low-resource as well as unseen characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Text-to-Image Synthesis. Most work in text-to-image synthesis has focused on the development of increasingly sophisticated generative adversarial networks (GANs) <ref type="bibr" target="#b7">[8]</ref>. We introduce the story continuation task and propose a new dataset for the task. Most story visualization models follow the framework introduced in Story-GAN <ref type="bibr" target="#b10">[27]</ref>, which comprises a recurrent text encoder, an image generator, and image as well as story discriminators to train the GAN [46]. [54] add textual alignment models and a path-based image discriminator, while [25] add dilated convolution and weighted activation degree to the discriminators. [43] add figurebackground segmentation to the model in the form of generators and discriminators. <ref type="bibr" target="#b15">[32]</ref> and <ref type="bibr" target="#b14">[31]</ref> use dual learning and structured inputs respectively to improve story visualization. We use their models as starting point and add modifications that leverage pretrained transformers for our proposed story continuation task.</p><p>Parameter-Efficient Training. Methods like adapter-tuning [12, 15, 30, 45] and prompt-based tuning [23, 26] add a small number of trainable parameters to the frozen weights of a pretrained model, which are then learned for the target task. Sparse updating of parameters <ref type="bibr" target="#b8">[9,</ref><ref type="bibr">53]</ref> and low-rank decomposition matrices [16] also provide parameter-efficient methods for finetuning. <ref type="bibr">[11,</ref><ref type="bibr" target="#b16">33]</ref> coaobine these approaches for a unified approach to finetuning pretrained models. <ref type="bibr" target="#b0">[1]</ref> 'retrofit' a pre-trained language model with cross-attention layers to retrieve relevant tokens at each timestep of word prediction in natural language generation. We use retro-fitting and prompt-tuning to adapt a pretrained image synthesis model to story continuation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>As discussed in Sec. 1, story visualization has limited applicability in real-world settings because the task formulation does not allow models to generalize to new story elements. Hence, we propose the story continuation task and present our StoryDALL-E and StoryGANc models for the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Story Continuation</head><p>Given a sequence of sentences S = [s 1 , s 2 , ..., s T ] forming a narrative, story visualization is the task of generating a corresponding sequence of imagesX = [x 1 ,x 2 , ...,x T ], following <ref type="bibr" target="#b10">[27]</ref>. S contains a story, where the captions are temporally ordered and describe the same narrative. This task has many different potential applications such as facilitating the creation of comics or creating visualizations in an educational setting. However, due to the way that the story visualization task is formulated, current models are far from being applied to these settings. The models rely on the images seen in the training data, to generate new visualizations for input stories during the inference phase. Thus, they can only recreate the characters as already found in the training set. Additionally, the captions in story visualization datasets are focused on the narrative, which limits the amount of information that is provided to the model, including descriptions of characters or settings, background etc. Much of this is inferred by the model, leading to generations that might be drastically different than expected, and it is unrealistic to expect the models to generate completely new visual attributes without sufficient instructions in the caption. Story continuation addresses these issues by providing initial information about the story setting and characters.</p><p>In the story continuation task, the first image of the sequence x 1 is provided as additional input to the model. By including an initial ground truth scene as input, the model has access to the appearances of characters, the setting in which the story takes place, and more. When making subsequent scenes, the model then no longer needs to create all the visual features from scratch, but can instead copy from the initial frame. This first image addresses both the generalization issue and the limited information issue in current story visualization models. We refer to this first frame as source frame and the remaining frames in the sequence [x 2 , ....., x t ] as target frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">StoryDALL-E</head><p>The DALL-E generative model is trained using a simple language-modeling objective on the sequence of discrete image tokens for the task of text-to-image synthesis <ref type="bibr" target="#b21">[38]</ref>. With massive amounts of data, such models learn the implicit alignment between text tokens and image tokens, which can be leveraged for downstream tasks like story continuation. The two main aspects that differentiate the story continuation task from text-to-image synthesis are: (1) sequence of captions vs. single caption, and (2) source frame vs. no source frame. Hence, in order to convert the text-to-image synthesis model into a story continuation model, we add two task-specific modules to the native DALL-E architecture. First, we use a global story encoder to pool information from all captions and produce a story embedding, which provides global context of the story at each timestep. Next, we 'retro-fit' the model with cross-attention layers in order to accept the source frame as additional input. We refer to our proposed model as StoryDALL-E (see <ref type="figure" target="#fig_2">Figure 2</ref>). All parameters of StoryDALL-E are updated during the finetuning of the model. In the parameter-efficient version of StoryDALL-E, we learn a sequence of embeddings for the story continuation task and provide it as a prompt to the model for task-specific instructions. During training, the pretrained model weights are frozen and these task-specific modules are trained from scratch.</p><p>Global Story Encoder. Most previous works in story visualization utilize recurrent encoders in the form of LSTM networks <ref type="bibr" target="#b10">[27]</ref> or memory-augmented encoders <ref type="bibr" target="#b14">[31,</ref><ref type="bibr" target="#b15">32]</ref>, to accept a sequence of captions as input. However, recurrent architectures are memory as well as time-intensive because of sequential processing. Hence, we propose to use a self-attention (f self ) based global story encoder, which takes the sentence embeddings for all captions as input and generates contextualized story embeddings for each time-step using parallel processing (see <ref type="figure" target="#fig_2">Figure 2</ref>). Additionally, we initialize sinusoid positional embeddings (S pos )</p><p>to provide information about the position of the target frame within the story, and add those to the story embeddings: S global = f self (S + S pos ). These embeddings are prepended to the word embeddings for the caption at that timestep and sent as input to the generative model.</p><p>Retro-fitted Cross-Attention Blocks. Next, we want to 'retro-fit' the DALL-E model with the ability to copy relevant elements from the source image, in order to promote generalizability to unseen visual attributes. This will allow the model to generate visual stories with completely new characters, as long as they are present in the source frame. Hence, we adapt the model to 'condition' the generation of target frame on the source frame by adding a cross-attention block to each self-attention block of the native DALL-E architecture. The image embeddings of the source frame are used in the cross-attention layer as key (K) and value (V ), while the output from the preceding self-attention layer is used as query (Q). As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, the DALL-E self-attention block consists of the self-attention (f i self ), feed-forward (f i dense ) and normalization (f norm ) layers. Given an input z i to the ith self-attention block, the output z i+1 is:</p><formula xml:id="formula_0">z i+1 = f norm (f i dense (f i self (z i ))).</formula><p>In StoryDALL-E, we insert a cross-attention layer such that the output z i+1 is:</p><formula xml:id="formula_1">z i+1 = f norm (f i dense (f i cross (f i self (z i ), c img )))<label>(1)</label></formula><p>where f i cross is the cross-attention layer in the ith transformer block and c image is the sequence of embedding representations for the conditioning image. The self-attention layers are constrained to perform causal masking for computing attention weights due to the nature of the image synthesis task. However, within the cross-attention layer, the input is free to attend over the entire source frame which eases the next token prediction task by augmenting the model with relevant information. The cross-attention layers are trained from scratch.</p><p>The StoryDALL-E architecture can be fully fine-tuned to learn the weights of the above-mentioned task-specific modules, while updating the weights of the pretrained model as necessary, on the target task as well as dataset. However, <ref type="bibr" target="#b0">[1]</ref> show that freezing of pretrained weights during training of retro-fitted models can also lead to similar performance as models trained from scratch, with lesser training data. Further, it provides a parameter-efficient approach that can be trained/deployed with a smaller amount of computational resources. Hence, we additionally explore prompt-tuning <ref type="bibr" target="#b9">[26]</ref> of the StoryDALL-E model.</p><p>Prompt. Prompt-tuning is an alternative <ref type="bibr" target="#b9">[26]</ref> to full model fine-tuning where the pretrained model weights are frozen and instead, a small sequence of task-specific vectors is optimized for the downstream task. We initialize a parameterization network M LP (.), which takes a matrix of trainable parameters P ? ? of dimensions P idx and dim(h i ) as input and generates the prompt P ? . These trainable matrices are randomly initialized and trained from scratch on the downstream task and dataset. P ? is appended to the word embeddings of input caption, along with the global story embeddings. Together, these additional embedding vectors act as 'virtual tokens' of a task-specific prompt, and are attended to by each of the caption as image tokens. Formally, the input h i to the ith self-attention layer in the auto-regressive transformer is organized as follows:</p><formula xml:id="formula_2">h i = ? ? ? ? ? P ? [j, :] if j ? [0, P idx ) S global if j == P idx f i (z j , h &lt;j ) otherwise (2) where f i (.) is the ith transformer block in StoryDALL-E.</formula><p>With the aforementioned additions, we convert the pretrained DALL-E into StoryDALL-E model for the story continuation task. A pretrained VQVAE encoder <ref type="bibr" target="#b18">[35]</ref> is used to transform RGB images into small 2D grids of image tokens, which are flattened and concatenated with the modified inputs in StoryDALL-E (see Appendix for details). Finally, StoryDALL-E is trained to model the joint distribution over the tokens of text s and image x: p(x) = d j=1 p(x j |x &lt;i ; s). New parameters as well as pretrained weights are optimized in full-model finetuning whereas only the parameters of the prompt, story encoder and cross-attention layers are optimized during prompt-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">StoryGANc</head><p>Generative Adversarial Networks (GANs) have enjoyed steady progress at many image generation tasks such as style transfer <ref type="bibr">[20]</ref>, conditional image generation [49], image-to-image translation [17] over the last decade. Unlike transformers, they do not need to be pretrained on massive datasets, and can be trained for narrow domains with smaller datasets, which makes it an appealing method. Several recent works in story visualization have demonstrated the effectiveness of GANs for this task <ref type="bibr" target="#b10">[27,</ref><ref type="bibr" target="#b15">32,</ref><ref type="bibr">44]</ref>. Hence, we also develop a GAN-based model, Sto-ryGANc, for the story continuation task and compare its performance to that of StoryDALL-E on the proposed datasets (see Appendix for figure and details). StoryGANc follows the general framework of the StoryGAN model <ref type="bibr" target="#b10">[27]</ref> i.e., it is composed of a recurrent text encoder, an image generation module, and two discriminators -image and story discriminator. We modify this framework to accept the source frame as input for the story continuation task, and use it for improving the generation of target frames. Our StoryGANc model is implemented as follows:</p><p>Pre-trained Language Model Encoder. We use a pretrained language model (such as RoBERTa <ref type="bibr" target="#b12">[29]</ref> or CLIP text encoder <ref type="bibr" target="#b20">[37]</ref>) as the caption encoder. These models are pretrained on large unimodal or multimodal datasets of language, which is of great utility for understanding the semantic concepts present in input captions. To ensure that the model has access to all captions, we append the captions together and use a special token to denote which caption is currently being generated.</p><p>Contextual Attention. The story representation from the encoder is combined with the image embeddings of the first frame of the image sequence using contextual attention [52] between the two inputs. The resulting representation is fed through a generator module which recurrently processes each caption, and produces a corresponding image.</p><p>Discriminators. The story discriminator takes all of the generated images and uses 3D convolution to create a single representation and then makes a prediction as to whether the generated story is real or fake. The image discriminator performs the same function but only focuses on individual images. The KL-Divergence loss enforces gaussian distribution on the latent representations learnt by GAN. Finally, the model is trained end-to-end using the objective function: min ? G max ? I ,? S L KL + L img + L story , where ? G , ? I and ? S denote the parameters of the text encoder + image generator, and image and story discriminators respectively. During inference, the trained weights ? G are used to generate a visual story for a given input of captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Datasets</head><p>Since story continuation is a reframing of the story visualization tasks, existing story visualization datasets can be adapted for story continuation by assigning the first frame in the sequence as source frame and the rest as target frames. However, such existing story visualization datasets like PororoSV <ref type="bibr" target="#b10">[27]</ref> and Flint-stonesSV [10] are also homogeneous datasets with recurring characters i.e., the characters used during evaluation already appear in the training set. It is not possible to evaluate the generalization capacity of story continuation models using these datasets. Hence, we propose a new dataset in this paper.</p><p>DiDeMoSV. DiDeMo [13] is a video captioning dataset containing 10,000 short clips with more than 40,000 text descriptions temporally localized with the videos. Each of the clips was randomly sampled from the YFCC100M [47] dataset which is based upon Flickr. This results in videos that cover a large breadth of real-world scenarios, containing many different settings, actions, entities, and more. The dataset contains 11550/2707/3378 samples in training, validation and test respectively, with each sample containing three consecutive frames. This dataset challenges story continuation models to generate diverse inputs, covering many more story elements, in contrast to existing story visualization datasets. In order to do this, models must maximize their usage of the initial scene input and need to incorporate additional general visual knowledge, whether this is done through transfer learning or additional data.</p><p>We also use the existing PororoSV <ref type="bibr" target="#b10">[27]</ref> and FlintstonesSV datasets [10], containing 10191/2334/2208 and 20132/2071/2309 samples respectively, to evaluate our story continuation models. Each sample contains 5 consecutive frames. There are 9 and 7 main characters in PororoSV and FlintstonesSV respectively, that appear throughout the dataset. For story continuation, we use the first frame as A man wearing a red hat and shirt is standing in a room. He talks..</p><p>Betty is in a room with dropped jaw responding to something she sees.</p><p>Betty is walking down the road while speaking.</p><p>Wilma is talking in the room. Betty walks in to face her.</p><p>Betty was listening to Barney vent. Betty wants to leave the room but...</p><p>A monkey jumps onto the building and hops away. The monkey disappears off-screen. The monkey is sitting on a stair set. source frame and the rest of the four frames in the sequence as target frames. Evaluation is only performed on the generation of target frames. See <ref type="figure" target="#fig_3">Figure 3</ref> for examples from the three story continuation datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We use the pretrained weights from popular open-source minDALL-E (1.3B parameters) which is trained on 14 million text-image pairs from the CC3M [42] and CC12M <ref type="bibr" target="#b2">[3]</ref> datasets, to initialize our models. 1 minDALL-E uses the pretrained VQGAN-VAE <ref type="bibr" target="#b5">[6]</ref> for discretizing image inputs. We experiment with pretrained CLIP <ref type="bibr" target="#b20">[37]</ref> (38M parameters) and distilBERT [41] (110M parameters) text encoders for the StoryGANc models. The StoryDALL-E models are trained for 5 epochs with learning rates of 1e-04 (AdamW, Cosine Scheduler) and 5e-04 (AdamW, Linear Decay Scheduler) for full-model fine-tuning and prompttuning setups respectively. Checkpoints are saved at the end of every epoch. The StoryGANc models are trained for 120 epochs with learning rates 1e-04 and 1e-05 for the generator and discriminators respectively. Checkpoints are saved every 10 epochs. These models are trained on single A6000 GPUs. We use the FID score for saving the best checkpoints in our experiments. The FID score calculates the difference between the ground truth and generated images by computing the distance between two feature vectors. Following <ref type="bibr" target="#b10">[27]</ref> and <ref type="bibr" target="#b15">[32]</ref>, we also compute the character classification scores (F1 Score and Frame Acc.) for the PororoSV and FlintstonesSV datasets. See Appendix for details. <ref type="table">Table 1</ref>. Results on the test sets of PororoSV, FlintstonesSV and DiDeMoSV (DSV) datasets from various models. Scores are based on FID (lower is better), character classification F1, and frame accuracy (F-Acc.; higher is better) evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>PororoSV  <ref type="bibr" target="#b15">[32]</ref>. With prompt-tuning, we observe that StoryDALL-E models manage to capture the background elements of the scene but fail to properly recreate the characters in the frame. The frame accuracy score, which is based on exact match overlap of multiple characters in the predicted scene with those in ground truth, remains low for all models, suggesting that both methods struggle to compose multiple roles in a single image <ref type="bibr" target="#b4">[5]</ref>.</p><formula xml:id="formula_3">FlintstonesSV DSV FID ? Char-F1? F-Acc? FID ? Char-F1? F-Acc? FID? StoryGANc (BERT</formula><p>For the more challenging DiDeMoSV dataset, the fully finetuned StoryDALL-E model outperforms the GAN models by a wide margin in terms of FID score. It should be noted here that PororoSV and FlintstonesSV have a finite set of recurring animated characters throughout the dataset, whereas DiDeMoSV is derived from a multitude of real-world scenarios with no overlap in characters between training and evaluation sets. While the addition of a source frame makes it easier for the model to replicate it in the target frames, the generation is significantly more difficult due to the diversity in evaluation samples. However, since the <ref type="table">Table 2</ref>. Ablation results of finetuned StoryDALL-E on validation sets of PororoSV, FlintstonesSV and DiDeMoSV (DSV) datasets. Scores are based on FID (lower is better), character classification F1 and frame accuracy (F-Acc.; higher is better) evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>PororoSV  DiDeMoSV dataset contains images from the real-world domain, the pretrained knowledge of StoryDALL-E derived from Conceptual Captions is useful for generating relevant and coherent images for the dataset, while StoryGANc largely fails to do so.</p><formula xml:id="formula_4">FlintstonesSV DSV FID ? Char-F1? F-Acc? FID ? Char-F1? F-Acc? FID? StoryDALL-E 21.</formula><p>Ablations. <ref type="table">Table 2</ref> contains results from ablation experiments on finetuned StoryDALL-E on the validation sets of the three story continuation datasets. The primary modifications we make to DALL-E in order to adapt it into StoryDALL-E, are the cross-attention layers and global story embeddings. We perform minusone experiments on StoryDALL-E by removing each of these components and observing the effect on FID results on validation sets. First, we remove the cross-attention layers from StoryDALL-E, which reverts the model to the story visualization setting where the model no longer receives the first image as input, and is evaluated on the generation of the rest of the frames in the visual story.</p><p>With this ablation, we see a large increase in FID scores across all datasets. Without a source image to guide the generated output, the quality of illustration drops rapidly, especially for the new DiDeMo dataset. The removal of global story embeddings results in a text-to-image synthesis setting with the first frame as additional input. In this scenario, we see smaller drops in FID, indicating that the global context is not as important as the ability to copy from an initial image.</p><p>In the third row, we remove both, cross-attention layers and story embeddings, which relegates the setting to a text-to-image synthesis task, and observe a large increase in FID scores across all datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>StoryDALL-E (finetuned)</head><p>We see the second violinist for the first time.</p><p>Man in white stripped shirt gets up and walks away.</p><p>Entire frame focused on the violinist.</p><p>Fred is talking on the television in the living room dressed as a superhero.</p><p>Fred dressed as a superhero is talking on the tv screen in the living room.</p><p>Barney sits beside Fred as he talks in an angry manner in the living room.</p><p>Fred and Barney are sitting in a room. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Human Evaluation</head><p>We additionally conduct human evaluation on our models' outputs hoping to better capture the overall quality of the generated stories. We have a human annotator compare generated visual stories from our StoryDALL-E (finetuning) and StoryGANc (BERT) models. They are provided with predictions from each dataset and the corresponding ground truth captions and asked to pick the better prediction (or tie) in terms of visual quality, consistency, and relevance <ref type="bibr" target="#b10">[27]</ref>. Results are presented in <ref type="table" target="#tab_4">Table 3</ref>. The StoryDALL-E model outperforms Sto-ryGANc model in terms of visual quality and relevance, achieving higher % of wins in each of the three datasets (except relevance in FlintstonesSV). These results follow from the fact that StoryDALL-E uses the VQGAN-VAE <ref type="bibr" target="#b5">[6]</ref> which is designed for reconstructing higher resolution images. Moreover, it has access to large pretraining data, which improves alignment between semantic concepts in captions and regions in images. We see wins in terms of consistency for PororoSV and DiDeMoSV predictions from StoryDALL-E models. But, the absolute numbers for consistency and relevance show that there is still room for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Analysis</head><p>In this section, we perform experiments to analyze aspects of the StoryDALL-E model and the story continuation task. First, we perform qualitative analyses of the predictions from StoryDALL-E. Next, we quantify the effect of the retrofitted cross-attention layers and visualize the attention heads. See Appendix for an analysis of the diverse semantic content in the DiDeMoSV dataset. <ref type="figure" target="#fig_4">Figure 4</ref> contains sampled outputs from both of our models for the three story continuation datasets. In each of these examples, StoryDALL-E generates higher quality images than StoryGANc. The difference is especially stark for PororoSV and FlintstonesSV datasets since StoryDALL-E is exposed to the characters during training and has additional guidance from source frame during inference. In the case of DiDeMoSV, the generations from StoryGANc are largely incomprehensible, which could be attributed to the unseen semantic concepts such as 'violinist' which did not appear in the training set. In contrast, StoryDALL-E is exposed to various real-world concepts during pretraining, which can be leveraged during generation. For instance, the pretrained knowledge, as well as the copying mechanism, help the StoryDALL-E model comprehend 'television' and generate an image for 'Fred is talking in the television' (see <ref type="figure" target="#fig_4">Figure 4</ref>(b)). However, the overall quality of the images from StoryDALL-E also does not approach human produced images. As discussed in Sec. 6, it is especially true for frames containing multiple characters. This suggests that while current models are able to attempt the task, there is still much work to be done before consistent and coherent images are commonly produced by the models. We also examine the ability of StoryDALL-E to recreate scarce characters from the training set (see <ref type="figure" target="#fig_6">Figure 5</ref>(a)) and generate unseen characters (see <ref type="figure" target="#fig_6">Figure 5(b)</ref>), when guided by the copying mechanism via cross-attention layers. We find that the copying mechanism allows for better generation of shape and form for less-frequent characters in PororoSV. Similarly, we identified non-recurring characters in the FlintstonesSV dataset and observed the corresponding generated images, when StoryDALL-E has access to a previous frame where they appear. StoryDALL-E succeeds at partially copying visual aspects of the characters, such as the purple skirt (top) and blue uniform (bottom).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Qualitative Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Retro-fitted Cross-Attention</head><p>We examine the attention scores computed in the retro cross-attention layer and present examples in <ref type="figure" target="#fig_6">Figure 5</ref>(c). The cross-attention layers in StoryDALL-E receive vector representations for the source image and compute the crossattention output using the source frame as key/value and the target frame as query. In the first example (left), the target frame is copying visual attributes of the pink bird with the most emphasis, as be seen from the higher attention scores for the image tokens roughly in the center of the source frame. For the second example (right), the source frame and target frames are nearly similar; the attention scores are highest in the diagonal of the plot. The resulting images in both samples contain many visual attributes already found in the source image, demonstrating that the cross-attention layer is effective at enabling conditional  image generation. See Appendix for correlation scores between source image and frames generated with or without condition using StoryDALL-E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">New Results &amp; Demo with DALL-E Mega</head><p>Following our approach of using pretrained text-to-image synthesis models for story continuation, we repeat our experiments with the recently released DALL-E Mega for the final version of the paper. <ref type="bibr" target="#b1">2</ref> DALL-E Mega is pretrained on 15 million images from the Conceptual Captions dataset [42] and follows an encoder-decoder architecture, as opposed to the decoder-only architecture used in minDALL-E. It relies on the pretrained BART encoder [24] for encoding the input captions as well as an improved VQGAN-VAE for discretized encoding of images. In order to adapt DALL-E Mega for story continuation, we retro-fit the decoder in the pretrained model with cross-attention layers and a global story encoder as outlined in Sec. 3. These additional cross-attention layers facilitate copying from a source image, and the story encoder enables generation of a sequence of frames for the story continuation task. We refer to the StoryDALL-E model based on the pretrained DALL-E Mega as the mega-StoryDALL-E model in this paper. In the fully-finetuned version of the mega-StoryDALL-E model, we finetune the encoder as well as the decoder on story continuation datasets. Results are shown in <ref type="table">Table 4</ref>. We observe up to 3% improvement in FID scores over StoryDALL-E across all datasets. Smaller improvements are observed for character classification scores with the use of DALL-E Mega. Examples are shown in <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>We make the mega-StoryDALL-E model trained on the Pororo dataset available for testing through an openly accessible and easy-to-use in-browser <ref type="table">Table 4</ref>. Results on the test sets of PororoSV, FlintstonesSV and DiDeMoSV (DSV) datasets from mega-StoryDALL-E. Scores are based on FID (lower is better), character classification F1 and frame accuracy (F-Acc.; higher is better) evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>PororoSV  demo system (see <ref type="figure" target="#fig_7">Figure 6</ref>). <ref type="bibr" target="#b2">3</ref> From <ref type="figure" target="#fig_1">Figures 1, 6</ref> and demo examples, we find that the model performs well at visualizing stories with up to three characters across all frames and struggles at generating coherent visuals for more than three characters, which is also in line with our findings in Sec. 7.1. The model copies visual elements from the source image and copies to each of the generated frames in the story, hence maintaining a continuous flow in narration by virtue of conditioning on an initial scene. mega-StoryDALL-E performs best at generating overtly visual actions such as 'making cookies', 'walking', 'reading a book'. Further, it is capable of generating semantic concepts that do not appear in the story continuation dataset, such as 'doughnut' and 'lion', by leveraging the pretrained knowledge of DALL-E Mega when possible. Most of the scenes in the Pororo dataset occur within the setting of a snowy village with wooden houses surrounded by trees and snow. Hence, the model usually generates scenes with similar visual elements.</p><formula xml:id="formula_5">FlintstonesSV DSV FID ? Char-F1? F-Acc? FID ? Char-F1? F-Acc? FID? StoryDALL-E (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We introduce a new task called story continuation in order to make the story visualization task more conducive for real-world use cases. We present a new dataset DiDeMoSV, in addition to reformatting two existing story visualization datasets for story continuation. Our model StoryDALL-E, based on a retrofitting approach for adapting pretrained transformer-based text-to-image synthesis models, outperforms GAN-based models on the story continuation datasets. We also added new, improved results and a demo system using the more recent, larger DALL-E Mega model. We hope that the dataset and models motivate future work in story continuation and that our work encourages the exploration of text-to-image synthesis models for more complex image synthesis tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Background</head><p>In this section, we give a brief introduction to the original story visualization task and auto-regressive transformers for text-to-image synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Story Visualization</head><p>Given a sequence of sentences S = [s 1 , s 2 , ..., s T ] forming a narrative, story visualization is the task of generating a corresponding sequence of imagesX = [x 1 ,x 2 , ...,x T ], following <ref type="bibr" target="#b10">[27]</ref>. The sentences form a coherent story with recurring plot and characters. The generative model for this task has two main modules: story encoder and image generator. The sentence encoder E caption (.) takes word embeddings {w ik } for sentence s k at each timestep k and generates contextualized embeddings {c ik }. These embeddings are then used to generate the corresponding images. The terms caption and sentence are used interchangeably throughout the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Pretrained Text-to-Image Synthesis Models (DALL-E)</head><p>The DALL-E model introduced in <ref type="bibr" target="#b21">[38]</ref> is a text-to-image synthesis pipeline which comprises of a discrete variational autoencoder (dVAE) in the first stage and an autoregressive transformer in the second stage:</p><p>Stage 1. The Vector Quantized Variational Autoencoder (VQVAE) <ref type="bibr" target="#b18">[35]</ref> consists of an encoder that learns to map high dimensional input data (x) to a discretized latent space, and a decoder that reconstructs x from the quantized encodings x q . The model is trained using the reconstruction loss and commitment loss <ref type="bibr">[48]</ref>. In DALL-E, the VQVAE is trained to transform RGB image into a small 2D grid of image tokens, where each token can assume a discrete value from a codebook of predefined length.</p><p>Stage 2. The VQVAE encoder from Stage 1 is used to infer the grid of discretized image tokens which is flattened and concatenated with the input text tokens, and an autoregressive transformer is used to model the joint distribution over the text and image tokens. For a given text input s and target image x, these models learn the distribution of image tokens p(x) as,</p><formula xml:id="formula_6">p(x) = d i=1 p(x i |x i&lt;i ; s)|x &lt; i)<label>(3)</label></formula><p>The models are composed of stacked multi-head self-attention layers with causal masking and are optimized via maximum likelihood. Each self-attention block is followed by a MLP feedforward layer, as per the standard design of transformers. The prediction of image tokens at each time step is influenced by the text tokens and previously predicted image tokens via the self-attention layer. Using this framework, DALL-E obtains impressive, state-of-the-art results on a variety of text-to-image tasks by leveraging large-scale pre-training on multimodal datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Method Details</head><p>In this section, we provide additional details about the StoryDALL-E and StoryGANc models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 StoryDALL-E</head><p>Retro Cross-Attention Layer Density. We experiment with different densities of cross-attention layers in our implementation of StoryDALL-E. In the densest variation, we introduce the retro layer in every self-attention block of minDALL-E, effectively increasing the number of parameters in the model by nearly 60%. We vary the density of the retro layer for one in every 1-5 self-attention block(s), and run experiments for each of these variations. Our best model has a density of one retro layer in every third self-attention block.</p><p>Objective. Following the original DALL-E implementation <ref type="bibr" target="#b21">[38]</ref>, the StoryDALL-E model is trained on a combination of text loss and image loss. The losses are cross-entropy losses for the respective modalities, and the combined objective is,</p><formula xml:id="formula_7">L = ? Ntext i=1 t i log(p(t i ) ? Nimg i=1 m i log(p(m i )</formula><p>where N text and N img are the caption lengths and image sequence lengths, set to 64 and 256 in our model respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 StoryGANc</head><p>StoryGANc follows the general framework of the StoryGAN model <ref type="bibr" target="#b10">[27]</ref> i.e., it is composed of a recurrent text encoder, an image generation module, and two discriminators -image and story discriminator. We modify this framework to accept the source frame as input for the story continuation task, and use it for improving the generation of target frames. Our StoryGANc model is implemented as follows:</p><p>Pre-trained Language Model Encoder. In the current state-of-the-art story visualization models <ref type="bibr" target="#b15">[32]</ref>, recurrent transformer-based text encoders like MART <ref type="bibr">[22]</ref> and MARTT <ref type="bibr" target="#b14">[31]</ref> are learnt from scratch for encoding the captions. However, while the memory module contains information about prior captions, there is no way for the current caption to directly attend to words in prior or subsequent captions. This is crucial in a story where causality plays such a large role, e.g., which characters need to appear in the scene, even if they don't appear in the current caption, has there been any modifications to the background that need to appear in the current scene, etc. Furthermore, general world knowledge is crucial for successfully generating unseen stories in our datasets, which is possible with pretrained knowledge. Therefore, we propose using a pretrained language model (such as RoBERTa <ref type="bibr" target="#b12">[29]</ref> or CLIP text encoder <ref type="bibr" target="#b20">[37]</ref>) as the caption encoder. These models are pretrained on large unimodal or multimodal datasets of language; their latent knowledge of the world is of great utility for understanding the semantic concepts present in input captions. For the RoBERTa encoder <ref type="bibr" target="#b12">[29]</ref>, to ensure that the model has access to all captions, we append the captions together and feed all of them into each timestep. We use a special token to denote which caption is currently being generated. The representation from the first token h 0 is used as the caption representation. For the CLIP encoder <ref type="bibr" target="#b20">[37]</ref>, we add an additional self-attention block that takes the caption representation for each timestep and produces the contextualized representations that have been computed by attending to all other timesteps.</p><p>Contextual Attention. We then combine the story representation with the image embeddings of the first frame of the image sequence using contextual attention. First, we reshape the story representation as a 2D matrix and extract 3 ? 3 patches {t x,y } as convolutional filters. Then, we match them against potential patches from the source frame {s x ? ,y ? } by measuring the normalized inner product as,</p><formula xml:id="formula_8">p x,y,x ? ,y ? = ? s x,y ||s x,y || , t x ? ,y ? ||t x ? ,y ? || ?<label>(4)</label></formula><p>where p x,y,x,y ? represents the similarity between the patch centered in target frame (x, y) and source frame (x ? , y ? ). We compute the similarity score for all dimensions along (x ? , y ? ) for the patch in target frame (x, y) and find the best match from the softmax-scaled similarity scores.</p><p>[52] implement this efficiently using convolution and channel-wise softmax; we use their implementation in our StoryGANc model. The extracted patches are used as deconvolutional filters and added to the target frame s. The resulting representation is fed through a generator module which processes each caption and produces an image. We use the generator module outlined in <ref type="bibr" target="#b10">[27]</ref>.</p><p>Discriminators. Finally, the loss is computed for the generated image sequence. There are 3 different components that provide the loss for the model. The first is a story discriminator, which takes all of the generated images and uses 3D convolution to create a single representation and then makes a prediction as to whether the generated story is real or fake. Additionally, there is an image discriminator, which performs the same function but only focuses on individual images. Finally, the model is trained end-to-end using the objective function:</p><formula xml:id="formula_9">min ? G max ? I ,? S L KL + L img + L story</formula><p>where ? G , ? I and ? S denote the parameters of the text encoder + generator, and image and story discriminator respectively. L img and L story are crossentropy losses for classifying ground truth and synthetic images into real and fake categories respectively. L KL is the Kullback-Leibler (KL) divergence between the learned distribution h 0 and the standard Gaussian distribution, to enforce smoothness over the conditional manifold in latent semantic space <ref type="bibr" target="#b10">[27]</ref>. During inference, the trained weights ? G are used to generate a visual story for a given input of captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Dataset Construction</head><p>We propose the new dataset DiDeMoSV, which is derived from the Didemo dataset <ref type="bibr">[13]</ref>. Below, we present details about collection and cleaning of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Dataset Construction</head><p>Prior work in story visualization has repurposed datasets from other tasks. We follow this trend and repurpose video captioning datasets in our work. Story visualization and video captioning share many components. In video captioning, an agent must produce a caption, or series of captions, that describe the content of a video. Story visualization can be thought of as video captioning in reverse, where frames are generated based on the captions. However, simply reversing the direction of the task is not sufficient in this case because the other difference between the two tasks is that story visualization has one frame per caption, whereas videos have many frames; a single caption is typically paired with a video time stamp, denoting which section of the video the caption aligns with. Therefore, to convert video captioning into story visualization, an appropriate method is needed to select which single frame should be used to represent the content of the caption. We employ the self-critical image captioning model <ref type="bibr">[40]</ref> for intelligently selecting the frame most aligned with the caption. Each of the clips that correspond to a caption is multiple seconds long. Not all of the frames will be equally aligned with the caption. Characters might be moving leaving blur effects, the scene might change a bit early or late in the clip, or there might be superfluous actions that occur. To initially shrink the number of frames that we must consider, we first sample frames at fixed intervals throughout the video. In the case of DiDeMoSV, we sample 10 frames. Each of the frames is then fed through the self critical image model and is ranked according to the sum of the log likelihood for each word in the caption being generated. We then use the top-ranked frame as the image for the given caption. The resulting image-caption sequence after this step is on average 4 frames long for DiDeMoSV. To maximize the amount of data that we have and make the task feasible, we split these image-caption sequences into a sequence of 3 frames. We use a sliding window approach to create these sequences, allowing for overlap between sequences. However, we also ensure that the train, val, and test splits contain separate videos. We then proceed with our image pre-processing steps.</p><p>The main pre-processing step that we explore is to convert the real-world images into cartoon images, to emphasize focus on the main characters of the image rather than the trivial details of the background. Rather than models focusing on making images realistic, we want them to focus on accurately representing the stories themselves in visual form. To cartoonize the images we use CartoonGAN <ref type="bibr" target="#b3">[4]</ref>. Each of the extracted frames is fed through this network and the resulting output is used in the final dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Experimental Details</head><p>Pretrained Weights. While the VAE checkpoints for the original DALL-E model have been released, the transformer weights have not. We explored training the transformer component from scratch on our data, but found that it did not perform well. Therefore, we explored other publicly available efforts to reproduce DALL-E and settled on a popular open-source version minDALL-E which is composed of 1.3 billion parameters and trained on 14 million text-image pairs from the CC3M [42] and CC12M <ref type="bibr" target="#b2">[3]</ref> datasets. <ref type="bibr" target="#b3">4</ref> minDALL-E uses the pretrained VQGAN-VAE <ref type="bibr" target="#b5">[6]</ref> for discretizing image inputs. We adapt the pretrained model minDALL-E to StoryDALL-E and then prompt-tune/fine-tune the retro-fitted model on our target datasets.</p><p>We experiment with pretrained CLIP <ref type="bibr" target="#b20">[37]</ref> (38M parameters) and distilBERT [41] (110M parameters) text encoders for the LM-StoryGAN models. The CLIP image encoder is used to extract image embeddings for the source frame in the story continuation task. The universal sentence transformer <ref type="bibr" target="#b1">[2]</ref> is used to extract sentence embeddings for captions, that are sent as input to the global story encoder in StoryDALL-E.</p><p>Training Details. We conduct experiments in the story continuation setting, i.e., the models receive the first frame as input condition. The StoryDALL-E and mega-StoryDALL-E models are trained for 5 epochs with learning rates of 1e-04 (AdamW, Cosine Scheduler) and 5e-04 (AdamW, Linear Decay Scheduler) for fine-tuning and prompt-tuning setups respectively. We use a cosine schedule with warmup from 0 in the first 750 training steps. The minimum learning rate is 0.1 times the maximum learning rate. Checkpoints are saved at the end of every epoch. In full-model finetuning settings, the pretrained weights are finetuned with a smaller learning rate of 1e-05. The LMStoryGAN models are trained for 120 epochs with learning rates 1e-04 and 1e-05 for the generator and discriminators respectively. Checkpoints are saved every 10 epochs. These models are trained on 1-2 A6000 GPUs.</p><p>For the publicly available demo, we have continued training the StoryDALL-E and mega-StoryDALL-E models for up to 50 epochs, which takes up to 10 days on 2 A6000 GPUs and exhibits improved performance over the checkpoints reported in the paper. See the codebase for links to the demo and the checkpoints used therein. <ref type="bibr" target="#b4">5</ref> Evaluation Metrics. We consider 3 automatic evaluation techniques. The first is FID score, which calculates the difference between the ground truth and generated images by computing the distance between two feature vectors. We follow prior work and use Inception-v3 as our image encoding model.</p><p>Following <ref type="bibr" target="#b10">[27]</ref> and <ref type="bibr" target="#b15">[32]</ref>, we also compute the character classification scores for the Pororo and Flintstones datasets, which are adapted from video QA datasets with recurring characters. We use the Inception-v3 models trained for character classification on these respective datasets for computing the F1 Score and frame accuracy (exact match). Since the DiDeMoSV dataset does not have recurring characters, we do not evaluate performance of our models on these datasets using character classification. <ref type="table">Table 5</ref>. Results on the validation sets of PororoSV, FlintstonesSV and DiDeMoSV (DSV) datasets from various models. Scores are based on FID (lower is better), character classification F1 and frame accuracy (F-Acc.; higher is better) evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>PororoSV </p><formula xml:id="formula_10">FlintstonesSV DSV FID ? Char-F1? F-Acc? FID ? Char-F1? F-Acc?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Results</head><p>In this section, we present the results on validation sets of the three story continuation datasets discussed in <ref type="table">Table 1</ref> in main text.</p><p>Validation Set Results. We present results on the validation set of the three story continuation datasets discussed in main text i.e. PororoSV, FlintstonesSV and DiDeMoSV, in <ref type="table">Table 5</ref>. The fully-finetuned StoryDALL-E model performs the best across all datasets in terms of FID score. The gains are seen in FID, due the high visual quality of the images generated by StoryDALL-E. However, the character classification and frame accuracy scores for the StoryDALL-E are close to those of StoryGANc for the FlintstonesSV dataset and relatively lower for the PororoSV dataset, in spite of being of better visual quality (as per manual analysis). This might be attributed to the fact that GAN-based models tend to generate some finer details of a character while sacrificing shape and form, which is recognized by character classification models as a faithful reconstruction. On the other hand, StoryDALL-E models focus on shape and form and tend to blur other defining characteristics, which are appealing to human eyes but fail to be recognized by the classification model. Due to the higher resolution images generated by VQGAN-VAE <ref type="bibr" target="#b5">[6]</ref>, the visual quality of images produced by StoryDALL-E is highly preferred over predictions from the StoryGANc models. Similarly, the latent pretrained knowledge of DALL-E promotes generation of images that align well with the input captions, and results in higher wins for the StoryDALL-E model. The %wins and %loss are nearly uniform for the attribute consistency in this larger experiment, for the PororoSV and DiDeMoSV datasets. Predictions from the StoryDALL-E model are found to be more consistent than those of StoryGANc for the FlintstonesSV dataset. See predictions from StoryDALL-E for the PororoSV, FlintstonesSV and DiDeMoSV datasets in <ref type="figure" target="#fig_1">Figures 10, 11</ref> and 12 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional Analysis</head><p>In this section, we examine various aspects of the story continuation task, models and datasets. First, we demonstrate the advantages of the story continuation task over the story visualization task. Next, we calculate correlations between the source images and generated images from StoryDALL-E, with and without condition, to demonstrate the utility of cross-attention layers. Finally, we discuss the semantic content of our proposed DiDeMoSV dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Story Visualization vs. Story Continuation</head><p>In <ref type="figure" target="#fig_10">Figure 8</ref>, we present a comparison of predictions from the state-of-the-art story visualization model VLCStoryGAN <ref type="bibr" target="#b14">[31]</ref> and our story continuation model StoryGANc for a sample from the test set of the PororoSV dataset. Story Visualization relies only on the input captions to generate the images from scratch. However, as discussed in Section 3.1 in the main text, the captions in story visualization datasets are short and do not contain information about the setting and background elements. As a result, the predictions from story visualization models rely on data seen in the training set to infer arbitrary visual elements. In <ref type="figure" target="#fig_10">Figure 8</ref>, the story takes place in a snowy field with trees (top), but the prediction from VLCStoryGAN (middle) depicts the story as taking place indoors. When the first frame is given as additional input to our model StoryGANc in the story continuation task, the models borrows the snowy fields from the source frame and creates the story within that setting (bottom). Hence, story continuation is a more realistic and practical version of story visualization that can enable significant progress in research and faster transfer of technology from research to real-world use cases. Our experiments and datasets demonstrate the utility of this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Correlation between Source and Generated Images</head><p>We also measure the cosine similarity between the source frames and the generated frames from StoryDALL-E, with and without the retro-fitted crossattention layer for conditioning on a source image, as a representation of the correlation between the two sets of images. We encode the images using the CLIP image encoder ViT-B/16 and report the mean and standard deviation of cosine similarity values for each dataset (see <ref type="table" target="#tab_9">Table 6</ref>). We see up to 0.3 points increase in correlation between the source image and generated image for all three datasets with the use of the conditioning mechanism.  As discussed in Section C, DiDeMoSV is collected from Flickr and the most common nouns indeed reflect this. Most of the captions are descriptive in that they describe the contents of the scene, the location of the objects/people in the scene, and the actions that are taking place in the scene. In DiDeMoSV, the focus is on the breadth of information that must be considered in the form of actions, objects, and settings. The graph for the frequency of verbs across the captions in the DiDeMoSV dataset (see (B) in <ref type="figure" target="#fig_11">Figure 9</ref>) illustrates the complexity of the actions that are being undertaken by agents in the story. It can be seen that most of the actions are simplistic and related to movement, such as "walks", "comes", "starts", "turns", "goes", etc. A lot of the verbs are also centered around vision, such as "see", "seen", and "looks". While these words corroborate our prior insights reflecting the relative simplicity of the stories in DiDeMoSV, they also are crucial for understanding simplistic event chains. An understanding of these simple verbs and the way that they affect the story goes a long way towards facilitating story continuation, especially in the many settings of DiDeMoSV.</p><p>Part (C) in <ref type="figure" target="#fig_11">Figure 9</ref> contains a breakdown of the objects that appear in the DiDeMoSV images. To generate these graphs, we use Yolov3 [39] to process each of the images in the respective datasets. The 'person' class is the dominant class in both datasets. This intuitively makes sense due to the initial data sources from which the respective video captioning datasets were constructed. Additionally, it matches the pattern that is observed in the caption noun analysis, where the nouns in both datasets are most frequently referring to people. However, we can also see that there are limitations of the Yolov3 model. There are frequently occurring nouns, such as 'camera' in DiDeMoSV that are not able to appear in our image analysis because these do not have corresponding classes in the model. We use the default confidence threshold of 0.25 in the Yolo model, which generates predictions for only 76% of DiDeMoSV images.</p><p>Our analysis demonstrates the diversity of the DiDeMoSV dataset, and showcases it as a challenging benchmark for the story continuation task, in addition to PororoSV and FlintstonesSV.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>arXiv:2209.06192v1 [cs.CV] 13 Sep 2022Ground TruthSource FrameThere are eight glasses of different colors of fluids. Eddy is standing in front of the glasses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Examples of predictions for (A) PororoSV (B) FlintstonesSV and (C) DiDe-MoSV story continuation datasets from the mega-StoryDALL-E model. Source frame refers to the initial frame provided as additional input to the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of our StoryDALL-E architecture for the prompt-tuning setting. The frames are encoded using pretrained VQVAE and sent as inputs to the pretrained DALL-E. The inputs are prepended with input-agnostic prompt (in prompt-tuning setting only) and global story embeddings corresponding to each sample in the story continuation dataset. The output of StoryDALL-E is decoded using VQ-VAE to generate the predicted image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Examples from the PororoSV (top), FlintstonesSV (middle) and DiDeMoSV (bottom) datasets. In the story continuation setting, the first frame is used as input to the generative model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Examples of predictions for (A) PororoSV (B) FlintstonesSV and (C) DiDe-MoSV story continuation datasets from finetuned StoryDALL-E and StoryGANc models. Source frame refers to the initial frame provided as additional input to the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Generation of unseen characters. (c) Visualization of attention heads in cross-attention layers of StoryDALLE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Examples of generation from StoryDALL-E in (a) low-resource scenarios and (b) of unseen characters. (c) Plots of attention scores computed in retro cross-attention layers for examples of source frames (x-axis) and target frames (y-axis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>A snapshot of the openly-available in-browser demo for mega-StoryDALL-E trained on the Pororo dataset. The right panel displays the images generated by the model for the captions entered by the user in the left panel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>10. Gupta, T., Schwenk, D., Farhadi, A., Hoiem, D., Kembhavi, A.: Imagine this! scripts to compositions to videos. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 598-613 (2018)3, 4, 9  11. He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., Neubig, G.: Towards a unified view of parameter-efficient transfer learning. In: International Conference on Learning Representations (2021) 4 12. Henderson, J., Ruder, S., et al.: Compacter: Efficient low-rank hypercomplex adapter layers. In: Advances in Neural Information Processing Systems (2021) 4 13. Hendricks, L.A., Wang, O., Shechtman, E., Sivic, J., Darrell, T., Russell, B.: Localizing moments in video with natural language. In: Proceedings of the IEEE International Conference on Computer Vision (ICCV) (2017) 3, 9, 24 14. Hinz, T., Heinrich, S., Wermter, S.: Semantic object accuracy for generative textto-image synthesis. IEEE transactions on pattern analysis and machine intelligence (2020) 4 15. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., Gelly, S.: Parameter-efficient transfer learning for nlp. In: International Conference on Machine Learning. pp. 2790-2799. PMLR (2019) 4 16. Hu, E.J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al.:Lora: Low-rank adaptation of large language models. In:InternationalConference on Learning Representations (2021) 4 17. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with conditional adversarial networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1125-1134 (2017) 2, 8 18. Johnson, J., Hariharan, B., Van Der Maaten, L., Fei-Fei, L., Lawrence Zitnick, C., Girshick, R.: Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2901-2910 (2017) 4 19. Kang, M., Park, J.: Contragan: Contrastive learning for conditional image generation. In: NeurIPS (2020) 4 20. Karras, T., Laine, S., Aila, T.: A style-based generator architecture for generative adversarial networks. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 4401-4410 (2019) 2, 8 21. Kim, K.M., Heo, M.O., Choi, S.H., Zhang, B.T.: Deepstory: video story qa by deep embedded memory networks. In: Proceedings of the 26th International Joint Conference on Artificial Intelligence. pp. 2016-2022 (2017) 4 22. Lei, J., Wang, L., Shen, Y., Yu, D., Berg, T., Bansal, M.: Mart: Memory-augmented recurrent transformer for coherent video paragraph captioning. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 2603-2614 (2020) 23 23. Lester, B., Al-Rfou, R., Constant, N.: The power of scale for parameter-efficient prompt tuning. In: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. pp. 3045-3059 (2021) 4 24. Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., Zettlemoyer, L.: Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871-7880 (2020) 15 25. Li, C., Kong, L., Zhou, Z.: Improved-storygan for sequential images visualization. Journal of Visual Communication and Image Representation 73, 102956 39. Redmon, J., Farhadi, A.: Yolov3: An incremental improvement. arXiv (2018) 30 40. Rennie, S.J., Marcheret, E., Mroueh, Y., Ross, J., Goel, V.: Self-critical sequence training for image captioning. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 7008-7024 (2017) 25 41. Sanh, V., Debut, L., Chaumond, J., Wolf, T.: Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. In: 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing (NeurIPS) (2019) 10, 26 42. Sharma, P., Ding, N., Goodman, S., Soricut, R.: Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 2556-2565 (2018) 10, 15, 26 43. Song, Y.Z., Rui Tam, Z., Chen, H.J., Lu, H.H., Shuai, H.H.: Character-preserving coherent story visualization. In: European Conference on Computer Vision. pp. 18-33. Springer (2020) 4 44. Song, Y.Z., Tam, Z.R., Chen, H.J., Lu, H.H., Shuai, H.H.: Character-preserving coherent story visualization. In: Proceedings of the European Conference on Computer Vision (ECCV) (2020) 2, 8 45. Sung, Y.L., Cho, J., Bansal, M.: Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5227-5237 (2022) 4 46. Sz?cs, G., Al-Shouha, M.: Modular storygan with background and theme awareness for story visualization. In: International Conference on Pattern Recognition and Artificial Intelligence. pp. 275-286. Springer (2022) 4 47. Thomee, B., Shamma, D.A., Friedland, G., Elizalde, B., Ni, K., Poland, D., Borth, D., Li, L.J.: Yfcc100m: The new data in multimedia research. Communications of the ACM 59(2), 64-73 (2016) 9 48. Van Den Oord, A., Vinyals, O., et al.: Neural discrete representation learning.Advances in neural information processing systems 30 (2017) 21 49. Xu, T., Zhang, P., Huang, Q., Zhang, H., Gan, Z., Huang, X., He, X.: Attngan:Fine-grained text to image generation with attentional generative adversarial networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1316-1324 (2018) 4, 8 50. Yan, W., Zhang, Y., Abbeel, P., Srinivas, A.: Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157 (2021) 4 51. Yin, G., Liu, B., Sheng, L., Yu, N., Wang, X., Shao, J.: Semantics disentangling for text-to-image generation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 2327-2336 (2019) 4 52. Yu, J., Lin, Z., Yang, J., Shen, X., Lu, X., Huang, T.S.: Generative image inpainting with contextual attention. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5505-5514 (2018) 9, 23 53. Zaken, E.B., Goldberg, Y., Ravfogel, S.: Bitfit: Simple parameter-efficient finetuning for transformer-based masked language-models. In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). pp. 1-9 (2022) 4 54. Zeng, G., Li, Z., Zhang, Y.: Pororogan: An improved story visualization model on pororo-sv dataset. In: Proceedings of the 2019 3rd International Conference on Computer Science and Artificial Intelligence. pp. 155-159 (2019) 4 55. Zhang, H., Koh, J.Y., Baldridge, J., Lee, H., Yang, Y.: Cross-modal contrastive learning for text-to-image generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 833-842 (2021) 4 56. Zhang, H., Xu, T., Li, H., Zhang, S., Wang, X., Huang, X., Metaxas, D.: Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In: ICCV (2017) 4 57. Zhu, M., Pan, P., Chen, W., Yang, Y.: Dm-gan: Dynamic memory generative adversarial networks for text-to-image synthesis. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5802-5810 (2019) 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Illustration of our StoryGANc architecture. The captions are first encoded using a pretrained language model to produce contextualized representations. These representations are sent to a contextual attention module along with the source frame, and the resulting representation is sent to the image generator. The generated frames are sent to a story and image discriminator, and the corresponding cross-entropy losses for detection real/fake images are used to train the StoryGANc model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Comparison of predictions from state-of-the-art story visualization model VLC-StoryGAN (middle) and our story continuation model StoryGANc (bottom) for a sample from the PororoSV dataset (top).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9</head><label>9</label><figDesc>contains counts for (A) noun chunks, (B) verbs and (C) object classes in DiDeMoSV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 9 .Fig. 10 .Fig. 11 .Fig. 12 .</head><label>9101112</label><figDesc>Plots for frequency of (A) noun chunks and (B) verbs in the captions and (C) objects in the frames of the DiDeMoSV dataset. Generated samples from StoryDALL-E for the PororoSV dataset. Generated samples from StoryDALL-E for the FlintstonesSV dataset. Generated samples from StoryDALL-E for the DiDeMoSV dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table 1contains the FID, character classification F1 score and frame accuracy results on the test sets of PororoSV and FlintstonesSV datasets using various models in our experiments. We train two variations of the StoryGANc model with the distilBERT and CLIP text encoders. Our model StoryDALL-E is trained under two settings, one where the pretrained weights are frozen during training and the other where the pretrained weights are also finetuned on the target dataset. In practice, we find it necessary to finetune the pretrained text and image embeddings within the transformers, which are pretrained on real-world images, in the prompt tuning setting in order to adapt them to different domains such as cartoons. This results in nearly 30% trainable parameters during prompt-tuning, as compared to full-model finetuning. With fully finetuned StoryDALL-E, we see drastic improvements in FID score for the PororoSV and FlinstonesSV datasets, over the StoryGANc model, demonstrating the superior visual quality of the generated visual stories. The character classification scores remain the same for FlintstonesSV and drop by 6% and 14% for PororoSV with use of finetuned and prompt-tuned StoryDALL-E respectively. GAN-based models like StoryGANc are able to recreate distinct and finer details of a character which leads to higher accuracy scores using a classification model, such as the Inception-v3 used in our experiments</figDesc><table><row><cell>)</cell><cell>72.98 43.22</cell><cell>17.09 91.37 70.45</cell><cell>55.78 91.43</cell></row><row><cell>StoryGANc (CLIP)</cell><cell>74.63 39.68</cell><cell>16.57 90.29 72.80</cell><cell>58.39 92.64</cell></row><row><cell cols="2">StoryDALL-E (prompt) 61.23 29.68</cell><cell>11.65 53.71 42.48</cell><cell>32.54 64.58</cell></row><row><cell cols="2">StoryDALL-E (finetuning) 25.90 36.97</cell><cell>17.26 26.49 73.43</cell><cell>55.19 32.92</cell></row><row><cell>6 Results</cell><cell></cell><cell></cell><cell></cell></row></table><note>Main Quantitative Results.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Results from human evaluation (Win% / Lose% / Tie%). Win% = % times stories from StoryDALL-E was preferred over StoryGANc, Lose% for vice-versa. Tie% represents remaining samples.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Visual Quality Relevance Consistency</cell></row><row><cell>PororoSV</cell><cell>94/0/6</cell><cell>44/28/28</cell><cell>56/26/18</cell></row><row><cell>FlintstonesSV</cell><cell>90/2/8</cell><cell>32/38/30</cell><cell>42/32/26</cell></row><row><cell>DiDeMoSV</cell><cell>64/0/36</cell><cell>38/0/62</cell><cell>32/48/20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>Mean and standard deviation of correlation between source image and generated images from StoryDALL-E without and with conditioning on the source image.</figDesc><table><row><cell cols="3">Dataset without condition with condition</cell></row><row><cell>PororoSV</cell><cell>0.23 ? 0.04</cell><cell>0.26 ? 0.04</cell></row><row><cell>FlintstonesSV</cell><cell>0.38 ? 0.05</cell><cell>0.41 ? 0.03</cell></row><row><cell>DiDeMoSV</cell><cell>0.16 ? 0.04</cell><cell>0.19 ? 0.01</cell></row></table><note>F.3 Semantic Analysis of the DiDeMoSV dataset.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/kakaobrain/minDALL-E</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/kuprel/min-dalle</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">See Model Card [34] &amp; Demo at https://github.com/adymaharana/storydalle.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/kakaobrain/minDALL-E 5 https://github.com/adymaharana/storydalle</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. We thank the reviewers for their useful feedback. This work was supported by ARO Award W911NF2110220, DARPA KAIROS Grant FA8750-19-2-1004, NSF-AI Engage Institute DRL-211263. The views, opinions, and/or findings contained in this article are those of the authors, not the funding agency.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improving language models by retrieving from trillions of tokens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11175</idno>
		<title level="m">Universal sentence encoder</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Conceptual 12m: Pushing webscale image-text pre-training to recognize long-tail visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cartoongan: Generative adversarial networks for photo cartoonization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Dall-eval: Probing the reasoning skills and social biases of text-to-image generative transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.04053</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Clipdraw: Exploring text-to-drawing synthesis through language-image encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Frans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Soros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Witkowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.14843</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning with diff pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jvcir.2020.102956</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S10473203203018264" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Storygan: A sequential conditional gan for story visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on CVPR</title>
		<meeting>the IEEE Conference on CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Cpgan: full-spectrum content-parsing generative adversarial networks for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.08562</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Mahabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Integrating visuospatial, linguistic, and commonsense structure into story visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maharana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
	<note>2021) 2, 4, 6</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving generation and evaluation of visual stories via semantic consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maharana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unipelt: A unified framework for parameter-efficient language model tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Model cards for model reporting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zaldivar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vasserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Spitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Raji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gebru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on fairness, accountability, and transparency</title>
		<meeting>the conference on fairness, accountability, and transparency</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<title level="m">Conditional image generation with pixelcnn decoders. Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mirrorgan: Learning text-to-image generation by redescription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
	<note>PMLR (2021) 2, 8, 10</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note>PMLR (2021) 2, 3, 4, 5</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
