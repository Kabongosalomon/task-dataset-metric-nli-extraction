<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DEFORMABLE TEMPORAL CONVOLUTIONAL NETWORKS FOR MONAURAL NOISY REVERBERANT SPEECH SEPARATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Ravenscroft</surname></persName>
							<email>jwravenscroft1@sheffield.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Sheffield</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Goetze</surname></persName>
							<email>s.goetze@sheffield.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Sheffield</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hain</surname></persName>
							<email>t.hain@sheffield.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Sheffield</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DEFORMABLE TEMPORAL CONVOLUTIONAL NETWORKS FOR MONAURAL NOISY REVERBERANT SPEECH SEPARATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-speech separation</term>
					<term>deformable convolution</term>
					<term>dy- namic neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Speech separation models are used for isolating individual speakers in many speech processing applications. Deep learning models have been shown to lead to state-of-the-art (SOTA) results on a number of speech separation benchmarks. One such class of models known as temporal convolutional networks (TCNs) has shown promising results for speech separation tasks. A limitation of these models is that they have a fixed receptive field (RF). Recent research in speech dereverberation has shown that the optimal RF of a TCN varies with the reverberation characteristics of the speech signal. In this work deformable convolution is proposed as a solution to allow TCN models to have dynamic RFs that can adapt to various reverberation times for reverberant speech separation. The proposed models are capable of achieving a 11.1 dB average scale-invariant signal-todistortion ratio (SISDR) improvement over the input signal on the WHAMR benchmark. A relatively small deformable TCN model of 1.3M parameters is proposed which gives comparable separation performance to larger and more computationally complex models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The separation of overlapping speech signals is an area that has been widely studied and which has many applications <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. Deep learning models have demonstrated impressive results on separating clean speech mixtures <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. However this performance still degrades heavily under noisy reverberant conditions <ref type="bibr" target="#b5">[6]</ref>. This performance loss can be alleviated somewhat with careful hyper-parameter optimization but a significant performance gap still exists <ref type="bibr" target="#b6">[7]</ref>.</p><p>The Conv-TasNet speech separation model has been widely studied and adapted for a number of speech enhancement tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. Conv-TasNet generally performs very well on clean speech mixtures with a very low computational cost compared to the most performant speech separation models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> on the WSJ0-2Mix benchmark <ref type="bibr" target="#b12">[13]</ref>. As such, it is still used in many related areas of research <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>. Recent research efforts in speech separation have focused on producing more resource efficient models even if they do not produce the most SOTA results on separation benchmarks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. Previous work has investigated adaptations to Conv-TasNet with additional modifications such as multi-scale convolution and gating mechanisms applied to the outputs of convolutional layers but these significantly increase the computational complexity <ref type="bibr" target="#b13">[14]</ref>. The Conv-TasNet model uses a sequence model known as a TCN. It was recently shown that the optimal RF of TCNs in dereverberation models varies with reverberation time when the model size is sufficiently large <ref type="bibr" target="#b8">[9]</ref>. Furthermore, it was shown that multi-dilation TCN models can be trained implicitly to weight differently dilated convolutional kernels to optimally focus within the RF on more or less temporal context according to the reverberation time in the data for dereverberation tasks <ref type="bibr" target="#b14">[15]</ref>, i.e. for larger reverberation times more weight was given to kernels with larger dilation factors.</p><p>In this work deformable depthwise convolutional layers <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref> are proposed as a replacement for standard depthwise convolutional layers <ref type="bibr" target="#b3">[4]</ref> in TCN based speech separation models for reverberant acoustic conditions. Deformable convolution allows each convolutional layer to have an adaptive RF. When used as a replacement for standard convolution in a TCN this enables the TCN to have a RF that can adapt to different reverberant conditions. Using shared weights <ref type="bibr" target="#b13">[14]</ref> and dynamic mixing <ref type="bibr" target="#b18">[19]</ref> are also explored as ways to reduce model size and improve performance. A PyTorch library for training deformable 1D convolutional layers as well as a Speech-Brain <ref type="bibr" target="#b19">[20]</ref> recipe for reproducing results (cf. Section 5) are provided.</p><p>The remainder of the paper proceeds as follows. In Section 2 the signal model is discussed. The deformable temporal convolutional network (DTCN) is introduced in Section 3. Section 4 discusses the experimental setup, data and baseline systems. Results are given in Section 5. Section 6 provides analysis of the proposed models and conclusions are provided in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SIGNAL MODEL</head><p>A noisy reverberant mixture of C speech signals sc[i] for discrete sample index i convolved with room impulse responses (RIRs) hc[i] and corrupted by an additive noise signal ?[i] is defined as</p><formula xml:id="formula_0">x[i] = C c=1 hc * sc[i] + ?[i]<label>(1)</label></formula><p>where * is the convolution operator. The goal in this work is to estimate the direct speech signal s dir,c [i] and remove the reverberant reflections srev,c[i] where</p><formula xml:id="formula_1">x[i] = C c=1 (s dir,c [i] + srev,c[i]) + ?[i].<label>(2)</label></formula><p>3. DEFORMABLE TEMPORAL CONVOLUTIONAL SEPARATION NETWORK</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>The separation network uses a mask-based approach similar to <ref type="bibr" target="#b3">[4]</ref>. The noisy reverberant microphone signal is first segmented into Lx arXiv:2210.15305v2 [cs.SD] 28 Oct 2022 blocks of length LBL with a 50% overlap defined as</p><formula xml:id="formula_2">x = [x[0.5( ? 1)LBL], . . . , x[0.5(1 + )LBL ? 1]]<label>(3)</label></formula><p>for frame ? {1, . . . , Lx}. Motivated by <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, the frames in (3) are encoded by a 1D convolutional layer with trainable weights B ?</p><formula xml:id="formula_3">R L BL ?N such that w = Henc (x B)<label>(4)</label></formula><p>with a rectified linear unit (ReLU) activation function Henc :</p><formula xml:id="formula_4">R 1?N ? R 1?N .</formula><p>Encoded features w are used as the input to a mask estimation network to produce masks m ,c for each speaker c ? {1, . . . , C}. The masks are then applied to the the encoded features using the Hadamard product, i.e. w m ,c resulting in v ,c . The encoded estimate v ,c for speaker c can be decoded from the same space back into the time domain using the inverse filter of B, denoted as U ? R N ?L BL , such that</p><formula xml:id="formula_5">s ,c = v ,c U<label>(5)</label></formula><p>where? ,c is the estimated clean speech signal for frame in the time domain. These frames are then combined following the overlap-add method. The entire network model diagram is shown in <ref type="figure" target="#fig_1">Fig. 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Mask Estimation Network</head><p>In this subsection the deformable depthwise convolution (DD-Conv) layer is introduced as a replacement for depthwise convolution (D-Conv) layers and then the DTCN network is described in full. The mask estimation network consists of channelwise layer normalization (cLN) and a bottleneck pointwise convolution (P-Conv) layer which transforms the feature dimension from N to B followed by the DTCN which is followed by a P-Conv and ReLU activation to compute a sequence of masks m ,: with dimension C ? N [4].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Deformable Depthwise Convolution (DD-Conv)</head><p>The formulation of DD-Conv in this section is adapted from <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b16">[17]</ref>. The D-Conv operation of kernel size P , dilation factor f and convolutional kernel weights for the gth channel of an input with G channels denoted yg ? R Lx at the th frame is defined as</p><formula xml:id="formula_6">D( , yg, kg) = P p=1 kg[p] yg[ + f ? (p ? 1)].<label>(6)</label></formula><p>The corresponding DD-Conv operator with learnable continuous offset of the pth kernel weight denoted ? ,p at frame is defined as</p><formula xml:id="formula_7">C( , yg, kg, ? ,1:P ) = P p=1 kg[p] yg[ + f ? (p ? 1) + ? ,p ].<label>(7)</label></formula><p>Note that ? ,p only varies temporally and not across channels. It is feasible to vary these values across channels but in this work to reduce computational complexity offsets are only varied temporally.  <ref type="figure" target="#fig_2">Figure 2</ref>. To simplify notation let ? p = +f ?(p?1)+? ,p . Linear interpolation is used to compute values of y[? p] from input sequence y such that</p><formula xml:id="formula_8">y[? p] = ? p +1 u= ? p max(0, 1 ? |u ? ? p|)y[u].<label>(8)</label></formula><p>In practice the interpolation function is designed to constrain the deformable convolutional kernel so it cannot exceed a maximum RF of</p><formula xml:id="formula_9">P ? (f ? 1) + 1 by replacing u = ? p with u = min( ? p , + P ? f ? 1)</formula><p>in the bottom of the summation of (8). This constrains the kernel with the benefit of improving interpretability for the overall scope of the DTCN described in the following subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Deformable Temporal Convolutional Mask Estimation Network</head><p>The DTCN is formulated in the same way as the Conv-TasNet TCN described in <ref type="bibr" target="#b20">[21]</ref>. This implementation deviates slightly from the original Conv-TasNet <ref type="bibr" target="#b3">[4]</ref> by neglecting the skip connections (SCs) and associated P-Conv layers. It was found empirically that these SC layers have an negligible impact on performance (? 0.1 dB SISDR) while having a significant negative impact on model size (? 35% parameter increase). The DTCN is composed of X ? R convolutional blocks where X, R ? Z + <ref type="bibr" target="#b20">[21]</ref>. Each convolutional block consists of a P-Conv which projects the feature dimension from B to H, DD-Conv that performs a depthwise operation across the H channels and another P-Conv layer which projects the feature dimension back to B from H. The DD-Conv proceeded by P-Conv layer forms a deformable depthwise-separable convolution (DDS-Conv) structure. Depthwise-separable convolution (DS-Conv), i.e. a P-Conv proceeded by any D-Conv layer, is used as a replacement for standard convolutional layers as it is more parameter efficient and mathematically equivalent <ref type="bibr" target="#b3">[4]</ref>. In each convolutional block the DD-Conv has an increasing dilation factor f for each additional block in a stack of X blocks as in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21]</ref>. The dilation factor f increases in powers of two through the stack such that f ? {1, 2, . . . , 2 X?1 }. Note that in D-Conv the dilation factor determines the fixed RF whereas in the proposed DD-Conv the dilation factor defines only the maximum possible RF of the kernel. The stack of X convolutional blocks is then repeated R times where the dilation factor is reset to 1 at the beginning of each stack. Using shared weights (SW) for each repeat is experimented with as this significantly reduces the model size similar to <ref type="bibr" target="#b13">[14]</ref>. The offsets ? ,p are computed using DS-Conv following the initial P-Conv in the block, referred to as the offset sub-network. A parametric ReLU (PReLU) activation is used at the output as this allows for both negative and positive offsets. Residual connections are applied around each of the convolutional blocks similar to the TCN described in <ref type="bibr" target="#b20">[21]</ref>. A schematic of the convolutional blocks is shown in <ref type="figure" target="#fig_3">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL SETUP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data</head><p>Two datasets are used to evaluate the proposed DTCN network. The first is a clean speech mixture corpus known as WSJ0-2Mix <ref type="bibr" target="#b21">[22]</ref>. In WSJ0-2Mix, two speech segments are overlapped at SNRs of 0 to 5 dB. The second is a noisy reverberant speech mixture corpus known as WHAMR <ref type="bibr" target="#b5">[6]</ref>. Speech segments in WHAMR are convolved with simulated RIRs then summed together with mixing SNRs of 0 to 5 dB. Ambient noise sources from outdoor pedestrian areas are added at SNRs of ?6 to 3 dB (cf. <ref type="formula" target="#formula_0">(1)</ref>). Dynamic mixing (DM), i.e. simulating new training data each epoch, is also experimented with at training time as it has been shown to lead to improved separation performance <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23]</ref>. Speed perturbation training is also performed as part of the DM process as described in <ref type="bibr" target="#b4">[5]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model Configuration</head><p>These values correspond to the optimal TCN network in <ref type="bibr" target="#b3">[4]</ref>. Note LBL equates to 2 ms at 8 kHz. Five DTCN model configurations are evaluated:</p><formula xml:id="formula_11">{X, R} ? {{3, 8}, {4, 6}, {5, 5}, {6, 4}, {8, 3}}.<label>(10)</label></formula><p>These configurations are selected as they have a similar or the same number of convolutional blocks to the optimal model configuration in <ref type="bibr" target="#b3">[4]</ref> </p><p>is used to train the DTCN models. Permutation invariant training (PIT) is used to solve the speaker permutation problem <ref type="bibr" target="#b24">[25]</ref>. Two GitHub repositories have been released in conjunction with this work. The first 1 is a Pytorch library for performing 1D de-1 URL to dc1d pip repository: github.com/jwr1995/dc1d formable convolution. The second 2 is a model and recipe for reproducing our results with the DTCN model using the speechbrain <ref type="bibr" target="#b19">[20]</ref> framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Metrics</head><p>A number of metrics are used to evaluate the performance of the proposed DTCN models. SISDR and signal-to-distortion ratio (SDR) <ref type="bibr" target="#b25">[26]</ref> are used to measure residual distortion in the signal. Perceptual evaluation of speech quality (PESQ) <ref type="bibr" target="#b26">[27]</ref> and extended short-time objective intelligibility (ESTOI) <ref type="bibr" target="#b27">[28]</ref> are used to measure speech quality and intelligibility, respectively. Speechto-reverberation modulation energy ratio (SRMR) <ref type="bibr" target="#b28">[29]</ref> is used to measure residual speech reverberation for the WHAMR corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS</head><p>The results for various peformance measures against each DTCN configuration's RF on the clean speech WSJ0-2Mix evaluation are shown in <ref type="figure" target="#fig_6">Fig. 4</ref> where they are also compared against their corresponding TCN configurations. Note that the DTCN models have a small increase in number of parameters over the TCN models (? 6%) due to the offset estimation subnetworks. Performance improvements can be seen across all configurations but is more significant with the models which have a RF of 0.19s to 0.51s. The improvement in most metrics at the highest RF 1.53s is marginal and for the intelligibility metric ESTOI the performance is identical to the TCN. <ref type="figure">Fig. 5</ref> shows respective results for each DTCN configuration's RF against the performance measures on noisy reverberant WHAMR data. Note that SDR has been replaced by SRMR to provide a measure of reverberation. The DTCN again shows improvement over the TCN across all measures and model configurations. The performance also increases more consistently as the RF increases. The performance convergence seen on the clean speech mixtures in <ref type="figure" target="#fig_6">Fig. 4</ref> at the largest RF, R = 1.53s, is not seen in the results for the noisy reverberant data in <ref type="figure">Fig. 5</ref>. These findings suggest that deformable convolution is useful in particular for noisy reverberant data.</p><p>In <ref type="table" target="#tab_0">Table 1</ref> the proposed DTCN model is compared against other speech separation models in terms of size, efficiency and performance. Comparing for model size the proposed DTCN outperforms  <ref type="bibr" target="#b3">[4]</ref> and recurrent SkiM-KS8 model <ref type="bibr" target="#b29">[31]</ref>. When DM is used in training, the DTCN outperforms the much larger convolutional SuDo-RM-RF 1.0x++ model <ref type="bibr" target="#b10">[11]</ref>. Using SW reduces the model size by two thirds but is still able to give comparable performance to the SuDoRM-RF 0.5x model of similar size and much improved performance when DM is also used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ANALYSIS</head><p>In the following the offset values of the best performing model configuration {X, R} = 8, 3 are analysed with the aim to provide insight as to how temporal offsets ? ,p in <ref type="formula" target="#formula_7">(7)</ref>, cf. also <ref type="figure" target="#fig_2">Fig. 2</ref>, behave relative to one another. The 2nd convolutional block of the 2nd repeat in the DTCN (i.e. the 10th block overall) was selected for analysis as it was found to have the highest average offset variance over the WHAMR evaluation set. The motivation for this choice is that it is assumed that blocks with offsets of larger variances are more indicative of the benefits of using deformable convolution. A correlation analysis was performed between each of three offset values averaged per utterance?p, corresponding to the three kernel weight positions. <ref type="figure" target="#fig_7">Fig. 6</ref> shows scatter plots for the mean of the middle and outermost offsets denoted?2 and?3, respectively, against the mean offset value of the first kernel sample point?1 for every example in the evaluation set. A strong negative correlation (? = ?0.99) can be observed between?1 and?3 indicating that the deformation is causing the RF of the kernel to shrink and grow more than shifting its focal point. A less strong negative correlation (? = ?0.88) was found between?1 and?2 indicating similar behaviour. The comparison of?2 against?3 is omitted from <ref type="figure" target="#fig_7">Fig 6</ref> for brevity but these mean offset values were found to have a positive correlation of ? = 0.81.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>In this paper deformable convolution was proposed as a method to improve TCNs for noisy reverberant speech separation. It was shown that the DTCN model is particularly useful for noisy reverberant conditions as performance increases were less consistent in the case of anechoic speech separation with a sufficiently large receptive field. Using shared weights and dynamic mixing led to further performance improvements resulting in a small model size for the DTCN compared to other separation models which give comparable performance. Finally, it was shown that the DTCN offsets vary the size of the receptive field of convolutional blocks in the network relative to the input data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]. This work was also funded in part by 3M Health Information Systems, Inc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Mask-based separation network, exemplary for 2 speakers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Single channel example of Deformable depthwise convolution (bottom) on pseudo-random signal (shown in black) with a kernel size of 6, dilation factor of 2 and stride of 11. R denotes the RF of the kernel. Dotted lines indicate original sampling position of kernel weights before deformation. An illutsration of the DD-Conv operation is shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Layers inside deformable temporal convolutional blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Feature</head><label></label><figDesc>dimensions N , B and H, kernel size P and encoder block size LBL are fixed: {N, B, H, P, LBL} = {512, 128, 512, 3, 16}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>?10 log 10 ? 2 2</head><label>102</label><figDesc>, i.e. {X, R} = {8, 3}. The SISDR loss function [24] defined as L(?, s dir ) c ,s dir,c s dir,c s dir,c ? c ? ? c ,s dir,c s dir,c s dir,c 2 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 :</head><label>4</label><figDesc>Performance measures over RF for WSJ0-2Mix clean speech mixtures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>Mean offset values?2 (top) and?3 (bottom) of the 10th convolutional block of the DTCN model plotted against the mean offset value of the first kernel weighttau1 for each example in the WHAMR evaluation set. Pearson correlation coefficients are denoted with ?. Dashed black line indicates line of best fit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of various DTCN models against other speech separation models with varying size and complexity. Dynamic Mixing is abbreviated to DM. Shared weights are abbreviated to SW. Compuational efficiency is expressed in mutiply-acccumulate operations (MACs). Where possible, all MACs values have been estimated using thop [30] unless a citation is provided.</figDesc><table><row><cell></cell><cell cols="2">WSJ0-2Mix</cell><cell cols="2">WHAMR</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="6">?SISDR ?SDR ?SISDR ?SDR Model size GMACs</cell></row><row><cell>Conv-TasNet [4]</cell><cell>15.3</cell><cell>15.6</cell><cell>9.2 [6]</cell><cell>-</cell><cell>5.1M</cell><cell>5.2</cell></row><row><cell>Conv-TasNet (w/o SC)</cell><cell>15.4</cell><cell>15.7</cell><cell>9.7</cell><cell>9.1</cell><cell>3.4M</cell><cell>3.5</cell></row><row><cell>SkiM-KS8 [31]</cell><cell>17.4</cell><cell>17.8</cell><cell>-</cell><cell>-</cell><cell>5.9M</cell><cell>4.9 [31]</cell></row><row><cell>Tiny-SepformerS-32 [12]</cell><cell>15.2</cell><cell>16.0</cell><cell>-</cell><cell>-</cell><cell>5.3M</cell><cell>-</cell></row><row><cell>SuDoRM-RF 1.0x++ [11]</cell><cell>17</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2.7M</cell><cell>2.3</cell></row><row><cell>SuDoRM-RF 0.5x [11]</cell><cell>15.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.4M</cell><cell>1.2</cell></row><row><cell>FurcaNeXt [14]</cell><cell>-</cell><cell>18.4</cell><cell>-</cell><cell>-</cell><cell>51.4M</cell><cell>-</cell></row><row><cell>SepFormer+DM [5]</cell><cell>22.3</cell><cell>22.4</cell><cell>14.0</cell><cell>13.0</cell><cell>26M</cell><cell>69.6 [32]</cell></row><row><cell>QDPN+DM [23]</cell><cell>23.6</cell><cell>-</cell><cell>14.4</cell><cell>-</cell><cell>200M</cell><cell>-</cell></row><row><cell>DTCN (proposed)</cell><cell>15.6</cell><cell>15.9</cell><cell>10.2</cell><cell>9.3</cell><cell>3.6M</cell><cell>3.7</cell></row><row><cell>DTCN+DM (proposed)</cell><cell>17.2</cell><cell>17.4</cell><cell>11.1</cell><cell>10.3</cell><cell>3.6M</cell><cell>3.7</cell></row><row><cell>DTCN+SW (proposed)</cell><cell>15.0</cell><cell>15.3</cell><cell>10.0</cell><cell>9.3</cell><cell>1.3M</cell><cell>3.7</cell></row><row><cell>DTCN+SW+DM (proposed)</cell><cell>16.1</cell><cell>16.3</cell><cell>10.1</cell><cell>9.5</cell><cell>1.3M</cell><cell>3.7</cell></row><row><cell cols="3">Fig. 5: Performance measures over RF for WHAMR noisy reverber-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ant speech mixtures.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>the larger convolutional Conv-TasNet model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">URL to DTCN recipe: github.com/jwr1995/DTCN</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Far-Field Automatic Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Drude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="124" to="148" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An Introduction to Blind Source Separation of Speech Signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benesty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Supervised Speaker Embedding De-Mixing in Two-Speaker Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SLT Workshop 2021</title>
		<imprint>
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Conv-TasNet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention is all you need in speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Subakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2021</title>
		<imprint>
			<date type="published" when="2021-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">WHAMR!: Noisy and reverberant single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020</title>
		<imprint>
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Monaural source separation: From anechoic to reverberant environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cord-Landwehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Boeddeker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zorila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Doddipatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWAENC 2022</title>
		<imprint>
			<date type="published" when="2022-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Beam-TasNet: Time-domain audio separation network meets frequency-domain beamformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ochiai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ikeshita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Araki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-05" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Receptive Field Analysis of Temporal Convolutional Networks for Monaural Speech Dereverberation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ravenscroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goetze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EUSIPCO 2022</title>
		<imprint>
			<date type="published" when="2022-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improving speaker discrimination of target speech extraction with time-domain speakerbeam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ochiai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zmolikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tawara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Araki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-05" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Compute and memory efficient universal sound source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Signal Processing Systems</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="245" to="259" />
			<date type="published" when="2022-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Tiny-sepformer: A tiny time-domain transformer network for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-09" />
		</imprint>
	</monogr>
	<note>in Interspeech 2022</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Single-Channel Multi-Speaker Separation using Deep Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Furcanext: Endto-end monaural speech separation with dynamic gated dilated temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ma</surname></persName>
		</author>
		<editor>MultiMedia Modeling, Y. M. Ro, W.-H. Cheng, J. Kim, W.-T. Chu, P. Cui, J.-W. Choi, M.-C. Hu, and W. De Neve</editor>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer International Publishing</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Utterance weighted multi-dilation temporal convolutional networks for monaural speech dereverberation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ravenscroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goetze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWAENC 2022</title>
		<imprint>
			<date type="published" when="2022-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A 1-d deformable convolutional neural network for the quantitative analysis of capnographic sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bhagya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suchetha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors Journal</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="6672" to="6678" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Wavesplit: End-to-end speech separation by speaker clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2840" to="2849" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">SpeechBrain: A general-purpose speech toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Plantinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rouhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Subakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dawalatabad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rastorgueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Grondin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Aris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04624</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Att-TasNet: Attending to Encodings in Time-Domain Audio Speech Separation of Noisy, Reverberant Speech Mixtures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ravenscroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goetze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">QDPN -Quasi-dual-path Network for single-channel Speech Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rixen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Renz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2022</title>
		<imprint>
			<date type="published" when="2022-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">TasNet: Time-domain audio separation network for real-time, single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech and Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1901" to="1913" />
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SDR -Half-baked or Well Done?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019</title>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hekstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An algorithm for predicting the intelligibility of speech masked by modulated noise maskers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2009" to="2022" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A non-intrusive quality and intelligibility measure of reverberant and dereverberated speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Falk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chan</surname></persName>
		</author>
		<ptr target="https://pypi.org/project/thop" />
	</analytic>
	<monogr>
		<title level="m">Thop: Pytorch-opcounter</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="19" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Skim: Skipping memory lstm for low-latency real-time continuous speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2022</title>
		<imprint>
			<date type="published" when="2022-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Resource-efficient separation transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Subakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lepoutre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Grondin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.09507</idno>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
