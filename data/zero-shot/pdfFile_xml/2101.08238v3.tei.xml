<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AXM-Net: Implicit Cross-Modal Feature Alignment for Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ammarah</forename><surname>Farooq</surname></persName>
							<email>ammarah.farooq@surrey.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Vision, Speech and Signal Processing (CVSSP)</orgName>
								<orgName type="institution">University of Surrey</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Awais</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Vision, Speech and Signal Processing (CVSSP)</orgName>
								<orgName type="institution">University of Surrey</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Surrey Institute for People-centred AI (SI-PAI)</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Sensus Futuris Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Kittler</surname></persName>
							<email>j.kittler@surrey.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Vision, Speech and Signal Processing (CVSSP)</orgName>
								<orgName type="institution">University of Surrey</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Surrey Institute for People-centred AI (SI-PAI)</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Sensus Futuris Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Syed</forename><forename type="middle">Safwan</forename><surname>Khalid</surname></persName>
							<email>s.khalid@surrey.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Vision, Speech and Signal Processing (CVSSP)</orgName>
								<orgName type="institution">University of Surrey</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AXM-Net: Implicit Cross-Modal Feature Alignment for Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cross-modal person re-identification (Re-ID) is critical for modern video surveillance systems. The key challenge is to align cross-modality representations induced by the semantic information present for a person and ignore background information. This work presents a novel convolutional neural network (CNN) based architecture designed to learn semantically aligned cross-modal visual and textual representations. The underlying building block, named AXM-Block, is a unified multi-layer network that dynamically exploits the multi-scale knowledge from both modalities and re-calibrates each modality according to shared semantics. To complement the convolutional design, contextual attention is applied in the text branch to manipulate long-term dependencies. Moreover, we propose a unique design to enhance visual partbased feature coherence and locality information. Our framework is novel in its ability to implicitly learn aligned semantics between modalities during the feature learning stage. The unified feature learning effectively utilizes textual data as a super-annotation signal for visual representation learning and automatically rejects irrelevant information. The entire AXM-Net is trained end-to-end on CUHK-PEDES data. We report results on two tasks, person search and cross-modal Re-ID. The AXM-Net outperforms the current state-of-theart (SOTA) methods and achieves 64.44% Rank@1 on the CUHK-PEDES test set. It also outperforms its competitors by &gt;10% in cross-viewpoint text-to-image Re-ID scenarios on CrossRe-ID and CUHK-SYSU datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Person re-identification (Re-ID) has become a principal component of intelligent video surveillance systems that aims to retrieve a queried person from a large database of pedestrian images. The database typically contains nonoverlapping camera viewpoints with respect to the query images. Depending on the type of information provided as a query, the task is referred to as person Re-ID or person search in the literature. Person search <ref type="bibr" target="#b18">(Li et al. 2017b</ref>) aims to find a person based on the natural language description of the person while images are provided as a query in person Re-ID. In person search, there is no constraint on the camera viewpoints of the person, i.e., in the visual gallery person may have the same pose for which the description is Copyright ? 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. given. Nevertheless, in both tasks, it is critical to learn discriminative feature representations which are unique to an individual and well-aligned within the class for finer matching.</p><p>The literature is packed with numerous deep learning based person re-identification approaches. These methods aim to learn robust person representations <ref type="bibr" target="#b29">(Zhou et al. 2019;</ref><ref type="bibr" target="#b9">Dai et al. 2019)</ref>, apply various attention mechanisms <ref type="bibr" target="#b24">(Xia et al. 2019;</ref><ref type="bibr" target="#b4">Chen, Deng, and Hu 2019)</ref>, look for crossdomain knowledge transfer <ref type="bibr" target="#b16">(Jing et al. 2020b;</ref><ref type="bibr" target="#b8">Chen, Zhu, and Gong 2019)</ref> and so on. Cross-modal person Re-ID is another important aspect of Re-ID task <ref type="bibr" target="#b25">(Yan, Kittler, and Mikolajczyk 2018;</ref><ref type="bibr">Farooq et al. 2020a,b;</ref><ref type="bibr" target="#b19">Lu et al. 2020;</ref><ref type="bibr" target="#b16">Jing et al. 2020b</ref>). The dependency on available image queries limits the practical application of a vision-based system. For example, in the case of criminal search, often the CCTV footage (or image) of a criminal is not available. Therefore, police rely on the unique cues of the criminal from the witnesses' descriptions often given in terms of natural language description. In such cases, with no images available, this descriptive information serves as a query for person Re-ID. Hence, employing a multi-modal Re-ID system can overcome the limitations of image-based systems.</p><p>This work focuses on cross-modal person Re-ID using visual and textual information of the person. The aim of the work is to design a system that semantically aligns crossmodal and cross-pose representations implicitly by focusing on the information present in the two modalities. By implicit alignment we mean alignment of two modalities without using external cues, like segmentation, human body landmarks, attributes prediction from image etc. Note, that existing methods <ref type="bibr" target="#b0">(Aggarwal, Radhakrishnan, and Chakraborty 2020;</ref><ref type="bibr" target="#b22">Wang et al. 2020a;</ref><ref type="bibr" target="#b14">Jing et al. 2018</ref>) rely on complex external cues and explicit alignment of the feature embeddings. There are several challenges while dealing with two distinct modalities. First, the structure of appearance information in both modalities is quite different. Presumably, images have persons always standing upright while the person description can have any sequence. Second, it is critical to learn a network that can extract the semantics in data instead of memorising corresponding image-text pairs for the identities seen during training. Third, the attributes information present in the features, for example, colour and type of clothes person is wearing, the activity of the person, accessories, etc, should be aligned across the modalities to learn the associations among image parts and textual phrases and disregard background noise ( <ref type="figure" target="#fig_0">Figure 1</ref>). Henceforth, the terms 'semantic concepts' and 'attributes' of the person are used interchangeably.</p><p>The main idea of this work is to align the visual and textual features of a person to enable cross-modal search seamlessly. To achieve this goal, we present AXM-Net, a novel convolutional neural network (CNN) based architecture designed to deal with the challenges mentioned above and capable of learning semantically aligned cross-modal feature representations. The underlying building block consists of multiple streams of feature maps capturing a variable amount of intra-modal information and an alignment network that is trained based on the semantics present in both modalities. The output representations are, hence, attended by the fused cross-modal details. The alignment network leverages multi-context intra-modality information and cross-modal attributes to boost informative concepts and suppress the noisy or background information.</p><p>Apart from exploiting inter-modal semantic knowledge, we also propose modality-specific contextual attention mechanisms to effectively extract within-modal relevant information. To be specific, we introduce a visual part-based feature coherence module to locally enhance or suppress the features. We also note that a semantic attribute can be shared across parts, for example, long maxi dress covers upper body as well lower body of a person etc. Therefore, we also focus on consistency of a feature across parts. Since the unified feature learning is performed using a CNN backbone, the sequential nature of the language modality demands learning long-term associations among the person attributes. For example, a description may contain information about the head (hairs, style) at the beginning, followed by a description for the lower body, and again carry information about the head (wearing hat). Hence, we also employ contextual attention to learn these useful associations among the textual attributes. Our contributions can be summarised as follows:</p><p>? Unified cross-modal semantic alignment block (AXM-Block) is proposed to mutually learn representations based on person attributes. To our knowledge, this is the first work to employ implicit semantic alignment across modalities at feature learning stage in the Re-ID setting. ? We put forward effective image parts-based feature coherency learning mechanism. The proposed module attends local spatial region based details while exploiting context from other parts of the image. ? Extensive experiments demonstrate the superiority of the proposed AXM-Net over the current SOTA by 3% on the CUHK-PEDES <ref type="bibr" target="#b18">(Li et al. 2017b</ref>) benchmark and by a wide margin on CrossRe-ID and CUHK-SYSU <ref type="bibr">(Farooq et al. 2020a,b)</ref> in all retrieval scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>The task of person search by natural language descriptions was introduced by <ref type="bibr" target="#b18">(Li et al. 2017b</ref>). The proposed method was a CNN-RNN network to learn global level cross-modal features. The following works <ref type="bibr" target="#b17">(Li et al. 2017a;</ref><ref type="bibr" target="#b6">Chen, Xu, and Luo 2018)</ref> also focused on similar network architectures and a little improvement was observed in performance. Major improvements were shown by <ref type="bibr" target="#b26">Zhang and Lu 2018;</ref><ref type="bibr" target="#b28">Zheng et al. 2020b</ref>) where researchers start exploiting global-local associations and improving the feature embedding space. More recent works <ref type="bibr" target="#b15">(Jing et al. 2020a;</ref><ref type="bibr" target="#b23">Wang et al. 2020b</ref>,a; Aggarwal, Radhakrishnan, and Chakraborty 2020) started employing auxiliary learning branches to explicitly make use of pose keypoints, person attributes, segmentation masks, body parts and textual phrases. These approaches brought improvements as compared to using only global features. <ref type="bibr" target="#b27">Zheng et. al. (Zheng et al. 2020a)</ref> proposed to use Gumbel attention to extract strongly aligned features by performing global, phrase and word-level matchings across the image and sentence. The SOTA work <ref type="bibr" target="#b13">(Gao et al. 2021</ref>) also used multiscale features by employing a staircase network to extract the visual features at global, regional and patch level, and then aligned these features with textual feature by applying cross-modal non-local attention. As mentioned earlier, cross-modal searches help to overcome limitations of vision only systems. For text based person Re-ID, <ref type="bibr" target="#b25">(Yan, Kittler, and Mikolajczyk 2018)</ref> published the pioneering work and reported results on the CUHK03 and Viper datasets under multiple retrieval scenarios. Recent works <ref type="bibr">(Farooq et al. 2020a,b)</ref> proposed to jointly optimise the two modalities and applied canonical correlation analysis to enhance similarity between the corresponding features. These works also reported results for larger cross-modal test splits including CrossRe-ID and CUHK-SYSU. All the above mentioned works extract visual features and textual features from separate backbones and then perform cross-modal alignment which mostly requires auxiliary tasks to increase matching performance. However, the proposed method emphasises on leveraging the multi-level representations with the aim of latent alignment of the crossmodal features during feature extraction stage. and global vision and textual branches. The details of each part are presented in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-modal AXM-Net Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture of the AXM-Block</head><p>First, we present the design of the proposed cross-modal semantic alignment block called as AXM-block shown in <ref type="figure" target="#fig_2">Figure 3</ref>. The design principle of the block is to exploit multi-scale and multi-modal information to align semantic information between the modalities. We define X r = (V r , T r ) to be the visual and textual features maps at convolution filter scales r = 3, 5, 7, 9. We apply a global average pool operation to collect the global concepts present in each feature map generating x r = (v r , t r ) vectors of length spanning the entire channel dimension. These vectors are then passed to the alignment network together with the input feature maps.x r = ?(M LP (x r )) (1) The alignment network consists of a two-layer MLP with a sigmoid activation (?) at the end to produce a scaling vectorx r = (? r ,t r ) corresponding to each feature vector. These scaling vectors convey the importance of each channel with respect to the shared information between modalities across different scales. Feature alignment is performed by multiplying each scaling vector by the corresponding input feature map. Finally, all attended multi-scale features are fused by summation and passed to the next layer.</p><formula xml:id="formula_0">(? ,T ) = r=3,5,7,9 (V r ? r , T r t r )<label>(2)</label></formula><p>Unified Feature Learning using AXM-Block</p><p>Intrinsically, the semantic information present in both modalities is the same, as both are describing the same person. However, the corresponding semantic concepts and their relationships may be available at different scales and locations within each modality. We propose a novel idea to align these representations across modalities based on the semantic information during feature extraction phase. For this purpose, we propose to use the AXM-Block as the basic building block of our feature learning backbone. The unified feature learning backbone consists of stacked AXMblocks. Intra-modal multi-scale learning captures the locally critical context with respect to a spatial location. In contrast, inter-modal multi-scale alignment dynamically drives the two modalities to conform to each other. Each stream of channels contains a coarser-to-finer level of semantics present in the input image or text. Likewise, cross-modal multi-scale learning captures and semantically aligns an unaligned coarser-to-finer level of semantics present in the multi-modal input. It is important to understand the crossmodal unified feature learning for semantic alignment to differentiate from typical multi-scale feature learning methods <ref type="bibr" target="#b29">(Zhou et al. 2019;</ref><ref type="bibr" target="#b7">Chen, Zhu, and Gong 2017;</ref><ref type="bibr" target="#b1">Cai, Wang, and Cheng 2019)</ref>.To illustrate this difference, we show baseline results in the experiments section for a model that leverages intra-modal multi-scale information but fails to outperform AXM-Net. The proposed cross-modal crossscale alignment is the key element of the AXM-Net, giving it an advantage over baseline and SOTA methods. The details of the layer-wise architecture of the feature learning backbone in the AXM-Net are provided in <ref type="table" target="#tab_1">Table 1</ref>. The implicit learning approach of AXM-Net utilises the textual input as a super-annotation signal and does not require auxiliary learning tasks such as segmentation, body landmark detection or word/phrase level matching, which is the case in the current SOTA methods (Aggarwal, Radhakr- ishnan, and Chakraborty 2020; <ref type="bibr" target="#b22">Wang et al. 2020a;</ref><ref type="bibr" target="#b27">Zheng et al. 2020a</ref>). Since, the textual input does not have any background clutter, the effect of visual background is reduced. The alignment with textual feature during feature learning shown in <ref type="figure">Figure 4</ref>.  Visual Part-based Feature Coherence (VFC)</p><p>The person Re-ID task depends on the discriminative local cues characterising each individual. The global visual branch focuses on the person as a whole while preserving shared semantics across modalities. However, it is important to pay attention to local cues and dependencies within each modality. Recent method ) used the sum of global channel-wise and spatial attention to attend intra-modal information. In channel attention, each channel is attended as a whole, while each pixel location is attended in spatial attention. In our case, we want to strengthen the relationship of semantic attributes to the associated body part as well as reinforce feature coherency across parts. To do this, we divide the image feature maps into K horizontal strips v k where k = 1, 2, ??, K . We apply max pool on each strip to get the most relevant features for a particular part. These features are then passed onto a MLP network to learn the relationship between features and their locality. The output (p k ) of the network attends each channel feature separately in each image part. Hence, it links the features to their location. For example, sneakers and laces are expected in foot region while a hat is expected on the head. This network is shared among all image strips to retain cross-part feature correspondence, like a long coat covers upper as well as lower body.</p><formula xml:id="formula_1">p k = ?(M LP (M axP ool(v k ))) (3) v k = p k v k (4) V p = conv 1?1 (concat(v k ))<label>(5)</label></formula><p>The attended image strips are concatenated back and fused into a single feature V P using linear layers for ID classification. The implicit feature alignment along with the VFC module offers a computational benefit by not requiring individual attribute/phrase level matching or multi-label learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attention for Text (TSA)</head><p>Similar to relationship among image parts, textual features are also required to model long-term dependencies among the phrases. To model these long-term dependencies we take inspiration from <ref type="bibr" target="#b2">Cao et al. 2019</ref>) and employ a non-local self attention by directly computing interactions between the pairs of textual features. This step is important to complement our convolutional design for unified feature learning. We take feature maps from the last convolution block of the feature learning network as input to selfattention module. Intuitively, these features represent the important semantic attributes present in the person's description. Each spatial index represents a response from a local spatial region (phrases). The TSA module takes the feature from each spatial position, computes its affinity (A) with all other features(context) and attends input feature maps accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Objective Function</head><p>The loss function for training is the sum of cross-entropy (CE) loss of the three feature branches, triplet-loss <ref type="bibr" target="#b3">(Chechik et al. 2009</ref>) and a simple affinity loss defined below. Specifically, the weights of the classifier layer are shared among all the branches to enforce intra-identity feature alignment. The CE loss is denoted as L IDjoint . The other losses are optimised in a pairwise manner for each visual and textural feature pair.</p><formula xml:id="formula_2">L T otal = ? 1 L IDjoint + ? 2 [L trip (V g , T ) + L trip (V p , T )] + ? 3 [L af f (V g , T ) + L af f (V p , T )] (6)</formula><p>where, ? i , i=1,2,3 determines the proportion of each loss in the total. Given tuples (V a , T + , T ? ), (T a , V + , V ? ), L trip is a margin (?) based ranking loss, defined over similarity(S) of cross-modal positive and negative feature pairs as follows:</p><formula xml:id="formula_3">L trip = max[0, ? ? (S(V a , T + ) ? (S(V a , T ? )] + max[0, ? ? (S(V + , T a ) ? (S(V ? , T a )] (7)</formula><p>Cross-modal Affinity Loss: In order to enhance the retrieval performance, we propose to use a simple yet effective affinity loss. The image features and the corresponding textual features should be aligned and have high similarity, ideally +1 (in the case of cosine similarity) and non corresponding features should be uncorrelated and have low affinity. Given the representations for an image-text pair, affinity is measured in terms of the cosine similarity score between image feature V and text feature T . The affinity loss is implemented as a binary cross entropy criterion, defined over {V i , T j , y ij } where y ij = 1 for matching pairs and y ij = 0 for non-matching ones. <ref type="formula">(8)</ref> where ? is sigmoid function applied to features similarity.</p><formula xml:id="formula_4">L af f = ?[ y ij . log(? (S(V i , T j ))) + (1 ? y ij ) . log(? (1 ? S(V i , T j ))) ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments and the Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We follow a two stage training strategy to train the AXM-Net. In the first stage, we focus on learning the textual branch, the VFC branch and all fully connected layers from scratch, while keeping the vision backbone fixed to pretrained ImageNet weights. For the first stage the training follows the standard classification paradigm considering each person as an individual class and only using L IDjoint . We also apply label smoothing to our cross entropy loss. We use batch size 64, weight decay 5e-4 and initial learning rate 0.01 with stochastic gradient descent optimisation. Images are resized to 384 ? 128. Each textual description is mapped to a 768 dimensional BERT embedding <ref type="bibr" target="#b10">(Devlin et al. 2018)</ref> and resized as 1 ? 56 ? 768 where 56 is the maximum sentence length. We kept the word embedding layer fixed during training. We adopted random flipping, random erasing for images, and random circular shift of sentences as data augmentation. To achieve computational efficiency, we employ depth-wise separable convolutions at each layer. We used equal contribution(?) of each loss and margin (?) equal to 0.5. During inference, the vision and text features are extracted separately as the weights of the unified feature learning backbone are independent of data samples. We use the sum of both vision features (V G + V P ) and text feature for evaluation. The performance is measured based on the cosine similarity between these features and reported in terms of Rank@1 and mean average precision (mAP).</p><p>Datasets CUHK-PEDES: The CUHK person description data <ref type="bibr" target="#b18">(Li et al. 2017b</ref>  <ref type="bibr" target="#b11">(Farooq et al. 2020a</ref>). There are 5532 IDs for training and 2099 IDs for testing. The corresponding descriptions have been extracted from CUHK-PEDES data. The final gallery and query splits con-tain 5070/10140 and 3271/6550 images/descriptions respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with State-of-the-Art Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Person Search</head><p>We summarise the performance of the proposed AXM-Net with the SOTA methods on the CUHK-PEDES data in <ref type="table" target="#tab_4">Table 2</ref>. The methods have been grouped according to the type of representations used for learning. We implement the baseline network with multi-scale features for both modalities and a joint classifier layer. The baseline method performs best among all global level techniques which signifies the benefit of having multi-scale features but still it falls behind the other complex methods <ref type="bibr" target="#b0">Aggarwal, Radhakrishnan, and Chakraborty 2020;</ref><ref type="bibr" target="#b23">Wang et al. 2020b;</ref><ref type="bibr" target="#b27">Zheng et al. 2020a</ref>) due to lack of cross-modal alignment. It is worth noting that our method is simple but powerful and enhances the semantic alignment between modalities without any explicit supervision from segmented body parts ), attributes (Aggarwal, Radhakrishnan, and Chakraborty 2020) and phrases/words <ref type="bibr" target="#b27">(Zheng et al. 2020a</ref>). The proposed AXM-Net with simple L IDjoint loss outperforms current SOTA by a large margin, achieving over 62% Rank@1 performance. By using affinity and triplet loss together, we set the new SOTA Rank@1 of 64.44%.</p><p>Results on Cross-modal Re-ID We follow the evaluation protocol of <ref type="bibr" target="#b12">(Farooq et al. 2020b</ref>) for cross-modal re-identification. For fair-comparison, we used image size 224?224 and word2vec <ref type="bibr" target="#b20">(Mikolov et al. 2013</ref>) embedding. In <ref type="table" target="#tab_6">Tables 3, V ?</ref> ? V indicates image based search, T ? ? V indicates description to image search and VT ? ? V indicates using both modalities for query and vision as gallery. We report the detailed results including Rank@5 and Rank@10 for all the datasets in the supplementary materials. For CrossRe-ID and CUHK-SYSU, we compare the results with the recently reported work <ref type="bibr" target="#b12">(Farooq et al. 2020b)</ref>. It is mentioned as JT+CCA in Tables 3. For both datasets, the proposed AXM-Net outperformed the previous method by a significant margin in all retrieval scenarios. Note that, now the T ? ? V indicates a description of a person from a different pose, and the gallery images have different poses. We can witness the potential of semantic alignment acrossmodalities in this challenging case. The improvement in Rank@1 performance shows the capability of the AXM-Net in matching viewpoint-invariant semantic details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Component Analysis</head><p>We perform an ablation study for the proposed framework by adding the components step-by-step. The study is performed on the CUHK-PEDES test set with joint crossentropy loss L IDjoint . All hyper-parameters are kept the same for training in all settings. <ref type="table" target="#tab_7">Table 4</ref> presents the corresponding results. The baseline model has the same architecture as the global visual and textual branch, including multiscale features for both modalities but without alignment. We list our observations as follows:</p><p>? Effect of Individual Components. Models: 1, 2, 3 correspond to the contributions of individual components.       <ref type="table" target="#tab_7">Table 4</ref>. We observe a performance drop of 4.58%, as compared to the proposed design. It is important to extract the global feature and perform a local enhancement separately to retain the cross-modal alignment learnt by the backbone network. ? The effect of Single Stage versus Two Stage Learning. As mentioned earlier, we used a two stage training for network learning. We also test a single stage policy in which we tune all the parameters together with the same initialisation setting. The single stage model clearly shows the difference between the two policies. Hence, it is recommended to learn weakly aligned textual features beforehand to support best convergence. ? Design Parameters for the VFC Branch. We consider several parameters for designing the part-based visual feature branch as presented in <ref type="table" target="#tab_8">Table 5</ref>. First of all, we test with separate MLP networks for each strip of image features. We find that a shared network helps feature coherency as well as reducing the number of parameters. It supports our idea of context sharing between image parts and signifies the connectedness of various semantic concepts across the strips. Next, we note that the global max pool helps in capturing local cues by focusing on the highest responses in each region. For each branch of AXM-Net, we also optimise the number of FC layers as it is critical to avoid any performance degradation and network overfitting. We observe from the table that having an identical linear layer structure not always implies the best performing solution. Being a richer modality and focusing on information from the whole image, two FC layers help in the global branch to learn better representations. We also consider the feature dropping technique <ref type="bibr" target="#b9">(Dai et al. 2019</ref>) to obtain robust features. However, it did not help either by random batch drop block or horizontal strip (part) drop.</p><formula xml:id="formula_5">CrossRe-ID CUHK-SYSU V ? ? V T ? ? V VT ? ? V V ? ? V T ? ? V VT ? ? V Rank@1</formula><p>? Position of Alignment In <ref type="table">Table 6</ref>, we investigate the effect of cross-modal alignment in different stages. The best performance is achieved by aligning at all levels.</p><p>The semantic concepts at lower layers are not well defined but still adds a little benefit in our case. It is due to the fact that the first strided (7?7) convolution on image captures rich set of primary features, also practised in modern vision Transformers. Hence, the lower blocks also get significant contextual information of the image.  <ref type="table">Table 6</ref>: Effect of implicit alignment in different blocks.</p><p>? The Effect of Language Embedding. We notice in Table 7 that the performance of the system can be improved by using complex and richer language embeddings. We hope to make further gains by combining language modelling with cross-modal feature learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rank@1</head><p>Word2Vec BERT AXM-Net + joint ID 59.44 61.27 AXM-Net + joint ID + triplet + affinity 61.9 63.0 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We presented a novel AXM-Net model to address the challenges of cross-modal person re-identification. Our innovation involves: 1) a unified feature learning block, called AXM-Block, which implicitly aligns the semantic features across the visual and textual modalities, and 2) an effective design for reinforcing feature coherence among image parts. In contrast to existing methods, the proposed AXM-Net is the first framework that is based on an integrated cross-modal feature alignment and learning stage in Re-ID context. An important advantage of the proposed method is that it does not rely on external cues for explicit alignment of feature embeddings. The experimental results show that our network defines new SOTA performance on the CUHK-PEDES benchmark and also demonstrates the potential of the proposed network for more challenging cross-modal person Re-ID applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of semantic alignment for visual and textual features. The semantic information in the features should be aligned to enhance the cross-modal associations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>shows the block diagram of the proposed AXM-Net, which includes a unified feature learning network, visual part-based feature coherence (VFC) learning branchFigure 2: Illustrative diagram of our cross-modal AXM-Net, which generates global visual feature V G , part based visual feature V P and textual feature T . Softmax loss L IDjoint is a function of all the features. Matching losses are trained pairwise with the textual feature for each visual feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>AXM-Block design. Unified cross-modal feature learning block. 3?3 indicates kernel size of the convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Architecture of the unified feature learning network.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>) is the only large-scale benchmark available for cross-modal person search. It has 13003 person IDs with 40,206 images and 80,440 descriptions. There are 11003,1000,1000 pre-defined IDs for training, validation and test sets. The training and test set include 34054/68126 and 3074/6156 images/descriptions respectively. CrossRe-ID Dataset: For cross-modal Re-ID, we evaluate the models on the protocol introduced by (Farooq et al. 2020b) on the test split of CUHK-PEDES data. The gallery and query splits have been carefully separated across viewpoints. The descriptions are also varying across viewpoints. The dataset includes 824 unique IDs. There are 1511/3022 and 1096/2200 images/descriptions in gallery and query sets respectively. CUHK-SYSU: We evaluate our model on the test protocol provided by</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparison with SOTA models on the CUHK-PEDES dataset</figDesc><table><row><cell>Model</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison on cross-modal Re-ID.</figDesc><table><row><cell>Query ? ? Gallery</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on the AXM-Net on CUHK-PEDES test set with Word2vec text embedding and 224 ? 224 image size</figDesc><table><row><cell cols="2">MLP Weights Rank@1</cell><cell>Pooling Type</cell><cell>Rank@1</cell></row><row><cell>Separate</cell><cell>57.62</cell><cell>Average (GAP)</cell><cell>57.36</cell></row><row><cell>Shared</cell><cell>57.93</cell><cell>Max (GMP)</cell><cell>58.82</cell></row><row><cell cols="4">Feature Drop Rank@1 No. of FC Layers Rank@1</cell></row><row><cell>Random</cell><cell>57.62</cell><cell>1</cell><cell>58.45</cell></row><row><cell>Part</cell><cell>58.33</cell><cell>2</cell><cell>58.06</cell></row><row><cell>No drop</cell><cell>59.44</cell><cell>Proposed</cell><cell>59.44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Design parameters for the VFC branch components together. ? The Effect of the Unified Visual Feature Branch. We experiment with the unified visual branch by removing the global branch and keeping only the VFC based part branch. It is indicated as Unified Visual in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Effect of choice of language embedding.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work has been supported in part by the EPSRC Grants; FACER2VM (EP/N007743/1), MVSE (EP/V002856/1), JADE2 (EP/T022205/1), and the EPSRC/dstl/MURI project EP/R018456/1.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Text-based Person Search via Attribute-aided Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">B</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2617" to="2625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-scale bodypart mask guided attention for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gcnet: Non-local networks meet squeeze-excitation networks and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale online learning of image similarity through ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Iberian Conference on Pattern Recognition and Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="11" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mixed High-Order Attention Network for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving deep visual representation for person re-identification by global and local image-language association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="54" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving text-based person search by spatial matching and adaptive threshold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1879" to="1887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Person reidentification by deep learning multi-scale representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2590" to="2600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Instance-Guided Context Rendering for Cross-Domain Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Batch DropBlock network for person re-identification and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3691" to="3701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A Convolutional Baseline for Person Re-Identification Using Vision and Language Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farooq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Khalid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00808</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cross Modal Person Re-identification with Visual-Textual Queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farooq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Khalid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Joint Conference on Biometrics (IJCB)</title>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03036</idno>
		<title level="m">Contextual Non-Local Alignment over Full-Scale Representation for Text-Based Person Search</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Pose-Guided Multi-Granularity Attention Network for Text-Based Person Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.08440</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pose-guided multi-granularity attention network for text-based person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11189" to="11196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cross-Modal Cross-Domain Moment Alignment Network for Person Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Identity-aware textual-visual matching with latent co-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1890" to="1899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Person search with natural language description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1970" to="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cross-Modality Person Re-Identification With Shared-Specific Feature Transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Nonlocal neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">2020a</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vi-Taa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07327</idno>
		<title level="m">Visual-Textual Attributes Alignment in Person Search by Natural Language</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">IMG-Net: inner-cross-modal attentional multigranular network for description-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">43028</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Second-Order Non-Local Attention Networks for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Poellabauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Person Re-Identification with Vision and Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2136" to="2141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep cross-modal projection learning for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="686" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical Gumbel Attention Network for Text-Based Person Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia, MM &apos;20</title>
		<meeting>the 28th ACM International Conference on Multimedia, MM &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3441" to="3449" />
		</imprint>
	</monogr>
	<note>ISBN 9781450379885</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dual-Path Convolutional Image-Text Embeddings with Instance Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Communications, and Applications (TOMM)</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Omni-scale feature learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3702" to="3712" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
