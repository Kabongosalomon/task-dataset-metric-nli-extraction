<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MonoGRNet: A Geometric Reasoning Network for Monocular 3D Object Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengyi</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MonoGRNet: A Geometric Reasoning Network for Monocular 3D Object Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Localizing objects in the real 3D space, which plays a crucial role in scene understanding, is particularly challenging given only a single RGB image due to the geometric information loss during imagery projection. We propose Mono-GRNet for monocular 3D object detection and localization from a single RGB image via geometric reasoning in both the observed 2D projection and the unobserved depth dimension. MonoGRNet is a single, unified network composed of four task-specific subnetworks, responsible for 2D object detection, instance depth estimation (IDE), 3D localization and local corner regression. Unlike the pixel-level depth estimation that needs per-pixel annotations, we propose a novel IDE method that directly predicts the depth of the targeting 3D bounding box's center using sparse supervision. The 3D localization is further achieved by estimating the position in the horizontal and vertical dimensions. Finally, Mono-GRNet is jointly learned by optimizing the locations and poses of the 3D bounding boxes in the global context. We demonstrate that MonoGRNet achieves state-of-the-art performance on challenging datasets. See the project website</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Typical object localization or detection from a RGB image estimates 2D boxes that bound visible parts of the objects belonging to the specific classes on the image plane. However, this kind of result cannot provide geometric perception in the real 3D world for scene understanding, which is crucial for applications, such as robotics, mixed reality, and autonomous driving.</p><p>In this paper, we address the problem of localizing amodal 3D bounding boxes (ABBox-3D) of objects at their full extents from a monocular RGB image. Unlike 2D analysis on the image plane, 3D localization with the extension to an unobserved dimension, i.e., depth, not solely enlarges the searching space but also introduces inherent ambiguity of 2D-to-3D mapping, increasing the task's difficulty significantly.</p><p>Most state-of-the-art monocular methods <ref type="bibr" target="#b12">(Xu and Chen 2018;</ref><ref type="bibr" target="#b13">Zhuo et al. 2018</ref>) estimate pixel-level depths and then regress 3D bounding boxes. Nevertheless, pixel-level depth Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. estimation does not focus on object localization by design. It aims to minimize the mean error for all pixels to get an average optimal estimation over the whole image, while objects covering small regions are often neglected , which drastically downgrades the 3D detection accuracy.</p><p>We propose MonoGRNet, a unified network for amodal 3D object localization from a monocular image. Our key idea is to decouple the 3D localization problem into several progressive sub-tasks that are solvable using only monocular RGB data. The network starts from perceiving semantics in 2D image planes and then performs geometric reasoning in the 3D space.</p><p>A challenging problem we overcome is to accurately estimate the depth of an instance's 3D center without computing pixel-level depth maps. We propose a novel instance-level depth estimation (IDE) module, which explores large receptive fields of deep feature maps to capture coarse instance depths and then combines early features of a higher resolution to refine the IDE.</p><p>To simultaneously retrieve the horizontal and vertical position, we first predict the 2D projection of the 3D center. In combination with the IDE, we then extrude the projected center into real 3D space to obtain the eventual 3D object location. All the components are integrated into the end-toend network, MonoGRNet, featuring its three 3D reasoning branches illustrated in <ref type="figure">Fig. 1</ref>, and is finally optimized by a joint geometric loss that minimizes the 3D bounding box discrepancy in the global context.</p><p>We argue that RGB information alone can provide almost accurate 3D locations and poses of objects. Experiments on the challenging KITTI dataset demonstrate that our network outperforms the state-of-art monocular method in 3D object localization with the least inference time. In summary, our contributions are three-fold:</p><p>? A novel instance-level depth estimation approach that directly predicts central depths of ABBox-3D in the absence of dense depth data, regardless of object occlusion and truncation.</p><p>? A progressive 3D localization scheme that explores rich feature representations in 2D images and extends geometric reasoning into 3D context.</p><p>? A unified network that coordinates localization of objects in 2D, 2.5D and 3D spaces via a joint optimization, which arXiv:1811.10247v2 [cs.CV] 31 Mar 2020 <ref type="figure">Figure 1</ref>: MonoGRNet for 3D object localization from a monocular RGB image. MonoGRNet consists of four subnetworks for 2D detection(brown), instance depth estimation(green), 3D location estimation(blue) and local corner regression(yellow). Guided by the detected 2D bounding box, the network first estimates depth and 2D projection of the 3D box's center to obtain the global 3D location, and then regresses corner coordinates in local context. The final 3D bounding box is optimized in an end-to-end manner in the global context based on the estimated 3D location and local corners. (Best viewed in color.) performs efficient inference (taking ?0.06s/image).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Our work is related to 3D object detection and monocular depth estimation. We mainly focus on the works of studying 3D detection and depth estimation, while 2D detection is the basis for coherence.  <ref type="bibr" target="#b12">Xu and Chen 2018;</ref><ref type="bibr" target="#b0">Chabot et al. 2017;</ref><ref type="bibr" target="#b7">Kehl et al. 2017</ref>), multi-view RGB <ref type="bibr" target="#b2">(Chen et al. 2017;</ref>, to <ref type="bibr">RGB-D (Qi et al. 2017;</ref><ref type="bibr" target="#b11">Song and Xiao 2016;</ref><ref type="bibr" target="#b8">Liu et al. 2015;</ref><ref type="bibr" target="#b13">Zhang et al. 2014)</ref>. While geometric information of the depth dimension is provided, the 3D detection task is much easier. Given RGB-D data, FPointNet (Qi et al. 2017) extrudes 2D region proposals to a 3D viewing frustum and then segments out the point cloud of interest object. MV3D <ref type="bibr" target="#b2">(Chen et al. 2017)</ref> generates 3D object proposals from bird's eye view maps given LIDAR point clouds, and then fuses features in RGB images, LIDAR front views and bird's eye views to predict 3D boxes. 3DOP  exploits stereo information and contextual models specific to autonomous driving. The most related approaches to ours are using a monocular RGB image. Information loss in the depth dimension significantly increases the task's difficulty. Performances of state-of-the-art such methods still have large margins to RGB-D and multi-view methods. Mono3D ) exploits segmentation and context priors to generate 3D proposals. Extra networks for semantic and instance segmentation are required, which cost more time for both training and inference. Xu et al.(Xu and Chen 2018) leverage a pretrained disparity estimation model (Mahjourian, Wicke, and Angelova 2018) to guide the geometry reasoning. Other methods <ref type="bibr" target="#b0">(Chabot et al. 2017;</ref><ref type="bibr" target="#b7">Kehl et al. 2017</ref>) utilize 3D CAD models to generate synthetic data for training, which provides 3D object templates, object poses and their corresponding 2D projections for better supervision. All previous methods exploit additional data and networks to facilitate the 3D perception, while our method only requires annotated 3D bounding boxes and no extra network needs to train. This makes our network much light weighted and efficient for training and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monocular Depth Estimation.</head><p>Recently, although many pixel-level depth estimation networks <ref type="bibr" target="#b2">Eigen and Fergus 2015)</ref> have been proposed, they are not sufficient for 3D object localization. When regressing the pixel-level depth, the loss function takes into account every pixel in the depth map and treats them without significant difference. In a common practice, the loss values from each pixel are summed up as a whole to be optimized. Nevertheless, there is a likelihood that the pixels lying in an object are much fewer than those lying in the background, and thus the low average error does not indicate the depth values are accurate in pixels contained in an object. In addition, dense depths are often estimated from disparity maps that may produce large errors at far regions, which may downgrade the 3D localization performance drastically.</p><p>Different from the abovementioned pixel-level depth estimation methods, we are the first to propose an instancelevel depth estimation network which jointly takes semantic and geometric features into account with sparse supervision data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>We propose an end-to-end network, MonoGRNet, that directly predicts ABBox-3D from a single RGB image. Mono-GRNet is composed of a 2D detection module and three geometric reasoning subnetworks for IDE, 3D localization and ABBox-3D regression. In this section, we first formally define the 3D localization problem and then detail MonoGR-Net for four subnetworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Definition</head><p>Given a monocular RGB image, the objective is to localize objects of specific classes in the 3D space. A target object is represented by a class label and an ABBox-3D, which bounds the complete object regardless of occlusion or truncation. An ABBox-3D is defined by a 3D center C = (X c , Y c , Z c ) in global context and eight corners O = {O k }, k = 1, ..., 8, related to local context. The 3D location C is calibrated in the camera coordinate frame and the local corners O are in a local coordinate frame, shown in <ref type="figure" target="#fig_0">Fig. 2 (b)</ref> and (c) respectively.</p><p>We propose to separate the 3D localization task into four progressive sub-tasks that are resolvable using only a monocular image. First, the 2D box B 2d with a center b and size (w, h) bounding the projection of the ABBox-3D is detected. Then, the 3D center C is localized by predicting its depth Z c and 2D projection c. Notations are illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. Finally, local corners O with respect to the 3D center are regressed based on local features. In summary, we formulate the ABBox-3D localization as estimating the following parameters of each interest object: </p><formula xml:id="formula_0">B 3d = (B 2d , Z c , c, O)<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monocular Geometric Reasoning Network</head><p>MonoGRNet is designed to estimate four components, B 2d , Z c , c, O, with four subnetworks respectively. Following a CNN backbone, they are integrated into a unified framework, as shown in <ref type="figure">Fig. 1</ref>.</p><p>2D Detection. The 2D detection module is the basic module that stabilizes the feature learning and also reveals regions of interest to the subsequent geometric reasoning modules.</p><p>We leverage the design of the detection component in <ref type="bibr" target="#b12">(Teichmann et al. 2016)</ref>, which combines fast regression <ref type="bibr" target="#b10">(Redmon et al. 2016</ref>) and size-adaptive RoiAlign , to achieve a convincing speed-accuracy ratio. An input image I of size W ?H is divided into an S x ?S y grid G, where a cell is indicated by g. The output feature map of the backbone is also reduced to S x ? S y . Each pixel in the feature map corresponding to an image grid cell yields a prediction. The 2D prediction of each cell g contains the confidence that an object of interest is present and the 2D bounding box of this object, namely, (P r g obj , B g 2d ), indicated by a superscript g. The 2D bounding box B 2d = (? x b , ? y b , w, h) is represented by the offsets (? x b , ? y b ) of its center b to the cell g and the 2D box size (w, h).</p><p>The predicted 2D bounding boxes are taken as inputs of the RoiAlign ) layers to extract early features with high resolutions to refine the predictions and reduce the performance gap between this fast detector with proposalbased detectors.</p><p>Instance-Level Depth Estimation. The IDE subnetwork estimates the depth of the ABBox-3D center Z c . Given the divided grid G in the feature map from backbone, each grid cell g predicts the 3D central depth of the nearest instance within a distance threshold ? scope , considering depth information, i.e., closer instances are assigned for cells, as illustrated in <ref type="figure">Fig. 3 (a)</ref>. An example of predicted instance depth for each cell is shown in <ref type="figure">Fig. 3 (c)</ref>.</p><p>The IDE module consists of a coarse regression of the region depth regardless of the scales and specific 2D location of the object, and a refinement stage that depends on the 2D bounding box to extract encoded depth features at exactly the region occupied by the target, as illustrated in <ref type="figure" target="#fig_2">Fig. 4</ref>. Grid cells in deep feature maps from the CNN backbone have larger receptive fields and lower resolution in comparison with that of shallow layers. Because they are less sensitive to the exact location of the targeted object, it is reasonable to regress a coarse depth offset Z cc from deep layers. Given the detected 2D bounding box, we are able to perform RoiAlign to the region containing an instance in early feature maps with a higher resolution and a smaller receptive field. The aligned features are passed through fully connected layers to regress a delta ? Zc in order to refine the instance-level depth value. The final prediction is Z c = Z cc + ? Zc .</p><p>3D Location Estimation. This subnetwork estimates the location of 3D center C = (X c , Y c , Z c ) of an object of interest in each grid g. As illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, the 2D center b and the 2D projection c of C are not located at the same position due to perspective transformation. We first regress the projection c and then backproject it to the 3D space based on the estimated depth Z c .</p><p>In a calibrated image, we elaborate the projection mapping from a 3D point X</p><formula xml:id="formula_1">= (X, Y, Z) to a 2D point x = (u, v), ? 3D ?2D : X ? x, by u = f x * X/Z + p x , v = f y * Y /Z + p y<label>(2)</label></formula><p>where f x and f y are the focal length along X and Y axes, p x and p y are coordinates of the principle point. Given known Z, the backprojection mapping ? 2D ?3D : (x, Z) ? X takes the form:</p><formula xml:id="formula_2">X = (u ? p x ) * Z/f x , Y = (v ? p y ) * Z/f y (3)</formula><p>Since we have obtained the instance depth Z c from the IDE module, the 3D location C can be analytically computed using the 2D projected center c according to Equation 3. Consequently, the 3D estimation problem is converted to a 2D keypoint localization task that only relies on a monocular image.</p><p>Similar to the IDE module, we utilize deep features to regress the offsets ? c = (? xc , ? yc ) of a projected center c to the grid cell g and calculate a coarse 3D location C s = ? 2D ?3D (? c + g, Z c ). In addition, the early features with high resolution are extracted to regress the delta ? C between the predicted C s and the groundtruth C to refine the final 3D location, C = C s + ? C .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Box Corner Regression</head><p>This subnetwork regresses eight corners, i.e., O = {O k }, k = 1, ..., 8, in a local coordinate frame. Since each grid cell predicts a 2D bounding box in the 2D detector, we apply RoiAlign to the cell's corresponding region in early feature maps with high resolution and regress the local corners of the 3D bounding box.</p><p>In addition, regressing poses of 3D boxes in camera coordinate frame is ambiguous <ref type="bibr" target="#b12">(Xu et al. 2018;</ref><ref type="bibr" target="#b4">Gao et al. 2018;</ref><ref type="bibr" target="#b6">Guizilini and Ramos 2018)</ref>. Even the poses of two 3D boxes are different, their projections could be similar when observed from certain viewpoints. We are inspired by Deep3DBox (Mousavian et al. 2017), which regresses boxes in a local system according to the observation angle.</p><p>We construct a local coordinate frame, where the origin is at the object's center, the z-axis points straight from the camera to the center in bird's eye view, the x-axis is on the right of z-axis, and the y-axis does not change, illustrated in <ref type="figure" target="#fig_0">Fig. 2  (c)</ref>. The transformation from local coordinates to camera coordinates are involved with a rotation R and a translation C, and we obtain O cam</p><formula xml:id="formula_3">k = RO k + C, where O cam k</formula><p>are the global corner coordinates. This is a single mapping between the perceived and actual rotation, in order to avoid confusing the regression model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Functions</head><p>Here we formally formulate four task losses for the above subnetworks and a joint loss for the unified network. All the predictions are modified with a superscript g for the corresponding grid cell g. Groundtruth observations are modified by the (?) symbol.</p><p>2D Detection Loss. The object confidence is trained using softmax (s?) cross entropy (CE) loss and the 2D bounding boxes B 2d = (x b , y b , w, h) are regressed using a masked L1 distance loss. Note that w and h are normalized by W and H. The 2D detection loss are defined as:</p><formula xml:id="formula_4">L conf = CEg ?G (s ? (P r g obj ), P r g obj ) L bbox = g 1 obj g ? d(B g 2d , B g 2d ) L 2d = L conf + ?L bbox<label>(4)</label></formula><p>where P r obj and P r obj refer to the confidence of predictions and groundtruths respectively, d(?) refers to L1 distance and 1 obj g masks off the grids that are not assigned any object. The mask function 1 obj g for each grid g is set to 1 if the distance between g b is less then ? scope , and 0 otherwise. The two components are balanced by ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instance Depth</head><p>Loss. This loss is a L1 loss for instance depths:</p><formula xml:id="formula_5">L zc = g 1 obj g ? d(Z g cc , Z g c ) L z? = g 1 obj g ? d(Z g cc + ? g Zc , Z g c ) L depth = ?L zc + L z?<label>(5)</label></formula><p>where ? &gt; 1 that encourages the network to first learn the coarse depths and then the deltas.</p><p>3D Localization Loss. This loss sums up the L1 loss of 2D projection and 3D location:</p><formula xml:id="formula_6">L 2d c = g 1 obj g ? d(g + ? g c , c g ) L 3d c = g 1 obj g ? d(C g s + ? g C , C g ) L location = ?L 2d c + L 3d c<label>(6)</label></formula><p>where ? &gt; 1 to make it possible to learn the projected center first and then refine the final 3D prediction.</p><p>Local Corner Loss. The loss is the sum of L1 loss for all corners:</p><formula xml:id="formula_7">L corners = g k 1 obj g ? d(O k , O k )<label>(7)</label></formula><p>Joint 3D Loss. Note that in the above loss functions , we decouple the monocular 3D detection into several subtasks and respectively regress different components of the 3D bounding box. Nevertheless, the prediction should be as a whole, and it is necessary to establish a certain relationship among different parts. We formulate the joint 3D loss as the sum of distances of corner coordinates in the camera coordinate frame:</p><formula xml:id="formula_8">L joint = g k 1 obj g ? d(O cam k , O cam k ) (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>Network Setup. The architecture of MonoGRNet is shown in <ref type="figure">Fig. 1</ref>. We choose VGG-16 <ref type="bibr" target="#b10">(Matthew and Rob 2014)</ref> as the CNN backbone, but without its FC layers. We adopt KittiBox <ref type="bibr" target="#b12">(Teichmann et al. 2016)</ref> for fast 2D detection and insert a buffer zone to separate 3D reasoning branches from the 2D detector. In the IDE module, a depth encoder structure similar in DORN ) is integrated to capture both local and global features. We present detailed settings for each layer in the supplemental material. There are 46 weighted layers in total, with only 20 weighted layers for the deepest path (i.e., from the input to the IDE output), due to the parallel 3D reasoning branches. In our design, there are 7.7 million parameters in all the 2D and 3D modules, which is approximately 6.2% of the fully connected layers in the original VGG-16.</p><p>Training. The VGG-16 backbone is initialized with the pretrained weights on ImageNet. In the loss functions, we set ? = ? = ? = 10. L2 regularization is applied to the model parameters with a decay rate of 1e-5. We first train the 2D detector, along with the backbone, for 120K iterations using the Adam optimizer (Kingma and Ba 2015). Then the 3D reasoning modules, IDE, 3D localization and local corners, are trained for 80K iterations with the Adam optimizer. Finally, we use SGD to optimize the whole network in an end-to-end fashion for 40K iterations. The batch-size is set to 5, and the learning rate is 1e-5 throughout training. The network is trained using a single GPU of NVidia Tesla P40.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment</head><p>We evaluate the proposed network on the challenging KITTI dataset <ref type="bibr" target="#b5">(Geiger, Lenz, and Urtasun 2012)</ref>, which contains 7481 training images and 7518 testing images with calibrated camera parameters. Detection is evaluated in three regimes: easy, moderate and hard, according to the occlusion and truncation levels. We compare our method with the state-of-art monocular 3D detectors, MF3D (Xu and Chen 2018) and Mono3D . We also present the results of a stereo-based 3D detector 3DOP <ref type="bibr" target="#b1">(Chen et al. 2015)</ref> for reference. For a fair comparison, we use the train1/val1 split following the setup in <ref type="bibr" target="#b2">Chen et al. 2017)</ref>, where each set contains half of the images.</p><p>Metrics. For evaluating 3D localization performance, we use the mean errors between the central location of predicted 3D bounding boxes and their nearest ground truths. For 3D detection performance, we follow the official settings of KITTI benchmark to evaluate the 3D Average Precision (AP 3D ) at different Intersection of Union (IoU) thresholds.</p><p>3D Localization Estimation. We evaluate the three dimensional location errors (horizontal, vertical and depth) according to the distances between the targeting objects and the camera centers. The distances are divided into intervals of 10 meters. The errors are calculated as the mean differences between the predicted 3D locations and their nearest ground truths in meters. Results are presented in <ref type="figure" target="#fig_3">Fig. 5</ref>  errors, especially for in the depth dimension, increase as the distances grow because far objects presenting small scales are more difficult to learn.</p><p>The results indicate that our approach (red curve) outperforms Mono3D by a significant margin, and is also superior to 3DOP, which requires stereo images as input. Another finding is that, in general, our model is less sensitive to the distances. When the targets are 30 meters or farther away from the camera, our performance is the most stable, indicating that our network deals with far objects(containing small image regions) best.</p><p>Interestingly, horizontal and vertical errors are an order of magnitude smaller than that of depth, i.e., the depth error dominants the overall localization error. This is reasonable because the depth dimension is not directly observed in the 2D image but is reasoned from geometric features. The proposed IDE module performs superior to the others for the easy and moderate regimes and comparable to the stereobased method for the hard regime.</p><p>3D Object Detection. 3D detection is evaluated using the AP 3D at 0.3, 0.5 and 0.7 3D IoU thresholds for the car class. We compare the performance with two monocular methods, Mono3D and MF3D. The results are reported in <ref type="table" target="#tab_3">Table 1</ref>. Since the authors of MF3D have not published their validation results, we only report the AP 3D at 0.5 and 0.7 presented in their paper. Experiments show that our method outperforms the state-of-art monocular detectors mostly and is comparable to the stereo-based method.</p><p>Our network is designed for efficient applications and a fast 2D detector with no region proposal is adopted. The inference time achieves about 0.06 seconds per image on a Geforce GTX Titan X, which is much less than the other three methods. On the other hand, this design at some degree sacrifices the accuracy of 2D detection. Our 2D AP of the moderate regime at 0.7 IoU threshold is 78.14%, about 10% lower than the region proposal based methods that generate a large number of object proposals to recall as many groundtruth as possible. Despite using a relatively weak 2D detector, our 3D detection achieves the state-of-the-art performance, resorting to our IDE and 3D localization module. Note that the 2D detection is a replaceable submodule in our network, and is not our main contribution.</p><p>Local 3D Bounding Box Regression. We evaluate the regression of local 3D bounding boxes with the size (height, width, length) and orientation metrics. The height, width,  length of a 3D bounding box can be easily calculated from its eight corners. The orientation is measured by the azimuth angles in the camera coordinate frame. We present the mean errors in <ref type="table" target="#tab_5">Table 2</ref>. Our network demonstrates a better capability to learn the size and orientation of a 3D bounding box from merely photometric features. It is worth noting that in our local corner regression module, after RoiAlign layers, all the objects of interest are rescaled to the same size to introduce scale-invariance, yet the network still manages to learn their real 3D sizes. This is because our network explores the image features that convey projective geometries and semantic information including types of objects (e.g., SUVs are generally larger than cars) to facilitate the size and orientation estimation.</p><p>Qualitative Results. Qualitative visualization is provided for three typical situations, shown in <ref type="figure" target="#fig_4">Fig. 6</ref>. In common street scenes, our predictions are able to successfully recall the targets. It can be observed that even though the vehicles are heavily truncated by image boundaries, our network still outputs precise 3D bounding boxes. Robustness to such a corner case is important in the scenario of autonomous driving to avoid collision with lateral objects. For cases where some vehicles are heavily occluded by others, i.e., in (g), (h) and (i), our 3D detector can handle those visible vehicles but fails in detecting invisible ones. In fact, this is a general limitation of perception from monocular RGB images, which can be solved by incorporating 3D data or multi-view data to obtain informative 3D geometric details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>A crucial step for localizing 3D center C is estimating its 2D projection c, since c is analytically related to C. Although the 2D bounding box's center b can be close to c, as is illustrated in <ref type="figure" target="#fig_0">Fig. 2 (a)</ref>, it does not have such 3D significance. When we replace c with b in 3D reasoning, the horizontal location error rises from 0.27m to 0.35m, while the verti- cal error increases from 0.09m to 0.69m. Moreover, when an object is truncated by the image boundaries, its projection c can be outside the image, while b is always inside. In this case, using b for 3D localization can result in a severe discrepancy. Therefore, our subnetwork for locating the projected 3D center is indispensable.</p><p>In order to examine the effect of coordinate transformation before local corner regression, we directly regress the corners offset in camera coordinates without rotating the axes. It shows that the average orientation error increases from 0.251 to 0.442 radians, while the height, width and length errors of the 3D bounding box almost remain the same. This phenomenon corresponds to our analysis that switching to object coordinates can reduce the rotation ambiguity caused by projection, and thus enables more accurate 3D bounding box estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We have presented the MonoGRNet for 3D object localization from a monocular image, which achieves superior performance on 3D detection, localization and pose estimation among the state-of-the-art monocular methods. A novel IDE module is proposed to predict precise instance-level depth, avoiding extra computation for pixel-level depth estimation and center localization, regardless of far distances between objects and camera. Meanwhile, we distinguish the 2D bounding box center and the projection of 3D center for a better geometric reasoning in the 3D localization. The object pose is estimated by regressing corner coordinates in a local coordinate frame that alleviates ambiguities of 3D rotations in perspective transformations. The final unified network integrates all components and performs inference efficiently.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Notation for 3D bounding box localization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Scopes of instances on grid cells (b) An image with 2D bounding box (c) Instance depth mapFigure 3: Instance depth. (a) Each grid cell g is assigned to a nearest object within a distance ? scope to the 2D bbox center b i . Objects closer to the camera are assigned to handle occlusion. Here Z 1 c &lt; Z 2 c . (b) An image with detected 2D bounding boxes. (c) Predicted instance depth for each cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Instance depth estimation subnet. This shows the inference of the red cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>3D Localization Errors in the horizontal, vertical and depth dimensions according to the distances between objects and camera centers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative Results. Predicted 3D bounding boxes are drawn in orange, while ground truths are in blue. Lidar point clouds are plotted for reference but not used in our method. Camera centers are at the bottom-left corner. (a), (b) and (c) are common cases when predictions recall the ground truths, while (d), (e) and (f) demonstrate the capability of our model handling truncated objects outside the image. (g), (h) and (i) show the failed detections when some cars are heavily occluded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>/ 71.41 52.22 / 57.78 49.64 / 51.91 46.04 / 55.04 34.63 / 41.25 30.09 / 34.55 / 55.02 29.48 / 36.73 26.44 / 31.27 10.53 / 22.03 5.69 / 13.63 5.39 / 11.60 Ours Mono 0.06 72.17 / 73.10 59.57 / 60.66 46.08 / 46.86 50.51 / 54.21 36.97 / 39.69 30.82 / 33.06 13.88 / 24.97 10.19 / 19.44 7.62 / 16.30</figDesc><table><row><cell>Method</cell><cell>Type</cell><cell>Time (s)</cell><cell>Easy</cell><cell>AP 3D / AP BEV (IoU=0.3) Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell>AP 3D / AP BEV (IoU=0.5) Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell cols="3">AP 3D / AP BEV (IoU=0.7) Moderate</cell><cell>Hard</cell></row><row><cell>3DOP</cell><cell>Stereo</cell><cell>4.2</cell><cell cols="8">69.79 6.55 / 12.63</cell><cell>5.07 / 9.49</cell><cell>4.10 / 7.59</cell></row><row><cell>Mono3D</cell><cell>Mono</cell><cell>3</cell><cell cols="6">28.29 / 32.76 23.21 / 25.15 19.49 / 23.65 25.19 / 30.50 18.20 / 22.39 15.22 / 19.16</cell><cell cols="2">2.53 / 5.22</cell><cell>2.31 / 5.19</cell><cell>2.31 / 4.13</cell></row><row><cell>MF3D</cell><cell>Mono</cell><cell>0.12</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>47.88</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>. The</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>3D Detection Performance. Average Precision of 3D bounding boxes on the same KITTI validation set and the inference time per image. Note that the stereo-based method 3DOP is not compared but listed for reference.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>3D Bounding Box Parameters Error.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep manta: A coarseto-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Teuli?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chateau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognit.(CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2040" to="2049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2147" to="2156" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision<address><addrLine>Fergus</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
	<note>IEEE CVPR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<idno>Fu et al. 2018</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Dssd: Deconvolutional single shot detector. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Group-pair convolutional neural networks for multiview based 3d object retrieval</title>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lenz</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition (CVPR)</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Iterative continuous convolution for 3d template matching and global localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ramos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Guizilini and Ramos</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ssd-6d: Making rgb-based 3d detection and 6d pose estimation great again</title>
		<idno type="arXiv">arXiv:1703.06870</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV 2017)</title>
		<meeting>the International Conference on Computer Vision (ICCV 2017)<address><addrLine>Venice, Italy; Ba</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="22" to="29" />
		</imprint>
	</monogr>
	<note type="report_type">Mask r-cnn. arXiv preprint</note>
	<note>International Conference for Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Higher-order crf structural segmentation of 3d reconstructed surfaces</title>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5667" to="5675" />
		</imprint>
	</monogr>
	<note>European conference on computer vision (ECCV)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ko?eck?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08488</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Computer Vision and Pattern Recognition (CVPR). Ren et al. 2017. Faster r-cnn: towards real-time object detection with region proposal networks. IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Emphasizing 3d properties in recurrent multi-view aggregation for 3d shape retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teichmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07695</idno>
		<idno>Xu et al. 2018</idno>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2345" to="2353" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Computer Vision and Pattern Recognition (CVPR)</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint segmentation of images and scanned point cloud in large-scale street scenes with low-annotation cost</title>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="4763" to="4772" />
		</imprint>
	</monogr>
	<note>3d box proposals from a single monocular image of an indoor scene</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
