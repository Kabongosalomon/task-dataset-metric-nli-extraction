<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhida</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yewei</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanxin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Liu</surname></persName>
							<email>liujiaxiang@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichong</forename><surname>Yin</surname></persName>
							<email>yinweichong@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
							<email>fengshikun01@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent progress in diffusion models has revolutionized the popular technology of text-to-image generation. While existing approaches could produce photorealistic high-resolution images with text conditions, there are still several open problems to be solved, which limits the further improvement of image fidelity and text relevancy. In this paper, we propose ERNIE-ViLG 2.0, a large-scale Chinese text-to-image diffusion model, which progressively upgrades the quality of generated images by:</p><p>(1) incorporating fine-grained textual and visual knowledge of key elements in the scene, and (2) utilizing different denoising experts at different denoising stages. With the proposed mechanisms, ERNIE-ViLG 2.0 not only achieves the stateof-the-art on MS-COCO with zero-shot FID score of 6.75, but also significantly outperforms recent models in terms of image fidelity and image-text alignment, with side-by-side human evaluation on the bilingual prompt set ViLG-300.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>the scene, thus facing the risk of text-image misalignment, such as the attribute confusion problem, especially for text prompts containing multiple objects with specific attributes . Second, when opening the horizon from individual step to the whole denoising process, we can found that the requirements of different denoising stages are also not identical. In the early stages, the input images are highly noised, and the model is required to outline the semantic layout and skeleton out of almost pure noise. In contrast, in the later steps close to the image output, denoising mainly means improving the details based on an almost completed image <ref type="bibr" target="#b24">(Rombach et al., 2021)</ref>. In practice, existing models usually use one U-Net for all steps, which means that the same set of parameters has to learn different denoising capabilities.</p><p>To address these issues, in this paper, we propose ERNIE-ViLG 2.0, an improved text-to-image diffusion model with knowledge-enhanced mixture-of-denoising-experts, to incorporate extra knowledge about the visual scene and decouple the denoising capabilities in different steps. Specifically, we employ a text parser and an object detector to extract key elements of the scene in the input text-image pair, and then guide the model to pay more attention to their alignment in the learning process, so as to hope the model could handle the relationships among various objects and attributes. Moreover, we divide the denoising steps into several stages and employ specific denoising "experts" for each stage. With the mixture of multiple experts, the model can involve more parameters and learn the data distribution of each denoising stage better, without increasing the inference time, as only one expert is activated in each denoising step.</p><p>With the extra knowledge from the visual scene and the mixture-of-denoising-experts mechanism, we train ERNIE-ViLG 2.0 and scale up the model size to 24B parameters. Experiments on MS-COCO show that our model exceeds previous text-to-image models by setting a new state-of-the-art of 6.75 zeros-shot FID-30k score, and detailed ablation studies confirm the contributions of each proposed strategy. Apart from automatic metrics, we also collect 300 bilingual text prompts that could assess the quality of generated images from different aspects and enable a fair comparison between English and Chinese text-to-image models. The human evaluation results again indicate that ERNIE-ViLG 2.0 outperforms other recent methods, including DALL-E 2 and Stable Diffusion, by a significant margin both in terms of image-text alignment and image fidelity.</p><p>To sum up, the main contributions of this work are as follows:</p><p>? ERNIE-ViLG 2.0 incorporates textual and visual knowledge into the text-to-image diffusion model, which effectively improves the ability of fine-grained semantic control and alleviates the problem of object-attribute mismatching in generated images. ? ERNIE-ViLG 2.0 proposes the mixture-of-denoising-experts mechanism to refine the denoising process, which can adapt to the characteristics of different denoising steps and scale up the model to 24B parameters, making it the largest text-to-image model at present. ? ERNIE-ViLG 2.0 achieves a new state-of-the-art zero-shot FID-30k score of 6.75 on the MS-COCO dataset and surpasses DALL-E 2 and Stable Diffusion in human evaluation on the Chinese-English bilingual prompt set ViLG-300.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>During the training process, the text-to-image diffusion model receives paired inputs (x, y) consisting of an image x with its text description y, and the ultimate goal is to generate x based on y. To achieve this, a text encoder f ? (?) first encodes y as f ? (y), then a denoising network ? (?) conditioned on f ? (y) learns to generate x from a Gaussian noise. To help the model generate high-quality images that accurately match the text description (i.e., text prompt), ERNIE-ViLG 2.0 enhances the text encoder f ? (?) and the denoising network ? (?) with textual and visual knowledge of the key elements in the scene. Furthermore, ERNIE-ViLG 2.0 employs mixture-of-denoising-experts to refine the image generation process, where different experts are responsible for different generation steps in the denoising process. The overall architecture of ERNIE-ViLG 2.0 is shown in <ref type="figure" target="#fig_0">Figure 2</ref> and the details of the models are described in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminary</head><p>Denoising diffusion probabilistic models (DDPM) are a class of score-based generative models that have recently shown delightful talents in the field of text-to-image generation <ref type="bibr" target="#b8">(Ho et al., 2020</ref>  diffusion process of DDPM aims to iteratively add diagonal Gaussian noise to the initial data sample x and turn it into an isotropic Gaussian distribution after T steps:</p><formula xml:id="formula_0">$ !! !" !# % ... ... ... ... " ",$ " ",!! " ",!" " ",!# ! ... " ",% " ",! Mixture-of-Denoising-Experts (MoDE) append ? dog ? bowl MoDE ... ... !,# !,$ !,%</formula><formula xml:id="formula_1">x t = ? ? t x t?1 + ? 1 ? ? t t , t ? {1, . . . , T }</formula><p>(1) where the sequence {x t } starts with x 0 = x and ends with x T ? N (0, I), the added noise at each step is t ? N (0, I), and {? t } 1...T is a pre-defined schedule <ref type="bibr" target="#b28">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b30">Song et al., 2021b)</ref>. The denoising process is the reverse of diffusion, which converts the Gaussian noise x T ? N (0, I) back into the data distribution x 0 through iterative denoising steps t = T, . . . , 1. During training, for a given image x, the model calculates x t by sampling a Gaussian noise ? N (0, I):</p><formula xml:id="formula_2">x t = ?? t x 0 + ? 1 ?? t ,<label>(2)</label></formula><p>where? t = t s=1 ? s . Given x t , the target of the denoising network ? (?) is to restore x 0 by predicting the noise . It is learned via the loss function</p><formula xml:id="formula_3">L = E x, ?N (0,I),t || ? ? (x t , t)|| 2 2 .<label>(3)</label></formula><p>With the predicted ? (x t , t), we can have the prediction of x 0 at step t by converting Equation <ref type="formula" target="#formula_2">(2)</ref>:</p><formula xml:id="formula_4">x 0,t = 1 ?? t (x t ? ? 1 ?? t ? (x t , t)).<label>(4)</label></formula><p>In <ref type="figure" target="#fig_0">Figure 2</ref>, we visualize the sampled x t and the predictedx 0,t for several timesteps during training.</p><p>In the inference process of DDPM, x 0 is unknown, so the model iteratively generates x t?1 based on x t andx 0,t :</p><formula xml:id="formula_5">x t?1 = 1 ?? t?1 1 ?? t ? ? t x t + 1 ? ? t 1 ?? t ?? t?1x0,t + (1 ?? t?1 )(1 ? ? t ) 1 ?? t t , t ? {T, . . . , 1},<label>(5)</label></formula><p>where t ? N (0, I) is a sampled Gaussian noise.</p><p>The denoising network ? (?) is typically implemented by U-Net <ref type="bibr" target="#b8">(Ho et al., 2020)</ref>. To allow ? (?) to condition on text prompts, a text encoder f ? (?) first extracts the text representation f ? (y) ? R ny?dy , which is then fed into ? (?) via a cross-modal attention layer . Formally, the U-Net representation ? i (x t ) ? R nx?d is concatenated with the text representation f ? (y) after projection, and then goes through an attention layer to achieve cross-modal interaction,</p><formula xml:id="formula_6">Q = ? i (x t )W (i) Q , K = [? i (x t )W (i) Kx ; f ? (y)W (i) Ky ], V = [? i (x t )W (i) Vx ; f ? (y)W (i) Vy ], (6) Attention(Q, K, V ) = softmax QK ? d V,<label>(7)</label></formula><p>where i is the index for U-Net layers, [; ] is the concatenation operator, W</p><formula xml:id="formula_7">(i) Q , W (i) Kx , W (i) Vx ? R d?d and W (i) Ky , W (i)</formula><p>Vy ? R dy?d are learnable projection layers, n x and n y are the length of encoded image and text, respectively.</p><p>During inference, given a text prompt y, the denoising U-Net ? (?) predicts the image sample x conditioned on the text y with classfier-free guidance <ref type="bibr" target="#b9">(Ho and Salimans, 2021)</ref> and Denoising Diffusion Implicit Models (DDIM) sampling <ref type="bibr" target="#b29">(Song et al., 2021a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Knowledge-Enhanced Diffusion Model</head><p>The text-to-image model receives a text prompt that describes the scene of an image, and then depicts the scene contained therein, including the crucial objects with corresponding attribute details. In other words, both text and image are intended to express a visual scene, in which key elements have different expressions, such as keywords in the text or salient regions in the image. However, the naive diffusion model does not distinguish the importance of elements and indiscriminately iterates the denoising process. ERNIE-ViLG 2.0 incorporates extra text and visual knowledge into the learning stage, hoping to enhance the fine-grained semantic perception of the diffusion model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Textual</head><p>Knowledge. An ideal text-to-image generation model is expected to focus on all the critical semantics mentioned in the text prompt. To distinguish function words and words describing key semantics, we adopt an off-the-shelf word segmentation and part-of-speech toolkit to extract lexical knowledge of the input prompt, and then improve the learning process by (1) inserting special tokens into the input sequence and (2) increasing the weight of tokens with specific part-of-speech tags in the attention layer. Specifically, we selected 50% of samples and inserted special tokens at the beginning of each word, in which each part-of-speech tag corresponds to a special token. Besides, for the selected samples, we also strengthen the attention weight of keywords based on the lexical analysis results. In this way, Equation <ref type="formula" target="#formula_6">(7)</ref> is modified to,</p><formula xml:id="formula_8">Attention(Q, K, V ) = softmax W a ? (QK ) ? d V,<label>(8)</label></formula><p>where W a ? R nx?(nx+ny) is an auxiliary weight matrix that used to scale the vanilla attention, and</p><formula xml:id="formula_9">W ij a = 1 + w a tok i ? {x}, tok j ? {x, y key } 1 otherwise.<label>(9)</label></formula><p>Here w ij a is the scaling factor of the attention weight between token tok i and tok j , w a is a hyperparameter, x refers to all the image tokens, and y key denotes the keywords in text 2 . <ref type="figure" target="#fig_0">Figure 2</ref> gives an example, where the special tokens "[a]" and "[n]" are inserted for adjectives and nouns, respectively, and the keywords such as "brown" and "dog" are marked with dark color.</p><p>Visual Knowledge. Similar to notional words in the text prompt, there are also many salient regions in an image, such as people, trees, buildings, and objects explicitly mentioned in the input. To extract such visual knowledge, we apply the object detector provided by <ref type="bibr" target="#b0">Anderson et al. (2018)</ref> to 50% of training samples, and then select eye-catching objects from the results with heuristic strategies. Since the loss function of the diffusion model directly acts on the image space, we can assign higher weights to corresponding regions by modifying Equation <ref type="formula" target="#formula_3">(3)</ref>, thus promoting the model to focus on the generation of these objects:</p><formula xml:id="formula_10">L = E z, ?N (0,I),t W l ? || ? ? (z t , t)|| 2 2 ,<label>(10)</label></formula><formula xml:id="formula_11">W ij l = 1 + w l los ij ? {x key } 1 otherwise.<label>(11)</label></formula><p>Here W l ? R n h ?nw is the weight matrix, n h and n w are the height and weight of image space, w l is a hyper-parameter, los ij is the loss item in i-th row and j-th column of image space, x key is the regions that corresponding to key objects. As <ref type="figure" target="#fig_0">Figure 2</ref> illustrates, the regions of "dog" and "cat" are assigned with larger weights in the calculation of the denoising loss L . Now a new problem arises: as a kind of fine-grained knowledge, it is inevitable that the selected objects may not appear in the text prompt, thus perplexing the model in learning the alignment between words and objects. Based on the result of object detection, an intuitive idea is first to obtain the object and attribute category of each region, then combine corresponding class labels with the original text prompt to achieve the fine-grained description, thus ensuring the final input contains both coarse and fine granularity information. For instance, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>, the detected object "bowl" is not included in the caption, so we append it to the original description. Besides, we also employ an image captioning model <ref type="bibr" target="#b36">(Wang et al., 2022a)</ref> to generate text for images, and randomly replace the original prompt with generated captions, because the generated captions of many images are more concise and reveal more accurate semantics than the original prompts.</p><p>Most notably, the above strategies are only limited to the training stage. By randomly selecting a part of samples to equip these additional enhancement strategies, the model is supposed to sense the hints of knowledge from various perspectives, and generate higher quality images for the given text in the inference stage, even without special tokens, attention strengthening, or text refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Mixture-of-Denoising-Experts</head><p>Recall that the diffusion process is to iteratively corrupt the image with Gaussian noises by a series of diffusion steps t = 1, . . . , T , and DDPM <ref type="bibr" target="#b8">(Ho et al., 2020)</ref> are trained to revert the diffusion process by denoising steps t = T, . . . , 1. During the denoising process, all steps aim to denoise a noised input, and they together convert a completely random noise into a meaningful image gradually. Although sharing the same goal, the difficulty of these denoising steps varies according to the noise ratio of input. <ref type="figure" target="#fig_0">Figure 2</ref> illustrates such difference by presenting some examples of x t and the denoising predictionx 0,t during training. For timesteps t near T , such as t = T, t k , t j in the figure, the input of the denoising network x t is highly noised, and the network of these steps mainly tackles a generation task, i.e., generating an image from a noise. On the contrary, for timesteps t near 1, such as t = t i , 1 in the figure, the input x t is close to the original image, and the network of these steps needs to refine the image details.</p><p>From the deduction in Section 2.1, DDPM makes no specific assumption on the implementation of denoising network, that is, the denoising process does not require the same denoising network for all steps in theory. However, most of the previous text-to-image diffusion approaches <ref type="bibr" target="#b24">(Rombach et al., 2021;</ref><ref type="bibr">Saharia et al., 2022)</ref> follow the original DDPM implementation to adopt one denoising network for the whole denoising process. Considering that tasks of different timesteps are different, we conjecture that using the same set of parameters to learn different tasks might lead to suboptimal performance.</p><p>In view of this, we propose Mixture-of-Denoising-Experts (MoDE), where the primary motivation is to employ multiple specialized expert networks to fit different tasks at different timesteps. Since the inputs of adjacent timesteps are similar and so are the denoising tasks, we divide all the timesteps uniformly into n blocks (S 1 , ? ? ? , S i , ? ? ? , S n ), in which each block consists of consecutive timesteps and is assigned to one denoising expert. In other words, the timesteps in the same block are denoised by the same group of network parameters. In practice, we share the same text encoder for all denoising experts, and utilize different U-Net experts for different timestep blocks:</p><formula xml:id="formula_12">? (x t , t) = { ?,i (x t , t)}, t ? S i ,<label>(12)</label></formula><p>where ?,i (x t , t) denotes the i-th expert network. In this way, MoDE could improve the model performance by adopting expert networks to specially deal with different denoising stages.</p><p>When using more experts, each block contains fewer timesteps, so each expert could better focus on learning the characteristics of specific denoising steps assigned to it. Meanwhile, as only one expert network is activated at each step, increasing the number of experts does not affect the computation overhead during inference. Therefore, MoDE can flexibly scale up the parameters of diffusion model, allowing the experts to fit the data distribution better without increasing inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we first introduce the implementation details and settings of experiments. Then we present the evaluation of models with automatic metrics and human assessment. Last, we further analyze the results with quantitative ablation studies and qualitative showcases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implementation Details</head><p>To reduce the learning complexity, we use diffusion models to generate the representations of images in the latent space of an image auto-encoder following Latent Diffusion Models <ref type="bibr" target="#b24">(Rombach et al., 2021)</ref>. We first pre-train an image encoder to transform an image x ? R n h ?nw?3 from the pixel  <ref type="bibr" target="#b1">(Ding et al., 2021)</ref> 27.10 LAFITE  26.94 LDM <ref type="bibr" target="#b24">(Rombach et al., 2021)</ref> 12.61 ERNIE-ViLG <ref type="bibr" target="#b43">(Zhang et al., 2021b)</ref> 14.70 <ref type="bibr">GLIDE (Nichol et al., 2022)</ref> 12.24 Make-A-Scene <ref type="bibr" target="#b5">(Gafni et al., 2022)</ref> 11.84 DALL-E 2  10.39 CogView2 <ref type="bibr" target="#b2">(Ding et al., 2022)</ref> 24.00 Imagen <ref type="bibr">(Saharia et al., 2022)</ref> 7.27 Parti  7.23 ERNIE-ViLG 2.0 6.75 space into the latent spacex ? R n h /8?nw/8?4 and an image decoder to convert it back. Here n h and n w denote the image's original height and width. Then we fix the auto-encoder and train the diffusion model to generatex from the text prompt y. During inference, we adopt the pre-trained image decoder to turnx into the pixel-level image output.</p><p>ERNIE-ViLG 2.0 contains a transformer-based text encoder with 1.3B parameters and 10 denoising U-Net experts with 2.2B parameters each, which totally add up to about 24B parameters. For hyperparameters to incorporate knowledge, the attention weight scale w a for textual knowledge and the loss weight scale w l for visual knowledge are set to 0.01 and 0.1, respectively. For MoDE, the number of experts n = 10, meaning that the timesteps are divided into 10 blocks. The model is optimized by AdamW <ref type="bibr" target="#b14">(Loshchilov and Hutter, 2019)</ref>, with a fixed learning rate 0.9 ? 10 ?4 , ? 1 = 0.9, ? 2 = 0.999, and weight decay of 0.01. We train the whole model on 320 A100 GPUs for 18 days. The training of diffusion model is a DDPM <ref type="bibr" target="#b8">(Ho et al., 2020)</ref> with 1,000 denoising steps and linear noise schedule, and the inference uses DDIM <ref type="bibr" target="#b29">(Song et al., 2021a)</ref> with 50 steps.</p><p>The training data of ERNIE-ViLG 2.0 consists of 170M image-text pairs, including the publicly available English datasets like LAION <ref type="bibr" target="#b26">(Schuhmann et al., 2021)</ref> and a series of internal Chinese datasets. The image auto-encoder is trained on the same images. For images with English captions, we automatically translate them with Baidu Translate API 3 to get the Chinese version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Automatic Evaluation on MS-COCO. Following previous work <ref type="bibr" target="#b24">(Rombach et al., 2021;</ref><ref type="bibr">Saharia et al., 2022)</ref>, we evaluate ERNIE-ViLG 2.0 on MS-COCO 256 ? 256 with zero-shot FID-30k score, where 30,000 images from the MS-COCO validation set are randomly selected and the English captions are translated to Chinese through Baidu Translate API. <ref type="table" target="#tab_1">Table 1</ref> shows that ERNIE-ViLG 2.0 achieves new state-of-the-art performance of text-to-image generation, with 6.75 zero-shot FID-30k on MS-COCO. Inspired by DALL-E  and Parti , we rerank the batch-sampled images (with only 4 images per text prompt, comparing with 512 images used in DALL-E and 16 images used in Parti) based on the image-text alignment score, calculated by a pre-trained CLIP model , in which the text is the initial English caption in MS-COCO and the image is generated from the auto-translated Chinese caption. Besides, even without the reranking strategy, we find that ERNIE-ViLG 2.0 can also beat the latest diffusion-based models like DALL-E 2  and Imagen <ref type="bibr">(Saharia et al., 2022)</ref>, with the zero-shot FID-30k of 7.23.</p><p>Human Evaluation on ViLG-300. ERNIE-ViLG 2.0 takes Chinese prompts as input and generates high-resolution images, unlike recent English-oriented text-to-image models. Herein, we introduce ViLG-300, a bilingual prompt set that supports the systematic evaluation and comparison of Chinese   <ref type="figure">Figure 4</ref>: Performance with various strategies in ERNIE-ViLG 2.0. For comparison, we draw pareto curves with zero-shot FID-10k and CLIP scores with guidance scales <ref type="bibr">[2,</ref><ref type="bibr">3,</ref><ref type="bibr">4,</ref><ref type="bibr">5,</ref><ref type="bibr">6,</ref><ref type="bibr">7,</ref><ref type="bibr">8,</ref><ref type="bibr">9]</ref>.</p><p>and English text-to-image models. ViLG-300 contains 300 prompts from 16 categories, composed of DrawBench <ref type="bibr">(Saharia et al., 2022</ref>) (in English) and the prompt set used in ERNIE-ViLG <ref type="bibr" target="#b43">(Zhang et al., 2021b</ref>) (in Chinese). We first removed the language-related prompts in DrawBench (including text rendering, rare words, misspelled prompts) and MS-COCO prompts in ERNIE-ViLG, leaving 162 and 398 prompts, respectively, then randomly sampled 150 prompts from these two parts, manually translated and proofread these prompts to achieve the final parallel Chinese and English set. See Appendix A.1 for more details about the construction process. To promote further study, ViLG-300 will be open to the community soon.</p><p>With ViLG-300, we can make convincing comparisons between ERNIE-ViLG 2.0 and DALL-E 2 4 , Stable Diffusion 5 . For evaluation, five raters are presented with two sets of images generated by ERNIE-ViLG 2.0 and the compared model. Next, they are asked to compare these images from two dimensions of image-text alignment and image fidelity, and then select the model they prefer, or respond that there is no measurable difference between two models. Throughout the process, raters are unaware of which model the image is generated from, and we do not apply any filtering strategy to the rating results. <ref type="figure" target="#fig_2">Figure 3</ref> shows that human raters prefer ERNIE-ViLG 2.0 over all other models in both image-text alignment (56.5%?3.8% and 68.2%?3.8% when compared to DALL-E 2 and Stable Diffusion, respectively) and image fidelity (58.8%?3.6% to DALL-E 2, 66.5%?3.5% to Stable Diffusion, respectively), which again proves that ERNIE-ViLG 2.0 can generate high-quality images that conform to the text, with the help of knowledge enhancement and mixture-of-denoising-experts strategies. We provide comparisons of separate categories in Appendix A.2. Beyond text relevancy and image fidelity, we also observe that ERNIE-ViLG 2.0 can generate images with better sharpness and textures than baseline models. See also Appendix B for detailed discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analysis</head><p>To examine the effectiveness of our design philosophy, we conduct two groups of experiments. Similar to the main experiment, the automatic metrics cannot fully reflect the advantages and disadvantages  <ref type="figure">Figure 5</ref>: Samples from ViLG-300 with different knowledge enhancement strategies. It can be found that the impacts of textual and visual knowledge do not seem to overlap, and the combination of them is an effective solution to facilitate accurate semantic control and high image fidelity.  <ref type="figure">Figure 6</ref>: The convergence curves of various models with knowledge enhancement strategies. We choose guidance scale 3 and 8 to draw the curves of FID-10k and CLIP Score, respectively.</p><p>of different models, so we also provide showcases to demonstrate the effectiveness of the two design strategies in ERNIE-ViLG 2.0 6 .</p><p>Knowledge Enhancement Strategies. In this part, we focus on the impact of various knowledge enhancement strategies by training a series of lightweight models, with 500M text encoders, 870M U-Nets, and 500M training samples. The pareto curves in <ref type="figure">Figure 4</ref>(a) and convergence curves in <ref type="figure">Figure 6</ref> demonstrate that incorporating extra knowledge in the learning process of diffusion models brings significant performance gains in image fidelity, image-text alignment, and convergence speed. Specifically, (1) the benefits of textual knowledge are mainly reflected in precise fine-grained semantic control (w/ textual), (2) only utilizing object knowledge may not be able to steadily promote  <ref type="figure">Figure 7</ref>: Samples from ViLG-300 with different number of denoising experts. When increasing the experts, the most noticeable evolution is that the texture of generated image becomes more natural and photorealistic. Limited by the layout, we simplify the prompt in the second column, and the input received by model actually is "??????????????????????????? ????????????????????????????????????(A stack of 3 cubes. A red cube is on the top, sitting on a red cube. The red cube is in the middle, sitting on a green cube. The green cube is on the bottom.)". the performance (w/ object), while taking synthetic descriptions into consideration is an effective solution to make full use of visual knowledge (w/ visual). <ref type="figure">Figure 5</ref> shows more visual comparisons to explicitly demonstrate the changes brought by each strategy. When dealing with complex prompts, the baseline model faces problems such as the absence of key objects or incorrect assignment of attributes. At this point, textual knowledge helps the model accurately understand the attributes of each object (especially the color), but the generated images sometimes fall into the new problem of distortion. Complementarily, visual knowledge promotes the generation of high-fidelity images, but it cannot well understand specific entities in the text. Eventually, the combination of two kinds of knowledge brings a perceivable improvement to the model performance, which ensures high fidelity and boost the image-text alignment in fine-grained visual scene.</p><p>Mixture-of-Denoising-Experts Strategies. Based on the above lightweight settings, we further train the baseline model with 500M samples, and then train 200M samples for each denoising expert. <ref type="figure">Figure 4(b)</ref> shows that with the increasing number of denoising experts, the overall performance is gradually improved, proving that scaling the size of U-Net is also an effective solution to achieve better image quality. More showcases about this experiment are provided in <ref type="figure">Figure 7</ref>. When the number of experts increases from 1 to 10, the model can not only better handle the coupling between different elements but also generate images with more natural textures. For instance, the numbers on clocks become clearer, the proportion of wolf and suit becomes more harmonious, and the model can generate more photorealistic pictures instead of cartoon drawings. <ref type="figure" target="#fig_4">Figure 8</ref> further visualizes the cross-attention maps from image features to text representations in denoising experts during the 1,000-step denoising process, where these steps shown are denoised by different experts. As can be seen from the figure, the attention patterns of different denoising timesteps vary. Specifically, the attention maps of timesteps t near 1,000 are almost evenly distributed over the whole image, which is because the input of these steps is close to Gaussian noise and the image layout is unclear, so all the image tokens have to attend to the text prompt to generate the image skeleton. When the timesteps are close to 1, attention maps concentrate more on foreground objects. For these timesteps, the input to the denoising network is close to the final image and the layout is clear, and only a few parts of the image need to focus on the text to supplement the details of the object. These observations again illustrate the difference among denoising timesteps and demonstrate the need to disentangle different timesteps with multiple experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Works</head><p>Text-to-Image Generation. Text-to-image generation is the task of synthesizing images according to natural language descriptions. Early works adopted generative adversarial networks (GANs) <ref type="bibr" target="#b6">(Goodfellow et al., 2014)</ref> to produce image samples based on text as condition <ref type="bibr" target="#b39">(Xu et al., 2018;</ref><ref type="bibr" target="#b34">Tao et al., 2022;</ref><ref type="bibr" target="#b42">Zhang et al., 2021a</ref>). Inspired by the success of transformers in generation tasks <ref type="bibr" target="#b35">(Vaswani et al., 2017)</ref>, researchers have also explored text-to-image generation as a sequenceto-sequence problem, with auto-regressive transformers as generators and text/image tokens as input/output sequences. Models armed with large-sized transformers trained on large-scale imagetext data such as ERNIE-ViLG <ref type="bibr" target="#b43">(Zhang et al., 2021b)</ref>, DALL-E , Cogview <ref type="bibr" target="#b1">(Ding et al., 2021)</ref>, Make-A-Scene <ref type="bibr" target="#b5">(Gafni et al., 2022)</ref>, and Parti  have displayed strong performance on both image fidelity and image-text alignment. Recently, another line of works have applied diffusion models <ref type="bibr" target="#b28">(Sohl-Dickstein et al., 2015)</ref>, shaping it as an iterative denoising task <ref type="bibr" target="#b8">(Ho et al., 2020;</ref><ref type="bibr" target="#b29">Song et al., 2021a)</ref>. By adding text condition in the denoising steps, practices such as LDM <ref type="bibr" target="#b24">(Rombach et al., 2021)</ref>, DALL-E 2 , and Imagen <ref type="bibr">(Saharia et al., 2022)</ref> constantly set new records in text-to-image generation. Based on diffusion models as the backbone, ERNIE-ViLG 2.0 proposes incorporating knowledge of scene and mixture-of-denoising-experts mechanism into the denoising process.</p><p>Knowledge-Enhanced Pre-trained Transformers. While large-sized transformers have benefited from pre-training on large-scale data, researchers have been adding knowledge to the transformers, guiding them to focus on key elements during learning. For language-only transformers, knowledgeenhanced models used knowledge masking strategy <ref type="bibr" target="#b32">(Sun et al., 2019;</ref><ref type="bibr" target="#b10">Joshi et al., 2020)</ref> or knowledgeaware pre-training tasks <ref type="bibr" target="#b33">(Sun et al., 2020</ref> to understand the language data distribution. As for vision-language multi-modal discrimination models, OSCAR , ERNIE-ViL  and ERNIE-Layout  leveraged object tags, scene graphs, and document layouts as extra knowledge to help the models better align language and vision modalities, respectively. Among multi-modal generation models, Make-A-Scene <ref type="bibr" target="#b5">(Gafni et al., 2022)</ref> emphasized the importance of object and face regions by integrating domain-specific perceptual knowledge. While current text-to-image diffusion models suffer from attribute misalignment problems , they have not employed any specific knowledge of objects. Herein, ERNIE-ViLG 2.0 utilizes the knowledge of key elements in images and text to enhance diffusion models, leading to better fine-grained image-text alignment in generated pictures.</p><p>Mixture-of-Expert. Mixture-of-Experts (MoE) in neural networks means dividing specific parts of the parameters into subsets, each of which is called an expert <ref type="bibr" target="#b3">Fedus et al., 2022)</ref>. During the forward pass, a router assigns experts to different input, and each input only interacts with the experts assigned to. The advantages of MoE lie in two aspects. First, since the router would assign the same experts to examples that share common characteristics, each expert can focus on a subset of examples of similar types or domains. Second, since only part of the parameters are activated (a) ??????</p><formula xml:id="formula_13">(b) ????????20??? (c) ??????? (d) ???? A hanging ornament with "?"</formula><p>in Chinese fashion A birthday cake with candles of "20" on it</p><p>Happy Chinese year of tiger in Chinese fashion Chinese painting of grapes <ref type="figure">Figure 9</ref>: Examples of character rendering. The model successfully renders characters that appear explicitly in the prompt, while for more difficult cases, the model only learns the position for now.</p><p>for each example, the amount of computation is relatively lightweight compared to activating all the parameters. Thus, MoE enables efficient training and inference with large-sized models. The implementation of the router is a critical part of MoE. In language tasks, the most common strategy is a matching algorithm that assigns each text token to several experts in the linear feed-forward layer <ref type="bibr" target="#b11">(Lepikhin et al., 2021;</ref><ref type="bibr" target="#b4">Fedus et al., 2021)</ref>. The algorithm could be dynamic and involves a trainable module to calculate the assignment, or it could be fixed and distribute experts to tokens or sequences according to their hash ID <ref type="bibr" target="#b23">(Roller et al., 2021)</ref> or domain . While most practices formulate multiple experts in only the linear layers, some works also use an entire language model as an expert . Beyond the natural language processing tasks, the idea of MoE have also been applied to vision models <ref type="bibr" target="#b19">(Puigcerver et al., 2021)</ref> and Mixture-of-Modality-Expert in multi-modal transformers <ref type="bibr">(Wang et al., , 2022b</ref><ref type="bibr" target="#b16">Mustafa et al., 2022)</ref>. In ERNIE-ViLG 2.0, the Mixture-of-Denoising-Experts mechanism takes multiple denoising U-Nets as experts. It uses the denoising step index as the fixed router to determine which expert to use. With multiple experts, we can exploit the advantage of MoE to make each expert focus on a group of denoising steps and scale up the model size without increasing inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Risks, Limitations, and Future Work</head><p>Model Usage and Data Bias. Text-to-image generation models trained by large-scale image-text data have all faced similar risks regarding to inappropriate usage of generation and data bias <ref type="bibr" target="#b24">(Rombach et al., 2021;</ref><ref type="bibr">Saharia et al., 2022)</ref>. Considering that text-to-image models help people realize their imagination with less effort, the malicious use of models may result in unexpected deceptive or harmful outcomes. Moreover, since the models are trained on datasets consisting of images and their alt-text crawled from websites, the generated images may exhibit social and cultural bias in the datasets and websites.</p><p>Character Rendering. Previous English text-to-image diffusion models with large-scale parameters have reported successful generation of common English words <ref type="bibr">Saharia et al., 2022)</ref>. When the models' parameters get smaller or the target words are less common, the models might produce an incorrect combination of characters or even wrongly written letters . For ERNIE-ViLG 2.0 in Chinese, character rendering remains challenging for two reasons. First, the training data contains both Chinese text-image pairs and English text-image pairs translated into Chinese. When a text prompt mentions characters, the characters in the image could be in Chinese or English, and it is hard for the model to learn the corresponding characters in both languages simultaneously. In the cases of successful character rendering that we observed, the characters are either words that are common in Chinese and do not have an exact match in English, such as "?" ("blessing, happiness, good luck" in English) in <ref type="figure">Figure 9</ref>(a), or numbers which are the same in English and Chinese images, such as "20" in <ref type="figure">Figure 9(b)</ref>. The second reason that makes character rendering difficult is probably that Chinese characters are complex combinations of strokes without basic components like English letters. The model does learn the appropriate locations for writing Chinese words, such as the banner in <ref type="figure">Figure 9</ref>(c) and the inscription in the top right corner of <ref type="figure">Figure 9(d)</ref>, but it only paints meaningless strokes in such places.</p><p>Variation of Mixture-of-Denoising-Experts. Section 3.3 shows that using more denoising experts leads to better model performance. It indicates that using parallel U-Net experts is an effective way to augment the denoising network. Due to the computation limitation, we only try using up to 10 experts in this work, while we believe that exploring more denoising experts and multiple text encoders as experts is a meaningful future direction. In this way, we can further scale up the model and allow it to learn the data distribution better with similar inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We present ERNIE-ViLG 2.0, the first Chinese large-scale text-to-image generation model based on diffusion models. To improve the fine-grained control of scene semantics, we incorporate visual and textual knowledge of the scene into diffusion models. To disentangle the model parameters for different denoising timesteps, we introduce MoDE in the denoising U-Net and scale up the model parameters to 24B with a relatively short inference time. Experiments show that our model achieves state-of-the-art on MS-COCO and that each proposed mechanism contributes to the final results. To allow fair comparisons between Chinese and English text-to-image models, we collect a bilingual prompt set ViLG-300. Human evaluation on ViLG-300 indicates that our model is preferred over baseline models in both text relevancy and image fidelity. Further analysis suggests that different knowledge sources improve the generation in different aspects, and that using more experts results in better image quality. In the future, we intend to enrich external image-text alignment knowledge and expand the usage of multiple experts to advance the generation further. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Detailed Human Evaluation</head><p>In this section, we supplement the part about human evaluation omitted in the main content, including the construction process of ViLG-300, the performance comparison on various categories, and the example qualitative comparison of different models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 The Construction of ViLG-300</head><p>While constructing ViLG-300, we remove language-related text prompts in DrawBench <ref type="bibr">(Saharia et al., 2022</ref>) since these are not comparable inputs for models in different languages. For text rendering in Chinese, we also have discussed in detail in Section 5. We also remove the MS-COCO category in ERNIE-ViLG <ref type="bibr" target="#b43">(Zhang et al., 2021b)</ref>, because MS-COCO has been used in the automatic evaluation, and the prompts are relatively simple for current text-to-image models, especially when evaluating the models' ability to understand complex scene. Note that there are two similar categories (i.e., Conflicting and Counterfactual) in DrawBench and ERNIE-ViLG that we do not align and merge. The reason is that the Conflicting category focuses on the impossible combination of things, while Counterfactual contains many prompts with negative descriptions, both of which are now difficult problems.</p><p>A.2 Detailed Results on ViLG-300 <ref type="figure">Figure 10</ref> shows the detailed performance comparison between ERNIE-ViLG 2.0 and DALL-E 2/Stable Diffusion on ViLG-300, and example qualitative comparisons are shown in <ref type="figure">Figure 11</ref> and 12. The most important conclusion is that ERNIE-ViLG 2.0 is quite skilled in dealing with text prompts with colors and complex scenes, and also has impressive performance in many categories, such as Geography, Scene, and Cartoon. Intuitively, we attribute the excellent performance to the knowledge injection that endows the model with the ability to perceive and understand various named entities and detailed descriptions, as well as the increase in the number of parameters brought by the mixture-of-denoising-experts strategies also makes the model even more powerful. At the same time, we also propose that further understanding of the number of objects and the relationship between them can be the focus of future text-to-image models.  <ref type="figure" target="#fig_2">Figure 13</ref> compares the image details of ERNIE-ViLG 2.0 and baseline models by zooming in small regions of the generated images. Technically, both ERNIE-ViLG 2.0 and Stable Diffusion generate image latent representation with diffusion models conditioned on text. While Stable Diffusion only produces 512?512 sized images, ERNIE-ViLG 2.0 could directly output images with 1024?1024 resolution. Therefore, the magnified parts of ERNIE-ViLG 2.0 are clearer than those of Stable Diffusion. As for DALL-E 2, it employs cascaded generation by first producing 64?64 images with text and then scaling it up to 1024?1024 resolution with two super-resolution models. Although it generates images of the same resolution as ERNIE-ViLG 2.0, the output of DALL-E 2 sometimes contains unnatural textures, such as the fluffy trees and rain drops in the magnified regions. Contrary to DALL-E 2, the textures of our model's outcome are more natural and photorealistic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Comparison of Image Quality</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The illustration of ERNIE-ViLG 2.0 model architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of ERNIE-ViLG 2.0 and DALL-E 2/Stable Diffusion on ViLG-300 with human evaluation. We report the user preference rates with 95% confidence intervals. Mixture-of-denoising-experts strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>The visualization of cross-attention maps in different denoising timesteps, where each value in the image space is the average of attention from this image token to all text tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :Figure 11 :Figure 12 :??</head><label>101112</label><figDesc>Detailed comparison of ERNIE-ViLG 2.0 and DALL-E 2/Stable Diffusion on ViLG-300 with human evaluation. We do not apply any filtering strategy and report the initial results here. Example qualitative comparisons between ERNIE-ViLG 2.0 and DALL-E 2/Stable Diffusion on DrawBench (Saharia et al., 2022) prompts from ViLG-300. Example qualitative comparisons between ERNIE-ViLG 2.0 and DALL-E 2/Stable Diffusion on ERNIE-ViLG (Zhang et al., 2021b) prompts from ViLG-300. ?? ?? ?? ??????? Green mountains, green waters, boats, scenery A puppy dancing in the rain Figure 13: Comparison of image quality by magnifying parts of images. ERNIE-ViLG 2.0 enables the generation of 1024?1024 images, with sharper and more natural details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of ERNIE-ViLG 2.0 and representative text-to-image generation models on MS-COCO 256 ? 256 with zero-shot FID-30k. We use classifier-free guidance scale 2.1 for our diffusion model and achieve the best performance.</figDesc><table><row><cell>Model</cell><cell>Zero-Shot FID-30k ?</cell></row><row><cell>DALL-E (Ramesh et al., 2021)</cell><cell>27.50</cell></row><row><cell>CogView</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Detailed categories and statistics of ViLG-300.</figDesc><table><row><cell>Source</cell><cell>Category</cell><cell>Description</cell><cell>Number</cell></row><row><cell></cell><cell>Color</cell><cell>objects with specified colors</cell><cell>22</cell></row><row><cell></cell><cell>Counting</cell><cell>objects with specified numbers</cell><cell>18</cell></row><row><cell></cell><cell>Positional</cell><cell>objects with specified spatial positioning</cell><cell>16</cell></row><row><cell>DrawBench</cell><cell>Conflicting Description</cell><cell>objects with conflicting interactions complex and long prompts describing an objects</cell><cell>10 20</cell></row><row><cell></cell><cell>DALL-E case</cell><cell>prompts from DALL-E (Ramesh et al., 2021)</cell><cell>19</cell></row><row><cell></cell><cell>Marcus</cell><cell>prompts from Marcus et al. (2022)</cell><cell>9</cell></row><row><cell></cell><cell>Reddit</cell><cell>prompts from DALL-E 2 Reddit</cell><cell>36</cell></row><row><cell></cell><cell>Simple</cell><cell>single-object with specified attributes</cell><cell>18</cell></row><row><cell></cell><cell>Complex</cell><cell>multi-objects with specified attributes and relationships</cell><cell>23</cell></row><row><cell></cell><cell cols="2">Counterfactual objects with impossible interactions or negative words</cell><cell>23</cell></row><row><cell>ERNIE-ViLG</cell><cell>Geography View</cell><cell>specific geographic entities objects with specified view angles</cell><cell>24 16</cell></row><row><cell></cell><cell>Scene</cell><cell>objects with specified time and scenes</cell><cell>14</cell></row><row><cell></cell><cell>Style</cell><cell>objects with specified styles</cell><cell>16</cell></row><row><cell></cell><cell>Cartoon</cell><cell>anthropomorphic animals or cartoon characters</cell><cell>16</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The keywords is defined as notional words in modern Chinese (i.e., nouns, verbs, adjectives, numerals, quantifiers, and pronouns).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://fanyi.baidu.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://openai.com/dall-e-2/ 5 https://beta.dreamstudio.ai/dream</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Here we show the impact of design strategies with lightweight settings and a small amount of training samples, the metrics and cases cannot be regarded as the final performance of ERNIE-ViLG 2.0.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00636</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cogview: Mastering text-to-image generation via transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021</title>
		<imprint>
			<date type="published" when="2021-12-06" />
			<biblScope unit="page" from="19822" to="19835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Cogview2: Faster and better text-toimage generation via hierarchical transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2204.14217</idno>
		<idno>abs/2204.14217</idno>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A review of sparse expert models in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2209.01667</idno>
		<idno>abs/2209.01667</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno>abs/2101.03961</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Make-a-scene: Scene-based text-to-image generation with human priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oran</forename><surname>Gafni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oron</forename><surname>Ashual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shelly</forename><surname>Sheynin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2203.13131</idno>
		<idno>abs/2203.13131</idno>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Demix layers: Disentangling domains for modular language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.407</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022<address><addrLine>Seattle, WA, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-07-10" />
			<biblScope unit="page" from="5557" to="5576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06" />
		</imprint>
	</monogr>
	<note>virtual</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Classifier-free diffusion guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00300</idno>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gshard: Scaling giant models with conditional computation and automatic sharding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-05-03" />
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Branch-train-merge: Embarrassingly parallel training of expert language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Althoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2208.03306</idno>
		<idno>abs/2208.03306</idno>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Oscar: Object-semantics aligned pretraining for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58577-8_8</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-08-23" />
			<biblScope unit="volume">12375</biblScope>
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XXX</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
	<note type="report_type">OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2204.13807</idno>
		<imprint/>
	</monogr>
	<note>and Scott Aaronson. 2022. A very preliminary analysis of DALL-E 2. CoRR, abs/2204.13807</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Multimodal contrastive learning with limoe: the language-image mixture of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2206.02770</idno>
		<idno>abs/2206.02770</idno>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">GLIDE: towards photorealistic image generation and editing with text-guided diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Quinn Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2022</title>
		<meeting><address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022-07" />
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="16784" to="16804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Ernie-layout: Layout knowledge enhanced pre-training for visually-rich document understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinxu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2210.06155</idno>
		<idno>abs/2210.06155</idno>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scalable transfer learning with expert models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">Riquelme</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?dric</forename><surname>Renggli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr? Susano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-05-03" />
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-07-24" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Hierarchical text-conditional image generation with CLIP latents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2204.06125</idno>
		<idno>abs/2204.06125</idno>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-07-24" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="8821" to="8831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hash layers for large sparse models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021</title>
		<imprint>
			<date type="published" when="2021-12-06" />
			<biblScope unit="page" from="17555" to="17566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
		<idno>abs/2112.10752</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Photorealistic text-to-image diffusion models with deep language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2205.11487</idno>
		<idno>abs/2205.11487</idno>
		<editor>Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. 2022</editor>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Seyed Kamyar Seyed Ghasemipour</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">LAION-400M: open dataset of clip-filtered 400 million image-text pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Kaczmarczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><surname>Mullis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aarush</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenia</forename><surname>Jitsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aran</forename><surname>Komatsuzaki</surname></persName>
		</author>
		<idno>abs/2111.02114</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-11" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
	<note>of JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-05-03" />
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Score-based generative modeling through stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-05-03" />
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Large-scale knowledge enhanced pre-training for language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weibao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhou</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/2107.02137</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">ERNIE: enhanced representation through knowledge integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danxiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno>abs/1904.09223</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">ERNIE 2.0: A continual pre-training framework for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8968" to="8975" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">DF-GAN: A simple and effective baseline for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyuan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing-Kun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR52688.2022.01602</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-06-18" />
			<biblScope unit="page" from="16494" to="16504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">OFA: unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2022</title>
		<meeting><address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022-07-23" />
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="23318" to="23340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Image as a foreign language: Beit pretraining for all vision and vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bjorck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiliang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kriti</forename><surname>Aggarwal</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2208.10442</idno>
		<idno>abs/2208.10442</idno>
		<editor>Owais Khan Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei. 2022b</editor>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Vlmo: Unified vision-language pretraining with mixture-of-modality-experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno>abs/2111.02358</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attngan: Fine-grained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00143</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="1316" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ernie-vil: Knowledge enhanced vision-language representations through scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiji</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021-02-02" />
			<biblScope unit="page" from="3208" to="3216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Scaling autoregressive models for content-rich text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yu Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunjan</forename><surname>Baid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Burcu Karagol Ayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2206.10789</idno>
		<ptr target="CoRR,abs/2206.10789" />
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cross-modal contrastive learning for text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yu Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR46437.2021.00089</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual</title>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2021-06-19" />
			<biblScope unit="page" from="833" to="842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Ernie-vilg: Unified generative pre-training for bidirectional vision-language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yewei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqiang</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno>abs/2112.15283</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">LAFITE: towards language-free training for text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Tensmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/2111.13792</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">DM-GAN: dynamic memory generative adversarial networks for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minfeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingbo</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00595</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="5802" to="5810" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
