<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recursively Refined R-CNN: Instance Segmentation with Self-RoI Rebalancing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Rossi</surname></persName>
							<email>leonardo.rossi@unipr.itimplab.ce.unipr.it</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IMP Lab -D.I.A</orgName>
								<orgName type="institution">University of Parma</orgName>
								<address>
									<settlement>Parma</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recursively Refined R-CNN: Instance Segmentation with Self-RoI Rebalancing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Instance Segmentation ? Object Detection ? RoI Rebalancing ? Deep Learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Within the field of instance segmentation, most of the state-of-the-art deep learning networks rely nowadays on cascade architectures [1], where multiple object detectors are trained sequentially, re-sampling the ground truth at each step. This offers a solution to the problem of exponentially vanishing positive samples. However, it also translates into an increase in network complexity in terms of the number of parameters. To address this issue, we propose Recursively Refined R-CNN (R 3 -CNN) which avoids duplicates by introducing a loop mechanism instead. At the same time, it achieves a quality boost using a recursive re-sampling technique, where a specific IoU quality is utilized in each recursion to eventually equally cover the positive spectrum. Our experiments highlight the specific encoding of the loop mechanism in the weights, requiring its usage at inference time. The R 3 -CNN architecture is able to surpass the recently proposed HTC [4] model, while reducing the number of parameters significantly. Experiments on COCO minival 2017 dataset show performance boost independently from the utilized baseline model. The code is available online at https: //github.com/IMPLabUniPr/mmdetection/tree/r3_cnn.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Computer vision is a field of continuous experimentation, where new and better performing algorithms are developed every day and are able to operate in environments with increasingly extreme conditions. In particular, object detection, and instance segmentation as its narrower extension, offers complex challenges which are utilized in various applications, including medical diagnostics <ref type="bibr" target="#b2">[3]</ref>, autonomous driving <ref type="bibr" target="#b10">[11]</ref>, visual product search <ref type="bibr" target="#b13">[14]</ref>, and many others. All these applications demand high-performing systems in terms of prediction quality, as well as low memory usage. Therefore, a desirable architecture is as light as possible regarding the parameter count since it reduces the search space and enhances generalization, while retaining high-quality detection and segmentation performance.</p><p>However, often these two goals are conflicting. The R 3 -CNN architecture and the corresponding training mechanism that we propose present a trade-off between these two conflicting goals. We show that our model is able to obtain the same performance of complex networks (such as HTC <ref type="bibr" target="#b3">[4]</ref>) with a network as light as Mask R-CNN <ref type="bibr" target="#b8">[9]</ref>.</p><p>The accuracy of instance segmentation systems is strongly based on the concept of intersection over union <ref type="bibr">(IoU)</ref>, which is used to identify the detection precision with respect to the ground truth. The higher this value is, the more accurate and the less noisy the predictions are. However, by increasing the IoU threshold, a problem called exponentially vanishing positive samples <ref type="bibr" target="#b0">[1]</ref> (EVPS) is also introduced, meaning that it can give rise to the problem of good proposals scarcity compared to low-quality ones. This usually leads to a training that is excessively biased towards low-quality predictions. In order to solve this issue, Cascade R-CNN <ref type="bibr" target="#b0">[1]</ref> first, and its descendant HTC later, introduced a cascade mechanism where multiple object detectors are trained sequentially in order to take advantage of the previous one and to increase the prediction quality gradually. This means that each stage performs two tasks: first, the detector is training itself, and, then, it is also devoted to identifying the region proposals for subsequent stages. Unfortunately, this also translates into an increase in network complexity in terms of the number of parameters.</p><p>In this work, we propose a new way to balance positive samples by exploiting the re-sampling technique, introduced by the cascade models. Our proposed technique generates new proposals with a pre-selected IoU quality in order to equally cover all IoU values. We carry out an extensive ablation study and compare our results with the state of the art in order to demonstrate the advantages of the proposed solution and its applicability to different existing architectures.</p><p>The main contributions of this paper are the following:</p><p>-An effective solution to deal with the EVPS problem with a single-detector model, rebalancing the proposals with respect to the IoU thresholds through a recursive re-sampling mechanism. This mechanism has the goal of eventually feeding the network with an equal distribution of samples. -An exhaustive ablation study on all the components of our R 3 -CNN architecture in order to evaluate how the performance is affected by each component. -Our R 3 -CNN is introduced into major state-of-the-art models to demonstrate that it boosts the performance independently from the baseline model used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Multi-stage Detection/Instance Segmentation. The early works on object detection and instance segmentation were based on the assumption that single-stage end-to-end networks are sufficient to recognize and segment the objects. For instance, YOLO network <ref type="bibr" target="#b18">[19]</ref> optimizes localization and classification in one step. Starting with the R-CNN network <ref type="bibr" target="#b7">[8]</ref>, the idea of a two-stage architecture was introduced, where, in the first stage, a network called RPN (Region Proposal Network) analyzes the whole image and identifies the regions where the probability of finding an object is high. In the second stage, another network performs a more refined analysis on each single region. After this seminal work, others have further refined this idea. The Cascade R-CNN architecture <ref type="bibr" target="#b0">[1]</ref> uses multiple bounding-box heads connected sequentially, where each one refines the proposals produced by the previous one. The minimum IoU required for positive examples is increased at each stage, taking into account a different set of proposals.</p><p>Other studies <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25]</ref> introduced a similar cascade concept, but applied to the RPN network, where multiple RPNs are sequenced and the results from the previous stage are fed into the next stage. Our work is inspired by HTC network <ref type="bibr" target="#b3">[4]</ref>, which introduces a particular cascade operation also on the mask extraction modules. However, all these multi-stage networks are quite complex in terms of the number of parameters. IoU distribution imbalance. Authors in <ref type="bibr" target="#b16">[17]</ref> describe the problem as a skewed IoU distribution observed in bounding boxes used in training and evaluation. In <ref type="bibr" target="#b20">[21]</ref>, the authors highlight the significant imbalance between background and foreground RoI examples and present a hard example mining algorithm to easily select the most significant ones. While in their case the aim is balancing the background (negative) and the foreground (positive) RoIs, in our work the primary goal is to balance RoIs across the entire positive spectrum of the IoU. In <ref type="bibr" target="#b17">[18]</ref>, an IoU-balanced sampling technique is proposed to mine hard examples. However, the sampling always takes place on the results of the RPN which, as we will see, is not very optimized to provide high-quality RoIs. In our case, we apply re-sampling to the detector itself, which has, on average, a much higher probability of returning more significant RoIs. In <ref type="bibr" target="#b5">[6]</ref>, the sources of false positives are analyzed and an extra independent classification head is introduced to be plugged into the original architecture to reduce hard false positives. In <ref type="bibr" target="#b25">[26]</ref>, the authors introduce a new IoU-prediction branch which supports classification and localization. They propose to manually generate samples around ground truths instead of using RPN for localization and IoU prediction branches in training. In <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>, overfitting due to EVPS problem for large thresholds is addressed using multiple detectors connected sequentially. They re-sample the ground truth in a sequential manner to progressively improve hypothesis quality. Unlike them, we tackle the problem with a single detector and a single segmentation head. In <ref type="bibr" target="#b15">[16]</ref>, they offer an interpretation similar to ours about the fact that IoU imbalance has an adverse effect on performance. However, while they use an algorithm to systematically generate the RoIs with the chosen quality, we rely only on the capabilities of the detector itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Recursively Refined R-CNN</head><p>In this section, first we briefly introduce the idea behind multi-stage processing. Then we describe our R 3 -CNN architecture with its evolution from a sequential to a recursive pipeline, which offers a change of perspective on training. As shown in <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>, the HTC (Hybrid Task Cascade) multi-stage architecture <ref type="bibr" target="#b3">[4]</ref> mainly follows the idea that a single detector is unlikely to be able to train uniformly on all quality levels of IoU. The cascade architecture tries to solve the EVPS problem by training multiple regressors connected sequentially, each of which is specialized in a predetermined and growing IoU minimum threshold. Each regressor performs a conversion of its localization results into a new list of proposals for the following regressor. Although this type of architecture clearly improves the overall performance, it also introduces a considerable number of new parameters into the network. In fact, with respect to its predecessor Mask R-CNN, the number of detection and segmenta- tion modules triples. To reduce the complexity of cascade networks and to address the EVPS problem, we design a lighter architecture with single detection and mask heads uniformly trained on all IoU levels. Authors in <ref type="bibr" target="#b0">[1]</ref> underlined the cost-sensitive learning problem <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15]</ref>, where the optimization of different IoU thresholds requires various loss functions. Inspired by this study, we address the problem using multiple selective training, which focuses on a specific IoU quality in each step and recursively feeds them into the detector. The intuition is that the detector training and its ability to return an adequate number of proposals of a certain quality level will happen at the same time.</p><p>In <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>, the new R 3 -CNN architecture along with our training paradigm are shown. In this loop (recursive) architecture, the detector and the RoI pooling modules are connected in a cycle. As in HTC, the first set of RoI proposals is provided by the RPN. After that, the RoI pooler crops and converts them to fixed-size feature maps, which are used to train the B1 block. Then, with an appropriate IoU threshold, the ground truth re-sampling takes place by the B1 block to generate a new proposal set. The result is then used both in the segmentation module M1 and as the new input for the pooler which closes the loop. By the IoU threshold manipulation, the network can force the detection to extract those RoIs with IoU quality levels which are typically missed. The cycle continues three times (3x loop) to guarantee the rebalancing of RoI levels. <ref type="figure" target="#fig_1">Fig. 2</ref> (a) shows the generated RoI distribution for each IoU level in Mask R-CNN as well as the EVPS problem. The distribution of the rebalanced samples by our model, on the other hand, can be seen in <ref type="figure" target="#fig_1">Fig. 2 (b)</ref> and (c). For the latter, it is worth emphasizing some important details emerging from these graphs: (i) Considering only the first loop trend, R 3 -CNN looks quite similar to Mask R-CNN; (ii) Conversely, considering the sum of the first two loops, our distribution looks much more balanced; (iii) The third loop significantly increases the number of high-quality RoIs. Despite the fact that our architecture contains a single detector, its behavior shows a unique and well-defined trend in terms of RoI distribution within different loops. We believe this is the reason why R 3 -CNN outperforms Mask R-CNN. It is able to mimic the Cascade R-CNN behavior in the RoI distribution (as also shown in <ref type="bibr" target="#b0">[1]</ref>), achieved by HTC, but using only a single detector and significantly fewer parameters.</p><p>For a given loop t, let us define h as the sole classifier and f as the sole regressor which is trained for a selected IoU threshold u t , with u t &gt; u t?1 , by minimizing the loss function of Cascade R-CNN <ref type="bibr" target="#b0">[1]</ref>: where x t represents the input features of the t-th loop, b t = f x t?1 , b t?1 is the new sampled set of proposals coming from the previous loop (with b 0 coming from the RPN), g is the ground truth, and ? is a positive coefficient. y t represents the label of x t given the IoU threshold u t , the proposals b t and the ground truth label g y with the following equation:</p><formula xml:id="formula_0">L(x t , g) = L cls h x t , y t + ? y t 1 L loc f x t , b t , g<label>(1)</label></formula><formula xml:id="formula_1">y t = g y if IoU (b t , g) u t 0 otherwise<label>(2)</label></formula><p>At inference time, the same loop procedure is applied and all the predictions are merged together by computing the mean of the classification values. As it will be shown in the experiments, using loops also at inference (or evaluation) time is not optional, meaning that the loop mechanism is intrinsic to the weights of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Evaluation Metrics</head><p>Dataset. As the majority of recent literature on instance segmentation, we perform our tests on the MS COCO 2017 dataset <ref type="bibr" target="#b12">[13]</ref>. The training dataset consists of more than 117,000 images and 80 different classes of objects. Evaluation Metrics. We used the same evaluation functions offered by the python pycocotools software package. All the evaluation phases have been performed on the COCO minival 2017 validation dataset, which contains 5000 images. We report the Average Precision (AP) with different IoU thresholds for both bounding box and segmentation tasks. The main metric (AP ) is computed with IoUs from 0.5 to 0.95. Others include AP 50 and AP 75 with 0.5 and 0.75 minimum IoU thresholds, and AP s , AP m and AP l for small, medium and large objects, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation details</head><p>To perform a fair comparison, we obtain all the reported results by training the networks with the same hardware and, when possible, the same software configuration. When available, the original code released by the authors or the corresponding implementation in MMDetection <ref type="bibr" target="#b4">[5]</ref> framework were used. Our code is also developed within this framework. In the case of HTC, we did not consider the semantic segmentation branch.</p><p>We performed a distributed training on 2 servers, each equipped with 2x16 IBM POWER9 cores, 256 GB of memory and 4 x NVIDIA Volta V100 GPUs with Nvlink 2.0 and 16GB of memory. Each training consists of 12 epochs with Stochastic Gradient Descent (SGD) optimization algorithm, an initial learning rate of 0.02, a weight decay of 0.0001, and a momentum of 0.9. The learning rate decays at epochs 8 and 11. We used batch size of 2 for each GPU. We fixed the long edge and short edge of the images to 1333 and 800, maintaining the aspect ratio. ResNet 50 <ref type="bibr" target="#b9">[10]</ref> was used as the backbone. If not specified differently, the number of loops in training and evaluation are the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis of R 3 -CNN</head><p>Description. In this part, we demonstrate the potentiality offered by a naive three-stage loop compared to Mask R-CNN and the original three-stage cascade HTC. To have a fair comparison, we select the optimal configuration for the HTC network as baseline and also apply it to training our R 3 -CNN. In the advanced version, we replace fully-connected layers from detection head with lightweight convolutions with kernel 7 ? 7 and a Non-Local block <ref type="bibr" target="#b23">[24]</ref> with incremented kernel size of 7 ? 7 to better exploit information. We also build a brand new branch using only convolutions and Non-Local blocks to include a new learning task to improve segmentation as described in <ref type="bibr" target="#b11">[12]</ref>. Since our naive version has slightly fewer parameters than Mask R-CNN, it is also insightful to compare it with our model. Finally, we also want to demonstrate the following important claim: it does not matter the way or order with which the IoU thresholds are changed (either incrementally or decrementally), since in both cases a more balanced IoU distribution is achieved (see <ref type="figure" target="#fig_1">Figure 2</ref>). Results. In <ref type="table" target="#tab_0">Table 1</ref>, we report speed in evaluation and memory usage in training, distinguishing between memory usage of the entire training process and model size (proportional to the number of parameters). Comparing the naive version (row #4) with HTC (row #3), it can be seen that our model has significantly fewer parameters and is more memory efficient. While the segmentation precision (S AP ) is practically the same, there is a slight loss in B AP . Also, the speed of naive is slightly better than HTC. Regarding the advanced version (row #6), it surpasses the HTC accuracy in both tasks, while saving a significant number of parameters and using the same amount of memory in training. The only disadvantage is the reduced speed due to Non-Local blocks.</p><p>Compared to Mask R-CNN (row #1), the naive R 3 -CNN has the same complexity, but achieves a much higher precision in both tasks. To further investigate how well our recursive mechanism works, we also compare it with Mask R-CNN trained with triple number of epochs (row #2). While more training helps Mask R-CNN produce a higher precision, it is still outperformed by naive R 3 -CNN. This demonstrates that our loop mechanism is not simply another way of training the network for more epochs, but that it represents a different and more effective training strategy. This can be explained by the fact that while in Mask R-CNN the RoI proposals are always provided by the RPN, in our case they are provided by the detection head which generates higher quality and more balanced RoIs (see <ref type="figure" target="#fig_1">Figure 2</ref>). Finally, to show that the order of changes in IoU threshold is not crucial to performance, in rows #4 and #5, we report a comparison between increasing and decreasing IoU thresholds through loops. Although there is a slight degradation of precision using the decreasing training strategy, it is almost negligible due to a more balanced IoU distribution achieved in both cases, but skewed to high-quality RoIs in the first case and low-quality in the latter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation study on the evaluation phase</head><p>Description. In this subsection we focus on how the results are affected by the number of cycles in the evaluation phase. We consider the naive version mentioned above as pre-trained model, which consists of three loops in the training and evaluation phases. Results. From <ref type="table">Table 2</ref>, it is evident that the loop mechanism is of paramount importance for the evaluation phase too. In fact, when we train the network with the 3x loops and then evaluate it with one loop, it performs even worse than Mask R-CNN (row #2). On the contrary, with two loops, the result is significantly better (row #3). It also underperforms the three-loop evaluation only slightly (row #4). Therefore, this version could be considered a good compromise between execution time and detection quality. From four loops onward, the performance tends to remain almost stable. This is consistent with our initial hypothesis of a link between evaluation and the loop mechanism, and confirms that in order to have higher performance with more than three loops in the evaluation, we also need to increase the number of loops in the training phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation study on the training phase</head><p>Description. In this experiment, the network is trained with a number of loops varying from 1 to 4. The number of loops for the evaluation changes accordingly. Results. The comparative results are reported in <ref type="table">Table 3</ref>. The precision of the single loop (row #2) is comparable to Mask R-CNN, and not much different from the abovementioned model with one-loop evaluation (row #2 of <ref type="table">Table 2</ref>). This connection with the previous experiment suggests that the detector is strictly optimized on the corresponding sample distribution. The performance is improved by the training strategies with two and three loops, though significantly by the former and only slightly over that by the latter. Regarding more than 3 loops (row #5), the improvement is negligible.  <ref type="table" target="#tab_0">Table 1</ref> row #6) and renamed R 3 -CNN-L model. The experiment tested different state-of-theart models, namely GRoIE <ref type="bibr" target="#b19">[20]</ref>, GC-net <ref type="bibr" target="#b1">[2]</ref>, DCN <ref type="bibr" target="#b26">[27]</ref>, with and without the R 3 -CNN-L version. When compatible, we also merged the original model with HTC as baseline. For example, HTC+GC-Net is composed of both HTC and GC-Net merged together.</p><p>Results. The results are summarized in <ref type="table" target="#tab_2">Table 4</ref>, where best results for each comparison are reported in bold, while the second best is in red. From the table we can see that almost all the best and second-best scores belong to R 3 -CNN architectures, both in object detection and instance segmentation. However, there are two cases in which this does not happen. The first one is the combination of GC-Net and HTC which outperforms R 3 -CNN-L in AP m by 0.6% (row #7), and the other one is the combination of DCN and HTC which outperforms R 3 -CNN-L in AP l by the same amount (row #10). Apart from these rare cases, these experiments confirm that the proposed R 3 -CNN consistently brings benefits to existing object detection and instance segmentation models, in terms of both precision and reduced number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we introduced the R 3 -CNN architecture to address the issue of exponentially vanishing positive samples in training by rebalancing the training proposals with respect to the IoU thresholds, through a recursive re-sampling mechanism in a single detector architecture. We demonstrated that a good training needs to take into account the diversity of IoU quality of the RoIs used to learn, more than aiming to have only high quality RoIs. Our extensive set of experiments and ablation studies provide a comprehensive understanding of the benefits and limitations of the proposed models.  demonstrates its usefulness when used in conjunction with several state-of-the-art models, achieving considerable improvements over the existing models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Network design. (a) HTC: a multi-stage network which trains each head in a cascade fashion. (b) R 3 -CNN: our architecture which introduces a loop mechanism to self-train the heads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The IoU histogram of training samples for Mask R-CNN with a 3x schedule (36 epochs) (a), and R 3 -CNN where each loop uses different IoU thresholds [0.5, 0.6, 0.7], decreasingly (b) and increasingly (c). Better seen in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>R</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparing trainable parameters with the bounding box and segmentation average precision. K: thousand. Column Lt: number of stages. Speed is image per second. TS: Training strategies. Inc: progressively increasing and Dec: decreasing IoU quality through loops.</figDesc><table><row><cell># Model</cell><cell cols="4"># Params TS Lt H BAP SAP Speed Mem. usage Model size</cell></row><row><cell>1 Mask (1x)</cell><cell>44,170 K -</cell><cell>1 1 38.2 34.7 11.5</cell><cell>4.4 GB</cell><cell>339 MB</cell></row><row><cell>2 Mask (3x)</cell><cell>44,170 K -</cell><cell>1 1 39.2 35.5 5.4</cell><cell>4.4 GB</cell><cell>339 MB</cell></row><row><cell>3 HTC</cell><cell cols="2">77,230 K Inc 3 3 41.7 36.9 5.4</cell><cell>6.8 GB</cell><cell>591 MB</cell></row><row><cell>4 R 3 -CNN (naive)</cell><cell cols="2">43,912 K Inc 3 1 40.9 36.8 5.5</cell><cell>5.9 GB</cell><cell>337 MB</cell></row><row><cell>5 R 3 -CNN (naive)</cell><cell cols="2">43,912 K Dec 3 1 40.4 36.7 5.5</cell><cell>5.9 GB</cell><cell>337 MB</cell></row><row><cell cols="3">6 R 3 -CNN (advanced) 50,072 K Inc 3 1 42.0 38.2 1.0</cell><cell>6.8 GB</cell><cell>384 MB</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Impact of evaluation loops Le in a 3loop and one-head-per-type R 3 -CNN model. Row #4 is the naive R 3 -CNN in Table 1. Impact of the number of training loops in a one-head-per-type R 3 -CNN model. Row #4 is the naive R 3 -CNN in Table 1. 4.6 Extensions on R 3 -CNN Description. Our final experiments show that R 3 -CNN model can be plugged in seamlessly to several state-of-the-art architectures for instance segmentation, consistently improving their performance, which demonstrates its generalizability. In this experiment, we select our best-performing version previously called advanced (see</figDesc><table><row><cell># Model Lt H Le BAP SAP 1 Mask 1 1 1 38.2 34.7 2 R 3 -CNN 3 1 1 37.9 35.4 3 3 1 2 40.5 36.6 4 3 1 3 40.9 36.8 5 3 1 4 40.9 36.7 6 3 1 5 40.9 36.7</cell><cell># Model Lt H BAP SAP 1 Mask 1 1 38.2 34.7 2 1 1 37.7 34.7 3 2 1 40.4 36.4 R 3 -CNN 4 3 1 40.9 36.8 5 4 1 40.9 36.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>3 -CNN offers a good flexibility to use intermediate versions between the naive version and HTC, permitting to play with the number of loops, depending if we privilege precision, number of parameters or speed. Overall, the proposed R 3 -CNN architecture Bounding Box (object detection) Mask (instance segmentation) # Method AP AP50 AP75 APs APm AP l AP AP50 AP75 APs APm AP l 1 Mask 37.3 58.9 40.4 21.7 41.1 48.2 34.1 55.5 36.1 18.0 37.6 46.7 2 HTC 41.7 60.4 45.2 24.0 44.8 54.7 36.9 57.6 39.9 19.8 39.8 50.1 3 R 3 -CNN-L 42.0 61.0 46.3 24.5 45.2 55.7 38.2 58.0 41.4 20.4 41.0 52.8 4 GRoIE 38.6 59.4 42.1 22.5 42.0 50.5 35.8 56.5 38.4 19.2 39.0 48.7 5 R 3 -CNN-L+GRoIE 42.0 61.2 45.6 24.4 45.2 55.7 39.1 58.8 42.3 20.7 42.1 54.3 6 GC-Net 40.5 62.0 44.0 23.8 44.4 52.7 36.4 58.7 38.5 19.7 40.2 49.1 7 HTC+GC-Net 43.9 63.1 47.7 26.2 47.7 57.6 38.7 60.4 41.7 21.6 42.2 52.5 8 R 3 -CNN-L+GC-Net 44.3 64.1 48.4 27.0 47.1 58.9 40.2 61.1 43.5 22.6 42.8 56.0 9 DCN 41.9 62.9 45.9 24.2 45.5 55.5 37.6 60.0 40.0 20.2 40.8 51.6 10 HTC+DCN 44.7 63.8 48.6 26.5 48.2 60.2 39.4 61.2 42.3 21.9 42.7 54.9 11 R 3 -CNN-L+DCN 44.8 64.3 48.9 26.6 48.3 59.6 40.4 61.3 44.0 22.3 43.6 56.1 Performance of the state-of-the-art models with and without R 3 -CNN model. Bold values are best results, red ones are second-best values.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gcnet: Non-local networks meet squeeze-excitation networks and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dcan: Deep contour-aware networks for object instance segmentation from histology images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Revisiting rcnn: On awakening the classification power of faster rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The foundations of cost-sensitive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Seventeenth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust inter-vehicle distance estimation method based on monocular vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mask scoring r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Matchingcnn meets knn: Quasi-parametric human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cost-sensitive boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Masnadi-Shirazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generating positive bounding boxes for balanced training of object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oksuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Cam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kalkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imbalance problems in object detection: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oksuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Cam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kalkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Libra r-cnn: Towards balanced learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A novel region of interest extraction layer for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prati</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13665</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cascade rpn: Delving into high-quality region proposal network with adaptive convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">X</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Region proposal by guided anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cascade region proposal and global context for deep object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">395</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05190</idno>
		<title level="m">Iou-uniform r-cnn: Breaking through the limitations of rpn</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
