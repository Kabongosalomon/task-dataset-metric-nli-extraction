<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NP-Match: When Neural Processes meet Semi-Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Massiceti</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Pavlovic</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Neophytou</surname></persName>
						</author>
						<title level="a" type="main">NP-Match: When Neural Processes meet Semi-Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semi-supervised learning (SSL) has been widely explored in recent years, and it is an effective way of leveraging unlabeled data to reduce the reliance on labeled data. In this work, we adjust neural processes (NPs) to the semi-supervised image classification task, resulting in a new method named NP-Match. NP-Match is suited to this task for two reasons. Firstly, NP-Match implicitly compares data points when making predictions, and as a result, the prediction of each unlabeled data point is affected by the labeled data points that are similar to it, which improves the quality of pseudolabels. Secondly, NP-Match is able to estimate uncertainty that can be used as a tool for selecting unlabeled samples with reliable pseudo-labels. Compared with uncertainty-based SSL methods implemented with Monte Carlo (MC) dropout, NP-Match estimates uncertainty with much less computational overhead, which can save time at both the training and the testing phases. We conducted extensive experiments on four public datasets, and NP-Match outperforms state-of-theart (SOTA) results or achieves competitive results on them, which shows the effectiveness of NP-Match and its potential for SSL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks have been widely used in computer vision tasks <ref type="bibr" target="#b20">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b47">Simonyan &amp; Zisserman, 2014;</ref><ref type="bibr" target="#b51">Szegedy et al., 2015;</ref><ref type="bibr" target="#b13">He et al., 2016)</ref> due to their strong performance. Training deep neural net-Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).</p><p>works relies on large-scale labeled datasets, but annotating large-scale datasets is time-consuming, which encourages researchers to explore semi-supervised learning (SSL). SSL aims to learn from few labeled data and a large amount of unlabeled data, and it has been a long-standing problem in computer vision and machine learning <ref type="bibr" target="#b46">Zhang et al., 2021;</ref><ref type="bibr" target="#b42">Rizve et al., 2021;</ref><ref type="bibr" target="#b38">Pham et al., 2021;</ref><ref type="bibr" target="#b26">Li &amp; Zhou, 2014;</ref><ref type="bibr" target="#b27">Liu et al., 2010;</ref><ref type="bibr" target="#b1">Berthelot et al., 2019;</ref><ref type="bibr" target="#b35">2020)</ref>. In this work, we focus on SSL for image classification.</p><p>Most recent approaches to SSL for image classification are based on the combination of consistency regularization and pseudo-labeling <ref type="bibr" target="#b25">Li et al., 2021;</ref><ref type="bibr" target="#b42">Rizve et al., 2021;</ref><ref type="bibr" target="#b46">Zhang et al., 2021;</ref><ref type="bibr" target="#b32">Nassar et al., 2021;</ref><ref type="bibr" target="#b38">Pham et al., 2021;</ref><ref type="bibr" target="#b14">Hu et al., 2021)</ref>. They can be further classified into two categories, namely, deterministic <ref type="bibr" target="#b25">Li et al., 2021;</ref><ref type="bibr" target="#b46">Zhang et al., 2021;</ref><ref type="bibr" target="#b32">Nassar et al., 2021;</ref><ref type="bibr" target="#b38">Pham et al., 2021;</ref><ref type="bibr" target="#b14">Hu et al., 2021)</ref> and probabilistic ones <ref type="bibr" target="#b42">(Rizve et al., 2021)</ref>. A deterministic approach aims at directly making predictions, while a probabilistic approach tries to additionally model the predictive distribution, such as using Bayesian neural networks (BNNs), which are implemented by Monte Carlo (MC) dropout <ref type="bibr" target="#b8">(Gal &amp; Ghahramani, 2016)</ref>. As a result, the former cannot estimate the uncertainty of the model's prediction, and unlabeled samples are selected only based on high-confidence predictions. In contrast, the latter can give uncertainties for unlabeled samples, and the uncertainties can be combined with high-confidence predictions for picking or refining pseudo-labels.</p><p>Current SOTA methods for the semi-supervised image classification task are deterministic, including FixMatch , CoMatch <ref type="bibr" target="#b25">(Li et al., 2021)</ref>, and FlexMatch <ref type="bibr" target="#b46">(Zhang et al., 2021)</ref>, which have achieved promising results on public benchmarks. In contrast, progress on probabilistic approaches lags behind, which is mainly shown by the fact that there are only few studies on this task and MC dropout becomes the only option for implementing the probabilistic model <ref type="bibr" target="#b42">(Rizve et al., 2021)</ref>. In addition, MC dropout also dominates the uncertainty-based approaches to other SSL tasks <ref type="bibr" target="#b45">(Sedai et al., 2019;</ref><ref type="bibr" target="#b46">Shi et al., 2021;</ref><ref type="bibr" target="#b54">Wang et al., 2021;</ref><ref type="bibr">Yu et al., 2019;</ref><ref type="bibr">Zhu et al., 2020)</ref>. MC dropout, however, is time-consuming, requiring several feedforward passes to get uncertainty at both the training and the testing stages, especially when some large models are used.</p><p>To solve this drawback and to further promote the related research, we need to find better probabilistic approaches for SSL. Considering that MC dropout is an approximation to the Gaussian process (GP) model <ref type="bibr" target="#b8">(Gal &amp; Ghahramani, 2016)</ref>, we turn to another approximation model called neural processes (NPs) <ref type="bibr" target="#b10">(Garnelo et al., 2018b)</ref>, which can be regarded as an NN-based formulation that approximates GPs. Similarly to a GP, a neural process is also a probabilistic model that defines distributions over functions. Thus, an NP is able to rapidly adapt to new observations, with the advantage of estimating the uncertainty of each observation. There are two main aspects that motivate us to investigate NPs in SSL. Firstly, GPs have been preliminarily explored for some SSL tasks <ref type="bibr" target="#b48">(Sindhwani et al., 2007;</ref><ref type="bibr" target="#b15">Jean et al., 2018;</ref><ref type="bibr">Yasarla et al., 2020)</ref>, because of the property that their kernels are able to compare labeled data with unlabeled data when making predictions. NPs share this property, since it has been proved that NPs can learn non-trivial implicit kernels from data <ref type="bibr" target="#b10">(Garnelo et al., 2018b)</ref>. As a result, NPs are able to make predictions for target points conditioned on context points. This feature is highly relevant to SSL, which must learn from limited labeled samples in order to make predictions for unlabeled data, similarly to how NPs are able to impute unknown pixel values (i.e., target points) when given only a small number of known pixels (namely, context points) <ref type="bibr" target="#b10">(Garnelo et al., 2018b)</ref>. Due to the learned implicit kernels in NPs <ref type="bibr" target="#b10">(Garnelo et al., 2018b)</ref> and the successful application of GPs to different SSL tasks <ref type="bibr" target="#b48">(Sindhwani et al., 2007;</ref><ref type="bibr" target="#b15">Jean et al., 2018;</ref><ref type="bibr">Yasarla et al., 2020)</ref>, NPs could be a suitable probabilistic model for SSL, as the kernels can compare labeled data with unlabeled data in order to improve the quality of pseudo-labels for the unlabeled data at the training stage. Secondly, previous GP-based works for SSL do not explore the semi-supervised large-scale image classification task, since GPs are computationally expensive, which usually incur a O(n 3 ) runtime for n training points. But, unlike GPs, NPs are more efficient than GPs, providing the possibility of applying NPs to this task. NPs are also computationally significantly more efficient than current MC-dropout-based approaches to SSL, since, given an input image, they only need to perform one feedforward pass to obtain the prediction with an uncertainty estimate.</p><p>In this work, we take the first step to explore NPs in largescale semi-supervised image classification, and propose a new probabilistic method called NP-Match. NP-Match still rests on the combination of consistency regularization and pseudo-labeling, but it incorporates NPs to the top of deep neural networks, and therefore it is a probabilistic approach. Compared to the previous probabilistic method for semisupervised image classification <ref type="bibr" target="#b42">(Rizve et al., 2021)</ref>, NP-Match not only can make predictions and estimate uncertainty more efficiently, inheriting the advantages of NPs, but also can achieve a better performance on public benchmarks. Summarizing, the main contributions of this paper are:</p><p>? We propose NP-Match, which adjusts NPs to SSL, and explore its use in semi-supervised large-scale image classification. To our knowledge, this is the first such work. In addition, NP-Match has the potential to break the monopoly of MC dropout as the probabilistic model in SSL.</p><p>? We experimentally show that the Kullback-Leibler (KL) divergence in the evidence lower bound (ELBO) of NPs <ref type="bibr" target="#b10">(Garnelo et al., 2018b)</ref> is not a good choice in the context of SSL, which may negatively impact the learning of global latent variables. To tackle this problem, we propose a new uncertainty-guided skew-geometric Jensen-Shannon (JS) divergence (JS G? u ) for NP-Match.</p><p>? We show that NP-Match outperforms SOTA results or achieves competitive results on four public benchmarks, demonstrating its effectiveness for SSL. We also show that NP-Match estimates uncertainty faster than the MCdropout-based probabilistic model, which can improve the training and the test efficiency.</p><p>The rest of this paper is organized as follows. In Section 2, we review related methods. Section 3 presents NP-Match and the uncertainty-guided skew-geometric JS divergence (JS G? u ), followed by the experimental settings and results in Section 4. In Section 5, we give a summary and an outlook on future research. The source code is available at: https://github.com/Jianf-Wang/NP-Match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We now briefly review related works, including semi-supervised learning (SSL) for image classification, Gaussian processes (GPs) for SSL, and neural processes (NPs).</p><p>SSL for image classification. Most methods for semisupervised image classification in the past few years are based on pseudo-labeling and consistency regularization. Pseudo-labeling approaches rely on the high confidence of pseudo-labels, which can be added to the training data set as labeled data, and those approaches can be classified into two classes, namely, disagreement-based models and selftraining models. The former models aim to train multiple learners and exploit the disagreement during the learning process <ref type="bibr" target="#b39">(Qiao et al., 2018;</ref><ref type="bibr" target="#b4">Chen et al., 2018)</ref>, while the latter models aim at training the model on a small amount of labeled data, and then using its predictions on the unlabeled data as pseudo-labels <ref type="bibr" target="#b23">(Lee, 2013;</ref><ref type="bibr">Zhai et al., 2019;</ref><ref type="bibr">Wang et al., 2020;</ref><ref type="bibr" target="#b38">Pham et al., 2021)</ref>. Consistency-regularizationbased approaches work by performing different transformations on an input image and adding a regularization term to make their predictions consistent <ref type="bibr" target="#b0">(Bachman et al., 2014;</ref><ref type="bibr" target="#b44">Sajjadi et al., 2016;</ref><ref type="bibr" target="#b21">Laine &amp; Aila, 2017;</ref><ref type="bibr" target="#b1">Berthelot et al., 2019;</ref><ref type="bibr">Xie et al., 2020)</ref>. Based on these two approaches, FixMatch  is proposed, which achieves new stateof-the-art (SOTA) results on the most commonly-studied SSL benchmarks. FixMatch  combines the merits of these two approaches: given an unlabeled image, weak data augmentation and strong data augmentation are performed on the image, leading to two versions of the image, and then FixMatch produces a pseudo-label based on its weakly-augmented version and a preset confidence threshold, which is used as the true label for its strongly augmented version to train the whole framework. The success of FixMatch inspired several subsequent methods <ref type="bibr" target="#b25">(Li et al., 2021;</ref><ref type="bibr" target="#b42">Rizve et al., 2021;</ref><ref type="bibr" target="#b46">Zhang et al., 2021;</ref><ref type="bibr" target="#b32">Nassar et al., 2021;</ref><ref type="bibr" target="#b38">Pham et al., 2021;</ref><ref type="bibr" target="#b14">Hu et al., 2021)</ref>. For instance, <ref type="bibr" target="#b25">Li et al. (2021)</ref> additionally design the classification head and the projection head for generating a class probability and a low-dimensional embedding, respectively. The projection head and the classification head are jointly optimized during training. Specifically, the former is learnt with contrastive learning on pseudo-label graphs to encourage the embeddings of samples with similar pseudo-labels to be close, and the latter is trained with pseudo-labels that are smoothed by aggregating information from nearby samples in the embedding space. <ref type="bibr" target="#b46">Zhang et al. (2021)</ref> propose to use dynamic confidence thresholds that are automatically adjusted according to the model's learning status of each class. <ref type="bibr" target="#b42">Rizve et al. (2021)</ref> propose an uncertainty-aware pseudolabel selection (UPS) framework for semi-supervised image classification. The UPS framework introduces MC dropout to obtain uncertainty estimates, which are then leveraged as a tool for selecting pseudo-labels. This is the first work using MC dropout for semi-supervised image classification.</p><p>GPs for SSL. Since NPs are also closely related to GPs, we review the application of GPs to different SSL tasks in this part. GPs, which are non-parametric models, have been preliminarily investigated in different semi-supervised learning tasks. For example, <ref type="bibr" target="#b48">Sindhwani et al. (2007)</ref> introduce a semi-supervised GP classifier, which incorporates the information of relationships among labeled and unlabeled data into the kernel. Their approach, however, has high computational costs and is thus only evaluated for a simple binary classification task on small datasets. Deep kernel learning <ref type="bibr">(Wilson et al., 2016)</ref> also lies on the spectrum between NNs and GPs, and has been integrated into a new framework for the semi-supervised regression task, named semi-supervised deep kernel learning <ref type="bibr" target="#b15">(Jean et al., 2018)</ref>, which aims to minimize the predictive variance for unlabeled data, encouraging unlabeled embeddings to be near labeled embeddings. Semi-supervised deep kernel learning, however, has not been applied to SSL image classification, and (similarly to semi-supervised GPs) also comes with a high (cubic) computational complexity. <ref type="bibr">Recently, Yasarla et al. (2020)</ref> proposed to combine GPs with UNet <ref type="bibr" target="#b43">(Ronneberger et al., 2015)</ref> for SSL image deraining. Here, GPs are used to get pseudo-labels for unlabeled samples based on the feature representations of labeled and unlabeled images. GPs have also been combined with graph convolutional networks for semi-supervised learning on graphs <ref type="bibr" target="#b33">(Ng et al., 2018;</ref><ref type="bibr" target="#b53">Walker &amp; Glocker, 2019;</ref><ref type="bibr" target="#b28">Liu et al., 2020)</ref>. Although many previous works explore GPs in different semi-supervised learning tasks, none of them investigates the application of GPs to semi-supervised large-scale image classification.</p><p>NPs. The origin of NPs can be traced back to conditional NPs <ref type="bibr" target="#b9">(Garnelo et al., 2018a)</ref>, which define conditional distributions over functions given a set of observations. Conditional NPs, however, do not introduce global latent variables for observations, which led to the birth of NPs <ref type="bibr" target="#b10">(Garnelo et al., 2018b)</ref>. In NPs, the mean-aggregator is used to summarize the encoded inputs of a task into a global latent variable, which is then used to make predictions on targets in the task. In recent years, several NP variants have been proposed to better approximate stochastic processes. For example, <ref type="bibr" target="#b17">Kim et al. (2019)</ref> consider that the mean-aggregator may cause difficulties for the decoder to pick relevant information with regard to making predictions, and they introduce a differentiable attention mechanism to solve this issue, resulting in new attentive NPs. <ref type="bibr" target="#b11">Gordon et al. (2020)</ref> consider that the translation equivariance is important for prediction problems, which should be considered. Therefore, they incorporate translation equivariance into NPs and design a new model, called convolutional conditional NPs. Besides, <ref type="bibr" target="#b30">Louizos et al. (2019)</ref> consider that using global latent variables is not flexible for encoding inductive biases. Thus, they propose to use local latent variables along with a dependency structure among them, resulting in new functional NPs. <ref type="bibr" target="#b24">Lee et al. (2020)</ref> point out the limitation of using a single Gaussian latent variable to model functional uncertainty. To solve the limitation, they propose to use bootstrapping for inducing functional uncertainty, leading to a new NP variant, called Bootstrapping Neural Processes (BNP). <ref type="bibr" target="#b3">Bruinsma et al. (2021)</ref> introduce a novel member of the NP family that incorporates translation equivariance and models the predictive distributions directly with Gaussian processes, called Gaussian NP (GNP). GNPs do not allow for correlations in the predictive distribution, but also provide universal approximation guarantees. Currently, NPs and their variants have been widely used in many different settings, including meta-learning <ref type="bibr" target="#b49">(Singh et al., 2019;</ref><ref type="bibr">Yoon et al., 2020;</ref><ref type="bibr" target="#b41">Requeima et al., 2019)</ref> and sequential data modelling <ref type="bibr" target="#b40">(Qin et al., 2019)</ref>, but they have not been studied in SSL. Our work is the first to leverage NPs for semi-supervised large-scale image recognition. We choose the most basic model from <ref type="bibr" target="#b10">(Garnelo et al., 2018b</ref>) (i.e., the original NPs), and we expect that future works can further study the application of other variants to this task.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we provide a brief introduction to neural processes (NPs) and a detailed description of our NP-Match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">NPs</head><p>NPs approximate stochastic processes via finite-dimensional marginal distributions <ref type="bibr" target="#b10">(Garnelo et al., 2018b)</ref>. Formally, given a probability space (?, ?, ?) and an index set X , a stochastic process can be written as {F (x, ?) :</p><p>x ? X }, where F (? , ?) is a sample function mapping X to another space Y for any point ? ? ?. For each finite sequence x 1:n , a marginal joint distribution function can be defined on the function values F (x 1 , ?), F (x 2 , ?), . . . , F (x n , ?), which satisfies two conditions given by the Kolmogorov Extension Theorem <ref type="bibr">(?ksendal, 2003)</ref>, namely, exchangeability and consistency. Assuming that a density ?, where d? = ?d?, and the likelihood density p(y 1:n |F (? , ?), x 1:n ) exist, the marginal joint distribution function can be written as:</p><formula xml:id="formula_0">p(y1:n|x1:n) = ?(?)p(y1:n|F (? , ?), x1:n)d?(?).<label>(1)</label></formula><p>The exchangeability condition requires the joint distributions to be invariant to permutations of the elements, i.e., p(y 1:n |x 1:n ) = p(?(y 1:n )|?(y 1:n )), where ? is a permutation of {1, . . . , n}. The consistency condition expresses that if a part of the sequence is marginalised out, then the resulting marginal distribution is consistent with that defined on the original sequence.</p><formula xml:id="formula_1">Letting (?, ?) be (R d , B(R d )), where B(R d ) denotes the</formula><p>Borel ?-algebra of R d , NPs parameterize the function F (? , ?) with a high-dimensional random vector z sampled from a multivariate Gaussian distribution. Then, F (x i , ?) can be replaced by g(x i , z), where g(?) denotes a neural network, and Eq. (1) becomes: p(y1:n|x1:n) = ?(z)p(y1:n|g(x1:n, z), x1:n)d?(z). <ref type="formula">(2)</ref> The training objective of NPs is to maximize p(y 1:n |x 1:n ), and the learning procedure reflects the NPs' property that they have the capability to make predictions for target points conditioned on context points <ref type="bibr" target="#b10">(Garnelo et al., 2018b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">NP-Match</head><p>As <ref type="figure" target="#fig_0">Figure 1</ref> shows, NP-Match is mainly composed of two parts: a deep neural network and an NP model. The deep neural network is leveraged for obtaining feature representations of input images, while the NP model is built upon the network to receive the representations for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">NP MODEL FOR SEMI-SUPERVISED IMAGE CLASSIFICATION</head><p>Since we extend the original NPs <ref type="bibr" target="#b10">(Garnelo et al., 2018b)</ref> to the classification task, p(y 1:n |g(x 1:n , z), x 1:n ) in Eq. <ref type="formula">(2)</ref> should define a categorical distribution rather than a Gaus-sian distribution. Therefore, we parameterize the categorical distribution by probability vectors from a classifier that contains a weight matrix (W) and a softmax function (?):</p><p>p(y 1:n |g(x 1:n , z), x 1:n ) = Categorical(?(Wg(x 1:n , z))).</p><p>(3) Note that g(?) can be learned via amortised variational inference, and to use this method, two steps need to be done: (1) parameterize a variational distribution over z, and (2) find the evidence lower bound (ELBO) as the learning objective. For the first step, we let q(z|x 1:n , y 1:n ) be a variational distribution defined on the same measure space, which can be parameterized by a neural network. For the second step, given a finite sequence with length n, we assume that there are m context points (x 1:m ) and r target points (x m+1: m+r ) in it, i.e., m + r = n. Then, the ELBO is given by (with proof in the appendix):</p><formula xml:id="formula_2">log p(y1:n|x1:n) ? E q(z|x m+1: m+r ,y m+1: m+r ) m+r i=m+1 log p(yi|z, xi)? log q(z|xm+1: m+r , ym+1: m+r ) q(z|x1:m, y1:m) + const.<label>(4)</label></formula><p>To learn the NP model, one can maximize this ELBO. Under the setting of SSL, we consider that only labeled data can be treated as context points, and either labeled or unlabeled data can be treated as target points, since the target points are what the NP model makes predictions for.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">NP-MATCH PIPELINE</head><p>We now introduce the NP-Match pipeline. We first focus on the configuration of the NP model, which is shown in the red dotted box in <ref type="figure" target="#fig_0">Figure 1</ref>. The NP model is mainly constructed by MLPs, memory banks, and a classifier. Specifically, the classifier is composed of the weight matrix (W) and the softmax function (?). Similarly to the original implementation of NPs, we build two paths with the memory banks and MLPs, namely, the latent path and the deterministic path. The decoder g(?) is also implemented with MLPs. The workflow of NP-Match at the training stage and the inference stage are different, which are shown in <ref type="figure" target="#fig_0">Figure 1</ref>, and they are introduced separately as follows.</p><formula xml:id="formula_3">Training mode. Given a batch of B labeled images L = {(x i , y i ) : i ? {1, .</formula><p>. . , B}} and a batch of unlabeled images U = {x u i : i ? {1, . . . , ?B}} at each iteration, where ? determines the relative size of U to L, we apply weak augmentation (i.e., crop-and-flip) on the labeled and unlabeled samples, and strong augmentation (i.e., RandAugment (Cubuk et al., 2020)) on only the unlabeled samples. After the augmentation is applied, the images are passed through the deep neural network, and the features are input to the NP model, which finally outputs the predictions and associated uncertainties. The detailed process can be summarized as follows. At the start of each iteration, NP-Match is switched to inference mode, and it makes predictions for the weaklyaugmented unlabeled data. Then, inference mode is turned off, and those predictions are treated as pseudo-labels for unlabeled data. After receiving the features, real labels, and pseudo-labels, the NP model first duplicates the labeled samples and treats them as context points, and all the labeled and unlabeled samples in the original batches are then treated as target points, since the NP model needs to make a prediction for them. Thereafter, the target points and context points are separately fed to the latent path and the deterministic path. As for the latent path, target points are concatenated with their corresponding real labels or pseudo labels, and processed by MLPs to get new representations. Then, the representations are averaged by a mean aggregator along the batch dimension, leading to an order-invariant representation, which implements the exchangeability and the consistency condition, and they are simultaneously stored in the latent memory bank, which is updated with a first-in-firstout strategy. After the mean aggregator, the order-invariant representation is further processed by other two MLPs in order to get the mean vector and the variance vector, which are used for sampling latent vectors via the reparameterization trick, and the number of latent vectors sampled at each feed-forward pass is denoted T . As for the deterministic path, context points are input to this path and are processed in the same way as the target points, until an order-invariant representation is procured from the mean aggregator. We also introduce a memory bank to the deterministic path for storing representations. Subsequently, each target point is concatenated with the T latent vectors and the orderinvariant representations from the deterministic path (note that, practically, the target point and the order-invariant representations from the deterministic path must be copied T times). After the concatenation operation, the T * r feature representations are fed into the decoder g(?) and then the classifier, which outputs T probability distributions over classes for each target point. The final prediction for each target point can be obtained by averaging the T predictions, and the uncertainty is computed as the entropy of the average prediction <ref type="bibr" target="#b16">(Kendall &amp; Gal, 2017)</ref>. The ELBO (Eq. (4)) shows the learning objective. Specifically, the first term can be achieved by using the cross-entropy loss on the labeled and unlabeled data with their corresponding real labels and pseudo-labels, while the second term is the KL divergence between q(z|x m+1: m+r , y m+1: m+r ) and q(z|x 1:m , y 1:m ).</p><p>Inference mode. Concerning a set of test images, they are also passed through the deep neural network at first to obtain their feature representations. Then, they are treated as target points and are fed to the NP model. Since the labels of test data are not available, it is impossible to obtain the order-invariant representation from test data. In this case, the stored features in the two memory banks can be directly used. As the bottom diagram of <ref type="figure" target="#fig_0">Figure 1</ref> shows, after the order-invariant representations are obtained from the memory banks, the target points are leveraged in the same way as in the training mode to generate concatenated feature representations for the decoder g(?) and then the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">UNCERTAINTY-GUIDED SKEW-GEOMETRIC JS DIVERGENCE</head><p>NP-Match, like many SSL approaches, relies on the use of pseudo-labels for the unlabeled samples. Pseudo-labels, however, are sometimes inaccurate and can lead to the neural network learning poor feature representations. In our pipeline, this can go on to impact the representation procured from the mean-aggregator and hence the model's estimated mean vector, variance vector, and global latent vectors (see "Latent Path" in <ref type="figure" target="#fig_0">Figure 1</ref>). To remedy this, similarly to how the KL divergence term in the ELBO (Eq. <ref type="formula" target="#formula_2">(4)</ref>) is used to learn global latent variables <ref type="bibr" target="#b10">(Garnelo et al., 2018b)</ref>, we propose a new distribution divergence, called the uncertainty-guided skew-geometric JS divergence (JS G? u ). We first formalize the definition of JS G? u :</p><p>Definition 1. Let (?, ?) be a measurable space, where ? denotes the sample space, and ? denotes the ?-algebra of measurable events. P and Q are two probability measures defined on the measurable space. Concerning a positive measure 1 , which is denoted as ?, the uncertainty-guided skew-geometric JS divergence (JS G? u ) can be defined as:</p><formula xml:id="formula_4">JS G? u (p, q) = (1 ? ?u) p log p G(p, q)? u d? + ?u q log q G(p, q)? u d?,<label>(5)</label></formula><p>where p and q are the Radon-Nikodym derivatives of P and Q with respect to ?, the scalar ? u ? [0, 1] is calculated based on the uncertainty, and G(p, q) ?u = p 1??u q ?u / ( ? p 1??u q ?u d?). The dual form of JS G? u is given by:</p><formula xml:id="formula_5">JS G? u * (p, q) = (1 ? ?u) G(p, q)? u log G(p, q)? u p d?+ ?u G(p, q)? u log G(p, q)? u q d?.<label>(6)</label></formula><p>The proposed JS G? u is an extension of the skew-geometric JS divergence first proposed by <ref type="bibr" target="#b35">Nielsen (2020)</ref>. Specifically, Nielsen (2020) generalizes the JS divergence with abstract means (quasi-arithmetic means <ref type="bibr" target="#b34">(Niculescu &amp; Persson, 2006)</ref>), in which a scalar ? is defined to control the degree of divergence skew. 2 By selecting the weighted geometric mean p 1?? q ? , such generalized JS divergence becomes the 1 Specifically, the positive measure is usually the Lebesgue measure with the Borel ?-algebra B(R d ) or the counting measure with the power set ?-algebra 2 ? . <ref type="bibr">2</ref> The divergence skew means how closely related the intermediate distribution (the abstract mean of p and q) is to p or q. skew-geometric JS divergence, which can be easily applied to the Gaussian distribution because of its property that the weighted product of exponential family distributions stays in the exponential family <ref type="bibr" target="#b36">(Nielsen &amp; Garcia, 2009</ref>). Our JS G? u extends such divergence by incorporating the uncertainty into the scalar ? to dynamically adjust the divergence skew. We assume the real variational distribution of the global latent variable under the supervised learning to be q * . If the framework is trained with real labels, the condition q(z|x m+1: m+r , y m+1: m+r ) = q(z|x 1:m , y 1:m ) = q * will hold after training, since they are all the marginal distributions of the same stochastic process. However, as for SSL, q(z|x m+1: m+r , y m+1: m+r ) and q(z|x 1:m , y 1:m ) are no longer equal to q * , as some low-quality representations are involved during training, which affect the estimation of q(z|x m+1: m+r , y m+1: m+r ) and q(z|x 1:m , y 1:m ). Our proposed JS G? u solves this issue by introducing an intermediate distribution that is calculated via G(q(z|x 1:m , y 1:m ), q(z|x m+1: m+r , y m+1: m+r )) ?u , where ? u = u cavg /(u cavg + u tavg ). Here, u cavg denotes the average value over the uncertainties of the predictions of context points, and u tavg represents the average value over that of target points. With this setting, the intermediate distribution is usually close to q * . For example, when u cavg is large, and u tavg is small, which means that there are many low-quality feature presentations involved for calculating q(z|x 1:m , y 1:m ), and q(z|x m+1: m+r , y m+1: m+r ) is closer to q * , then G(q(z|x 1:m , y 1:m ), q(z|x m+1: m+r , y m+1: m+r )) ?u will be close to q(z|x m+1: m+r , y m+1: m+r ), and as a result, the network is optimized to learn the distribution of the global latent variable in the direction to q * , which mitigates the issue to some extent. <ref type="bibr">3</ref> Concerning the variational distribution being supposed to be a Gaussian distribution, we introduce the following theorem (with proof in the appendix) for calculating JS G? u on Gaussian distributions: Theorem 1. Given two multivariate Gaussians N 1 (? 1 , ? 1 ) and N 2 (? 2 , ? 2 ), the following holds:</p><formula xml:id="formula_6">JS G? u (N1, N2) = 1 2 (tr(? ?1 ?u ((1 ? ?u)?1 + ?u?2))+ (1 ? ?u)(?? u ? ?1) T ? ?1 ?u (?? u ? ?1)+ ?u(?? u ? ?2) T ? ?1 ?u (?? u ? ?2)+ log[ det[?? u ] det[?1] 1??u det[?2] ?u ] ? D) JS G? u * (N1, N2) = 1 2 (log[ det[?1] 1??u det[?2] ?u det[?? u ] ] + ?u? T 2 ? ?1 2 ?2 ? ? T ?u ? ?1 ?u ?? u + (1 ? ?u)? T 1 ? ?1 1 ?1),<label>(7)</label></formula><p>where</p><formula xml:id="formula_7">? ?u = ((1 ? ? u )? ?1 1 + ? u ? ?1 2 ) ?1 and ? ?u = ? ?u ((1 ? ? u )? ?1 1 ? 1 + ? u ? ?1 2 ? 2 )</formula><p>, D denotes the number of dimension, and det[?] represents the determinant.</p><p>With Theorem 1, one can calculate JS G? u or its dual form JS G? u * based on the mean vector and the variance vector, and use JS G? u or JS G? u * to replace the original KL divergence term in the ELBO <ref type="figure" target="#fig_3">(Eq. (4)</ref>) for training the whole framework. When the two distributions are diagonal Gaussians, ? 1 and ? 2 can be implemented by diagonal matrices with the variance vectors for calculating JS G? u or JS G? u * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">LOSS FUNCTIONS</head><p>To calculate loss functions, reliable pseudo-labels are required for unlabeled data. In practice, to select reliable unlabeled samples from U and their corresponding pseudolabels, we preset a confidence threshold (? c ) and an uncertainty threshold (? u ). In particular, as for unlabeled data x u i , NP-Match gives its prediction p(y|Aug w (x u i )) and associated uncertainty estimate under the inference mode, where Aug w (?) denotes the weak augmentation. When the highest prediction score max(p(y|Aug w (x u i ))) is higher than ? c , and the uncertainty is smaller than ? u , the sample will be chosen, and we denote the selected sample as x uc i , since the model is certain about his prediction, and the pseudo-label of x uc i is? i = arg max(p(y|Aug w (x uc i ))). Concerning ?B unlabeled samples in U, we assume B c unlabeled samples are selected from them in each feedforward pass. According to the ELBO (Eq. (4)), three loss terms are used for training, namely, L cls , L u cls , and JS G? u . For each input (labeled or unlabeled), the NP model can give T predictions, and hence L cls and L u cls are defined as:</p><formula xml:id="formula_8">L cls = 1 B ? T B i=1 T j=1 H(y * i , pj(y|Augw(xi))), L u cls = 1 Bc ? T Bc i=1 T j=1 H(?i, pj(y|Augs(x uc i ))),<label>(8)</label></formula><p>where Aug s (?) denotes the strong augmentation, y * i represents the real label for the labeled sample x i , and H(?, ?) denotes the cross-entropy between two distributions. Thus, the total loss function is given by:</p><formula xml:id="formula_9">L total = L cls + ? u L u cls + ?JS G? u ,<label>(9)</label></formula><p>where ? u and ? are coefficients. During training, we followed previous work <ref type="bibr" target="#b46">Zhang et al., 2021;</ref><ref type="bibr" target="#b42">Rizve et al., 2021;</ref><ref type="bibr" target="#b25">Li et al., 2021)</ref> to utilize the exponential moving average (EMA) technique. It is worth noting that, in the real implementation, NP-Match only preserves the averaged representation over all representations in each memory bank after training, which just takes up negligible storage space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We now report on our experiments of NP-Match on four public image classification benchmarks. To save space, implementation details are given in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We conducted our experiments on four widely used public SSL benchmarks, including CIFAR-10 <ref type="bibr" target="#b19">(Krizhevsky et al., 2009</ref>), CIFAR-100 <ref type="bibr" target="#b19">(Krizhevsky et al., 2009</ref>), STL-10 <ref type="bibr" target="#b5">(Coates et al., 2011)</ref>, and ImageNet <ref type="bibr" target="#b7">(Deng et al., 2009</ref>). CIFAR-10 and CIFAR-100 contain 50,000 images of size 32 ? 32 from 10 and 100 classes, respectively. We evaluated NP-match on these two datasets following the evaluation settings used in previous works <ref type="bibr" target="#b46">Zhang et al., 2021;</ref><ref type="bibr" target="#b25">Li et al., 2021)</ref>. The STL-10 dataset has 5000 labeled samples with size 96 ? 96 from 10 classes and 100,000 unlabeled samples, and it is more difficult than CIFAR, since STL-10 has a number of out-of-distribution images in the unlabeled set. We follow the experimental settings for STL-10 as detailed in <ref type="bibr" target="#b46">(Zhang et al., 2021)</ref>. Finally, ImageNet contains around 1.2 million images from 1000 classes. Following the experimental settings in <ref type="bibr" target="#b46">(Zhang et al., 2021)</ref>, we used 100K labeled data, namely, 100 labels per class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Main Results</head><p>In the following, we report the main experimental results on the accuracy, the average uncertainty, the expected uncertainty calibration error, and the running time of NP-Match compared with SOTA approaches.</p><p>First, in <ref type="table" target="#tab_1">Table 1</ref>, we compare NP-Match with SOTA SSL image classification methods on CIFAR-10, CIFAR-100, and STL-10. We see that NP-Match outperforms SOTA results or achieves competitive results under different SSL settings. We highlight two key observations. First, NP-Match outperforms all other methods by a wide margin on all three benchmarks under the most challenging settings, where the number of labeled samples is smallest. Second, NP-Match is compared to UPS 4 , since the UPS framework is the MCdropout-based probabilistic model for semi-supervised image classification, and NP-Match completely outperforms them on all three benchmarks. This suggests that NPs can be a good alternative to MC dropout in probabilistic approaches to semi-supervised learning tasks.</p><p>Second, we analyse the relationship between the average class-wise uncertainty and accuracy at test phase on CIFAR-10 and STL10. From <ref type="figure">Figure 2</ref>, we empirically observe that: (1) when more labeled data are used for training, the average uncertainty of samples' predictions for each class  decreases. This is consistent with the property of NPs and GPs where the model is less uncertain with regard to its prediction when more real and correct labels are leveraged;</p><p>(2) the classes with higher average uncertainties have lower accuracy, meaning that the uncertainty is a good standard for choosing unlabeled samples.</p><p>Third, the expected uncertainty calibration error (UCE) of our method is also calculated to evaluate the uncertainty  <ref type="table">Table 3</ref>. Error rates of SOTA methods on ImageNet.</p><p>estimation. The expected UCE is used to measure the miscalibration of uncertainty <ref type="bibr" target="#b22">(Laves et al., 2020)</ref>, which is an analogue to the expected calibration error (ECE) <ref type="bibr" target="#b12">(Guo et al., 2017;</ref><ref type="bibr">Naeini et al., 2015)</ref>. The low expected UCE indicates that the model is certain when making accurate predictions and that the model is uncertain when making inaccurate predictions. More details about the expected UCE can be found in previous works <ref type="bibr" target="#b22">(Laves et al., 2020;</ref><ref type="bibr" target="#b18">Krishnan &amp; Tickoo, 2020)</ref>. The results of NP-Match and the MC-dropout-based model (i.e., UPS <ref type="bibr" target="#b42">(Rizve et al., 2021)</ref>) are shown in <ref type="table">Table 2</ref>; their comparison shows that NP-Match can output more reliable and well-calibrated uncertainty estimates.</p><p>Furthermore, we compare the running time of NP-Match and the MC dropout-based model (i.e., UPS <ref type="bibr" target="#b42">(Rizve et al., 2021)</ref>). We use a batch of 16 samples and two network architectures that are widely used in previous works <ref type="bibr">(Zagoruyko &amp; Komodakis, 2016;</ref><ref type="bibr" target="#b46">Zhang et al., 2021;</ref><ref type="bibr" target="#b50">Sohn et al., 2020;</ref><ref type="bibr" target="#b25">Li et al., 2021)</ref>, namely, WRN-28-2 on CIFAR-10 ( <ref type="figure">Figure 3  (a)</ref>) and WRN-28-8 on CIFAR-100 <ref type="figure">(Figure 3 (b)</ref>). In (a), we observe that when the number of predictions (T ) increases, the time cost of the UPS framework rises quickly, but the time cost of NP-Match grows slowly. In (b), we observe that the time cost gap between these two methods is even  <ref type="table">Table 4</ref>. Ablation studies of the proposed uncertainty-guided skew-geometric JS divergence and its dual form.</p><p>larger when a larger model is tested on a larger dataset. This demonstrates that NP-Match is significantly more computationally efficient than MC dropout-based methods.</p><p>Finally, <ref type="table">Table 3</ref> shows the experiments conducted on Im-ageNet. Here, NP-Match achieves a SOTA performance, suggesting that it is effective at handling challenging largescale datasets. Note that previous works usually evaluate their frameworks under distinct SSL settings, and thus it is hard to compare different methods directly. Therefore, we re-evaluate another two methods proposed recently under the same SSL setting with the same training details, namely, UPS and CoMatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>We now report our ablation studies on CIFAR-10, CIFAR-100, and STL-10. We also present further experiments, including a hyperparameter exploration, in the appendix.</p><p>We evaluate our uncertainty-guided skew-geometric JS divergence (JS G? u ) as well as its dual form (JS G? u * ), and compare them to the original KL divergence in NPs. In <ref type="table">Table 4</ref>, we see that NP-Match with KL divergence consistently underperforms relative to our proposed JS G? u and JS G? u * . This suggests that our uncertainty-guided skew-geometric JS divergence can mitigate the problem caused by low-quality feature representations. Between the two, JS G? u and JS G? u * achieve a comparable performance across the three benchmarks, and thus we select JS G? u to replace the original KL divergence in the ELBO (Eq. (4)) for the comparisons to previous SOTA methods in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Summary and Outlook</head><p>In this work, we proposed the application of neural processes (NPs) to semi-supervised learning (SSL), designing a new framework called NP-Match, and explored its use in semi-supervised large-scale image classification. To our knowledge, this is the first such work. To better adapt NP-Match to the SSL task, we proposed a new divergence term, which we call uncertainty-guided skew-geometric JS divergence, to replace the original KL divergence in NPs. We demonstrated the effectiveness of NP-Match and the proposed divergence term for SSL in extensive experiments, and also showed that NP-Match could be a good alternative to MC dropout in SSL.</p><p>Future works will explore the following two directions. First, due to the successful application of NPs to semi-supervised image classification, it is valuable to explore NPs in other SSL tasks, such as object detection and segmentation. Second, many successful NPs variants have been proposed since the original NPs <ref type="bibr" target="#b10">(Garnelo et al., 2018b</ref>) (see Section 2). We will also explore these in SSL for image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>Wang, X., <ref type="bibr">Kihara, D., Luo, J., and Qi, G.-J. Enaet</ref> </p><formula xml:id="formula_10">= C1e ? 1 2 (x T (? ?1 1u +? ?1 2u )x?x T (? ?1 1u +? ?1 2u )(? ?1 1u +? ?1 2u ) ?1 (? ?1 1u ? 1 +? ?1 2u ? 2 )?(? T 1 ? ?1 1u +? T 2 ? ?1 2u )x+(? T 1 ? ?1 1u ? 1 +? T 2 ? ?1 2u ? 2 )) = C1e ? 1 2 (x T (? ?1 1u +? ?1 2u )(x?(? ?1 1u +? ?1 2u ) ?1 (? ?1 1u ? 1 +? ?1 2u ? 2 ))?(? T 1 ? ?1 1u +? T 2 ? ?1 2u )x+(? T 1 ? ?1 1u ? 1 +? T 2 ? ?1 2u ? 2 ))<label>(13)</label></formula><p>NP-Match: When Neural Processes meet Semi-Supervised Learning</p><formula xml:id="formula_11">= C1e ? 1 2 (x T (? ?1 1u +? ?1 2u )(x?(? ?1 1u +? ?1 2u ) ?1 (? ?1 1u ? 1 +? ?1 2u ? 2 ))?(? T 1 ? ?1 1u +? T 2 ? ?1 2u )x+(? T 1 ? ?1 1u +? T 2 ? ?1 2u )(? ?1 1u +? ?1 2u ) ?1 (? ?1 1u ? 1 +? ?1 2u ? 2 )+C 2 ) = C1e ? 1 2 (x T (? ?1 1u +? ?1 2u )(x?(? ?1 1u +? ?1 2u ) ?1 (? ?1 1u ? 1 +? ?1 2u ? 2 ))?(? T 1 ? ?1 1u +? T 2 ? ?1 2u )(x?(? ?1 1u +? ?1 2u ) ?1 (? ?1 1u ? 1 +? ?1 2u ? 2 ))+C 2 ) = C1e ? 1 2 (x T (? ?1 1u +? ?1 2u )(x?(? ?1 1u +? ?1 2u ) ?1 (? ?1 1u ? 1 +? ?1 2u ? 2 ))?(? T 1 ? ?1 1u +? T 2 ? ?1 2u )(? ?1 1u +? ?1 2u ) ?1 (? ?1 1u +? ?1 2u )(x?(? ?1 1u +? ?1 2u ) ?1 (? ?1 1u ? 1 +? ?1 2u ? 2 ))+C 2 ) = C1e ? 1 2 ((x T ?(? T 1 ? ?1 1u +? T 2 ? ?1 2u )(? ?1 1u +? ?1 2u ) ?1 )(? ?1 1u +? ?1 2u )(x?(? ?1 1u +? ?1 2u ) ?1 (? ?1 1u ? 1 +? ?1 2u ? 2 ))+C 2 ) = C1e ? 1 2 ((x?(? ?1 1u +? ?1 2u ) ?1 (? ?1 1u ? 1 +? ?1 2u ? 2 )) T (? ?1 1u +? ?1 2u )(x?(? ?1 1u +? ?1 2u ) ?1 (? ?1 1u ? 1 +? ?1 2u ? 2 ))+C 2 ) = C3e ? 1 2 (x?(? ?1 1u +? ?1 2u ) ?1 (? ?1 1u ? 1 +? ?1 2u ? 2 )) T (? ?1 1u +? ?1 2u )(x?(? ?1 1u +? ?1 2u ) ?1 (? ?1 1u ? 1 +? ?1 2u ? 2 )) ,<label>(14)</label></formula><p>where</p><formula xml:id="formula_12">C 1 = (2?) ? D 2 det[? 1 ] ? 1??u 2 det[? 2 ] ? ?u 2</formula><p>, C 2 is a constant for aborting the terms used for completing the square relative to x, and C 3 = C 1 e ? 1 2 C2 . The last formula of Eq. <ref type="formula" target="#formula_0">(13)</ref> is an unnormalized Gaussian curve with covariance</p><formula xml:id="formula_13">(? ?1 1u +? ?1 2u ) ?1 and mean (? ?1 1u +? ?1 2u ) ?1 (? ?1 1u ? 1 +? ?1 2u ? 2 ). Therefore, we can get ? ?u = ((1?? u )? ?1 1 +? u ? ?1 2 ) ?1 and ? ?u = ? ?u ((1?? u )? ?1 1 ? 1 +? u ? ?1 2 ? 2 ).</formula><p>After the normalization step, we can get a Gaussian distribution N ?u (? ?u , ? ?u ).</p><p>As for JS G? u , we first calculate E N1 [logN 1 ? logN ?u ] as follows:</p><formula xml:id="formula_14">EN 1 [logN1 ? logN? u ] = 1 2 EN 1 [?logdet[?1] ? (x ? ?1) T ? ?1 1 (x ? ?1) + logdet[?? u ] + (x ? ?? u ) T ? ?1 ?u (x ? ?? u )] = 1 2 (log det[?? u ] det[?1] + EN 1 [?(x ? ?1) T ? ?1 1 (x ? ?1) + (x ? ?? u ) T ? ?1 ?u (x ? ?? u )]) = 1 2 (log det[?? u ] det[?1] + EN 1 [?tr[? ?1 1 ?1] + tr[? ?1 ?u (xx T ? 2x? T ?u + ?? u ? T ?u )]]) = 1 2 log det[?? u ] det[?1] ? D 2 + 1 2 EN 1 [tr[? ?1 ?u (xx T ? 2x? T ?u + ?? u ? T ?u )]] = 1 2 log det[?? u ] det[?1] ? D 2 + 1 2 EN 1 [tr[? ?1 ?u ((x ? ?1)(x ? ?1) T + 2?1x T ? ?1? T 1 ? 2x? T ?u + ?? u ? T ?u )]] = 1 2 log det[?? u ] det[?1] ? D 2 + 1 2 tr[? ?1 ?u (?1 + ?1? T 1 ? 2?? u ? T 1 + ?? u ? T ?u )] = 1 2 log det[?? u ] det[?1] ? D 2 + 1 2 tr[? ?1 ?u ?1] + 1 2 tr[? T 1 ? ?1 ?u ?1 ? 2? T 1 ? ?1 ?u ?? u + ? T ?u ? ?1 ?u ?? u )] = 1 2 log det[?? u ] det[?1] ? D 2 + 1 2 tr[? ?1 ?u ?1] + 1 2 (?? u ? ?1) T ? ?1 ?u (?? u ? ?1).<label>(15)</label></formula><p>The calculation of E N2 [logN 2 ? logN ?u ] is the same, and then, JS G? u is given by:</p><formula xml:id="formula_15">JS G? u = 1 ? ?u 2 log det[?? u ] det[?1] ? D(1 ? ?u) 2 + 1 ? ?u 2 tr[? ?1 ?u ?1] + 1 ? ?u 2 (?? u ? ?1) T ? ?1 ?u (?? u ? ?1)+ ?u 2 log det[?? u ] det[?2] ? D?u 2 + ?u 2 tr[? ?1 ?u ?2] + ?u 2 (?? u ? ?2) T ? ?1 ?u (?? u ? ?2) = 1 2 (log det[?? u ] 1??u det[?1] 1??u + log det[?? u ] ?u det[?2] ?u ) ? D 2 + 1 2 tr(? ?1 ?u ((1 ? ?u)?1 + ?u?2))+ 1 ? ?u 2 (?? u ? ?1) T ? ?1 ?u (?? u ? ?1) + ?u 2 (?? u ? ?2) T ? ?1 ?u (?? u ? ?2) = 1 2 (log[ det[?? u ] det[?1] 1??u det[?2] ?u ] ? D + tr(? ?1 ?u ((1 ? ?u)?1 + ?u?2)) + (1 ? ?u)(?? u ? ?1) T ? ?1 ?u (?? u ? ?1)+ ?u(?? u ? ?2) T ? ?1 ?u (?? u ? ?2)).<label>(16)</label></formula><p>As to the dual form JS G? u * , we calculate E N? u [logN ?u ? logN 1 ], which is given by:</p><formula xml:id="formula_16">1 2 log det[?1] det[?? u ] ? D 2 + 1 2 tr[? ?1 1 ?? u ] + 1 2 (?1 ? ?? u ) T ? ?1 1 (?1 ? ?? u ). (17) ?u ? ?1 ?u ?? u ).<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>The deep neural network configuration and training details are summarized in <ref type="table" target="#tab_5">Table 5</ref>. As for the NP-Match related hyperparameters, we set the lengths of both memory banks (Q) to 2560. The coefficient (?) is set to 0.01, and we sample T = 10 latent vectors for each target point. The uncertainty threshold (? u ) is set to 0.4 for CIFAR-10, CIFAR-100, and STL-10, and it is set to 1.2 for ImageNet. NP-Match is trained by using stochastic gradient descent (SGD) with a momentum of 0.9. The initial learning rate is set to 0.03 for CIFAR-10, CIFAR-100, and STL-10, and it is set to 0.05 for ImageNet. The learning rate is decayed with a cosine decay schedule <ref type="bibr" target="#b29">(Loshchilov &amp; Hutter, 2016)</ref>, and NP-Match is trained for 2 20 iterations. The MLPs used in the NP model all have two layers with M hidden units for each layer. For WRN, M is a quarter of the channel dimension of the last convolutional layer, and as for ResNet-50, M is equal to 256. To compete with the most recent SOTA method (Zhang et al., 2021), we followed this work to use the Curriculum Pseudo Labeling (CPL) strategy in our method and UPS <ref type="bibr" target="#b42">(Rizve et al., 2021)</ref>. We initialize each memory bank with a random vector. We ran each label amount setting for three times using different random seeds to obtain the error bars on CIFAR-10, CIFAR-100, and STL-10, but on ImageNet, we only ran for once. GeForce GTX 1080 Ti GPUs were used for the experiments on CIFAR-10, CIFAR-100, and STL-10, while Tesla V100 SXM2 GPUs were used for the experiments on ImageNet.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Hyperparameter Exploration</head><p>We did additional experiments on CIFAR-10 and STL-10 in terms of the hyperparameters related to the NP model in NP-Match, in order to explore how performance is affected by the changes of hyperparameters, which may provide some hints to readers for applying NP-Match to other datasets. We consider four hyperparameters in total, including the uncertainty threshold (? u ), the length of memory banks (Q), the coefficient of JS G? u (?), and the number of sampled latent vectors (T ). By <ref type="figure" target="#fig_3">Figure 4(a)</ref>, a reasonable ? u is important. Specifically, lower ? u usually leads to worse performance, because lower ? u enforces NP-Match to select a limited number of unlabeled data during training, which is equivalent to training the whole framework with a small dataset. Conversely, when ? u is too large, more uncertain unlabeled samples are chosen, whose pseudo-labels might be incorrect, and using these uncertain samples to train the framework can also lead to a poor performance. Furthermore, the difficulty of a training set also affects the setting of ? u , as a more difficult dataset usually has more classes and hard samples (e.g., ImageNet), which makes the uncertainties of predictions large, so that ? u should be adjusted accordingly. From <ref type="figure" target="#fig_3">Figure 4(b)</ref>, the performance becomes better with the increase of Q. When more context points are used, the more information is involved for inference, and then the NP model can better estimate the global latent variable and make predictions. This observation is consistent with the experimental results where the original NPs are used for image completion <ref type="bibr" target="#b10">(Garnelo et al., 2018b)</ref>. <ref type="figure" target="#fig_3">Figure 4(c)</ref> shows the ablation study of ? that controls the contribution of JS G? u to the total loss function, and when ? = 0.01, we can obtain the best accuracy on both datasets. By <ref type="figure" target="#fig_3">Figure 4(d)</ref>, if T is smaller than 5, then the performance will go down, but when T is further increased, then the performance of NP-Match is not influenced greatly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Overview of NP-Match: it contains a convolutional neural network (CNN) and an NP model that is shown in the red dotted box. The feature vectors come from the global average pooling layer in the CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Analysis of average class-wise uncertainty and accuracy. Expected UCEs (%) of the MC-dropout-based model (i.e., UPS<ref type="bibr" target="#b42">(Rizve et al., 2021)</ref>) and of NP-Match on the test sets of CIFAR-10 and STL-10. Time consumption of estimating uncertainty for the MCdropout-based model (i.e., UPS<ref type="bibr" target="#b42">(Rizve et al., 2021)</ref>) and NP-Match. The horizontal axis refers to the number of predictions used for the uncertainty quantification, and the vertical axis indicates the time consumption (sec).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Performance for different hyperparameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Berthelot et al., 2019)  36.19 (?6.48)  13.63 (?0.59) 6.66(?0.26)  67.59 (?0.66)  39.76 (?0.48)  27.78 (?0.29)  54.93 (?0.96)  34.52 (?0.32)  21.70 (?0.68)   ReMixMatch 9.88 (?1.03) 6.30 (?0.05) 4.84 (?0.01) 42.75 (?1.05) 26.03 (?0.35) 20.02 (?0.27) 32.12 (?6.24) 12.49 (?1.28) 6.74 (?0.14) UDA (Xie et al., 2020) 10.62 (?3.75) 5.16 (?0.06) 4.29 (?0.07) 46.39 (?1.59) 27.73 (?0.21) 22.49 (?0.23) 37.42 (?8.44) 9.72 (?1.15) 6.64 (?0.17) CoMatch (Li et al., 2021) 6.88 (?0.92) 4.90 (?0.35) 4.06 (?0.03) 40.02 (?1.11) 27.01 (?0.21) 21.83 (?0.23) 31.77 (?2.56) 11.56 (?1.27) 8.66 (?0.41) SemCo (Nassar et al., 2021) 7.87 (?0.22) 5.12 (?0.27) 3.80 (?0.08) 44.11 (?1.18) 31.93 (?0.33) 24.45 (?0.12) 34.17 (?2.78) 12.23 (?1.40) 7.49 (?0.29) Meta Pseudo Labels (Pham et al., 2021) 6.93 (?0.17) 4.94 (?0.04) 3.89 (?0.07) 44.23 (?0.99) 27.68 (?0.22) 22.48 (?0.18) 34.29 (?3.29) 9.90 (?0.Comparison with SOTA results on CIFAR-10, CIFAR-100, and STL-10. The error rates are reported with standard deviation.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>CIFAR-10</cell><cell></cell><cell></cell><cell>CIFAR-100</cell><cell></cell><cell></cell><cell>STL-10</cell><cell></cell></row><row><cell>Label Amount</cell><cell>40</cell><cell>250</cell><cell>4000</cell><cell>400</cell><cell>2500</cell><cell>10000</cell><cell>40</cell><cell>250</cell><cell>1000</cell></row><row><cell cols="9">MixMatch (96)</cell><cell>6.45 (?0.26)</cell></row><row><cell>FlexMatch (Zhang et al., 2021)</cell><cell>4.96 (?0.06)</cell><cell cols="7">4.98 (?0.09) 4.19 (?0.01) 39.94 (?1.62) 26.49 (?0.20) 21.90 (?0.15) 29.15 (?4.16) 8.23 (?0.39)</cell><cell>5.77 (?0.18)</cell></row><row><cell>UPS (Rizve et al., 2021)</cell><cell>5.26 (?0.29)</cell><cell cols="7">5.11 (?0.08) 4.25 (?0.05) 41.07 (?1.66) 27.14 (?0.24) 21.97 (?0.23) 30.82 (?2.16) 9.77 (?0.44)</cell><cell>6.02 (?0.28)</cell></row><row><cell>FixMatch (Sohn et al., 2020)</cell><cell>7.47 (?0.28)</cell><cell cols="7">4.86 (?0.05) 4.21 (?0.08) 46.42 (?0.82) 28.03 (?0.16) 22.20 (?0.12) 35.96 (?4.14) 9.81 (?1.04)</cell><cell>6.25 (?0.33)</cell></row><row><cell>NP-Match (ours)</cell><cell>4.91 (?0.04)</cell><cell cols="7">4.96 (?0.06) 4.11 (?0.02) 38.91 (?0.99) 26.03 (?0.26) 21.22 (?0.13) 14.20 (?0.67) 9.51 (?0.37)</cell><cell>5.59 (?0.24)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>?0.06) 5.20 (?0.02) 4.36 (?0.03) 39.15 (?1.53) 26.48 (?0.23) 21.51 (?0.17) 14.67 (?0.38) 9.92 (?0.24) 6.21 (?0.23) NP-Match with JS G? u * 4.93 (?0.02) 4.87 (?0.03) 4.19 (?0.04) 38.67 (?1.29) 26.24 (?0.17) 21.33 (?0.10) 14.45 (?0.55) 9.48 (?0.28) 5.47 (?0.19) NP-Match with JS G? u 4.91 (?0.04) 4.96 (?0.06) 4.11 (?0.02) 38.91 (?0.99) 26.03 (?0.26) 21.22 (?0.13) 14.20 (?0.67) 9.51 (?0.37) 5.59 (?0.24)</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>CIFAR-10</cell><cell></cell><cell></cell><cell>CIFAR-100</cell><cell></cell><cell></cell><cell>STL-10</cell><cell></cell></row><row><cell>Label Amount</cell><cell>40</cell><cell>250</cell><cell>4000</cell><cell>400</cell><cell>2500</cell><cell>10000</cell><cell>40</cell><cell>250</cell><cell>1000</cell></row><row><cell>NP-Match with KL</cell><cell>5.32 (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>: A selftrained framework for semi-supervised and supervised learning with ensemble transformations. IEEE Transactions on Image Processing, 30:1639-1647, 2020.Wilson, A. G., Hu, Z., Salakhutdinov, R., and Xing, E. P.</figDesc><table><row><cell>Deep kernel learning. In Artificial Intelligence and Statis-</cell></row><row><cell>tics, pp. 370-378. PMLR, 2016.</cell></row><row><cell>Xie, Q., Dai, Z., Hovy, E., Luong, M.-T., and Le, Q. V.</cell></row><row><cell>Unsupervised data augmentation for consistency train-</cell></row><row><cell>ing. Advances in Neural Information Processing Systems,</cell></row><row><cell>2020.</cell></row><row><cell>Yasarla, R., Sindagi, V. A., and Patel, V. M. Syn2Real</cell></row><row><cell>transfer learning for image deraining using Gaussian pro-</cell></row><row><cell>cesses. In Proceedings of the IEEE/CVF Conference</cell></row><row><cell>on Computer Vision and Pattern Recognition, pp. 2726-</cell></row><row><cell>2736, 2020.</cell></row><row><cell>Yoon, J., Singh, G., and Ahn, S. Robustifying sequential</cell></row><row><cell>neural processes. In International Conference on Ma-</cell></row><row><cell>chine Learning, pp. 10861-10870. PMLR, 2020.</cell></row><row><cell>Yu, L., Wang, S., Li, X., Fu, C.-W., and Heng, P.-</cell></row><row><cell>A. Uncertainty-aware self-ensembling model for semi-</cell></row><row><cell>supervised 3D left atrium segmentation. In International</cell></row><row><cell>Conference on Medical Image Computing and Computer-</cell></row><row><cell>Assisted Intervention, pp. 605-613. Springer, 2019.</cell></row><row><cell>Zagoruyko, S. and Komodakis, N. Wide residual networks.</cell></row><row><cell>British Machine Vision Conference, 2016.</cell></row><row><cell>Zhai, X., Oliver, A., Kolesnikov, A., and Beyer, L. S4l: Self-</cell></row><row><cell>supervised semi-supervised learning. In Proceedings of</cell></row><row><cell>the IEEE/CVF International Conference on Computer</cell></row><row><cell>Vision, pp. 1476-1485, 2019.</cell></row><row><cell>Zhang, B., Wang, Y., Hou, W., Wu, H., Wang, J., Oku-</cell></row><row><cell>mura, M., and Shinozaki, T. FlexMatch: Boosting semi-</cell></row><row><cell>supervised learning with curriculum pseudo labeling. Ad-</cell></row><row><cell>vances in Neural Information Processing Systems, 34,</cell></row><row><cell>2021.</cell></row><row><cell>Zhu, H., Li, Y., Bai, F., Chen, W., Li, X., Ma, J., Teo,</cell></row><row><cell>C. S., Tao, P. Y., and Lin, W. Grasping detection net-</cell></row><row><cell>work with uncertainty estimation for confidence-driven</cell></row><row><cell>semi-supervised domain adaptation. In 2020 IEEE/RSJ</cell></row><row><cell>International Conference on Intelligent Robots and Sys-</cell></row><row><cell>tems, pp. 9608-9613. IEEE, 2020.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Details of the training setting.</figDesc><table><row><cell>Dataset</cell><cell cols="2">CIFAR-10 CIFAR-100</cell><cell>STL-10</cell><cell>ImageNet</cell></row><row><cell>Model</cell><cell cols="4">WRN-28-2 WRN-28-8 WRN-37-2 ResNet-50</cell></row><row><cell>Weight Decay</cell><cell>5e-4</cell><cell>1e-3</cell><cell>5e-4</cell><cell>1e-4</cell></row><row><cell>Batch Size (B)</cell><cell></cell><cell>64</cell><cell></cell><cell>256</cell></row><row><cell>?</cell><cell></cell><cell>7</cell><cell></cell><cell>1</cell></row><row><cell>Confidence Threshold (? c )</cell><cell></cell><cell>0.95</cell><cell></cell><cell>0.7</cell></row><row><cell>EMA Momentum</cell><cell></cell><cell>0.999</cell><cell></cell><cell></cell></row><row><cell>? u</cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">As long as one of q(z|x1:m, y1:m) and q(z|xm+1: m+r , ym+1: m+r ) is close to q * , the proposed JS G? u mitigates the issue, but JS G? u still has difficulties to solve the problem when both of their calculations involve many low-quality representations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Note that UPS<ref type="bibr" target="#b42">(Rizve et al., 2021)</ref> does not use strong augmentations, thus we re-implemented it withRandAugment (Cubuk  et al., 2020)  for fair comparisons.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was partially supported by the Alan Turing Institute under the EPSRC grant EP/N510129/1, by the AXA Research Fund, and by the EPSRC grant EP/R013667/1. We also acknowledge the use of the EPSRC-funded Tier 2 facility JADE (EP/P020275/1) and GPU computing support by Scan Computers International Ltd.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>A. Derivation of ELBO (Eq. (4)) Proof. As for the marginal joint distribution p(y 1:n |x 1:n ) over n data points in which there are m context points and r target points (i.e., m + r = n), we assume a variation distribution q, and then: log p(y1:n|x1:n) = log z p(z, y1:n|x1:n) = log z p(z, y1:n|x1:n) q(z|xm+1: m+r , ym+1: m+r ) q(z|xm+1: m+r , ym+1: m+r ) </p><p>where "const" refers to E q(z|xm+1: m+r ,ym+1: m+r ) [log p(y 1:m )], which is a constant term. Concerning that p(z|x 1:m , y 1:m ) is unknown, we replace it with q(z|x 1:m , y 1:m ), and then we get:</p><p>log p(y1:n|x1:n) ? E q(z|x m+1: m+r ,y m+1: m+r ) m+r i=m+1 log p(yi|z, xi) ? log q(z|xm+1: m+r , ym+1: m+r ) q(z|x1:m, y1:m) + const.</p><p>B. Proof of Theorem 1</p><p>Now, we let ? ?1 1u = (1 ? ? u )? ?1 1 and ? ?1 2u = ? u ? ?1 2 , then:</p><p>Then, the calculation of E N? u [logN ?u ? logN 2 ] is the same, and JS G? u * is given by:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning with pseudo-ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3365" to="3373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MixMatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Remixmatch</surname></persName>
		</author>
		<title level="m">Semisupervised learning with distribution alignment and augmentation anchoring. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Gaussian neural process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">P</forename><surname>Bruinsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Foong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Approximate Bayesian Inference</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tri-net for semi-supervised deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2014" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ran-dAugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Np-Match ; Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
	<note>When Neural Processes meet Semi-Supervised Learning</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Conditional neural processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1704" to="1713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01622</idno>
		<title level="m">Neural processes</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">P</forename><surname>Bruinsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Foong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<title level="m">Convolutional conditional neural processes. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SimPLE: Similar pseudo label exploitation for semi-supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15099" to="15108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised deep kernel learning: Regression with unlabeled data by minimizing predictive variance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<title level="m">Attentive neural processes. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving model calibration with accuracy versus uncertainty optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tickoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Calibration of model uncertainty for dropout variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Laves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ihler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-P</forename><surname>Kortmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ortmaier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11584</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semisupervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Challenges in Representation Learning, International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">896</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bootstrapping neural processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6606" to="6615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">CoMatch: Semi-supervised learning with contrastive graph regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9475" to="9484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards making unlabeled data never hurt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="175" to="188" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large graph construction for scalable semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Uncertainty aware graph Gaussian process for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4957" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sgdr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The functional neural process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schutte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Obtaining well calibrated probabilities using bayesian binning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">M P</forename><surname>Np-Match</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hauskrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">When Neural Processes meet Semi-Supervised Learning Naeini</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>29th AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">All labels are not created equal: Enhancing semi-supervision via label grouping and co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Nassar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Herath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Abbasnejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Haffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7241" to="7250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bayesian semisupervised learning with graph Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Convex functions and their applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Niculescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-E</forename><surname>Persson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On a generalization of the Jensen-Shannon divergence and the Jensen-Shannon centroid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">221</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Garcia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0911.4863</idno>
		<title level="m">Statistical exponential families: A digest with flash cards</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>?ksendal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stochastic Differential Equations</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="65" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Meta pseudo labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11557" to="11568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep co-training for semi-supervised image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuille</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Recurrent attentive neural process for sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LIRE Workshop NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fast and flexible multi-task classification using conditional neural adaptive processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="7959" to="7970" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">In defense of pseudo-labeling: An uncertainty-aware pseudolabel selection framework for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Rizve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1163" to="1171" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Uncertainty guided semi-supervised segmentation of retinal layers in OCT images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sedai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Antony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="282" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Inconsistency-aware uncertainty estimation for semi-supervised medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Very deep convolutional networks for large-scale image recognition. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semi-supervised Gaussian process classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Keerthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1059" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Sequential neural processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahn</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fixmatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Graph convolutional Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6495" to="6504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Tripled-uncertainty guided mean teacher model for semi-supervised medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="450" to="460" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
