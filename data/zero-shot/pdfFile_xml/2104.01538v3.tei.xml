<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hypercorrelation Squeeze for Few-Shot Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahyun</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hypercorrelation Squeeze for Few-Shot Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot semantic segmentation aims at learning to segment a target object from a query image using only a few annotated support images of the target class. This challenging task requires to understand diverse levels of visual cues and analyze fine-grained correspondence relations between the query and the support images. To address the problem, we propose Hypercorrelation Squeeze Networks (HSNet) that leverages multi-level feature correlation and efficient 4D convolutions. It extracts diverse features from different levels of intermediate convolutional layers and constructs a collection of 4D correlation tensors, i.e., hypercorrelations. Using efficient center-pivot 4D convolutions in a pyramidal architecture, the method gradually squeezes high-level semantic and low-level geometric cues of the hypercorrelation into precise segmentation masks in coarse-to-fine manner. The significant performance improvements on standard fewshot segmentation benchmarks of PASCAL-5 i , COCO-20 i , and FSS-1000 verify the efficacy of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The advent of deep convolutional neural networks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b63">64]</ref> has promoted dramatic advances in many computer vision tasks including object tracking <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b44">45]</ref>, visual correspondence <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">48]</ref>, and semantic segmentation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b61">62]</ref> to name a few. Despite the effectiveness of deep networks, their demand for a heavy amount of annotated examples from large-scale datasets <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b34">35]</ref> still remains a fundamental limitation since data labeling requires substantial human efforts, especially for dense prediction tasks, e.g., semantic segmentation. To cope with the challenge, there have been various attempts in semi-and weakly-supervised segmentation approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b87">88]</ref> which in turn effectively alleviated the data-hunger issue. However, given only a few annotated training examples, the problem of poor generalization ability of the deep networks is yet the primary concern that many few-shot segmentation methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b88">89]</ref> struggle to address. In contrast, human visual system easily achieves generalizing appearances of new objects given extremely limited supervision. The crux of such intelligence lies at the ability in finding reliable correspondences across different instances of the same class. Recent work on semantic correspondence shows that leveraging dense intermediate features <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44]</ref> and processing correlation tensors with high-dimensional convolutions <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b70">71]</ref> are significantly effective in establishing accurate correspondences. However, while recent few-shot segmentation research began active exploration in the direction of correlation learning, most of them <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b79">80]</ref> neither exploit diverse levels of feature representations from early to late layers of a CNN nor construct pair-wise feature correlations to capture fine-grained correlation patterns. There have been some attempts <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b85">86]</ref> in utilizing dense correlations with multilevel features, but they are yet limited in the sense that they simply employ the dense correlations for graph attention, using only a small fraction of intermediate conv layers.</p><p>In this work we combine the two of the most influential techniques in recent research of visual correspondence, multi-level features and 4D convolutions, and deign a novel framework, dubbed Hypercorrelation Squeeze Networks (HSNet), for the task of few-shot semantic segmentation. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, our network exploits diverse geometric/semantic feature representations from many different intermediate CNN layers to construct a collection of 4D correlation tensors, i.e., hypercorrelations, which represent a rich set of correspondences in multiple visual aspects. Following the work of FPN <ref type="bibr" target="#b33">[34]</ref>, we adapt pyramidal design to capture both high-level semantic and low-level geometric cues for precise mask prediction in coarse-to-fine manner using deeply stacked 4D conv layers. To reduce computational burden caused by such heavy use of highdimensional convs, we devise an efficient 4D kernel via reasonable weight-sparsification which enables real-time inference while being more effective and light-weight than the existing ones. The improvements on standard few-shot segmentation benchmarks of PASCAL-5 i <ref type="bibr" target="#b60">[61]</ref>, COCO-20 i <ref type="bibr" target="#b34">[35]</ref>, and FSS-1000 <ref type="bibr" target="#b32">[33]</ref> verify the efficacy of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semantic segmentation. The goal of semantic segmentation is to classify each pixel of an image into one of the predefined object categories. Prevalent segmentation approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b75">76]</ref> typically employ encoderdecoder structure in their architecture; the encoder aggregates features along deep convolutional pathways and provides high-dimensional feature map in low-resolution and the corresponding decoder takes the output to predict segmentation mask by reversing this process <ref type="bibr" target="#b48">[49]</ref>. Although the methods clearly show the effectiveness of the encoderdecoder architecture in the task of semantic segmentation, offering useful insights to our study, they still suffer apparent disadvantages of data-driven nature of neural networks: lack of generalizibility under insufficient training data. Few-shot learning. To resolve the generalization problem, many recent approaches to image classification made various attempts in training deep networks with a few annotated examples <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b84">85]</ref>. Vinyals et al. <ref type="bibr" target="#b72">[73]</ref> propose matching networks for one-shot learning; the method utilizes a special kind of mini-batches called episodes to match training and testing environments, facilitating better generalization on novel classes. Snell et al. <ref type="bibr" target="#b64">[65]</ref> introduce prototypical networks which compute distances between representative embeddings, i.e., prototypes, for few-shot classification. With the growing interests in few-shot learning in classification domain, the problem of few-shot segmentation has attracted a great deal of attention as well. Shaban et al. <ref type="bibr" target="#b60">[61]</ref> propose one-shot semantic segmentation networks which (meta-) learns to generate parameters of FCN <ref type="bibr" target="#b61">[62]</ref>. Inspired by the prototypical networks <ref type="bibr" target="#b64">[65]</ref>, utilizing prototype representations to guide mask prediction in a query image became a popular paradigm in few-shot segmentation literature <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b88">89]</ref>.</p><p>Witnessing the limitation of prototypical approaches, e.g., loss of spatial structure due to masked average pooling <ref type="bibr" target="#b88">[89]</ref>, work of <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b85">86]</ref> build pair-wise feature correlations, e.g., graph attention, to retain the spatial structure of the images for fine-grained mask prediction. Note that both prototypical and graph-based methods fundamentally focus on learning to find reliable correspondences between support and query images for accurate mask prediction. In this work, we advance this idea and focus on learning to analyze correspondences using adequately designed learnable layers, e.g., 4D convolutions <ref type="bibr" target="#b57">[58]</ref>, for effective semantic segmentation. Learning visual correspondences. The task of visual correspondence aims to find reliable correspondences under challenging degree of variations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b59">60]</ref>. Many methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b80">81]</ref> typically built upon convolutional features pretrained on classification task <ref type="bibr" target="#b8">[9]</ref>, showing they serve as good transferable representations. Recent approaches to semantic correspondence <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44]</ref> show that efficiently exploiting different levels of convolutional features distributed over all intermediate layers clearly benefits matching accuracy. In wide-baseline matching literature, a trending choice is to employ 4D convolutions <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b70">71]</ref> on dense feature matches to identify spatially consistent matches by analyzing local patterns in 4D space. The use of multi-level features and relational pattern analysis using 4D convs are the two widely adopted techniques in the field of visual correspondence.</p><p>In this paper we adapt the two most influential methodologies in visual correspondence to tackle few-shot segmentation: multi-level features and 4D convolutions. Inspired by the previous matching methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b26">27]</ref>, which use multi-level features to build effective "appearance features", we construct high-dimensional "relational features" using intermediate CNN features and process them with a series of 4D convolutions. However, their quadratic complexity still remains a major bottleneck in designing cost-effective deep networks, constraining many previous matching methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b70">71]</ref> to use only a few 4D conv layers. To resolve the issue, we develop a light-weight 4D convolutional kernel by collecting only a small subset of vital parameters for effective pattern recognition, which eventually leads to an efficient decomposition into a pair of 2D conv kernels with a linear complexity. Our contributions can be summarized as follows:</p><p>? We present the Hypercorrelation Squeeze Networks that analyze dense feature matches of diverse visual aspects using deeply stacked 4D conv layers. ? We propose center-pivot 4D conv kernel which is more effective than the existing one in terms both accuracy and speed, achieving real-time inference. ? The proposed method sets a new state of the art on three standard few-shot segmentation benchmarks: PASCAL-5 i <ref type="bibr" target="#b60">[61]</ref>, COCO-20 i <ref type="bibr" target="#b34">[35]</ref>, and FSS-1000 <ref type="bibr" target="#b32">[33]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypercorrelation construction</head><p>Cosine similarity <ref type="figure">Figure 2</ref>: Overall architecture of the proposed network which consists of three main parts: hypercorrelation construction, 4D-convolutional pyramid encoder, and 2D-convolutional context decoder. We refer the readers to Sec. 4 for details of the architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Setup</head><p>The goal of few-shot semantic segmentation is to perform segmentation given only a few annotated examples. To avoid the risk of overfitting due to insufficient training data, we adopt widely used meta-learning approach called episodic training <ref type="bibr" target="#b72">[73]</ref>. Let us denote respective training and test sets as D train and D test which are disjoint with respect to object classes. Both sets consist of multiple episodes each of which is composed of a support set S = (I s , M s ) and a query set Q = (I q , M q ) where I * and M * are an image and its corresponding mask label respectively. During training, our model iteratively samples an episode from D train to learn a mapping from (I s , M s , I q ) to query mask M q . Once the model is trained, it uses the learned mapping for evaluation without further optimization, i.e., the model takes randomly sampled (I s , M s , I q ) from D test to predict query mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Approach</head><p>In this section, we present a novel few-shot segmentation architecture, Hypercorrelation Squeeze Networks (HSNet), which capture relevant patterns in multi-level feature correlations between a pair of input images to predict fine-grained segmentation mask in a query image. As illustrated in <ref type="figure">Fig. 2</ref>, we adopt an encoder-decoder structure in our architecture; the encoder gradually squeezes dimension of the input hypercorrelations by aggregating their local information to a global context, and the decoder processes the encoded context to predict a query mask. In Sec. 4.1-4.3, we demonstrate each pipeline in one-shot setting, i.e., the model predicts the query mask given I q and S = (I s , M s ). In Sec. 4.4, to mitigate large resource demands of 4D convs, we present a light-weight 4D kernel which greatly improves model efficiency in terms of both memory and time. In Sec. 4.5, we demonstrate how the model can be easily extended to K-shot setting, i.e., S = {(I s k , M s k )} K k=1 , without loss of generality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Hypercorrelation construction</head><p>Inspired by recent semantic matching approaches <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44]</ref>, our model exploits a rich set of features from the intermediate layers of a convolutional neural network to capture multi-level semantic and geometric patterns of similarities between the support and query images. Given a pair of query and support images, I q , I s ? R 3?H?W , the backbone network produces a sequence of L pairs of intermediate feature maps {(F q l , F s l )} L l=1 . We mask each support feature map F s l ? R C l ?H l ?W l using the support mask M s ? {0, 1} H?W to discard irrelevant activations for reliable mask prediction:</p><formula xml:id="formula_0">F s l = F s l ? l (M s ),<label>(1)</label></formula><p>where is Hadamard product and ? l (?) is a function that bilinearly interpolates input tensor to the spatial size of the feature map F s l at layer l followed by expansion along channel dimension such that ? l : R H?W ? ? R C l ?H l ?W l . For the subsequent hypercorrleation construction, a pair of query and masked support features at each layer forms a 4D correlation tensor? l ? R H l ?W l ?H l ?W l using cosine similarity:</p><formula xml:id="formula_1">C l (x q , x s ) = ReLU F q l (x q ) ?F s l (x s ) F q l (x q ) Fs l (x s ) ,<label>(2)</label></formula><p>where x q and x s denote 2-dimensional spatial positions of feature maps F q l andF s l respectively, and ReLU suppresses noisy correlation scores. From the resultant set of 4D correlations {? l } L l=1 , we collect 4D tensors if they have the same spatial sizes and denote the subset as {? l } l?Lp where L p is a subset of CNN layer indices {1, ..., L} at some pyramidal layer p. Finally, all the 4D tensors in {? l } l?Lp are concatenated along channel dimension to form a hypercorrelation C p ? R |Lp|?Hp?Wp?Hp?Wp where (H p , W p , H p , W p ), with abuse of notation, represents the spatial resolution of the hypercorrelation at pyramidal layer p. Given P pyramidal layers, we denote hypercorrelation pyramid as C = {C p } P p=1 , representing a rich collection of feature correlations from multiple visual aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">4D-convolutional pyramid encoder</head><p>Our encoder network takes the hypercorrelation pyramid C = {C p } P p=1 to effectively squeeze it into a condensed feature map Z ? R 128?H1?W1 . We achieve this correlation learning using two types of building blocks: a squeezing block f sqz p and a mixing block f mix p . Each block consists of three sequences of multi-channel 4D convolution, group normalization <ref type="bibr" target="#b77">[78]</ref>, and ReLU activation as illustrated in <ref type="figure">Fig. 3</ref>. In the squeezing block f sqz p , large strides periodically squeeze the last two (support) spatial dimensions of C p down to (H , W ) while the first two spatial (query) dimensions remain the same as ( <ref type="bibr" target="#b33">[34]</ref> structure, two outputs from adjacent pyramidal layers, p and p+1, are merged by element-wise addition after upsampling the (query) spatial dimensions of the upper layer output by a factor of 2. The mixing block f mix p : R 128?Hp?Wp?H ?W ? ? R 128?Hp?Wp?H ?W then processes this mixture with 4D convolutions to propagate relevant information to lower layers in a top-down fashion. After the iterative propagation, the output tensor of the lowest mixing block f mix 1 is further compressed by average-pooling its last two (support) spatial dimensions, which in turn provides a 2-dimensional feature map Z ? R 128?H1?W1 that signifies a condensed representation of the hypercorrelation C.</p><formula xml:id="formula_2">H p , W p ), i.e., f sqz p : R |Lp|?Hp?Wp?Hp?Wp ? ? R 128?Hp?Wp?H ?W where H p &gt; H and W p &gt; W . Similar to FPN</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">2D-convolutional context decoder</head><p>The decoder network consists of a series of 2D convolutions, ReLU, and upsampling layers followed by softmax function as illustrated in <ref type="figure">Fig. 2</ref>. The network takes the context representation Z and predicts two-channel map M q ? [0, 1] 2?H?W where two channel values indicate probabilities of foreground and background. During training, the network parameters are optimized using the mean of crossentropy loss between the predictionM q and the ground-truth M q over all pixel locations. During testing, we take the maximum channel value at each pixel to obtain final query mask predictionM q ? {0, 1} H?W for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Center-pivot 4D convolution</head><p>Apparently, our network with such a large number of 4D convolutions demands a substantial amount of resources due to the curse of dimensionality, which constrained many visual correspondence methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b70">71</ref>] to use only a few 4D conv layers. To address the concern, we revisit the 4D convolution operation and delve into its limitations. Then we demonstrate how a unique weight-sparsification scheme effectively resolves the issues.  <ref type="figure">Figure 3</ref>: Building blocks in Hypercorrelation Squeeze Networks. s and g denotes strides of 4D conv and the number of groups in group normalization <ref type="bibr" target="#b77">[78]</ref> respectively. Note p ? {1, 2} for f mix p .</p><p>4D convolution and its limitation. Typical 4D convolution parameterized by a kernel k ? Rk ?k?k?k on a correlation tensor c ? R H?W ?H?W at position (x, x ) ? R 4 * is formulated as</p><formula xml:id="formula_3">(c * k)(x, x ) = (p,p )?P(x,x ) c(p, p )k(p ? x, p ? x ),<label>(3)</label></formula><p>where P(x, x ) denotes a set of neighbourhood regions within the local 4D window centered on position (x, x ), i.e., P(x, x ) = P(x) ? P(x ) as visualized in <ref type="figure">Fig. 4</ref>. Although the use of 4D convolutions on a correlation tensor has shown its efficacy with good empirical performance in correspondence-related domains <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b70">71]</ref>, its quadratic complexity with respect to the size of input features still remains a primary bottleneck. Another limiting factor is over-parameterization of the high-dimensional kernel: Consider a single activation in an nD tensor convolved by nD conv kernel. The number of times that the kernel processes this activation is exponentially proportional to n. This implies some unreliable input activations with large magnitudes may entail some noise in capturing reliable patterns as a result of their excessive exposure to the highdimensional kernel. The work of <ref type="bibr" target="#b80">[81]</ref> resolves the former problem (quadratic complexity) using spatially separable 4D kernels to approximate the 4D conv with two separate 2D kernels along with additional batch normalization layers <ref type="bibr" target="#b22">[23]</ref> that settle the latter problem (numerical instability). In this work we introduce a novel weight-sparsification scheme to address both issues at the same time.</p><p>* The correlation tensor c is the output of cosine similarity (Eqn. 2) between a pair of feature maps, F, F ? R H?W , and x and x denote 2-dimensional spatial positions of the respective feature maps.</p><formula xml:id="formula_4">, ? = ? ? CP , ? = {( , ? ) ? , ? : = ? ? = ?} ? ? ? 4-dimensional feature space 4D convolution convolved at position ( , ? )</formula><p>4D convolutional kernel Center-pivot 4D convolutional kernel weightsparsification <ref type="figure">Figure 4</ref>: 4D convolution (left) and weights of 4D kernel <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b80">81]</ref> (middle) and center-pivot 4D kernel (right). Each black wire that connects two different pixel locations represents a single weight of the 4D kernel. The kernel size used in this example is (3, 3, 3, 3), i.e.,k = 3.</p><p>Center-pivot 4D convolution. Our goal is to design a lightweight 4D kernel that is efficient in terms of both memory and time while effectively approximating the existing ones <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b80">81]</ref>. We achieve this via a reasonable weightsparsification; from a set of neighborhood positions within a local 4D window of interest, our kernel aims to disregard a large number of activations located at fairly insignificant positions in the 4D window, thereby focusing on a small subset of relevant activations only. Specifically, we consider the activations at positions that pivots either one of 2-dimensional centers, e.g., x or x , as the foremost influential ones as illustrated in <ref type="figure">Fig. 4</ref>. Given 4D position (x, x ), we collect its neighbors if and only if they are adjacent to either x or x in its corresponding 2D subspace and define two respective sets as</p><formula xml:id="formula_5">P c (x, x ) = {(p, p ) ? P(x, x ) : p = x} and P c (x, x ) = {(p, p ) ? P(x, x ) : p = x }. The set of center-pivot neighbours is defined as P CP (x, x ) = P c (x, x ) ? P c (x, x )</formula><p>. Based on these two subsets of neighbors, center-pivot 4D convolution can be formulated as a union of two separate 4D convolutions:</p><formula xml:id="formula_6">(c * k CP )(x, x ) = (c * k c )(x, x ) + (c * k c )(x, x ) (4)</formula><p>where k c and k c are 4D kernels convolved on P c (x, x ) and</p><formula xml:id="formula_7">P c (x, x ) respectively. Note that (c * k c )(x, x ) is equivalent to convolutions with a 2D kernel k 2D c = k(0, :) ? Rk ?k performed on 2D slice of 4D tensor c(x, :). Similarly, with k 2D c = k(:, 0) ? Rk ?k , we reformulate Eqn. 4 as follows (c * k CP )(x, x ) = p ?P(x ) c(x, p )k 2D c (p ? x )<label>(5)</label></formula><formula xml:id="formula_8">+ p?P(x) c(p, x )k 2D c (p ? x),</formula><p>which performs two different convolutions on separate 2D subspaces, having a linear complexity. In Sec. 5.2, we experimentally demonstrate the superiority of the center-pivot 4D kernels over the existing ones <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b80">81]</ref> in terms of accuracy, memory, and time. We refer the readers to the Appendix A for a complete derivation of Eqn. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Extension to K-shot setting</head><p>Our network can be easily extended to K-shot setting:</p><formula xml:id="formula_9">Given K support image-mask pairs S = {(I s k , M s k )} K k=1</formula><p>and a query image I q , model performs K forward passes to provide a set of K mask predictions {M q k } K k=1 . We perform voting at every pixel location by summing all the K predictions and divide each output score by the maximum voting score. We assign foreground labels to pixels if their values are larger than some threshold ? whereas the others are classified as background. We set ? = 0.5 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head><p>In this section we evaluate the proposed method, compare it with recent state of the arts, and provide in-depth analyses of the results with ablation study.</p><p>Implementation details. For the backbone network, we employ VGG <ref type="bibr" target="#b63">[64]</ref> and ResNet <ref type="bibr" target="#b16">[17]</ref> families pre-trained on ImageNet <ref type="bibr" target="#b8">[9]</ref>, e.g., VGG16, ResNet50, and ResNet101. For VGG16 backbone, we extract features after every conv layer in the last two building blocks: from conv4_x to conv5_x, and after the last maxpooling layer. For ResNet backbones, we extract features at the end of each bottleneck before ReLU activation: from conv3_x to conv5_x. This feature extracting scheme results in 3 pyramidal layers (P = 3) for each backbone. We set spatial sizes of both support and query images to 400 ? 400, i.e., H, W = 400, thus having H 1 , W 1 = 50, H 2 , W 2 = 25, and H 3 , W 3 = 13. The network is implemented in PyTorch <ref type="bibr" target="#b50">[51]</ref> and optimized using Adam <ref type="bibr" target="#b23">[24]</ref> with learning rate of 1e-3. We freeze the pre-trained backbone networks to prevent them from learning class-specific representations of the training data.</p><p>Datasets. We evaluate the proposed network on three standard few-shot segmentation datasets: PASCAL-5 i [61], COCO-20 i <ref type="bibr" target="#b34">[35]</ref>, and FSS-1000 <ref type="bibr" target="#b32">[33]</ref>. PASCAL-5 i is created from PASCAL VOC 2012 <ref type="bibr" target="#b10">[11]</ref> with extra mask annotations <ref type="bibr" target="#b15">[16]</ref>, consisting of 20 object classes that are evenly divided into 4 folds: {5 i : i ? {0, 1, 2, 3}}. COCO-20 i consists of mask-annotated images from 80 object classes divided into 4 folds: {20 i : i ? {0, 1, 2, 3}}. Following common training/evaluation scheme <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b79">80]</ref>, we conduct cross-validation over all the folds; for each fold i, samples from the other remaining folds are used for training and 1,000 episodes from the target fold i are randomly sampled for evaluation. For every fold, we use the same    <ref type="table">Table 3</ref>: Mean IoU comparison on FSS-1000 <ref type="bibr" target="#b32">[33]</ref>. Some results are from <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b73">74]</ref>. model with the same hyperparameter setup following the standard cross-validation protocol. FSS-1000 contains maskannotated images from 1,000 classes divided into training, validation and test splits having 520, 240, and 240 classes respectively.</p><p>Evaluation metrics. We adopt mean intersection over union (mIoU) and foreground-background IoU (FB-IoU) as our evaluation metrics. The mIoU metric averages over IoU values of all classes in a fold: mIoU = 1 C C c=1 IoU c where C is the number of classes in the target fold and IoU c is the intersection over union of class c. FB-IoU ignores object classes and computes average of foreground and background IoUs: FB-IoU = 1 2 (IoU F + IoU B ) where IoU F and IoU B are respectively foreground and background IoU values in the target fold. As mIoU better reflects model generalization capability and prediction quality than FB-IoU does, we mainly focus on mIoU in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results and analysis</head><p>We evaluate the proposed model on PASCAL-5 i , COCO-20 i , and FSS-1000 and compare the results with recent methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b85">86]</ref>. <ref type="table" target="#tab_3">Table 1</ref> summarizes 1-shot and 5-shot results on PASCAL-5 i ; all of our models with three different backbones clearly set new state of the arts with the smallest the number of learnable parameters. With ResNet101 backbone, our 1-shot and 5-shot results respectively achieve 6.1%p and 4.8%p of mIoU improvements over <ref type="bibr" target="#b69">[70]</ref> and <ref type="bibr" target="#b3">[4]</ref>, verifying its superiority in few-shot segmentation task. As shown in Tab. 2, our model outperforms recent methods with a sizable margin on COCO-20 i as well, achieving 2.7%p (1-shot) and 6.8%p (5-shot) of mIoU improvements over <ref type="bibr" target="#b69">[70]</ref> with ResNet101 backbone. Also on the last benchmark, FSS-1000, our method sets a new state of the art, outperforming <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b73">74]</ref> as shown in Tab. 3.</p><p>We conduct additional experiments without support feature masking (Eqn. 1). Note that this setup is similar to co-segmentation problem <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b81">82]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation study</head><p>We conduct extensive ablation study to investigate the impacts of major components in our model: hypercorrelations, pyramidal architecture, and center-pivot 4D kernels. We also study how freezing backbone networks prevents overfitting and helps generalization on novel classes. All ablation study experiments are performed with ResNet101 backbone on PASCAL-5   Ablation study on pyramid layers. To see the impact of hypercorrelation C p at each layer p, we perform experiments in absence of each pyramidal layer. We train and evaluate our model using two different hypercorrelation pyramids, C (2:3) = {C 2 , C 3 } and C (3) = {C 3 }, and compare the results with ours C = {C p } 3 p=1 . <ref type="figure">Figure 6</ref> summarizes the results; given hypercorrelation pyramid without geometric information (C <ref type="figure">(2:3)</ref> ), our model fails to refine object boundaries in the final mask prediction as visualized in <ref type="figure" target="#fig_3">Fig. 7</ref>. Given a single hypercorrelation that only encodes semantic relations (C (3) ), the model predictions are severely damaged, providing only rough localization of the target objects. These results indicate that capturing patterns of both semantic and geometric cues is essential for fine-grained localization. Comparison between three different 4D kernels. We conduct ablation study on 4D kernel by replacing the proposed center-pivot 4D kernel with the original <ref type="bibr" target="#b57">[58]</ref> and spatially separable <ref type="bibr" target="#b80">[81]</ref> 4D kernels and compare their model size,  <ref type="table">Table 5</ref>: Comparison between three different 4D conv kernels in model size, per-episode inference time, memory consumption and FLOPs. For fair comparison, the inference times of all the models are measured on a machine with an Intel i7-7820X and an NVIDIA Titan-XP. per-episode inference time (1-shot), memory consumption, and floating point operations per second (FLOPs) with ours. <ref type="table">Table 5</ref> summarizes the results. The proposed kernel records the fastest inference time with the smallest memory/FLOPs requirements while being comparably effective than the other two. The results clearly support our claim that a large part of parameters in a high-dimensional kernel can safely be discarded without harming the quality of predictions; only a few relevant parameters are sufficient and even better for the purpose. While both the separable <ref type="bibr" target="#b80">[81]</ref> and our centerpivot 4D convolutions operate on two separate 2D convolutions, auxiliary transformation layers with multiple batch normalizations that make the separable 4D conv numerically stable in its sequential design result in twice larger number of parameters (4.4M vs. 2.6M) and slower inference time (28.48ms vs. 25.51ms) than ours.</p><p>The number of 4D layers in building blocks. We also perform experiments with varying number of 4D conv layers in the two building blocks: f sqz p and f mix p . <ref type="figure" target="#fig_4">Figure 8</ref> plots 1shot and 5-shot mIoU results on PASCAL-5 i with the model sizes. In the experiments, appending additional 4D layers (with a group norm and a ReLU activation) in the building blocks provides clear performance improvements up to three layers but the accuracy eventually saturates after all. Hence we use a stack of three 4D layers for both.</p><p>Finetuning backbone networks. To investigate the significance of learning 'feature correlations' over learning 'feature representation' in few-shot regime, we finetune our backbone network and compare learning processes of the finetuned model and ours (frozen backbone). <ref type="figure" target="#fig_5">Figure 9</ref>  a vast amount of past experiences, e.g., ImageNet classification. This is quite analogous to human vision perspective in the sense that we generalize novel concepts (what we see) by analyzing their relations to the past observations (what we know) <ref type="bibr" target="#b39">[40]</ref>. For additional experimental details, results and analyses, we refer the readers to the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have presented a novel framework that analyzes complex feature correlations in a fully-convolutional manner using light-weight 4D convolutions. The significant performance improvements on three standard benchmarks demonstrate that learning patterns of feature relations from multiple visual aspects is effective in fine-grained segmentation under limited supervision. We also demonstrated a unique way of discarding insignificant weights leads to an efficient decomposition of a 4D kernel into a pair of 2D kernels, thus allowing extensive use of 4D conv layers at a significantly small cost. We believe our investigation will further facilitate the use of 4D convolutions in other domains that require learning to analyze high-dimensional correlations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Complete derivation of the centerpivot 4D convolution</head><p>In this section, we extend Sec. 4.4 to provide a complete derivation of the center-pivot 4D convolution. Note that a typical 4D convolution parameterized by a kernel k ? Rk ?k?k?k on a correlation tensor c ? R H?W ?H?W at position (x, x ) ? R 4 is formulated as</p><formula xml:id="formula_10">(c * k)(x, x ) = (p,p )?P(x,x ) c(p, p )k(p ? x, p ? x ),<label>(6)</label></formula><p>where P(x, x ) denotes a set of neighbourhood regions within the local 4D window centered on position (x, x ), i.e., P(x, x ) = P(x) ? P(x ) as visualized in <ref type="figure">Fig. 4</ref>. Now we design a light-weight, efficient 4D convolution via a reasonable weight-sparsification; from a set of neighborhood positions within a local 4D window of interest, our kernel aims to disregard a large number of activations located at fairly insignificant positions in the 4D window, thereby focusing only on a small subset of relevant activations for capturing complex patterns in the correlation tensor. Specifically, we consider activations at positions that pivots either one of 2-dimensional centers, e.g., x or x , as the foremost influential ones. Given 4D position (x, x ), we collect its neighbors if and only if they are adjacent to either x or x in its corresponding 2D subspace and define two respective sets as P c (x, x ) = {(p, p ) ? P(x, x ) : p = x},</p><p>and P c (x, x ) = {(p, p ) ? P(x, x ) : p = x }.</p><p>The set of center-pivot neighbours P CP (x, x ) is defined as a union of the two subsets:</p><formula xml:id="formula_13">P CP (x, x ) = P c (x, x ) ? P c (x, x ).<label>(9)</label></formula><p>Based on this small subset of neighbors, center-pivot 4D (CP 4D) convolution can be formulated as a union of two separate 4D convolutions:</p><formula xml:id="formula_14">(c * k CP )(x, x ) = (c * k c )(x, x ) + (c * k c )(x, x ),<label>(10)</label></formula><p>where k c and k c are 4D kernels with their respective neighbours P c (x, x ) and P c (x, x ). Now consider below</p><formula xml:id="formula_15">(c * k c )(x, x ) = (p,p )?Pc(x,x ) c(p, p )k(p ? x, p ? x ) = p ?P(x ) c(x, p )k(x ? x, p ? x ) = p ?P(x ) c(x, p )k(0, p ? x ) = p ?P(x ) c(x, p )k 2D c (p ? x ),<label>(11)</label></formula><p>which is equivalent to convolution with kernel k 2D c = k(0, : ) ? Rk ?k performed on 2D slice of the 4D tensor c(x, :). Similarly,</p><formula xml:id="formula_16">(c * k c )(x, x ) = (p,p )?P c (x,x ) c(p, p )k(p ? x, p ? x ) = p?P(x) c(p, x )k(p ? x, x ? x ) = p?P(x) c(p, x )k(p ? x, 0) = p?P(x) c(p, x )k 2D c (p ? x),<label>(12)</label></formula><p>where k 2D c = k(:, 0) ? Rk ?k . Based on above derivations, we rewrite Eqn. 10 as follows</p><formula xml:id="formula_17">(c * k CP )(x, x ) = p ?P(x ) c(x, p )k 2D c (p ? x ) + p?P(x) c(p, x )k 2D c (p ? x),<label>(13)</label></formula><p>which performs two different convolutions on separate 2D subspaces, having a linear complexity.  Numerical comparisons of ablation study. We tabularize <ref type="figure">Figures 5 and 6</ref>, e.g., ablation study on hypercorrelations and pyramidal layers, in <ref type="table" target="#tab_6">Tables A3 and A4</ref> respectively. Achieving 4.5%p mIoU improvements over C deep p , our method clearly benefits from diverse feature correlations from multi-level CNN layers (C p ) as seen in Tab. A3. A large performance gap between C (2:3) and C <ref type="bibr" target="#b2">(3)</ref> in Tab. A4 (63.9 vs. 55.5) reveals that the intermediary second pyramidal layer (p = 2) is especially effective in robust mask prediction compared to the first pyramidal layer (p = 1).</p><p>Evaluation results without using ignore_label on PASCAL-5 i . The benchmarks of PASCAL-5 i <ref type="bibr" target="#b60">[61]</ref>, COCO-20 i <ref type="bibr" target="#b34">[35]</ref>, and FSS-1000 <ref type="bibr" target="#b32">[33]</ref> consist of segmentation mask annotations in which each pixel is labeled with either background or one of the predefined object categories. As pixelwise segmentation near object boundaries is ambiguous to perform even for human annotators, PASCAL-5 i uses a spe-  <ref type="table">Table A3</ref>: Numerical results of <ref type="figure">Figure 5</ref>. All experiments are performed with ResNet101 backbone <ref type="bibr" target="#b16">[17]</ref>. <ref type="bibr">Methods</ref> 1-shot 5-shot 5 0  <ref type="table" target="#tab_6">Table A4</ref>: Numerical results of <ref type="figure">Figure 6</ref>. All experiments are performed with ResNet101 backbone <ref type="bibr" target="#b16">[17]</ref>.</p><formula xml:id="formula_18">5 1 5 2 5 3 mean 5 0 5 1 5 2 5 3 mean C<label>(</label></formula><p>cial kind of label called ignore_label which marks pixel regions ignored during training and evaluation to mitigate the ambiguity ? . Most recent few-shot segmentation work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b85">86]</ref> adopt this evaluation criteria but we found that some methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b88">89]</ref> do not utilize ignore_label in their evaluations. Therefore, the methods are unfairly evaluated as fine-grained mask prediction near object boundaries is one of the most challenging part in segmentation problem. For fair comparisons, we intentionally exclude the methods of <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b88">89]</ref> from Tab. 1 and compare the results of our model evaluated without the use of ignore_label with those methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b88">89]</ref>. The results are summarized in Tab. A1. Even without using ignore_label, the proposed method sets a new state of the art with ResNet50 backbone, outperforming the previous best methods of <ref type="bibr" target="#b79">[80]</ref> and <ref type="bibr" target="#b35">[36]</ref> by (1-shot) 2.8%p and (5-shot) 5.4%p respectively. With VGG16 backbone, our method performs comparably effective to the previous best method <ref type="bibr" target="#b35">[36]</ref> while having the smallest learnable parameters. ? The use of ignore_label was originally adopted in PASCAL VOC dataset <ref type="bibr" target="#b10">[11]</ref>. The same evaluation criteria is naturally transferred to PASCAL-5 i <ref type="bibr" target="#b60">[61]</ref> as it is created from PASCAL VOC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. Qualitative results</head><p>Results without support feature masking. As demonstrated in Sec. 5.1, we conduct experiments without support feature masking (Eqn. 1), similarly to co-segmentation problem with stronger demands for generalizibility. <ref type="figure" target="#fig_0">Figure A1</ref> visualizes some example results on PASCAL-5 i dataset. Even without the use of support masks (in both training and testing), our model effectively segments target instances in query images. The results indicate that learning patterns of feature correlations from multiple visual aspects is effective in fine-grained segmentation as well as identifying 'common' instances in the support and query images. Additional qualitative results. We present additional qualitative results on PASCAL-5 i <ref type="bibr" target="#b60">[61]</ref>, COCO-20 i <ref type="bibr" target="#b45">[46]</ref>, and FSS-1000 <ref type="bibr" target="#b32">[33]</ref> benchmark datasets. All the qualitative results are best viewed in electronic forms. Example results in presence of large scale-differences, truncations, and occlusions are shown in <ref type="figure">Fig. A2</ref>, A3, and A4. <ref type="figure">Figure A5</ref> visualizes model predictions under large illumination-changes in support and query images. <ref type="figure" target="#fig_7">Figure A6</ref> visualizes some sample predictions given exceptionally small objects in either support or query images. As seen in <ref type="figure" target="#fig_3">Fig. A7</ref>, we found that our model sometimes predicts more reliable segmentation masks than ground-truth ones. Some qualitative results in presence of large intra-class variations and noisy clutters in background are shown in <ref type="figure" target="#fig_4">Fig. A8 and A9</ref>. Given only a single support image-annotation pair, our model effectively segments multiple instances in a query image as visualized in <ref type="figure" target="#fig_0">Fig. A10</ref>. <ref type="figure" target="#fig_0">Figure A11</ref> shows representative failure cases; our model fails to localize target objects in presence of severe occlusions, intra-class variances and extremely tiny support (or query) objects. As seen in <ref type="figure" target="#fig_0">Fig. A12</ref>, the model predictions become much reliable given multiple support image-mask pairs, i.e., K &gt; 1.</p><p>The code and data to reproduce all experiments in this paper is available at our project page: http://cvlab. postech.ac.kr/research/HSNet/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support image</head><p>Query image Prediction Ground-truth <ref type="figure" target="#fig_0">Figure A1</ref>: Example results without support feature masking (Eqn. 1) on PASCAL-5 i dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support set Query image Prediction</head><p>Ground-truth <ref type="figure" target="#fig_3">Figure A7</ref>: Our network occasionally predicts more accurate segmentation masks than human-annotated ground-truths.  <ref type="figure" target="#fig_0">Figure A10</ref>: One-to-many and many-to-many (1-shot) results on PASCAL-5 i <ref type="bibr" target="#b60">[61]</ref>, COCO-20 i <ref type="bibr" target="#b34">[35]</ref>, and FSS-1000 <ref type="bibr" target="#b32">[33]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support set Query image Prediction</head><p>Ground-truth <ref type="figure" target="#fig_0">Figure A11</ref>: Representative failure cases in presence of severe occlusions, intra-class variances and extremely tiny objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query image</head><p>Ground-truth Support set Prediction Support set Prediction 1-shot 5-shot <ref type="figure" target="#fig_0">Figure A12</ref>: Comparison between 1-shot and 5-shot results on PASCAL-5 i dataset <ref type="bibr" target="#b60">[61]</ref>. Multiple support images and mask annotations clearly help our model generate accurate mask predictions on query images in many challenging cases.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our model performs visual reasoning in coarse-to-fine manner by gradually squeezing high-dimensional hypercorrelation to the target segmentation mask with efficient 4D convolutions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc><ref type="bibr" target="#b60">[61]</ref> dataset. Ablation study on hypercorrelations. To study the effect of intermediate correlations {? l } l?Lp in hypercorrelation C p ? R |Lp|?Hp?Wp?Hp?Wp , we form single-channel hypercorrelations using only a single intermediate correlation. Specifically, we form two different single-channel hypercorrelations using the smallest (shallow) and largest (deep) layer indices in L p and denote the hypercorrelations as C shallowp , C deep p ? R 1?Hp?Wp?Hp?Wp , and compare the results with ours (C p ) in Fig. 5. The large performance gaps between C p and the single-channel hypercorrelations confirm that capturing diverse correlation patterns from dense intermediate CNN layers is crucial in effective pattern analyses. Performance degradation from C deep p to C shallow p indicates that reliable feature representations typically appear at deeper layers of a CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Ablation study on hypercorrelations on PASCAL-5 i [61] dataset in 1-shot (left) and 5-shot (right) mIoU results. Ablation study on pyramid layers on PASCAL-5 i [61] dataset in 1-shot (left) and 5-shot (right) mIoU results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Ablation study on hypercorrelation pyramid layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>The effect of depths in building blocks: f sqz p and f mix p .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>plots the training/validation curves of the finetuned model and ours on every fold of PASCAL-5 i . The finetuned model rapidly overfits to the training data, losing generic, comprehensive visual representations learned from large-scale dataset<ref type="bibr" target="#b8">[9]</ref>. Meanwhile, our model with frozen backbone provides better generalizibility with large trade-offs between training and validation accuracies. The results reveal that learning new appearances under limited supervision requires understanding their 'relations' to diverse visual patterns acquired from Learning curves (x-axis: epoch, y-axis: mIoU) on PASCAL-5 i . We carefully tuned the learning rate of the backbone and set it to 100 times smaller than the layers in HSNet (1e-5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure A2 :Figure A3 :Figure A4 :Figure A5 :</head><label>A2A3A4A5</label><figDesc>Qualitative (1-shot) results on PASCAL-5 i [61] dataset under large differences in object scales. Qualitative (1-shot) results on PASCAL-5 i [61] and COCO-20 i [35] datasets under large truncations and occlusions. Qualitative (1-shot) results on COCO-20 i [35] dataset under large differences in object scales. Qualitative (1-shot) results on COCO-20 i [35] dataset under large illumination-changes in support and query images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure A6 :</head><label>A6</label><figDesc>Qualitative (1-shot) results on COCO-20 i [35] dataset with exceptionally small objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Backbone network</cell><cell>Methods</cell><cell>20 0</cell><cell>20 1</cell><cell>20 2</cell><cell cols="3">1-shot 20 3 mean FB-IoU 20 0</cell><cell>20 1</cell><cell>20 2</cell><cell>5-shot 20 3 mean FB-IoU</cell></row><row><cell></cell><cell>PPNet [37]</cell><cell cols="4">28.1 30.8 29.5 27.7 29.0</cell><cell>-</cell><cell cols="3">39.0 40.8 37.1 37.3 38.5</cell><cell>-</cell></row><row><cell></cell><cell>PMM [80]</cell><cell cols="4">29.3 34.8 27.1 27.3 29.6</cell><cell>-</cell><cell cols="3">33.0 40.6 30.3 33.3 34.3</cell><cell>-</cell></row><row><cell>ResNet50 [17]</cell><cell>RPMM [80] PFENet [70]</cell><cell cols="4">29.5 36.8 28.9 27.0 30.6 36.5 38.6 34.5 33.8 35.8</cell><cell>--</cell><cell cols="3">33.8 42.0 33.0 33.3 35.5 36.5 43.3 37.8 38.4 39.0</cell><cell>--</cell></row><row><cell></cell><cell>RePRI [4]</cell><cell cols="4">32.0 38.7 32.7 33.1 34.1</cell><cell>-</cell><cell cols="3">39.3 45.4 39.7 41.8 41.6</cell><cell>-</cell></row><row><cell></cell><cell cols="5">HSNet (ours) 36.3 43.1 38.7 38.7 39.2</cell><cell>68.2</cell><cell cols="3">43.3 51.3 48.2 45.0 46.9</cell><cell>70.7</cell></row><row><cell></cell><cell>FWB [46]</cell><cell cols="4">17.0 18.0 21.0 28.9 21.2</cell><cell>-</cell><cell cols="3">19.1 21.5 23.9 30.1 23.7</cell><cell>-</cell></row><row><cell>ResNet101 [17]</cell><cell>DAN [74] PFENet [70]</cell><cell cols="4">-36.8 41.8 38.7 36.7 38.5 ---24.4</cell><cell>62.3 63.0</cell><cell cols="3">-40.4 46.8 43.2 40.5 42.7 ---29.6</cell><cell>63.9 65.8</cell></row><row><cell></cell><cell cols="5">HSNet (ours) 37.2 44.1 42.4 41.3 41.2</cell><cell>69.1</cell><cell cols="3">45.9 53.0 51.8 47.1 49.5</cell><cell>72.4</cell></row></table><note>Performance on PASCAL-5 i [61] in mIoU and FB-IoU. Some results are from [4, 37, 70, 74, 80]. Superscript ? denotes our model without support feature masking (Eqn. 1). Numbers in bold indicate the best performance and underlined ones are the second best.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Performance on COCO-20 i<ref type="bibr" target="#b45">[46]</ref> in mIoU and FB-IoU. The results of other methods are from<ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b79">80]</ref>.</figDesc><table><row><cell>Backbone network</cell><cell>Methods</cell><cell cols="2">mIoU 1-shot 5-shot</cell></row><row><cell></cell><cell>OSLSM [61]</cell><cell>70.3</cell><cell>73.0</cell></row><row><cell></cell><cell>GNet [55]</cell><cell>71.9</cell><cell>74.3</cell></row><row><cell>VGG16 [64]</cell><cell>FSS [33]</cell><cell>73.5</cell><cell>80.1</cell></row><row><cell></cell><cell>DoG-LSTM [2]</cell><cell>80.8</cell><cell>83.4</cell></row><row><cell></cell><cell>HSNet (ours)</cell><cell>82.3</cell><cell>85.8</cell></row><row><cell cols="2">ResNet50 [17] HSNet (ours)</cell><cell>85.5</cell><cell>87.8</cell></row><row><cell>ResNet101 [17]</cell><cell>DAN [74] HSNet (ours)</cell><cell>85.2 86.5</cell><cell>88.1 88.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Domain shift results. Subscripts denote backbone.</figDesc><table><row><cell>classes. As seen in the bottom row of Tab. 1, our model</cell></row><row><cell>without support masking still performs remarkably well,</cell></row><row><cell>achieving 1.4%p mIoU improvement over the previous best</cell></row><row><cell>method [70] in 1-shot setting whereas it rivals [4, 70] in</cell></row><row><cell>5-shot setting. This interesting result reveals that our model</cell></row><row><cell>is also capable of identifying 'common' instances across</cell></row><row><cell>different input images as well as predicting fine-grained seg-</cell></row><row><cell>mentation masks.</cell></row></table><note>Robustness to domain shift. To demonstrate the robustness of our method to domain shift, we evaluate COCO-trained HSNet on each fold of PASCAL-5 i following the recent work of [4]. We use the same training/test folds as in [4] where object classes in training and testing do not overlap. As seen in Tab. 4, our model, which is trained without any data augmentation methods with 18 times smaller number of trainable parameters compared to [4] (2.6M vs. 46.7M), performs robustly in presence of large domain gaps between COCO-20 i and PASCAL-5 i , surpassing [4] by 1.0%p in 5-shot setting, and further improves with a larger backbone, e.g., ResNet101. The results clearly show the robustness of our method to domain shift, and may further increase when trained with data augmentations used in [4, 70].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>71.4 62.3 61.7 64.9 70.8 74.8 67.4 67.5 70.1 11.3M 512.17 4.12 702.35 Separable 4D kernel [81] 66.1 72.0 63.2 62.6 65.9 71.2 74.1 67.2 68.1 70.2 4.4M 28.48 1.50 28.40 Center-pivot 4D kernel (ours) 67.3 72.3 62.0 63.1 66.2 71.8 74.4 67.0 68.3 70.4</figDesc><table><row><cell>Kernel type</cell><cell>5 0</cell><cell>5 1</cell><cell>1-shot 5 2</cell><cell>5 3</cell><cell>mean</cell><cell>5 0</cell><cell>5 1</cell><cell>5-shot 5 2</cell><cell>5 3</cell><cell>mean</cell><cell># learnable params</cell><cell>time (ms)</cell><cell cols="2">memory footprint FLOPs (GB) (G)</cell></row><row><cell>Original 4D kernel [58]</cell><cell cols="11">64.5 2.6M</cell><cell>25.51</cell><cell>1.39</cell><cell>20.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table A2 :</head><label>A2</label><figDesc>Results on K-shot with ResNet50 backbone network where K ? {1, 5, 10}. The results of other methods are from<ref type="bibr" target="#b3">[4]</ref>.</figDesc><table><row><cell>Appendix C. Additional results and analyses</cell></row><row><cell>Additional K-shot results. Following the work of [4, 70,</cell></row><row><cell>80], we conduct K-shot experiments with K ? {1, 5, 10}.</cell></row><row><cell>Table A2 compares our results with the recent methods [4,</cell></row><row><cell>70, 80] on PASCAL-5 i and COCO-20 i . The significant per-</cell></row><row><cell>formance improvements on both datasets clearly indicate the</cell></row><row><cell>effectiveness of our approach. Achieving 2.5%p and 4.6%p</cell></row><row><cell>mIoU improvements over the previous best method [4] on</cell></row><row><cell>respective PASCAL-5 i and COCO-20 i , our model again sets</cell></row><row><cell>a new state of the art in 10-shot setting as well, showing</cell></row><row><cell>notable improvements with larger K.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table A5 :</head><label>A5</label><figDesc>Hypercorrelation Squeeze Networks (HSNet) with VGG16 [64] backbone network. The reported number of parameters in VGG16 backbone network (14.7M) excludes those in fully-connected layers (unused in our model). The total number of 'learnable' parameters amounts to 2.6M. The number of intermediate features extracted from backbone network amounts to 7, i.e., L = 7. The CP 4D CONV refers to the proposed center-pivot 4D convolution.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>CP 4D CONV</cell><cell>?</cell><cell></cell></row><row><cell></cell><cell>C 2</cell><cell>(6, 25, 25, 25, 25)</cell><cell>C sqz 2</cell><cell>(128, 25, 25, 2, 2)</cell><cell>?</cell><cell>GROUP NORM</cell><cell>? ? 3</cell><cell>172K</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RELU</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>CP 4D CONV</cell><cell>?</cell><cell></cell></row><row><cell>Squeezing Block f sqz 1</cell><cell>C 1</cell><cell>(4, 50, 50, 50, 50)</cell><cell>C sqz 1</cell><cell>(128, 50, 50, 2, 2)</cell><cell>?</cell><cell>GROUP NORM</cell><cell>? ? 3</cell><cell>203K</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RELU</cell><cell></cell><cell></cell></row><row><cell></cell><cell>C sqz 3</cell><cell>(128, 13, 13, 2, 2)</cell><cell></cell><cell></cell><cell cols="3">BILINEAR INTERPOLATION ELEMENT-WISE ADDITION</cell><cell></cell></row><row><cell>Mixing Block f mix 2</cell><cell>C sqz 2</cell><cell>(128, 25, 25, 2, 2)</cell><cell>C mix 2</cell><cell>(128, 25, 25, 2, 2)</cell><cell>? ?</cell><cell>CP 4D CONV GROUP NORM RELU</cell><cell>? ? ? 3</cell><cell>886K</cell></row><row><cell></cell><cell>C mix 2</cell><cell>(128, 25, 25, 2, 2)</cell><cell></cell><cell></cell><cell cols="3">BILINEAR INTERPOLATION ELEMENT-WISE ADDITION</cell><cell></cell></row><row><cell>Mixing Block f mix 1</cell><cell>C sqz 1</cell><cell>(128, 50, 50, 2, 2)</cell><cell>C mix 1</cell><cell>(128, 50, 50, 2, 2)</cell><cell>? ?</cell><cell>CP 4D CONV GROUP NORM RELU</cell><cell>? ? ? 3</cell><cell>886K</cell></row><row><cell>Pooling Layer</cell><cell>C mix 1</cell><cell>(128, 50, 50, 2, 2)</cell><cell>Z</cell><cell>(128, 50, 50)</cell><cell cols="3">AVERAGE-POOLING</cell><cell>-</cell></row><row><cell>Decoder Layer</cell><cell>Z</cell><cell cols="2">(128, 50, 50)M q</cell><cell>(2, 400, 400)</cell><cell cols="3">SERIES OF 2D CONVS WITH BILINEAR INTERPOLATION</cell><cell>259K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table A6 :</head><label>A6</label><figDesc>Hypercorrelation Squeeze Networks (HSNet) with ResNet50 [17] backbone network. The reported number of parameters in ResNet50 backbone network (23.6M) excludes those in fully-connected layers (unused in our model). The total number of 'learnable' parameters amounts to 2.6M. The number of intermediate features extracted from backbone network amounts to 13, i.e., L = 13.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>CP 4D CONV</cell><cell>?</cell><cell></cell></row><row><cell></cell><cell>C 2</cell><cell>(23, 25, 25, 25, 25)</cell><cell>C sqz 2</cell><cell>(128, 25, 25, 2, 2)</cell><cell>?</cell><cell>GROUP NORM</cell><cell>? ? 3</cell><cell>185K</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RELU</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>CP 4D CONV</cell><cell>?</cell><cell></cell></row><row><cell>Squeezing Block f sqz 1</cell><cell>C 1</cell><cell>(4, 50, 50, 50, 50)</cell><cell>C sqz 1</cell><cell>(128, 50, 50, 2, 2)</cell><cell>?</cell><cell>GROUP NORM</cell><cell>? ? 3</cell><cell>203K</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RELU</cell><cell></cell><cell></cell></row><row><cell></cell><cell>C sqz 3</cell><cell>(128, 13, 13, 2, 2)</cell><cell></cell><cell></cell><cell cols="3">BILINEAR INTERPOLATION ELEMENT-WISE ADDITION</cell><cell></cell></row><row><cell>Mixing Block f mix 2</cell><cell>C sqz 2</cell><cell>(128, 25, 25, 2, 2)</cell><cell>C mix 2</cell><cell>(128, 25, 25, 2, 2)</cell><cell>? ?</cell><cell>CP 4D CONV GROUP NORM RELU</cell><cell>? ? ? 3</cell><cell>886K</cell></row><row><cell></cell><cell>C mix 2</cell><cell>(128, 25, 25, 2, 2)</cell><cell></cell><cell></cell><cell cols="3">BILINEAR INTERPOLATION ELEMENT-WISE ADDITION</cell><cell></cell></row><row><cell>Mixing Block f mix 1</cell><cell>C sqz 1</cell><cell>(128, 50, 50, 2, 2)</cell><cell>C mix 1</cell><cell>(128, 50, 50, 2, 2)</cell><cell>? ?</cell><cell>CP 4D CONV GROUP NORM RELU</cell><cell>? ? ? 3</cell><cell>886K</cell></row><row><cell>Pooling Layer</cell><cell>C mix 1</cell><cell>(128, 50, 50, 2, 2)</cell><cell>Z</cell><cell>(128, 50, 50)</cell><cell cols="3">AVERAGE-POOLING</cell><cell>-</cell></row><row><cell>Decoder Layer</cell><cell>Z</cell><cell cols="2">(128, 50, 50)M q</cell><cell>(2, 400, 400)</cell><cell cols="3">SERIES OF 2D CONVS WITH BILINEAR INTERPOLATION</cell><cell>259K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table A7 :</head><label>A7</label><figDesc>Hypercorrelation Squeeze Networks (HSNet) with ResNet101 [17] backbone network. The reported number of parameters in ResNet101 backbone network (42.6M) excludes those in fully-connected layers (unused in our model). The total number of 'learnable' parameters amounts to 2.6M. The number of intermediate features extracted from backbone network amounts to 30, i.e., L = 30.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Implementation details</head><p>For the backbone networks, we employ VGG <ref type="bibr" target="#b63">[64]</ref> and ResNet <ref type="bibr" target="#b16">[17]</ref> families pre-trained on ImageNet <ref type="bibr" target="#b8">[9]</ref>, e.g., VGG16, ResNet50, and ResNet101. For the VGG16 backbone, we extract features after every conv layer in the last two building blocks: from conv4_x to conv5_x, and after the last maxpooling layer. For the ResNet backbones, we extract features at the end of each bottleneck before ReLU activation: from conv3_x to conv5_x. This feature extracting scheme results in 3 pyramidal layers (P = 3) for every backbone. We set spatial sizes of both support and query images to 400 ? 400, i.e., H, W = 400, thus having H 1 , W 1 = 50, H 2 , W 2 = 25, and H 3 , W 3 = 13 for both ResNet50 and ResNet101 bakcbones and H 1 , W 1 = 50, H 2 , W 2 = 25, and H 3 , W 3 = 12 for the VGG16 backbone. The network is implemented in PyTorch <ref type="bibr" target="#b50">[51]</ref> and optimized using Adam <ref type="bibr" target="#b23">[24]</ref> with learning rate of 1e-3. We train our model with batch size of 20, 40, and 20 for PASCAL-5 i , COCO-20 i , and FSS-1000 respectively. We freeze the pretrained backbone networks to prevent them from learning class-specific representations of the training data. The intermediate tensor dimensions, the number of parameters of each layers and other additional details of the network are demonstrated in Tab. A5, A6, and A7 for respective backbones of VGG16, ResNet50, and ResNet101. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Infinite mixture prototypes for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelsey</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanul</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the texture bias for few-shot cnn segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Azad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>Abdur R Fayjie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Kauffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Pedersoli1</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dolz1</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hpatches: A benchmark and evaluation of handcrafted and learned local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassileios</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5173" to="5182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Few-shot segmentation without meta-learning: A good transductive inference is all you need?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malik</forename><surname>Boudiaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoel</forename><surname>Kervadec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Ziko Imtiaz Masud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Piantanida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dolz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modeling the background for incremental learning in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Cermelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weaklysupervised semantic segmentation via sub-category exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robinson</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Show, match and segment: Joint weakly supervised learning of semantic matching and object co-segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Few-shot semantic segmentation with prototype learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanqing</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference (BMVC)</title>
		<meeting>British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mark Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fgn: Fully guided network for few-shot instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarong</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simpropnet: Improved similarity propagation for few-shot image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Gairola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayur</forename><surname>Hemani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Proposal flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Proposal flow: Semantic correspondences from object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cross attention network for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Bingpeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attention-based multi-context guiding for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengwan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust image matching by dynamic feature selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference (BMVC)</title>
		<meeting>British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamic context correspondence network for semantic alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaiyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sideinfnet: A deep neural network for semi-automatic semantic segmentation with side information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yu Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duc</forename><forename type="middle">Thanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quang-Trung</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sfnet: Learning object-aware semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junghyup</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dohyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast online object tracking and segmentation: A unifying approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Siamrpn++: Evolution of siamese visual tracking with very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Correspondence networks with adaptive neighbourhood consensus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><forename type="middle">W</forename><surname>Costain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Howard-Jenkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 1</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 1</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Revisiting local descriptor based image-to-class measure for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dualresolution correspondence networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
		</author>
		<idno>2020. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fss-1000: A 1000-class dataset for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Yau Pun Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><forename type="middle">Doll?r</forename><surname>Microsoft</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<title level="m">Common objects in context</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Crnet: Cross-reference networks for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weide</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 1</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 1</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Songyang Zhang, and Xuming He. Part-aware prototype network for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Semantic correspondence as an optimal transport problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 1</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 1</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation via strong-weak dual-branch network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenfeng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Categorization and naming in children: Problems of induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Markman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Convolutional hough matching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="2940" to="2950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hyperpixel flow: Semantic correspondence with multi-layer neural features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">SPair-71k: A large-scale benchmark for semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10543</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv prepreint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning to compose hypercolumns for visual correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV), 2020. 1</title>
		<meeting>European Conference on Computer Vision (ECCV), 2020. 1</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Feature weighting and boosting for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Anchornet: A weakly supervised network to learn geometry-sensitive features for semantic matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<meeting>Medical Image Computing and Computer-Assisted Intervention (MICCAI)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Pau Rodr?guez L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Large kernel matters -improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Transductive episodic-wise adaptive metric for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limeng</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yemin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Conditional networks for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshops (ICLRW)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Few-shot segmentation propagation with guided networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07373</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Convolutional neural network architecture for geometric matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Efficient neighbourhood consensus networks via submanifold sparse convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Neighbourhood consensus networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garcia</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><forename type="middle">Bruna</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Estrach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Benchmarking 6dof outdoor visual localization in changing conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Toft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Hammarstrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Stenborg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Safari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatoshi</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">One-shot learning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirreza</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shray</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><surname>Boots</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference (BMVC</title>
		<meeting>British Machine Vision Conference (BMVC</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2002" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Amp: Adaptive masked proxies for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mennatullah</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Mining cross-image semantics for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Joint recovery of dense correspondence and cosegmentation in two images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsunori</forename><surname>Taniai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sudipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Differentiable meta-learning model for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinzhuo</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangkai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Prior guided feature enrichment network for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuotao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">GLU-Net: Global-local universal network for dense flow and correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prune</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 1</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 1</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Regularized loss for weakly supervised single class semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Veksler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Few-shot semantic segmentation with democratic attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>Xianbin Cao, and Xiantong Zhen</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Panet: Few-shot image semantic segmentation with prototype alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Hao Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingtian</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Dual super-resolution learning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yousong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yude</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Parn: Positionaware relation networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihua</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Jianbin Jiao, and Ye Qixiang. Prototype mixture models for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Volumetric correspondence networks for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Daisy filter flow: A generalized discrete approach to dense correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Yan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">A new local transformation module for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanman</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingbo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MultiMedia Modeling (MMM)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Fewshot learning via embedding adaptation with set-to-set functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Han-Jia Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>De-Chuan Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Deepemd: Few-shot image classification with differentiable earth mover&apos;s distance and structured classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Pyramid graph networks with connection attentions for region-based one-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiushuang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Canet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Splitting vs. merging: Mining object regions with discrepancy and intersection loss for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weide</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Sg-one: Similarity guidance network for one-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
