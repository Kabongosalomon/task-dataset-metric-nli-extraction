<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PedRecNet: Multi-task deep neural network for full 3D human pose and orientation estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Burgermeister</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Crist?bal</forename><surname>Curio</surname></persName>
						</author>
						<title level="a" type="main">PedRecNet: Multi-task deep neural network for full 3D human pose and orientation estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a multitask network that supports various deep neural network based pedestrian detection functions. Besides 2D and 3D human pose, it also supports body and head orientation estimation based on full body bounding box input. This eliminates the need for explicit face recognition. We show that the performance of 3D human pose estimation and orientation estimation is comparable to the state-of-theart. Since very few data sets exist for 3D human pose and in particular body and head orientation estimation based on full body data, we further show the benefit of particular simulation data to train the network. The network architecture is relatively simple, yet powerful, and easily adaptable for further research and applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The detection of pedestrians as well as their behavior remains a challenge in the field of autonomous driving. Previous work shows how to detect pedestrian actions using 2D pose recognition <ref type="bibr" target="#b0">[1]</ref>. In this work, we present a new multitask network, PedRecNet, that can estimate 3D poses in addition to 2D poses. 3D poses bring in the benefit of multiple perspectives on the skeleton, enabling detection of movement changes which may not be visible in 2D projections <ref type="bibr" target="#b1">[2]</ref>. In addition to 3D pose data, orientation information about a person's body and head is also relevant for human recognition systems. Especially in pedestrian recognition, this information can be valuable to perform path planning or to detect if a pedestrian notices a vehicle or not. Since 3D pose recognition and body as well as head orientation estimation are related it seems beneficial to bring this tasks together in one versatile network. All tasks in the PedRecNet are based on the same input data, so all tasks should be implemented in the same network using a multitask approach. Since there are only a few real datasets available for 3D pose recognition and especially for body and head orientation estimation, we use simulation data to improve these parts of the PedRecNet and to enable training in the first place. The skeleton-based action recognition results presented by <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b2">[3]</ref> support the assumption that using abstract pose information rather than just visual information enables the transfer of simulated training data to real data. We will corroborate this work hypothesis in more detail in experiments using simulated training data. In the following, we describe the developed network, the datasets used, and D. <ref type="bibr">Burgermeister</ref>  {Dennis.Burgermeister, Cristobal.Curio}@Reutlingen-University.de evaluate 3D human pose recognition as well as the body orientation estimation on several real and simulated datasets.</p><p>The entire system and novel simulation data has been made public under the MIT license <ref type="bibr" target="#b0">1</ref> .</p><p>Our contributions in this work are: 1) A multi-task network which supports 2D and 3D human pose estimation, as well as body and head orientation estimation on cropped full body input data. 2) An approach to 3D human pose estimation in which 3D human joint positions are encoded in the skeletal coordinate system. This makes the skeleton estimation independent of the camera parameters and can thus be better used in follow-up applications, such as action recognition which uses temporal data. 3) An integrated approach to body and head orientation estimation based on whole body bounding box input. This eliminates the need for face recognition to obtain a crop of the head bounding box. 4) Simulation data that provide, in particular, accurate head and body orientation data that are not available in standard data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Besides direct estimation of 3D human joint positions by a deep neural network <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> alternative approaches exist that first estimate 2D poses followed by 3D pose regression <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Some approaches show the application of model-based approaches <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref> to further improve a recognized 3D skeleton. The categorization in bottom-up <ref type="bibr" target="#b4">[5]</ref> and top-down approaches <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref> is also valid for 3D human pose estimation. Mehta et al. show an approach predicting three location-maps for the x, y and z position parameters per body joint <ref type="bibr" target="#b3">[4]</ref>. Those location maps encode the distance in x, y, or z direction from the coordinate root (hip center). The location of a joint on those location maps is retrieved from 2D pose heatmaps. The results are refined using a kinematic skeleton fitting method. They have also shown how to apply location maps in a bottom-up approach which also handles occlusion better by using redundancy in so-called occlusion-robust pose-maps by representing the decomposed body as torso, limbs, and heads <ref type="bibr" target="#b4">[5]</ref>. Luvizon et al. show a similar approach to encode depth information in a heatmap but use it only for the depth estimation <ref type="bibr" target="#b6">[7]</ref>. It is also possible to directly regress 3D poses from 2D heatmaps <ref type="bibr" target="#b9">[10]</ref> or directly from 2D pose coordinates <ref type="bibr" target="#b16">[16]</ref> which improves when using multiple frames as input <ref type="bibr" target="#b8">[9]</ref>. Another approach to retrieve 3D pose information from 2D poses is 3D catalog matching <ref type="bibr" target="#b7">[8]</ref>. Such approaches rely more heavily on the 2D human pose estimator's output than approaches that also use visual input. <ref type="bibr">Kolotouros et al. show</ref> how to reconstruct a volumetric model by estimating the parameters for the SMPL statistical body shape model <ref type="bibr" target="#b17">[17]</ref> and further improve the model by iteratively fitting on 2D joints <ref type="bibr" target="#b11">[12]</ref>. We present an approach similar to <ref type="bibr" target="#b6">[7]</ref>, but with a straightforward and performant method to retrieve the pose and depth heatmaps in section III.</p><p>Body and head orientation estimation approaches are usually handled as separate problems. The estimation of body orientation often originate in pedestrian-related works. Classical approaches often used classification of body parts, for example, by using a part descriptor in a sliding window fashion to classify position, scale, and orientation of body parts <ref type="bibr" target="#b18">[18]</ref>. Another approach focuses on combining pedestrian orientation and classification by clustering pedestrians in the four categories front, back, left, and right and train classification networks on those clusters, which combined scores serve as the full pedestrian classification <ref type="bibr" target="#b19">[19]</ref>. Another approach uses specific detectors for head and body orientation which are converted to a full continuous probability density function and stabilized over time by particle filtering <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b21">[21]</ref>. The authors also discretized the orientation space to 45-degree bins and used a HOG/linSVM based classification system <ref type="bibr" target="#b20">[20]</ref>. There is a lot of recent head orientation work using deep learning <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b26">[25]</ref>, <ref type="bibr" target="#b27">[26]</ref>, <ref type="bibr" target="#b28">[27]</ref>, which usually takes a head bounding box as input and thus is an additional step in the recognition pipeline. Such methods require a head bounding box with a reasonable resolution and thus high-resolution sensors or people not at distance from the camera sensor. Work on body orientation or combined body and head orientation estimation has not yet transitioned well to deep learning approaches. One possible reason could be the lack of appropriate datasets. There are not many standard datasets, and the existing ones are rather small and thus not suitable to train deep neural networks. Heo et al. try to overcome this issue on body orientation estimation by using a teacher-student learning framework in which they train a teacher network with labeled data and use this network to generate labels for an unlabeled dataset with which the student network is trained <ref type="bibr" target="#b29">[28]</ref>. They have also discretized the output orientation in 45-degree bins, turning the problem into a classification problem <ref type="bibr" target="#b29">[28]</ref>. Another work uses CNNs in a random forest that focuses on different body and head parts to recognize the human body and head orientation, with a focus on head orientation <ref type="bibr" target="#b30">[29]</ref>. Steinhoff and G?hring propose the usage of IMUs to generate more labeled training data for body and head orientation tasks, but IMU-based approaches are usually hard to sync, suffer from error accumulation, and do not contain global reference points <ref type="bibr" target="#b31">[30]</ref>. Wu et al. propose the application of 3D human pose estimation approaches as a basis for body and head pose estimation using a 3D pose estimation network as a backbone for a classification header which classifies the input in 72 orientation bins <ref type="bibr" target="#b32">[31]</ref>. We propose a regression approach which is based on full human 3D pose information, described in section III. Regressed orientation estimation offer various benefits, for example in time-dependent fine-grained actions like the head movement during looking for traffic. We show how to train such a network with simulated data to overcome the deficient number of labeled data in this field. The overall network architecture is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The PedRecNet expands the 2D human pose estimation approach proposed by Xiao et al. <ref type="bibr" target="#b33">[32]</ref>. The PedRecNet architecture is based on a ResNet50 backbone for feature extraction but other backbones could be used as well. The ResNet50 architecture was chosen as a compromise between accuracy and performance. The inputs I are always images cropped to the size of certain bounding boxes. First features? are extracted using the feature extraction part of the network. Next, the 2D human pose estimation part is based on three transpose convolution blocks with which joint heatmaps are generated from the extracted features <ref type="bibr" target="#b33">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>In PedRecNet, the 2D human pose estimation architecture was extended to include 3D human pose estimation. For this purpose, two transpose convolution blocks are used as a common basis and then split into a 2D and a 3D path. These have basically the same structure. The output in the 2D path corresponds to 2D image coordinates. The 3D path, leading to L p3d , corresponds to the estimation of the x and y coordinates of a joint relative to the hip and additional depth estimation of the z coordinate using a sigmoid map. Another change from the previous 2D pose estimation approach is the post-processing of the heatmaps. In the approach shown in <ref type="bibr" target="#b0">[1]</ref>, the heatmaps were output from the network, and a non-maximum suppression (NMS) was used to determine the coordinates. This has the disadvantage that the subpixel coordinate values are not available in the network. The NMS approach also implies that artificial heatmaps have to be generated to be used as training data labels. The PedRecNet applies a softargmax layer, which determines the coordinates from the heatmaps in the network inside the network. This allows to use estimated joint coordinates as inputs into other parts of the network. For example, the 3D joint positions are used as input into the orientation estimation part of the network, which is visualized as the path to L o . The 3D joint position coordinates are scaled up into a feature space via 1D transpose convolution blocks. These are concatenated with the visual features? such that the orientation estimation can use direct information from the image in addition to the, noisy and potentially erroneous, 3D pose. This concatenated feature vector is input into a fully connected layer which generates a one-dimensional heatmap for each, the polar angle ? and azimuthal angle ? (see <ref type="figure">Figure 2</ref>) of the body as well as the head. From these one-dimensional orientation heatmaps, the corresponding normalized angle is extracted using 1D softargmax. To classify the visibility of labels, we added a standard classification head to the network leading to the path to L conf . <ref type="figure">Fig. 2</ref>: Visualization of the orientation estimation for each, the body and the head. The orange dot shows an example point on a 3D sphere, visualizing an orientation. We use the standard notation from ISO 80000-2:2019 <ref type="bibr" target="#b34">[33]</ref> for spherical coordinates. As we only need the polar angle ? and azimuthal angle ? we use a unit sphere and with r = 1.</p><formula xml:id="formula_0">? = 0 ? |360 ? ? = 0 ? r = 1 ? ?</formula><p>The PedRecNet outputs 2D human joint positions as pixel coordinates in the bounding box's coordinate system, normalized between zero and one. The 3D human joint positions are output as 3D coordinates relative to the hip center position, and normalized between zero and one. Orientations are output as angles between 0 ? 180 ? for ? and 0 ? 360 ? for ?, and also normalized between zero and one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>The training and validation datasets of the PedRec network are composed of the COCO <ref type="bibr" target="#b35">[34]</ref>, the H36M <ref type="bibr" target="#b36">[35]</ref>, and selfgenerated simulated datasets. The datasets support different labels, especially orientation estimation labels are only available in the simulation data (cf <ref type="table" target="#tab_1">. Table I)</ref>.</p><p>Whole-body data, including the ? and ? angle of the body and head orientations, are only available in the simulated  <ref type="bibr" target="#b32">[31]</ref>) and TUD <ref type="bibr" target="#b37">[36]</ref> orientation annotations provide only body ? labels. The 2D and 3D pose estimation labels differ in the available labels as COCO, H36M and SIM use different skeleton structures. Legend:</p><formula xml:id="formula_1">Dataset Pose2D Pose3D Orientation COCO X O H36M X TUD [36] X X O SIM-ROM SIM-Circle</formula><p>data is available, X data is not available, O data is partially available.</p><p>datasets. Therefore, the validation of the orientation estimation on real data can be performed with these datasets only for the azimuthal angle ? of the body. All simulated datasets are created using motions, captured in a motion capture laboratory.</p><p>1) SIM-ROM: In the SIM-ROM dataset, a person was recorded performing an extended range of motions that should include as many poses as possible. The idea behind this dataset is to provide data from various performable body poses for 3D pose recognition.</p><p>2) SIM-C01: The SIM-C01 dataset is a large scale pedestrian action dataset containing actions ranging from simple walking, to hitchhiking, to tripping and falling. This dataset is used in this work for validation (SIM-C01V) only.</p><p>3) SIM-CIRCLE: The SIM-Circle dataset resulted from an analysis of the other datasets. As highlighted in <ref type="figure" target="#fig_1">Figure 3</ref>, there is a substantial difference in the distribution of the azimuthal angle ? of the body pose. The same could be observed for the distribution of the azimuthal angle ? of the head poses. In order to generate further data with a uniform distribution, the SIM-Circle dataset was created, in which 3D models walk clock-and counterclockwise in a circle at a uniform speed (see <ref type="figure" target="#fig_1">Figure 3d)</ref>. <ref type="table" target="#tab_1">Table II</ref>   This overview shows that the H36M dataset was recorded in a lab with limited space, which is why the bounding boxes are always relatively large. Comparing the 2D bounding box diameters of COCO, TUD, and SIM-CIRCLE, the distribution is similar, as each of the datasets contains individuals at various distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training procedure</head><p>The network was trained step by step in the following order:</p><p>1) 2D human pose estimation 2) 3D human pose estimation 3) Joint visibility 4) Head and body orientation The training was performed on real data (COCO+H36M) and then on simulation data in the same order. a) Loss Functions: We used the L 1 loss function for the 2D and 3D human joint coordinate regression losses L p2d and L p3d . The joint visibility loss L conf is the standard binary cross-entropy loss. In the orientation regression task, we represent circular data in a one-dimensional map. Thus we cannot use the standard L 1 or L 2 loss; for example, a prediction of 359 ? with a ground truth of 0 ? results in an error of 359 ? . This applies only for the azimuthal angle ?, the polar angle ? is defined between 0 ? and 180 ? , and thus the standard distance metrics can be used. As such, we applied the following loss functions:</p><formula xml:id="formula_2">L ? = N i=1 min (1 ? |m ? ? m ? |, |m ? ? m ? |) N (1) L ? = N i=1 |m ? ? m ? | N<label>(2)</label></formula><formula xml:id="formula_3">L o = L ? + L ? 2<label>(3)</label></formula><p>where N is the number of samples and m ? and m ? are the softargmax outputs for the azimuthal angle ? and the polar angle ? normalized between zero and one. As shown in equation 1 and 2 the L 1 loss is applied. For training data which only provides labels for the azimuthal angle ? only equation 1 is used.</p><p>In our experiments, N may be a subset of the entire training set, as various datasets with different supported labels (see table I) were combined. As such, each loss function contains a sample selection step before the actual calculation of the loss.</p><p>To weight the loss functions, we applied uncertainty loss, described by Kendall et al. <ref type="bibr" target="#b38">[37]</ref>, to balance the different loss outputs. The final loss function is:</p><formula xml:id="formula_4">L = 1 2? 2 1 L p2d + 1 2? 2 2 L p3d + 1 2? 2 3 L o + 1 ? 2 4 L conf + log (1 + 4 i=1 ? i ) (4)</formula><p>where ? 1?4 are learnable parameters. We use log (1 + ?) instead log (?) to ensure a positive loss value. b) Optimizer: For optimization we used the AdamW optimizer <ref type="bibr" target="#b39">[38]</ref> which is an slightly modified variant of the Adam optimizer <ref type="bibr" target="#b40">[39]</ref>. We applied the learning rate range test <ref type="bibr" target="#b41">[40]</ref> to get an initial learning rate of 4e ?3 . We used a standard weight decay of 1e ?2 . We also applied the 1cycle policy <ref type="bibr" target="#b42">[41]</ref> with which the learning rate is updated during the training process from a minimum learning rate of 4e ?3 <ref type="bibr" target="#b26">25</ref> to the maximum of 4e ?3 and afterward back to a minimum using cosine annealing. Smith showed that this approach results in faster convergence and usually better results <ref type="bibr" target="#b41">[40]</ref>. The network was trained with a training cycle of 15 epochs, from which 10 epochs were trained with frozen weights in the feature extractor. For the last five epochs with the feature extractor unfrozen, we reduce the learning rate for the feature extraction to 2e ?4 and for the other layers to 4e ?4 . The training cycle was repeated five times, which improved the performance slightly. c) Datasets: We used the training datasets of COCO <ref type="bibr" target="#b35">[34]</ref>, H36M <ref type="bibr" target="#b36">[35]</ref>, SIM-ROM, and SIM-Circle to train the PedRecNet. The validation is done on the validation parts of COCO, H36M, and the SIM-C01 dataset. We subsampled the H36M training dataset by ten, all samples from the other datasets were used. For the orientation estimation part, we used only labels from SIM-ROM and SIM-Circle during the initial training. COCO labels were used in an additional training step during orientation experiments (see section IV-.0.b). The dataset names are abbreviated in the results as follows: C stands for COCO, M for COCO (MEBOW <ref type="bibr" target="#b32">[31]</ref>), H for H36M and S for SIM-Circle and SIM-ROM combined. COCO is always the base dataset of PedRecNet and is therefore used as one of the training dataset in every experiment. d) Augmentations: We augmented the data by scaling the input by up to ?25%. We rotate the input by up to ?30 ? in 50% of the cases, but only when no orientation labels were used. The image is flipped in 50% of the cases.  <ref type="bibr" target="#b47">[45]</ref> ag   <ref type="bibr" target="#b8">[9]</ref>. An improvement of 4.7mm could be achieved by using temporal information. The performance from PedRecNet, depending on the datasets used, is very similar but decreases slightly when simulation data is added. However, this may also be because the simulation data is partly very different from the H36M data in terms of distances and body size of the persons. <ref type="figure">Figure 4</ref> shows some examples on 'in the wild' real data. The examples are from various sources and include different cameras, focal lengths, exposures and perspectives. Examples 4d-4f show that even in challenging situations a good 3D pose can be predicted. In contrast, we have reported some error cases in <ref type="figure">Figure 5</ref>. <ref type="figure">Figure 5a</ref> shows an extreme corner case where the pose detection fails completely. Note that the corner case dataset from our work <ref type="bibr" target="#b48">[46]</ref> was not used during training. In <ref type="figure">Figure 5b</ref> the pose is correct in principle, but the outstretched left arm is not correctly recognized. From the pose only, it is not clear that the person is just operating a traffic light switch. Example 5c shows a false recognition due to self occlusion. The body occludes the left arm, but the 3D pose recognition estimates it to be hidden behind the right arm and displays it stretched out accordingly. These false detections by occlusion could possibly be improved by further training data or the use of temporal context. b) Body orientation estimation: As described in the related work section, there are only few datasets in the area of body and head orientation estimation for full-body inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><formula xml:id="formula_5">- - - - - - - - - - - - - - -</formula><p>For a state-of-the-art comparison, we found the relatively new dataset <ref type="bibr" target="#b32">[31]</ref> suitable, which provides body orientation labels for the COCO dataset. However, it only contains the azimuthal angle ? and labels for the head pose are not included. We also used the TUD <ref type="bibr" target="#b37">[36]</ref> dataset in the analysis, although it only contains 309 samples in the validation dataset. Accordingly, the significance of the results here is relatively low. <ref type="table" target="#tab_1">Table IV</ref> gives an overview on the results on these datasets. It is to notice that with the PedRecNet we already achieve an Acc.(22.5 ? ) of 75.4% on the TUD <ref type="bibr" target="#b37">[36]</ref> dataset and 80.2% on the MEBOW dataset. For Acc.(45 ? ), which is often sufficient for real-world applications, we even achieve 98.1% on the TUD <ref type="bibr" target="#b37">[36]</ref> dataset and 94.7% on the MEBOW dataset. These are surprising results for not using any training data from the corresponding training datasets. Especially when compared to the earlier approaches of Hara et al. <ref type="bibr" target="#b49">[47]</ref> and Yu et al. <ref type="bibr" target="#b50">[48]</ref>, the PedRecNet gives better results without ever having seen any data from the TUD <ref type="bibr" target="#b37">[36]</ref> dataset. We are accordingly able to provide a solid baseline here purely with simulated data. It should be noted, however, that 3D pose data is also used for orientation estimation and the training for this has included real data from the H36M dataset. When the MEBOW training data are used in addition to the simulation data, the Acc.(22.5 ? ) and Acc.(45 ? ) improve by 11.5% and 2.3%, respectively, and are 2.2% and 1.2% worse than the results reported by Wu et al.. In total, 159 body orientations were predicted with an error above 45 ? . We analyzed these misclassifications further and detected erroneous ground truth labels for 37 images, some of which are shown in <ref type="figure" target="#fig_2">Figure 6</ref>. c) Head orientation estimation: For the head orientation (?) estimation, we use the SIM-C01V dataset. We consider only the ? estimate at this point because ? is underrepresented in the SIM-C01 dataset; the head orientations are relatively horizontal in the pedestrian actions in almost all cases. Accordingly, for the estimation of ?, further targeted experiments and new data recordings are needed in the future. The results for the estimation of the head ? orientation are shown in <ref type="table" target="#tab_9">Table V</ref>   Acc.(45 ? ), respectively. In general, however, performance on the body and head orientation estimates is relatively similar. The somewhat inferior performance can be explained by the head region's smaller image area than the body region. We are not currently aware of a larger and publicly dataset that includes head orientation images in addition to full-body images. Therefore, most approaches to head orientation estimation work with datasets that only contain cropped faces. In productive applications, face recognition can then be performed first, followed by a crop of the face, and orientation estimation can be performed based on this cropped face bounding box. In addition, most datasets only contain faces, which means that a side view or the back of the head cannot usually be used for orientation estimation. In our approach, the entire body is always considered, which enables head orientation estimation even for a side and back view of a person. However, based on subjective observation of 'in-the-wild' examples, we think that we can achieve similar performance on real data for head pose recognition as for body pose recognition when trained on simulation data only. We show 'in-the-wild' examples in <ref type="figure" target="#fig_3">Figure 7</ref>. <ref type="figure" target="#fig_3">Figure 7a</ref> shows a typical example, where one can nicely depict the different estimates of head versus body orientation. Example 7b shows a boy in a stroller, which shows that the orientation estimation gives good results even in non-upright positions. <ref type="figure" target="#fig_3">Figure 7c</ref> shows a person who was photographed from behind. Especially the correct head pose estimation is interesting, although the person wears a hood and only a small part of the nose is visible. Another interesting example is demonstrated in <ref type="figure" target="#fig_3">Figure 7d</ref>, where the orientation estimation is based on input data of a person shot from behind and only visible in a low-resolution image section of about 38 ? 78px.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>With PedRecNet, we presented a simple yet efficient architecture that performs multiple tasks simultaneously and can run on consumer hardware at over 15FPS even with multiple people. The network achieves performance that is comparable to current SOTA methods for 2D and 3D pose detection and orientation estimation. Our model combines all these tasks in a simple and extensible architecture which is straight forward to train. Thus, the introduced model is also well suited as a baseline for further research. We have further shown that we can train the orientation estimation purely with simulation data and achieve high accuracy on real data without requiring real sensor data for training. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Simplified PedRecNet architecture. The input I is an RGB cropped bounding box image of a human. The dotted connection lines indicate connections without gradient flow in the backward pass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Distribution of body ? orientations [ ? ] in the datasets used in this work. The plots show the distribution of the samples in a polar plot. The small peaks in SIM-CIRCLE are due to overlapping start and end frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 :</head><label>6</label><figDesc>MEBOW validation dataset: Examples of misclassifications. The red dotted arrow shows the annotated ground truth, the black arrow shows the prediction of PedRecNet. The misclassifications are caused by: (a) false ground truth label, (b) occlusion of the labeled person (the one in the back of the woman) and (c) PedRecNet misclassification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 :</head><label>7</label><figDesc>Head and body orientation estimation 'in the wild': Examples. Top: Cropped image of the person processed by the network. Bottom two rows: Predicted head and body orientation ?. ACKNOWLEDGMENT This project has been supported by the Continental AG as part of a research cooperation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and C. Curio are with the Cognitive</figDesc><table><row><cell>Systems</cell><cell>Group,</cell><cell>Computer</cell><cell>Science</cell><cell>Department,</cell><cell>Reutlingen</cell></row><row><cell>University,</cell><cell></cell><cell>Germany.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Overview of used datasets and the supported labels. COCO (MEBOW</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>provides an overview over the datasets used.</figDesc><table><row><cell>Data</cell><cell cols="4">COCO [34] H36M [35] TUD [36] SIM-ROM</cell><cell>SIM-CIRCLE</cell></row><row><cell>n</cell><cell>149.813</cell><cell>1.559.752</cell><cell>8.322</cell><cell>147.729</cell><cell>39.484</cell></row><row><cell>b 2D [px]</cell><cell>229</cell><cell>855</cell><cell>193</cell><cell>503</cell><cell>222</cell></row><row><cell>? 2D b min b 2D [px] [px]</cell><cell>148 28</cell><cell>83 585</cell><cell>44 45</cell><cell>502 31</cell><cell>133 43</cell></row><row><cell>max b 2D [px]</cell><cell>1.002</cell><cell>1.339</cell><cell>462</cell><cell>2.202</cell><cell>830</cell></row><row><cell>b 3D [mm]</cell><cell></cell><cell>1.712</cell><cell></cell><cell>1.816</cell><cell>1.885</cell></row><row><cell>? 3D b min b 3D [mm] [mm]</cell><cell></cell><cell>197 859</cell><cell></cell><cell>401 169</cell><cell>194 1.202</cell></row><row><cell>max b 3D [mm]</cell><cell></cell><cell>2.769</cell><cell></cell><cell>2.840</cell><cell>2.194</cell></row><row><cell>d [px]</cell><cell></cell><cell>5.170</cell><cell></cell><cell>7.205</cell><cell>10.609</cell></row><row><cell>? d [px]</cell><cell></cell><cell>750</cell><cell></cell><cell>5.578</cell><cell>5.779</cell></row><row><cell>min d [px]</cell><cell></cell><cell>2.530</cell><cell></cell><cell>22</cell><cell>2.321</cell></row><row><cell>max d [px]</cell><cell></cell><cell>7.690</cell><cell></cell><cell>21.289</cell><cell>25.754</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc></figDesc><table /><note>Statistics of datasets used in the PedRec experi- ments. b represents the 2D or 3D bounding box size and d the distance to the camera. The number of samples is notated as n, the mean of bounding boxes and the distances to the camera asb/d, and the standard deviation values are notated with ?.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>a) 3D pose estimation: For 3D human pose estimation, we first consider the performance compared to other methodsGong et al. '21  </figDesc><table><row><cell>Method</cell><cell>Prop.</cell><cell>Dir.</cell><cell>Disc.</cell><cell>Eat</cell><cell>Greet</cell><cell cols="5">Phone Photo Pose Purch. Sit</cell><cell cols="3">SitD. Smoke Wait</cell><cell cols="2">WalkD. Walk</cell><cell cols="2">WalkT. Avg</cell></row><row><cell>Chen et al. '17 [8]</cell><cell>g</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>82.4</cell></row><row><cell>Martinez et al. '17 [16]</cell><cell>g</cell><cell>51.8</cell><cell>56.2</cell><cell>58.1</cell><cell>59.0</cell><cell>69.5</cell><cell>78.4</cell><cell cols="2">55.2 58.1</cell><cell cols="2">74.0 94.6</cell><cell>62.3</cell><cell cols="2">59.1 65.1</cell><cell>49.5</cell><cell>52.4</cell><cell>62.9</cell></row><row><cell cols="2">Luvizon et al. '17 [42] gs</cell><cell>49.2</cell><cell>51.6</cell><cell>47.6</cell><cell>50.5</cell><cell>51.8</cell><cell>48.9</cell><cell cols="2">48.5 51.7</cell><cell cols="2">61.5 70.9</cell><cell>53.7</cell><cell cols="2">60.3 44.4</cell><cell>48.9</cell><cell>57.9</cell><cell>53.2</cell></row><row><cell>Yang et al. '18 [43]</cell><cell>-</cell><cell cols="2">51.5 58.9</cell><cell cols="2">50.4 57.0</cell><cell>62.1</cell><cell>65.4</cell><cell cols="2">49.8 52.7</cell><cell cols="2">69.2 85.2</cell><cell>57.4</cell><cell cols="2">58.4 43.6</cell><cell>60.1</cell><cell>47.7</cell><cell>58.6</cell></row><row><cell>Pavllo et al. '18 [9]</cell><cell>agt</cell><cell>45.1</cell><cell>47.4</cell><cell cols="2">42.0 46.0</cell><cell>49.1</cell><cell>56.7</cell><cell cols="2">44.5 44.4</cell><cell cols="2">57.2 66.1</cell><cell>47.5</cell><cell cols="2">44.8 49.2</cell><cell>32.6</cell><cell>34.0</cell><cell>47.1</cell></row><row><cell>Pavllo et al. '18 [9]</cell><cell>ag</cell><cell>47.1</cell><cell>50.6</cell><cell>49.0</cell><cell>51.8</cell><cell>53.6</cell><cell>61.4</cell><cell cols="2">49.4 47.4</cell><cell cols="2">59.3 67.4</cell><cell>52.4</cell><cell cols="2">49.5 55.3</cell><cell>39.5</cell><cell>42.7</cell><cell>51.8</cell></row><row><cell>Luvizon et al. '20 [7]</cell><cell>gs</cell><cell>43.2</cell><cell>48.6</cell><cell>44.1</cell><cell>45.9</cell><cell>48.2</cell><cell>43.5</cell><cell cols="2">44.2 45.5</cell><cell cols="2">57.1 64.2</cell><cell>50.6</cell><cell cols="2">53.8 40.0</cell><cell>44.0</cell><cell>51.1</cell><cell>48.6</cell></row><row><cell>Shan et al. '21 [44]</cell><cell>at</cell><cell>40.8</cell><cell>44.5</cell><cell>41.4</cell><cell>42.7</cell><cell>46.3</cell><cell>55.6</cell><cell cols="2">41.8 41.9</cell><cell cols="2">53.7 60.8</cell><cell>45.0</cell><cell cols="2">41.5 44.8</cell><cell>30.8</cell><cell>31.9</cell><cell>44.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE III :</head><label>III</label><figDesc>Results on the H36M dataset reported as mean per joint position error (MPJPE). Our novel PedRecNet is based on single-frame estimation. For better comparability, we also show results using flip-testing in addition to the results without any form of test-time augmentation. The performance of our method with an average MPJPE of 52.7mm is comparable to current SOTA approaches such as Gong et al.<ref type="bibr" target="#b47">[45]</ref> with 50.2mm and Luvizon et al.<ref type="bibr" target="#b6">[7]</ref> with 48.6mm. The use of temporal information leads to noticeable better results in this benchmark. This becomes clear when comparing the two results of Pavllo et al.</figDesc><table><row><cell>Legend of properties (Prop.),</cell></row><row><cell>influencing the results: (a) used test-time augmentation (in ours only flip test is applied), (g) used ground truth bounding</cell></row><row><cell>boxes, (s) sampled every 64th frame of validation set, (t) used temporal information</cell></row><row><cell>on the H36M validation dataset. The results of 3D pose</cell></row><row><cell>estimation are shown in Table III. It summarizes further</cell></row><row><cell>approaches, which differ in the methods used, input data, and</cell></row><row><cell>test methods. Some approaches use test-time augmentations</cell></row><row><cell>like flip-testing. Others use temporal information to improve</cell></row><row><cell>the results.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>.</head><label></label><figDesc>The results are slightly inferior to the body orientation estimation by 2.6% and 2.8% for Acc.(22.5 ? ) and</figDesc><table><row><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell><cell></cell><cell>(d)</cell><cell>(e)</cell><cell>(f)</cell></row><row><cell cols="7">Fig. 4: 3D Human pose estimation 'in the wild': PedRecNet examples. Top: Cropped image of the person inputed in the</cell></row><row><cell cols="3">network. Bottom: Predicted 3D human pose.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Network</cell><cell></cell><cell>Trainset</cell><cell>Testset</cell><cell>Acc.(22.5 ? )</cell><cell>Acc.(45 ? )</cell><cell>MAE( ? )</cell></row><row><cell cols="2">Wu et al.[31] (2020)</cell><cell>MEBOW</cell><cell cols="2">MEBOW 93.9</cell><cell>98.2</cell><cell>8.4</cell></row><row><cell>ours</cell><cell></cell><cell>MEBOW</cell><cell cols="2">MEBOW 92.3</cell><cell>97.0</cell><cell>9.7</cell></row><row><cell>ours</cell><cell></cell><cell cols="3">SIM+MEBOW MEBOW 91.7</cell><cell>97.0</cell><cell>10.0</cell></row><row><cell>ours</cell><cell></cell><cell>SIM</cell><cell cols="2">MEBOW 80.2</cell><cell>94.7</cell><cell>16.1</cell></row><row><cell cols="3">Hara et al.[47] (2017) TUD</cell><cell>TUD</cell><cell>70.6</cell><cell>86.1</cell><cell>26.6</cell></row><row><cell cols="2">Yu et al.[48] (2019)</cell><cell>TUD</cell><cell>TUD</cell><cell>75.7</cell><cell>96.8</cell><cell>15.3</cell></row><row><cell cols="2">Wu et al.[31] (2020)</cell><cell>MEBOW</cell><cell>TUD</cell><cell>77.3</cell><cell>99.0</cell><cell>14.3</cell></row><row><cell>ours</cell><cell></cell><cell>MEBOW</cell><cell>TUD</cell><cell>79.6</cell><cell>99.0</cell><cell>10.8</cell></row><row><cell>ours</cell><cell></cell><cell cols="2">SIM+MEBOW TUD</cell><cell>77.3</cell><cell>98.7</cell><cell>14.3</cell></row><row><cell>ours</cell><cell></cell><cell>SIM</cell><cell>TUD</cell><cell>75.4</cell><cell>98.1</cell><cell>16.0</cell></row><row><cell>ours</cell><cell></cell><cell>MEBOW</cell><cell>SIM-C01</cell><cell>76.2</cell><cell>97.0</cell><cell>16.6</cell></row><row><cell>ours</cell><cell></cell><cell cols="2">SIM+MEBOW SIM-C01</cell><cell>79.7</cell><cell>97.9</cell><cell>15.3</cell></row><row><cell>ours</cell><cell></cell><cell>SIM</cell><cell>SIM-C01</cell><cell>78.7</cell><cell>96.5</cell><cell>16.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IV :</head><label>IV</label><figDesc>Human body orientation (?) test results on the MEBOW, TUD<ref type="bibr" target="#b37">[36]</ref> and SIM-C01V datasets. The column trainset specifies the training dataset(s) used to train the specific networks. Testset specifies on which testsets the results are reported on. In addition to the accuracy in 22.5 ? and 45 ? intervals we report the mean average error (MAE).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Network Trainset</cell><cell>Testset</cell><cell>Acc.(22.5 ? )</cell><cell>Acc.(45 ? )</cell><cell>M AE</cell></row><row><cell></cell><cell></cell><cell>PedRec</cell><cell cols="3">SIM+MEBOW SIM-C01V 77.1</cell><cell>95.1</cell><cell>16.65</cell></row><row><cell></cell><cell></cell><cell>PedRec</cell><cell>SIM</cell><cell cols="2">SIM-C01V 76.3</cell><cell>94.8</cell><cell>17.43</cell></row><row><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell><cell></cell><cell></cell></row><row><cell cols="3">Fig. 5: 3D Human pose estimation in the wild: PedRecNet</cell><cell></cell><cell></cell></row><row><cell cols="3">false predictions. Top: Cropped image of the person inputed</cell><cell></cell><cell></cell></row><row><cell cols="3">in the network. Bottom: Predicted 3D human pose.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE V :</head><label>V</label><figDesc>PedRecNet: Head orientation test results for ?.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Simple yet efficient real-time posebased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ludl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gulde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Curio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd IEEE Int. Conf. on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="581" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Top-down influences on stereoscopic depth-perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>B?lthoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>B?lthoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="254" to="257" />
			<date type="published" when="1998-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enhancing Data-Driven Algorithms for Human Pose Estimation and Action Recognition Through Simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ludl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gulde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Curio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">VNect: Real-time 3D human pose estimation with a single RGB camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Single-Shot Multi-person 3D Pose Estimation from Monocular RGB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<meeting><address><addrLine>Verona</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="120" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">XNect: Realtime multi-person 3D motion capture with a single RGB camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-task Deep Learning for Real-Time 3D Human Pose Estimation and Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3D Human Pose Estimation = 2D Pose Estimation + Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="5759" to="5767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3D human pose estimation in video with temporal convolutions and semisupervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to Fuse 2D and 3D Image Cues for Monocular Body Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marquez-Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Venice</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="3961" to="3970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards 3D Human Pose Estimation in the Wild: A Weakly-Supervised Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Venice</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to Reconstruct 3D Human Pose and Shape via Model-Fitting in the Loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Seoul, Korea (South</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="2252" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Keep It SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>B. Leibe, J. Matas, N. Sebe, and M. Welling</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9909</biblScope>
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep Kinematic Pose Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 Workshops</title>
		<editor>Hua and H. J?gou</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9915</biblScope>
			<biblScope unit="page" from="186" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">2D/3D Pose Estimation and Action Recognition Using Multitask Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="5137" to="5146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Simple Yet Effective Baseline for 3d Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Venice</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="2659" to="2668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SMPL: A Skinned Multi-person Linear Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2015-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pictorial structures revisited: People detection and articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="1014" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Integrated pedestrian classification and orientation estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="982" to="989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Joint probabilistic pedestrian head and body orientation estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Flohr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dumitru-Guzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F P</forename><surname>Kooij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Intelligent Vehicles Symposium Proceedings</title>
		<meeting><address><addrLine>MI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="617" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Probabilistic Framework for Joint Pedestrian Head and Body Orientation Estimation</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1872" to="1882" />
			<date type="published" when="2015-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nose, Eyes and Ears: Head Pose Estimation by Locating Facial Keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting><address><addrLine>Brighton, United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-05" />
			<biblScope unit="page" from="1977" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fine-Grained Head Pose Estimation Without Keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Salt Lake City</title>
		<meeting><address><addrLine>UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="2155" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Self-Paced Deep Regression Forests with Consideration on Underrepresented Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frahm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="volume">12375</biblScope>
			<biblScope unit="page" from="271" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network-based Bernoulli heatmap for head pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">436</biblScope>
			<biblScope unit="page" from="198" to="209" />
			<date type="published" when="2021-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-task head pose estimation in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baumela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">An Efficient Multitask Neural Network for Face Alignment, Head Pose Estimation and Face Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07615</idno>
		<imprint>
			<date type="published" when="2021-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Estimation of Pedestrian Pose Orientation Using Soft Target Training Based on Teacher-Student Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1147</biblScope>
			<date type="published" when="2019-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Head and Body Orientation Estimation Using Convolutional Random Projection Forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="120" />
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pedestrian Head and Body Pose Estimation with CNN in the Context of Automated Driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steinhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>G?hring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Vehicle Technology and Intelligent Transport Systems</title>
		<meeting>the 6th International Conference on Vehicle Technology and Intelligent Transport Systems<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>SCITE-PRESS -Science and Technology Publications</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">MEBOW: Monocular Estimation of Body Orientation in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dawane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hanzra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06" />
			<biblScope unit="page" from="3448" to="3458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Simple Baselines for Human Pose Estimation and Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="472" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">International Organization for Standardization</title>
	</analytic>
	<monogr>
		<title level="m">ISO 80000-2:2019 -Quantities and units -Part 2: Mathematics</title>
		<imprint>
			<date type="published" when="2019-08" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Monocular 3D pose estimation and tracking by detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="623" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="7482" to="7491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cyclical Learning Rates for Training Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting><address><addrLine>Santa Rosa, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-03" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A disciplined approach to neural network hyper-parameters: Part 1 -learning rate, batch size, momentum, and weight decay</title>
		<idno type="arXiv">arXiv:1803.09820</idno>
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Human Pose Regression by Combining Indirect Part Detection and Contextual Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Graphics</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="15" to="22" />
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">3D Human Pose Estimation in the Wild by Adversarial Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="5255" to="5264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improving Robustness and Accuracy via Relative Information Encoding in 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="3446" to="3454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">PoseAug: A Differentiable Pose Augmentation Framework for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Using Simulation to Improve Human Pose Estimation for Corner Cases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ludl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gulde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thalji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Curio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st IEEE Int. Conf. on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="3575" to="3582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Designing Deep Convolutional Neural Networks for Continuous Object Orientation Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01499</idno>
		<imprint>
			<date type="published" when="2017-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Continuous Pedestrian Orientation Estimation using Human Keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<imprint>
			<date type="published" when="2019-05" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
