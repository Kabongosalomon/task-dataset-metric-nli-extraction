<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Kernel Distillation for Efficient Pose Estimation in Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
							<email>niexuecheng@u.nus.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Snap Inc. 3 ByteDance AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Luo</surname></persName>
							<email>linjie.luo@bytedance.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
							<email>ningzhang@berkeley.eduelefjia@nus.edu.sg</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Kernel Distillation for Efficient Pose Estimation in Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing video-based human pose estimation methods extensively apply large networks onto every frame in the video to localize body joints, which suffer high computational cost and hardly meet the low-latency requirement in realistic applications. To address this issue, we propose a novel Dynamic Kernel Distillation (DKD) model to facilitate small networks for estimating human poses in videos, thus significantly lifting the efficiency. In particular, DKD introduces a light-weight distillator to online distill pose kernels via leveraging temporal cues from the previous frame in a one-shot feed-forward manner. Then, DKD simplifies body joint localization into a matching procedure between the pose kernels and the current frame, which can be efficiently computed via simple convolution. In this way, DKD fast transfers pose knowledge from one frame to provide compact guidance for body joint localization in the following frame, which enables utilization of small networks in video-based pose estimation. To facilitate the training process, DKD exploits a temporally adversarial training strategy that introduces a temporal discriminator to help generate temporally coherent pose kernels and pose estimation results within a long range. Experiments on Penn Action and Sub-JHMDB benchmarks demonstrate outperforming efficiency of DKD, specifically, 10? flops reduction and 2? speedup over previous best model, and its state-of-the-art accuracy. * This work was partly done while Xuecheng was an intern as Snap Inc. Small CNN Pose Kernel Distillator Matching Frame t-1 Frame t Small CNN Matching Frame t+1 Small CNN Pose Kernel Distillator Matching (a) Our DKD Model RNN or Optical Flow Large CNN Classification Frame t-1 RNN or Optical Flow Large CNN Frame t Large CNN Frame t+1 Classification Classification (b) The Traditional Model</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human pose estimation in videos aims to generate framewise joint localization of the human body. It is important for many applications including surveillance <ref type="bibr" target="#b7">[8]</ref>, computer animation <ref type="bibr" target="#b17">[18]</ref>, and AR/VR <ref type="bibr" target="#b18">[19]</ref>. Compared to its still-image based counterpart, this task is more challenging due to its low-latency requirement and various distracting factors, e.g., motion blur, pose variation and viewpoint change. Prior CNN based methods to solve this task <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b19">20]</ref> usually use a large network to extract representative features for every frame and localize body joints based on them via pixel-wise classification. Some recent works also incorporate temporal cues from optical flow <ref type="bibr" target="#b8">[9]</ref> or RNN units <ref type="bibr" target="#b29">[30]</ref> to improve the performance, as shown in <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>. Despite their notable accuracy, these methods suffer expensive computation cost from the large model size, and hardly meet the low-latency requirement for realistic applications. The efficiency of video-based pose estimation still needs to be largely enhanced.</p><p>In this paper, we propose to enhance efficiency of human pose estimation in videos by fully leveraging temporal cues to enable small networks to localize body joints accurately. Such an idea is motivated by observing the computational bottleneck for prior models. Considering the temporal consistency across adjacent frames, it is not necessary to pass every frame through a large network for feature extraction. Instead, the model only needs to learn how to effectively transfer knowledge of pose localization in previous frames to the subsequent frames. Such transfer can help alleviate the requirements of large models and reduce the overall computational cost.</p><p>To implement the above idea, we design a novel Dynamic Kernel Distillation (DKD) model. As shown in <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>, DKD online distills pose knowledge from the previous frame into pose kernels through a light-weight distillator. Then, DKD simplifies body joint localization into a matching procedure between the pose kernels and the current frame through simple convolution. In this way, DKD fast re-uses pose knowledge from one frame and provides compact guidance for a small network to learn discriminative features for accurate human pose estimation.</p><p>In particular, DKD introduces a light-weight CNN based pose kernel distillator. It takes features and pose estimations of the previous frame as input and infers pose kernels suitable for the current frame. These pose kernels carry knowledge of body joint configuration patterns from the previous frame to the current frame, and guide a small network to learn compact features matchable to the pose kernels for efficient pose estimation. Accordingly, body joint localization is cast as a matching procedure via applying pose kernels on feature maps output from small networks with simple convolution to search for regions with similar patterns. Since it gets rid of the need for using large networks, DKD performs significantly faster than prior models. In addition, this 2D convolution based matching scheme is significantly cheaper than additional optical flow <ref type="bibr" target="#b8">[9]</ref>, the decoding phase of an RNN unit <ref type="bibr" target="#b29">[30]</ref> or the expensive 3D convolutions <ref type="bibr" target="#b15">[16]</ref>. Moreover, the distillator framewisely updates the pose kernels according to current joint representations and configurations. This dynamic feature makes DKD more flexible and robust in analyzing various scenarios in videos.</p><p>To further leverage temporal cues to facilitate the distillator to infer suitable pose kernels, DKD introduces a temporally adversarial training method that adopts a discriminator to help estimate consistent poses in consecutive frames. The temporally adversarial discriminator learns to distinguish the groundtruth change of joint confidence maps over neighboring frames from the predicted change, and thus supervises DKD to generate temporally coherent poses. In contrast to previous adversarial training methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5]</ref> that learn structure priors in the spatial dimension for recognition over still images, our method constrains the pose variations in the temporal dimension of videos, enforcing plausible changes of estimated poses in videos. In addition, this discriminator can be removed during the inference phase, thus introducing no additional computation.</p><p>The whole framework of the proposed DKD model is endto-end learnable. Comprehensive experiments on two widely used benchmarks Penn Action <ref type="bibr" target="#b32">[33]</ref> and Sub-JHMDB <ref type="bibr" target="#b14">[15]</ref> demonstrate the efficiency and effectiveness of our DKD model for resolving human pose estimation in videos. Our main contributions are in three folds: 1) We propose a novel model to facilitate small networks in video-based pose estimation with lifted efficiency, by using a light-weight distillator to online distill the pose knowledge and simplifying body joint localization into a matching procedure with simple convolution. 2) We introduce the first temporally adversarial training strategy for encouraging the coherence of estimated poses in the temporal dimension of videos. 3) Our model achieves outperforming efficiency, i.e. 10x flops reduction and 2x speedup over previous best model, also with state-ofthe-art accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>For human pose estimation in videos, existing CNN based methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b9">10]</ref> usually focus on leveraging temporal cues to extract complementary information for refining the preliminary results output from a large network for every frame. In <ref type="bibr" target="#b13">[14]</ref>, Iqbal et al. incorporate deep learned representations into an action conditioned pictorial structured model to refine pose estimation results of each frame. In <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b9">[10]</ref>, 3D convolutions are exploited on video clips for implicitly capturing the temporal contexts between frames. In <ref type="bibr" target="#b24">[25]</ref>, Song et al. propose a Thin-Slicing network that uses dense optical flow to warp and align heatmaps of neighboring frames and then performs spatial-temporal inference via message passing through the graph constructed by joint candidates and their relationships among aligned heatmaps. <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b19">[20]</ref> sequentially estimate human poses in videos following the Encoder-RNN-Decoder framework. Given a frame, this kind of framework first uses an encoder network to learn high-level image representations, then RNN units to explicitly propagate temporal information between neighboring frames and produce hidden states, and finally a decoder network to take hidden states as input and output pose estimation results of current frame. For ensuring good performance, however, these methods always require large network to compactly learn intermediate representations or preliminary poses. Their efficiency is rather limited.</p><p>Different from existing methods, our DKD model distills coherent pose knowledge from temporal cues and simplifies body joint localization as a matching problem, thus allowing small networks to accurately and efficiently estimate human poses in videos, which is explained in more detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formulation</head><p>We first mathematically formulate the proposed Dynamic Kernel Distillation (DKD) model for human pose estimation in videos. For a video V={I t } T t=1 including T frames, we use I t ?R M ?N ?3 to denote its tth frame, where M and N are the height and width of I t , respectively. DKD aims to estimate a set of confidence maps H={h t } T t=1 for all frames in V. The h t ?R m?n?K is of spatial size m?n, where K is the number of body joints, and each of its elements encodes the confidence of a joint at the corresponding position. Accordingly, DKD performs online human pose estimation frame-by-frame in a sequential manner, by leveraging temporal cues between neighboring frames. In particular, its core is composed of a pose kernel distillator with a temporally adversarial training strategy. Pose kernel distillation Given a frame I t , DKD introduces a pose kernel distillator ?(?) to transfer pose knowledge provided by I t to guide pose estimation in the next frame I t+1 . In particular, it leverages temporal cues represented with the combination of feature maps f t and confidence maps h t , to online distill pose kernels k t via a simple feed-forward computation</p><formula xml:id="formula_0">k t = ?(f t , h t ),<label>(1)</label></formula><p>where k t ?R S?S?C?K and S is the kernel size. The distilled pose kernels k t encode knowledge of body joint patterns and provide compact guidance for pose estimation in the posterior frame, which is learnable with light-weight networks. Accordingly, DKD exploits a small frame encoder F(?) to learn high-level image representations f t+1 of frame I t+1 to match these distilled pose kernels, alleviating the demand of large networks that troubles prior works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b19">20]</ref>. Then, DKD applies the distilled pose kernels k t on feature maps f t+1 , in a sliding window manner to search for the region with similar patterns as each body joint, namely,</p><formula xml:id="formula_1">h j t+1 = k j t ? f t+1 ,<label>(2)</label></formula><p>where ? denotes the convolution operation, and h j t+1 ?R m?n , k j t ?R S?S?C are the confidence map and pose kernels of the jth joint, respectively. With the above formulation, DKD casts human pose estimation to a matching problem and locates the position with maximum response on h j t+1 in the (t+1)th frame as the jth body joint. In this way, the pose kernel distillator equips DKD with the capability of transferring pose knowledge among neighboring frames and enables small network to estimate human pose in videos. Its distilled pose kernels can be applied to fast localize body joints with simple convolution, further improving the efficiency. In addition, it can directly leverage temporal cues of one frame to assist body joint localization in the following frame, without requiring auxiliary optical flow models <ref type="bibr" target="#b24">[25]</ref> or decoders appended to RNN units <ref type="bibr" target="#b19">[20]</ref>. It can also fast distill pose kernels in a one-shot manner, avoiding complex iterating utilized by previous online kernel learning models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27]</ref>. Moreover, it framewisely updates pose kernels and improves the robustness of our model to joint appearance and configuration variations.</p><p>It is worth noting that, for the first frame, due to the lack of preceding temporal cues, we utilize another pose model P(?), usually larger than F(?), to initialize its confidence map, i.e., h 1 =P(I 1 ). In particular, ?(?) together with F(?) and P(?) instantiate the pose generator. Given pose annotations {? t } T t=1 , to learn the pose generator, we define the loss as</p><formula xml:id="formula_2">L G = T t=1 2 (h t ,? t ),<label>(3)</label></formula><p>where 2 denotes the Mean Square Error loss.</p><p>Temporally adversarial training To further leverage temporal cues, DKD adopts the adversarial training strategy to learn proper supervision in the temporal dimension for improving the pose kernel distillator. Adversarial training was only exploited for images in the spatial dimension in prior works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5]</ref>. In contrast, our proposed temporally adversarial training strategy aims to provide constraints for pose changes in the temporal dimension, helping estimate coherent human poses in consecutive frames of videos. Inspired by <ref type="bibr" target="#b5">[6]</ref>, DKD introduces a discriminator D(?) to distinguish the changes of groundtruth confidence maps between neighboring frames from predicted ones. The discriminator D(?) takes as input two neighboring confidence maps (either from groundtruth or prediction) concatenated with the corresponding images, and reconstructs the change of the confidence maps. For real (groundtruth) samples? t and ? t+1 , the discriminator D(?) targets at approaching their changed t =? t+1 ?? t , while for fake (predicted) samples h t and h t+1 , keeping the reconstructed change away from d t =h t+1 ?h t . Therefore, the discriminator can better differentiate groundtruth change from erroneous predictions. In this way, the discriminator D(?) criticizes pixel-wise variations of confidence maps and judges whether joint positions are in rational movements, to encourage the pose kernel distillator to distill suitable pose kernels and ensure consistency of estimated poses between neighboring frames. To train the discriminator D(?), we define its loss function as</p><formula xml:id="formula_3">L D =? T ?1 t=1 2 (d f t , d t ) ? T ?1 t=1 2 (d r t ,d t ),<label>(4)</label></formula><p>where d r t =D(I t ,? t , I t+1 ,? t+1 ) denotes the output from the discriminator for real samples and d f t =D(I t , h t , I t+1 , h t+1 ) denotes the one for fake samples. ? is a variable for dynamically balancing the relative learning speed between the pose generator and temporally adversarial discriminator.</p><p>The temporally adversarial training conventionally follows a two-player minmax game. Therefore, the final objective function of the DKD model is written as</p><formula xml:id="formula_4">min P,F,? max D L G + ?L D ,<label>(5)</label></formula><p>where ? is a constant for weighting generator loss and discriminator loss, set as 0.1. The training process to optimize the above object function will be illustrated in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network architecture</head><p>Pose initializer For the first frame I 1 , DKD utilizes a pose initializer P(?) to directly estimate its confidence maps h 1 . Here, P(?) exploits the network following <ref type="bibr" target="#b28">[29]</ref>, which achieves outstanding performance with a simple architecture. The network follows a U-shape architecture. It first encodes down-sized feature maps from the input image, and then gradually recovers high-resolution feature maps by appending several deconvolution layers, as shown in <ref type="figure">Fig. 2 (b)</ref>. In particular, we use ResNet <ref type="bibr" target="#b12">[13]</ref> as the backbone and append two deconvolution layers, resulting in a total stride of the network of 8. The other settings follow <ref type="bibr" target="#b28">[29]</ref>.</p><p>Frame encoder DKD utilizes an encoder F(?) to extract high-level features f t of frame I t to match the pose kernels from the pose kernel distillator. Here, we design F(?) with the same network architecture as the pose initializer P(?), with only the last classification layer removed from P(?). Note, the backbone of F(?) is much smaller than P(?).</p><p>Pose kernel distillator The pose kernel distillator ?(?) in DKD takes as input the temporal information, represented by the concatenation of feature maps f t and confidence maps h t , and distills the pose kernels k t in a one-shot feed-forward manner. We implement ?(?) with a CNN, including three convolution layers followed by BatchNorm and ReLU layers and two pooling layers. Its architecture is shown in <ref type="figure">Fig. 2</ref> (c). This light-weight CNN guarantees the efficiency of ?(?). However, it is inefficient and infeasible for ?(?) to directly learn all kernels k t ?R S?S?C?K due to their large scale which brings high computational complexity and also the risk of overfitting. To avoid these issues, inspired by <ref type="bibr" target="#b2">[3]</ref>, DKD exploits ?(?) to learn the kernel bases k t instead of full size k t via performing the following factorization:</p><formula xml:id="formula_5">k t = U ? k t ? C V,<label>(6)</label></formula><p>where ? is the convolution operation, ? C the channel-wise convolution, and U ?R 1?1?C?K , V ?R 1?1?C?C are coefficients over the kernel bases k t ?R S?S?C . In this way, the size of actual outputs k t from the pose kernel distillator is smaller than original k t by a magnitude, thus enhancing the efficiency of the DKD model. To generate the confidence maps h t+1 of I t+1 , the calculation between k t and f t+1 is implemented with convolution layers. In particular, we first use a 1?1 convolution parameterized by V on f t+1 . Then we apply k t in a dynamic convolution layer <ref type="bibr" target="#b20">[21]</ref>, which is the same with traditional convolution layer, just replacing the pre-learned static convolution kernels with the dynamically learned ones. Finally, we adopt another 1?1 convolution with U to produce h t+1 . To scale the estimation results with the pose kernels, we add a BatchNorm layer in the last to facilitate the training. Temporally adversarial discriminator DKD utilizes the temporally adversarial discriminator D(?) to enhance the learning process of the pose kernel distillator with confidence map variations as auxiliary temporal supervision. We design D(?) with the same network backbone as the frame encoder F(?) to balance the learning capability between pose generator and discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training and inference</head><p>In this subsection, we will explain the training and inference process of the DKD model for human pose estimation in videos. Specifically, DKD exploits a temporally adversarial training strategy. The discriminator is optimized via maximizing the loss function defined in Eqn. (5) for distinguishing the changes of groundtruth confidence maps from estimated ones between neighboring frames. On the other hand, the generator produces a set of confidence maps for consecutive frames in a video and meanwhile fools the discriminator via making the changes of estimated poses approach those of groundtruth ones. To synchronize the learning speed between generator and discriminator, we follow <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref> to update ? in Eqn. (4) for each iteration i:</p><formula xml:id="formula_6">? i+1 = ? i + ? T ?1 t=1 2 (d r t ,d t ) ? T ?1 t=1 2 (d f t , d t )<label>(7)</label></formula><p>Algorithm 1: Training process for our DKD model.</p><formula xml:id="formula_7">input :video {It} T t=1 , groundtruth {?t} T t=1 , iteration number E initialization: L D ? 0, L G ? 0 for iteration i, i=1 to E do Forward pose initializer h 1 ? P(I 1 ) Update loss L G ? 2 (h 1 ,? 1 ) for frame t, t=1 to T do if t equals 1 then Encode image representations f 1 ? F(I 1 ) end else Forward discriminator d r t?1 ?D(I t?1 ,? t?1 , It,?t) Update loss L D ? L D ? 2 (d r t?1 ,d t?1 ) Update pose kernels k t?1 ? ?(f t?1 , h t?1 ) Encode image representations ft ? F(It) Estimate confidence map ht with Eqn. (2) Update loss L G ? L G + 2 (ht,?t) Forward discriminator d f t?1 ?D(I t?1 , h t?1 , It, ht) Update loss L D ? L D + ? i 2 (d f t?1 , d t?1 ) Update loss L G ? L G + ? 2 (d f t?1 , d t?1 ) end end Update discriminator D(?) with ?L D via backpropagation Update P(?), ?(?), and F(?) with L G via backpropagation Update ? i with Eqn. (7) end</formula><p>where ? is a hyper-parameter controlling the update rate and set as 0.1. ? is initialized as 0 and bounded in [0, 1]. As defined in Eqn. <ref type="bibr" target="#b6">(7)</ref>, when the generator successfully fools the discriminator, ? will be increased to make the optimizer emphasize improving the discriminator, and vice versa. The overall training process is illustrated in Algorithm 1.</p><p>During inference, the discriminator D(?) is removed. Given a video, DKD first utilizes the pose initializer P(?) to estimate the confidence maps h 1 of the first frame. Then, h 1 is combined with the feature maps f 1 from the encoder F(?) as input to the pose kernel distillator ?(?) for distilling the initial pose kernels k 1 . For the second and subsequent frames, DKD applies the framewisely updated pose kernels k t on the feature maps f t+1 =F(I t+1 ) of the posterior frame to estimate the confidence maps h t+1 . Finally, DKD outputs body joint positions for each frame by localizing the maximum responses on the corresponding confidence maps. The overall inference procedure of DKD is given in <ref type="figure">Fig. 2 (a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>Datasets We evaluate our model on two widely used benchmarks: Penn Action <ref type="bibr" target="#b32">[33]</ref> and Sub-JHMDB <ref type="bibr" target="#b14">[15]</ref>. Penn Action dataset is a large-scale unconstrained video dataset. It contains 2,326 video clips, 1,258 for training and 1,068 for testing. Each person in a frame is annotated with 13 body joints, including the coordinates and visibility. Following conventions, evaluations on the Penn Action dataset only consider the visible joints. Sub-JHMDB is another dataset for video based human pose estimation. It provides labels for 15 body joints. Different from Penn Action dataset, it only annotates visible joints for complete bodies. It contains 316 video clips with 11,200 frames in total. The ratio for the number of training and testing videos is roughly 3:1. In addition, it includes three different split schemes. Following previous works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25]</ref>, we separately conduct evaluations on these three splits and report the average precision.</p><p>Data augmentation For both the Penn Action dataset and Sub-JHMDB dataset, we perform data augmentation following conventional strategies, including random scaling with a factor from [0.8, 1.4], random rotation in [?40 ? , 40 ? ] and random flipping. The same augmentation setting is applied to all the frames in a training video clip. In addition, each frame is cropped based on the person center on the original image and padded to 256?256 as input for training.</p><p>Implementation For fair comparison with previous works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25]</ref>, we first pre-train the pose initializer and the frame encoder for single-person pose estimation on the MPII <ref type="bibr" target="#b0">[1]</ref> dataset. Then, we fine-tune the pre-trained models together with the randomly initialized pose kernel distillator and the temporally adversarial discriminator on Penn Action dataset and Sub-JHMDB dataset for 40 epochs, respectively. In particular, each training sample contains 5 frames, which are consecutively sampled from a video. We set the channel number C of the pose kernels k t as 256 and the kernel size S as 7. We implement our DKD model with Pytorch <ref type="bibr" target="#b23">[24]</ref> and use RMSprop as the optimizer <ref type="bibr" target="#b25">[26]</ref>. We set the initial learning rate as 0.0005 and drop it with a multiplier 0.1 at the 15th and 25th epochs. For evaluation, we perform seven-scale testing with flipping.</p><p>Evaluation metrics We evaluate the performance with PCK [32]-the localization of a body joint is considered to be correct if it falls within ??L pixels of the groundtruth. ? controls the relative threshold and conventionally set as 0.2. L is the reference distance, set as L= max(H, W ) following prior works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25]</ref> with H and W being height and width of the person bounding box. We term this metric as PCK normalized by person size. This metric is somewhat loose to precisely evaluate the model performance as person size is usually relatively large. Thereby, we follow the conventions of still-image based pose estimation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b27">28]</ref>, and also adopt another metric that takes torso size as reference distance. We term it as PCK normalized by torso size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation analysis</head><p>We first conduct ablation studies on Penn Action dataset to analyze the efficacy of each core component of our DKD model: the pose kernel distillator and the temporally adversarial training. We fix the backbone of the pose initializer as ResNet101. We vary the backbone of  Results are shown in Tab. 1. From Tab. 1, we can see that DKD(ResNet34) and DKD(ResNet50) use smaller networks for frame feature learning while achieve much better performance than Baseline(ResNet101) which is much deeper. We can also see DKD(ResNet18) achieves comparable performance to Baseline(ResNet101) (90.6% PCK vs 90.7% PCK), with up to 2? flop reduction (5.27G vs 11.02G Flops). These results verify the efficacy of DKD to enable small networks to estimate human pose in videos, bring efficiency enhancement while achieving outperforming accuracy. By comparing the DKD(ResNetx)-w/o-TATs and the Baseline(ResNetx)s, we find that the computation overhead of the pose kernel distillator is small, only bringing slight flops increase, e.g., with ResNet50 as backbone, from 7.66G to 8.65G. We can also find the pose kernel distillator improves frame-level performance for human pose estimation over baselines by 4.3% in average. Besides, DKD(ResNetx)w/o-TATs always outperform DKD(ResNetx)-w/o-PKDs, this implies the distilled pose kernels carry knowledge of body joint patterns and provide compact guidance for pose estimation between neighboring frames, which are absent in still-image based inference. The above results verify the efficacy of the pose kernel distillator for efficiently transferring pose knowledge to assist poses estimation in videos.</p><p>By comparing the time cost of the DKD(ResNetx)-w/o-PKDs and the Baseline(ResNetx)s, we find temporally adversarial training does not hurt inference speed, since the discriminator is used only in training. In addition, the temporally adversarial training consistently improves the baseline performance for all body joints, in particular for the joints difficult to localize, e.g., DKD(ResNet34)-w/o-PKD improves the accuracy of ankles from 85.3% PCK to 90.3% PCK. This demonstrates the proposed temporally adversarial training is effective for regularizing temporal changes over pose predictions during model training.</p><p>Combining temporally adversarial training with the pose kernel distillator, the full DKD model further boosts the performance over all the ablated models, showing they are complementary to each other. Especially, DKD(ResNetx)s achieves average 5.5% performance gain over the corresponding vanilla baselines Baseline(ResNetx)s.</p><p>To better reveal the advantages of our DKD model over single-frame based models, we visualize the confidence maps estimated from DKD(ResNet34) and Baseline (ResNet34) for the elbow and ankle in <ref type="figure" target="#fig_2">Fig. 3</ref>. By comparing <ref type="figure" target="#fig_2">Fig. 3 (b)</ref> and (c), we can observe that our DKD model produces pose kernels of the correct person of interest with more accurate response. In contrast, the baseline model produces false alarms on the elbow of another person in the frame. We can also see that the proposed model can produce consistent confidence maps for the hip in <ref type="figure" target="#fig_2">Fig. 3 (d)</ref> while the baseline model produces unstable estimations even with fixed hip <ref type="figure" target="#fig_2">Fig. 3 (e)</ref>. These results further validate the capability of the proposed model for generating accurate and temporally consistent human pose estimations in videos.</p><p>Next, we analyze how well our pose kernel distillator performs for propagating temporal information via comparing it with the state-of-the-art Convolutional LSTMs <ref type="bibr" target="#b19">[20]</ref>. We also compare our temporally adversarial training with the spatially one in <ref type="bibr" target="#b5">[6]</ref>. All the compared models adopt the ResNet101 as the backbone of the pose initializer and ResNet34 as the frame encoder. Except for the compared components, all the other settings are the same. Results are shown in Tab. 2. We use DKD(ResNet34)-w-LSTM to denote the model utilizing Convolutional LSTM for temporal cues propagation instead of our pose kernel distillator in the DKD model. We can observe that DKD(ResNet34)w-LSTM degrades the accuracy of DKD(ResNet34) for all body joints, especially for wrist and ankle. In addition, it increases the flops from 7.68G to 10.16G. These results evaluate the superiority of the pose kernel distillator in both efficiency and efficacy for transferring pose knowledge between neighboring frames over traditional RNN units. We use DKD(ResNet34)-w-SAT to denote the model in which our temporally adversarial training is replaced with the spatially one in <ref type="bibr" target="#b5">[6]</ref>. Specifically, <ref type="bibr" target="#b5">[6]</ref> introduces a discriminator to distinguish the single-frame groundtruth confidence maps from estimated ones for obtaining structural spatial constraints on poses. We can see DKD(ResNet34) consistently outperforms DKD(ResNet34)-w-SAT. In addition, by comparing DKD(ResNet34)-w-SAT with DKD(ResNet34)w/o-TAT in Tab. 1, spatially adversarial training only brings limited improvement. These results further verify the efficacy of using adversarial training in temporal dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons with state-of-the-arts</head><p>Tab. 3 show the comparisons of our DKD model with state-of-the-arts on Penn Action dataset. In particular, the method proposed in <ref type="bibr" target="#b19">[20]</ref> follows the Encoder-RNNs-Decoder framework with Convolutional LSTMs, while <ref type="bibr" target="#b24">[25]</ref> exploits optical flow models to align confidence maps of neighboring frames. We report the performance of our model with both person and torso size as reference distance under the PCK evaluation metric. For comparison with current best model <ref type="bibr" target="#b19">[20]</ref>, we report both its performance with PCK normalized by torso size, flops and running time <ref type="bibr" target="#b0">1</ref> . For our DKD model, we fix the backbone of the pose initializer as ResNet101. We vary the backbone of frame encoder ranging in ResNet18/34/50. Since both of state-of-the-arts <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b24">[25]</ref> use the same network as Convolutional Pose Machines (CPM) <ref type="bibr" target="#b27">[28]</ref>, we also experiment our DKD model with a frame encoder as a simplified version of CPM by replacing its kernels with size larger than 3 to 3?3 kernels, denoted as DKD(SmallCPM), to further verifying the efficacy of DKD to facilitate small networks in video-based pose estimation.   From Tab. 3, we can observe that our best model DKD(ResNet50) reduces the computation flops by a magnitude over <ref type="bibr" target="#b19">[20]</ref> (8.65G vs 70.98G) and achieves 2x faster speed (11ms vs 25ms per image), verifying the outperforming efficiency of our model. In addition, we can see under PCK normalized by person size, DKD(ResNet50) achieves comparable accuracy with state-of-the-art <ref type="bibr" target="#b19">[20]</ref>. When using PCK normalized by torso size, DKD(ResNet50) achieves superior accuracy over <ref type="bibr" target="#b19">[20]</ref> (92.9% PCK vs 92.6% PCK) and with better performance for all of the body joints. We also compare our model with <ref type="bibr" target="#b19">[20]</ref> via evaluating the performance with PCK normalized by torso size when varying threshold ? from 0 to 0.2 with 0.01 as the step size, and results are shown in <ref type="figure" target="#fig_4">Fig. 4 (a)</ref>. We can see that DKD consistently outperforms <ref type="bibr" target="#b19">[20]</ref> under more critic metrics by decreasing ?. These results demonstrate the superior speed and accuracy of our model for human pose estimation in videos.</p><p>By comparing DKD(SmallCPM) with <ref type="bibr" target="#b19">[20]</ref>, we can find our DKD model maintains high accuracy (92.4% PCK vs 92.6% PCK) in case of significant simplification to the network (9.96G vs 70.98G Flops). This result verifies the effectiveness of our DKD model for alleviating the demands of large networks for video-based human pose estimation.</p><p>To evaluate the effects of different frame encoder backbones on the efficiency and efficacy of DKD, we plot speed vs. accuracy analysis for different models in <ref type="figure" target="#fig_4">Fig. 4 (b)</ref>. We can observe that reducing depth of frame encoder backbone from ResNet50 to ResNet18 slightly degrades the accuracy, but speeds up 2x from 11ms to 6.5ms per image. In addition, we can see that DKD(ResNet18) achieves comparable  performance with <ref type="bibr" target="#b19">[20]</ref> but 4x faster. These results further validate the efficacy of our DKD model to facilitate small networks in video-based pose estimation. Tab. 4 show the comparisons of our DKD model with state-of-the-arts on Sub-JHMDB dataset. We can see that our DKD model achieves new state-of-the-art 94.0% PCK and performs best for all the body joints. When using the stricter metric PCK normalized by torso size, the superiority of our model over <ref type="bibr" target="#b19">[20]</ref> is more significant, achieving over 5% improvement (77.4% PCK vs 73.6% PCK) on average. In addition, we can find that our model well applies to smallscale datasets, such as Sub-JHMDB with only 316 videos. These small datasets are challenging since they provide only limited training samples, while in our DKD model, the oneshot pose kernel distillator is able to fast adapt pose kernels, without requiring a large number of training samples for iteratively tuning classifiers as in existing methods.</p><p>Qualitative results <ref type="figure" target="#fig_5">Fig. 5</ref> shows the qualitative results to visualize efficacy of the DKD model for human pose estimation in videos on Penn Action and Sub-JHMDB, respectively. We can observe DKD can accurately estimate human poses in various challenging scenarios, e.g., cluttered backgrounds (the 1st row of <ref type="figure" target="#fig_5">Fig. 5 (a)</ref>), scale variations (the 1st row of <ref type="figure" target="#fig_5">Fig. 5 (b)</ref>), motion blur (the 2nd rows of <ref type="figure" target="#fig_5">Fig. 5 (a) and (b)</ref>). In addition, it can leverage temporal cues to handle occa-sional disappearance of a body joint caused by occlusion, as shown in the 3rd row of <ref type="figure" target="#fig_5">Fig. 5 (a)</ref>, and encourage pose consistency in presence of fast and large-degree pose variations, as shown in the 3rd and 4th rows of <ref type="figure" target="#fig_5">Fig. 5 (b)</ref>. Moreover, it is robust to various view-point and lighting conditions, as shown in the 5th rows of <ref type="figure" target="#fig_5">Fig. 5 (a)</ref> and (b), respectively. These results further verify the effectiveness of DKD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presents a Dynamic Kernel Distillation (DKD) model for improving efficiency of human pose estimation in videos. In particular, it adopts a pose kernel distillator to online distill the pose kernels from temporal cues of one frame in a one-shot feed-forward manner. The distilled pose kernels encode knowledge of body joint patterns and provide compact guidance for pose estimation in the posterior frame. With these pose kernels, DKD simplifies body joint localization into a matching procedure with simple convolution. In this way, DKD fast transfers pose knowledge between neighboring frames and enables small networks to accurately estimate human poses in videos, thus significantly lifting the efficiency. DKD also introduces the temporally adversarial training strategy via constraining the changes of estimated confidence maps between neighboring frames. The whole framework can be end-to-end trained and inferred. Experiments on two benchmarks demonstrate that our model achieves state-of-the-art efficiency with only 1/10 flops and 2x faster speed of the previous best model, and also outperforming accuracy for human pose estimation in videos.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Comparison between (a) our DKD model and (b) the traditional model for video-based human pose estimation. DKD online distills coherent pose knowledge and simplifies body joint localization into a matching procedure, facilitating small networks to efficiently estimate human pose in videos while achieving outperforming accuracy. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Comparison of confidence maps estimated from the proposed model DKD(ResNet34) and the baseline one Baseline(ResNet34). (a) are input frames. (b) and (d) are estimated confidence maps from our model for right elbow and right hip, respectively, and (c) and (e) from baseline. Best viewed in color.frame encoder ranging in ResNet18/34/50, since it dominates the computational cost of pose estimation of our model. We use DKD(ResNetx) to denote our full model, where x?{18, 34, 50} represents the backbone depth of the frame encoder. We use DKD(ResNetx)-w/o-TAT to denote the model without the temporally adversarial training and DKD(ResNetx)-w/o-PKD the model without the pose kernel distillator. We use Baseline(ResNetx) to denote the singleimage pose estimation model without using temporal cues.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Extensive analysis for comparing our method with stateof-the-art [20] on (a) PCK over different thresholds with ? ranging from 0 to 0.2; (b) speed vs. accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative results on (a) Penn Action dataset and (b) Sub-JHMDB dataset. Best viewed in color and 2x zoom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Ablation studies on Penn Action dataset with PCK normalized by torso size as evaluation metric.</figDesc><table><row><cell>Methods</cell><cell cols="2">Flops(G) Head Sho. Elb. Wri. Hip Knee Ank. PCK</cell></row><row><cell>Baseline(ResNet101)</cell><cell cols="2">11.02 96.1 90.7 91.4 89.5 86.2 92.2 88.9 90.7</cell></row><row><cell>DKD(ResNet50)</cell><cell>8.65</cell><cell>96.6 93.7 92.9 91.2 88.8 94.3 93.7 92.9</cell></row><row><cell>DKD(ResNet50)-w/o-TAT</cell><cell>8.65</cell><cell>96.6 92.6 92.9 90.8 87.5 93.4 92.4 92.1</cell></row><row><cell cols="2">DKD(ResNet50)-w/o-PKD 7.66</cell><cell>96.0 91.8 92.4 90.4 88.3 93.5 89.8 91.6</cell></row><row><cell>Baseline(ResNet50)</cell><cell>7.66</cell><cell>96.0 90.5 89.4 87.6 83.8 89.7 86.0 88.8</cell></row><row><cell>DKD(ResNet34)</cell><cell>7.68</cell><cell>96.4 91.9 93.0 90.8 88.6 93.5 91.9 92.1</cell></row><row><cell>DKD(ResNet34)-w/o-TAT</cell><cell>7.68</cell><cell>96.4 91.2 92.7 89.9 87.3 93.3 90.9 91.4</cell></row><row><cell cols="2">DKD(ResNet34)-w/o-PKD 6.69</cell><cell>95.9 91.1 91.9 89.3 87.7 92.5 90.3 91.0</cell></row><row><cell>Baseline(ResNet34)</cell><cell>6.69</cell><cell>95.8 88.7 88.5 86.7 83.6 89.6 85.3 87.3</cell></row><row><cell>DKD(ResNet18)</cell><cell>5.27</cell><cell>95.7 90.0 92.2 89.4 86.8 92.3 89.5 90.6</cell></row><row><cell>DKD(ResNet18)-w/o-TAT</cell><cell>5.27</cell><cell>95.5 89.3 91.9 89.1 85.0 91.6 89.0 89.9</cell></row><row><cell cols="2">DKD(ResNet18)-w/o-PKD 4.28</cell><cell>95.0 89.1 92.4 88.7 85.5 91.4 87.7 89.7</cell></row><row><cell>Baseline(ResNet18)</cell><cell>4.28</cell><cell>94.7 86.0 87.7 84.6 81.1 87.4 84.3 86.1</cell></row><row><cell>(a)</cell><cell></cell><cell></cell></row><row><cell>(b)</cell><cell></cell><cell></cell></row><row><cell>(c)</cell><cell></cell><cell></cell></row><row><cell>(d)</cell><cell></cell><cell></cell></row><row><cell>(e)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of temporally vs. spatially adversarial training, and pose kernel distillator vs. Convolutional LSTM. The accuracy is measured with PCK normalized by torso size.</figDesc><table><row><cell>Methods</cell><cell cols="2">Flops(G) Head Sho. Elb. Wri. Hip Knee Ank. PCK</cell></row><row><cell>DKD(ResNet34)</cell><cell>7.68</cell><cell>96.4 91.9 93.0 90.8 88.6 93.5 91.9 92.1</cell></row><row><cell>DKD(ResNet34)-w-SAT</cell><cell>7.68</cell><cell>96.4 91.4 92.8 90.1 87.7 93.4 91.2 91.6</cell></row><row><cell cols="3">DKD(ResNet34)-w-LSTM 10.16 95.7 89.5 92.9 90.2 86.9 93.5 90.1 91.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison with state-of-the-arts on Penn Action dataset.</figDesc><table><row><cell>Methods</cell><cell cols="3">Flops(G) Time(ms) Head Sho. Elb. Wri. Hip Knee Ank. PCK</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Normalized by Person Size</cell></row><row><cell>Park et al. [23]</cell><cell>-</cell><cell>-</cell><cell>62.8 52.0 32.3 23.3 53.3 50.2 43.0 45.3</cell></row><row><cell>Nie et al. [22]</cell><cell>-</cell><cell>-</cell><cell>64.2 55.4 33.8 24.4 56.4 54.1 48.0 48.0</cell></row><row><cell>Iqal et al. [14]</cell><cell>-</cell><cell>-</cell><cell>89.1 86.4 73.9 73.0 85.3 79.9 80.3 81.1</cell></row><row><cell>Gkioxari et al. [11]</cell><cell>-</cell><cell>-</cell><cell>95.6 93.8 90.4 90.7 91.8 90.8 91.5 91.8</cell></row><row><cell>Song et al. [25]</cell><cell>-</cell><cell>-</cell><cell>98.0 97.3 95.1 94.7 97.1 97.1 96.9 96.5</cell></row><row><cell>Luo et al. [20]</cell><cell>70.98</cell><cell>25</cell><cell>98.9 98.6 96.6 96.6 98.2 98.2 97.5 97.7</cell></row><row><cell>DKD(SmallCPM)</cell><cell>9.96</cell><cell>12</cell><cell>98.4 97.3 96.1 95.5 97.0 97.3 96.6 96.8</cell></row><row><cell>DKD(ResNet50)</cell><cell>8.65</cell><cell>11</cell><cell>98.8 98.7 96.8 97.0 98.2 98.1 97.2 97.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Normalized by Torso Size</cell></row><row><cell>Luo et al. [20]</cell><cell>70.98</cell><cell>25</cell><cell>96.0 93.6 92.4 91.1 88.3 94.2 93.5 92.6</cell></row><row><cell>DKD(SmallCPM)</cell><cell>9.96</cell><cell>12</cell><cell>96.0 93.5 92.0 90.6 87.8 94.0 93.1 92.4</cell></row><row><cell>DKD(ResNet50)</cell><cell>8.65</cell><cell>11</cell><cell>96.6 93.7 92.9 91.2 88.8 94.3 93.7 92.9</cell></row></table><note>Runtime (ms/per image)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison with state-of-the-arts on Sub-JHMDB dataset. Normalized by Torso Size Luo et al. [20] 92.7 75.6 66.8 64.8 78.0 73.1 73.3 73.6 DKD(ResNet50) 94.4 78.9 69.8 67.6 81.8 79.0 78.8 77.4</figDesc><table><row><cell>Methods</cell><cell cols="2">Head Sho. Elb. Wri. Hip Knee Ank. PCK</cell></row><row><cell></cell><cell>Normalized by Person Size</cell><cell></cell></row><row><cell>Park et al. [23]</cell><cell>79.0 60.3 28.7 16.0 74.8 59.2</cell><cell>49.3 52.5</cell></row><row><cell>Nie et al. [22]</cell><cell>80.3 63.5 32.5 21.6 76.3 62.7</cell><cell>53.1 55.7</cell></row><row><cell>Iqal et al. [14]</cell><cell>90.3 76.9 59.3 55.0 85.9 76.4</cell><cell>73.0 73.8</cell></row><row><cell>Song et al. [25]</cell><cell>97.1 95.7 87.5 81.6 98.0 92.7</cell><cell>89.8 92.1</cell></row><row><cell>Luo et al. [20]</cell><cell>98.2 96.5 89.6 86.0 98.7 95.6</cell><cell>90.9 93.6</cell></row><row><cell cols="2">DKD(ResNet50) 98.3 96.6 90.4 87.1 99.1 96.0</cell><cell>92.9 94.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We reproduce the results of<ref type="bibr" target="#b19">[20]</ref> with PCK normalized by torso size via running the codes released by the authors on the repo: https://github.com/lawy623/LSTM Pose Machines. The running time is counted on GPU GTX 1080ti for both<ref type="bibr" target="#b19">[20]</ref> and our model.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>Jiashi Feng was partially supported by NUS IDS R-263-000-C67-646, ECRA R-263-000-C87-133 and MOE Tier-II R-263-000-D17-112.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Began: boundary equilibrium generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10717</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning feed-forward one-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jo?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adversarial posenet: A structure-aware convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self adversarial training for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Jung</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jui-Ting</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Structured feature learning for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human behavior analysis in video surveillance: A social signal processing perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramya</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Del Bue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="86" to="97" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Detect-and-track: Efficient pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Chained predictions using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Emrah Tasli, and Marten den Uyl. Human pose estimation in space and time using 3d cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agne</forename><surname>Grinciunaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amogh</forename><surname>Gudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pose for action-action for pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">tional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>3d convolu</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning effective human pose estimation from inaccurate annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Interactive control of avatars animated with human motion data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jehee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxiang</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">K</forename><surname>Reitsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><forename type="middle">S</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pollard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="491" to="500" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Augmented reality with human body interaction based on monocular 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung</forename><surname>Huei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Wen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACIVS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lstm pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mutual learning to adapt for joint human parsing and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint action recognition and pose estimation from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">N-best maximal decoders for part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Thin-slicing network: A deep structured model for pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">End-to-end representation learning for correlation filter based tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Shi Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Articulated human detection with flexible mixtures of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2878" to="2890" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">From actemes to action: A strongly-supervised representation for detailed action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
