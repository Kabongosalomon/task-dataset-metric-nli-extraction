<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast Weakly Supervised Action Segmentation Using Mutual Consistency</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Fast Weakly Supervised Action Segmentation Using Mutual Consistency</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Video understanding</term>
					<term>Action segmentation</term>
					<term>Weakly supervised learning !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Action segmentation is the task of predicting the actions for each frame of a video. As obtaining the full annotation of videos for action segmentation is expensive, weakly supervised approaches that can learn only from transcripts are appealing. In this paper, we propose a novel end-to-end approach for weakly supervised action segmentation based on a two-branch neural network. The two branches of our network predict two redundant but different representations for action segmentation and we propose a novel mutual consistency (MuCon) loss that enforces the consistency of the two redundant representations. Using the MuCon loss together with a loss for transcript prediction, our proposed approach achieves the accuracy of state-of-the-art approaches while being 14 times faster to train and 20 times faster during inference. The MuCon loss proves beneficial even in the fully supervised setting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>T HE production, availability, and consumption of video data is increasing every day at an exponential rate. With such growth comes the need to analyze, monitor, annotate, and learn from this huge amount of often publicly available data. The research community has therefore shown great interest in video data and approaches for action recognition on trimmed video clips have shown remarkable results in recent years <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Although these results on trimmed video clips are important, in many realworld scenarios we are often faced with untrimmed videos that contain multiple actions with different lengths <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>.</p><p>Since annotating exact temporal boundaries of actions in long videos is cumbersome and costly, action segmentation approaches that can utilize weaker types of supervision are important. In particular ordered lists of actions such as spoon powder -pour milk -stir milk, which are termed transcripts and describe the order of the actions as they occur in a training video but not when they occur, are a popular type of weak supervision <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. To learn from transcripts, previous approaches try to align the transcripts to the training videos, i.e., they infer frame-wise labels of each training video based on the provided transcripts. This alignment is then used as the pseudo ground truth for training. For the transcript alignment, the Viterbi algorithm is commonly used. It takes the estimated frame-wise class probabilities of a video and finds the best sequence of frame labels that does not violate the action order of the given transcript. While <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref> perform the alignment after each epoch for all training videos, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref> apply it at each iteration to a single video.</p><p>During inference, previous approaches except ISBA <ref type="bibr" target="#b14">[15]</ref> rely on segmentation through alignment. This means that Manuscript received <ref type="bibr">April 19, 2005</ref>; revised August 26, 2015.  <ref type="bibr" target="#b17">[18]</ref>. The average MoF is calculated over 5 training/inference iterations. The proposed approach (MuCon-full) provides the best trade-off between inference time and accuracy. More details are provided in Section 5.5.</p><p>given an unseen video from the test set, the methods search over all transcripts of the training set and take the transcript that best aligns with the test video. This is highly undesirable since the inference time increases in this case as the number of different transcripts in the training set increases. As a result, these approaches are inefficient as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. In contrast to segmentation through alignment approaches, ISBA is fast, but it does not achieve the accuracy of state-of-the-art approaches. This means that at the moment one can only choose between accurate or fast approaches, but one cannot have both.</p><p>In this work, we therefore fill this gap and propose an approach that is nearly as fast as ISBA and nearly as accurate as the state-of-the-art as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Instead of optimizing over all possible transcripts during inference, our approach directly predicts the transcript for a video as well as the frame-wise class probabilities using two branches as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. This means that the inference time does not depend on the number of transcripts used for training and that the best label sequence can be directly predicted from the estimated transcript and frame-arXiv:1904.03116v4 [cs.CV] 10 Jun 2021 wise class probabilities.</p><p>In addition, the proposed approach has also two major advantages during training. First, the branch, which predicts the transcripts, is directly trained with the type of supervision that is provided, namely transcripts. Second, the branch not only predicts the transcripts, but also the length of each action in the transcript. This means that the branch predicts already a full segmentation of the video without the need for the Viterbi algorithm, which also reduces the training time. Nevertheless, we need an additional loss to learn the action lengths. In contrast to previous works, we do not generate pseudo ground truth with hard labels during training, but we propose a novel differentiable mutual consistency (MuCon) loss that enforces that the representations estimated by the two branches are mutually consistent and match each other. Furthermore, we show that the approach can be trained using weak supervision, full supervision, or a mixture of the two, where only few videos are fully annotated.</p><p>We provide a thorough analysis of the proposed approach including a detailed statistical analysis. We show that the proposed network with the mutual consistency loss achieves an accuracy that is either on par or better than existing approaches. At the same time, it is 20 times faster during inference compared to the most accurate approach <ref type="bibr" target="#b16">[17]</ref> as shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Action segmentation in untrimmed videos with various levels of supervision has been studied in several works. We thus first describe approaches that use full supervision and then approaches that use weaker levels of supervision. Finally, we discuss sequence to sequence learning methods that are related to our work.</p><p>Fully Supervised Action Segmentation. Action segmentation in videos has already been tackled in many works <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Early approaches for action segmentation rely on multi-scale sliding window processing <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> or Markov models on top of frame-classifiers <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b22">[23]</ref>. These approaches are typically slow at inference time and do not provide strong temporal models. More recent approaches for fully supervised action segmentation aim to capture long-range temporal dependencies using temporal convolutions <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Some works for action segmentation focus on segmenting egocentric videos <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, including also additional sensors <ref type="bibr" target="#b18">[19]</ref>.</p><p>Weakly Supervised Action Segmentation. Weakly supervised action segmentation has gained increased attention over the last years. Bojanowski et al. <ref type="bibr" target="#b26">[27]</ref> introduced the Hollywood extended dataset and proposed a method based on discriminative clustering for the task of action alignment. Huang et al. <ref type="bibr" target="#b11">[12]</ref> have proposed to use an extended version of the connectionist temporal classification loss where they take the similarity of frames of the same action into account. Inspired by methods that are used in speech processing, Kuehne et al. <ref type="bibr" target="#b27">[28]</ref> proposed an approach based on hidden Markov models and use a Gaussian mixture model as observation model. It iteratively generates a pseudo ground truth for videos at the start of each epoch and refines the pseudo ground truth at the end of the epoch. The approaches <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b28">[29]</ref> build on this work by replacing the Gaussian mixture model by a recurrent neural network for short-range temporal modeling, but the methods keep the iterative nature that involves the generation of pseudo ground truth at each epoch.</p><p>While these methods rely on iterative approaches with two-step optimization, which does not allow for direct endto-end training, Richard et al. <ref type="bibr" target="#b13">[14]</ref> introduced the Neural Network Viterbi (NNV) method. This method generates the pseudo ground truth for each iteration instead of each epoch, but it needs an additional buffer which increases the training time. They use furthermore a global length model for actions, which is updated heuristically during training. Recently, Li et al. <ref type="bibr" target="#b16">[17]</ref> introduced an extension of NNV that outperformed existing works in terms of accuracy on standard benchmarks. They introduced a new constrained discriminative loss that discriminates between the energy of valid and invalid segmentations of the training videos. This results in a large improvement in accuracy compared to NNV.</p><p>Sequence to Sequence Learning. There has been a lot of work in sequence to sequence learning, mostly in natural language processing <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. However, our problem is different in one major aspect: the size mismatch between the video features and transcripts is often large. In the Breakfast dataset <ref type="bibr" target="#b17">[18]</ref>, for example, the average input video length is around 2100 frames and the longest video has around 9700 frames, while the average transcript length is 6.8 and the longest is 25. Because of this issue, we have specially designed our network to be able to temporally model such long sequences. Wei et al. <ref type="bibr" target="#b33">[34]</ref> proposed a sequence to sequence model for detecting temporal segments of interest, but not the action classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">WEAKLY SUPERVISED ACTION SEGMENTATION</head><p>Action segmentation is the task of predicting the action class for each frame of a video. More formally, given an input sequence of T D-dimensional frame-level features X 1:T = [x 1 , . . . , x T ], x t ? R D , the goal is to predict the output sequence of frame-level action labels? 1:T = [? 1 , . . . ,? T ], where? t ? C and C is the set of action classes. The framelevel action labels? 1:T can also be represented as an ordered sequence of M segments? 1:M where each segment? m is defined as an action label? m ? C and its corresponding length? m ? R + .</p><p>For fully supervised action segmentation, the target labels for every frame? t are known during training. This means that the target lengths? m are known. However, in weakly supervised action segmentation, the only supervisory signal is the ordered sequence of actions? 1:M = [? 1 , . . . ,? m ], often called video transcript, while the action lengthsL 1:M = [? 1 , . . . ,? M ] are unknown.</p><p>In order to estimate L 1:M , we exploit the fact that the two unknown target representations? 1:T and? 1:M for action segmentation are redundant and it is possible to generate one given the other. The main idea is therefore to predict both representations Y 1:T and S 1:M by different branches of the network such that they can supervise each other. In the ideal case, both predicted representations converge to the same solution during training. In order to train the model, Our proposed network consists of three subnetworks (gray). The temporal backbone ft embeds the input features in the hidden representation Z which is used for two branches. While the frame classification branch fc predicts framewise class probabilities Y for action segmentation, the segment generation branch fs predicts the segment representation S for action segmentation. We train our network using two loss functions. While the transcript prediction loss Lt enforces that the predicted transcript A matches the ground-truth transcript?, our proposed mutual consistency (MuCon) loss L? enforces that the two representations are consistent. however, we have to face the challenging problem that the mapping from one representation to the other needs to be differentiable, such that the loss that measures the mutual consistency of both representations is differentiable with respect to the predicted frame-wise class probabilities Y 1:T and the predicted segment lengths L 1:M . Since the transcript A 1:M is given, it does not need to be differentiable with respect to A 1:M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROPOSED METHOD</head><p>During training, we have for each video the input video features X 1:T and its corresponding transcript? 1:M . Since only weak labels in form of the transcripts are provided for training, we propose a) to use the weak labels directly for training a transcript prediction subnetwork and b) exploit the two representations discussed in Section 3 as mutual supervision.</p><p>The proposed network is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. The network consists of two branches that share the same backbone f t (X), which is a temporal convolutional network that maps the input video features X ? R T ?D to a latent video representation Z ? R T ?D . The backbone architecture is described in Section 4.1. After the shared backbone, the network has two branches. The top branch f c (Z) is a standard frame classification branch that estimates the class probabilities of each frame Y ? R T ?N , where N is the number of classes. The branch will be described in Section 4.2. The novelty of the network is the lower branch and the mutual consistency loss L ? , which will be described in Sections 4.3 and 4.4, respectively.</p><p>In contrast to previous works that commonly use a network to predict Y and Viterbi decoding on top of it for computing the loss based on the given transcript?, we use a second branch that predicts the segment representation S, i.e. A and L. This has two advantages during training. First, we can compare the predicted transcript A and the groundtruth transcript? for each video directly, i.e., the network is directly trained with the type of supervision that is provided. The corresponding loss is denoted by L t . Second, the network has two branches that supervise each other and we do not need an additional Viterbi decoding step. Since the two branches predict different representations, the novel mutual consistency (MuCon) loss L ? requires a differentiable mask generator such that the loss is differentiable with respect to Y and L; otherwise we could not train the network.</p><p>After training, we need to infer the action class for each frame of an unseen test video. In case of action segmentation, no additional transcripts are given for the test videos. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, we can use three different approaches for inference, which are denoted by MuCon-Y, MuCon-S, and MuCon-full. In case of MuCon-Y and MuCon-S, we use the predicted Y or S representation, respectively. While these settings are as fast but more accurate than ISBA <ref type="bibr" target="#b14">[15]</ref>, the accuracy is lower compared to the state-of-the-art. For the model MuCon-full, we use therefore the predictions of both branches. To this end, we use the predicted framewise class probabilities Y and the predicted transcript A to find the best sequence of action labels using Viterbi decoding as described in Section 4.8. Note that the approach is still much faster than other methods except of ISBA since we do not need to optimize over all possible transcripts. We now provide more details for each part of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Temporal Backbone</head><p>The temporal backbone f t (X), which is a temporal convolutional network <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b34">[35]</ref>, outputs the latent video representation Z ? R T ?D of the input video features X ? R T ?D . This hidden representation has a smaller temporal resolution due to temporal pooling. Similar to <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b23">[24]</ref>, our temporal backbone consists of a set of 1-dimensional dilated convolutional layers with increasing dilation sizes. More specifically, we first apply a 1-d convolution with kernel size 1 to perform dimensionality reduction. Then a set of 11 layers with increasing dilation rates are applied, followed by a single 1-d convolution with kernel size 1 that generates the output. The amount of dilation for each layer is 2 i . We perform temporal max pooling with a kernel size of Given the hidden video representation Z, we use a sequence to sequence network with attention. The transcript prediction loss Lt compares the predicted action class probabilities am with the ground-truth action label?m. (b) Mutual consistency loss L?. Given the predicted lengths L, a set of masks wm are generated using differentiable sampling by the mask generation module (MG). The loss then measures for each segment the consistency of the estimated framewise class probabilities Y with the ground-truth action?m.</p><p>2 after the layers 1, 2, 4, and 8. Additional details are given in Section 5.2. The design is similar to a single stage TCN <ref type="bibr" target="#b8">[9]</ref>, but it includes additional pooling layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Frame Classification Branch</head><p>The classification branch takes the shared latent video representation Z ? R T ?D as input and predicts the class probabilities Y ? R T ?N where N is the number of actions in the dataset. Due to the temporal pooling of the temporal backbone, Z has a lower temporal resolution compared to the input features. To compensate for this, we first upsample Z using nearest-neighbor interpolation to the desired temporal size T . Then we use a single 1-d convolution with kernel size 1, which takes as input the upsampled Z ? R T ?D and outputs Y ? R T ?N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Segment Generation Branch</head><p>The second branch on top of Z is the segment generation subnetwork which predicts the segments S. Each segment s m consists of predicted action probabilities a m and the estimated relative log length m of that segment. By predicting the log length, we implicitly enforce that the segment lengths are positive. We will discuss in Section 4.5.1 how the relative log length is mapped to the absolute length. The subnetwork and the transcript prediction loss L t are illustrated in <ref type="figure" target="#fig_2">Figure 3a</ref>.</p><p>We employ a conventional sequence to sequence network with attention <ref type="bibr" target="#b30">[31]</ref>. Given the hidden video representation Z, we use a bidirectional LSTM encoder to encode it. Our decoder is an LSTM recurrent neural network with MLP attention. Although these networks on their own struggle to learn a temporal model for long input sequences <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, the temporal backbone, which encodes temporal relations at higher resolution, makes it easier for the segment generation subnetwork to learn temporal dependencies as shown by our ablation experiments.</p><p>As illustrated in <ref type="figure" target="#fig_2">Figure 3a</ref>, the decoding starts with the starting symbol? start . At each step of the decoding process, the ground truth action label? m?1 from the previous step is added to the encoded input sequence by concatenating it to the result of the attention. The concatenated vector is then given as input to the LSTM decoder cell, which estimates probability scores a m using a fully connected MLP with two layers. Given these probability scores and the ground truth action label? m , we compute the action prediction loss L tm per segment using cross-entropy. The final transcript prediction loss is defined as the sum of the action prediction losses, i.e., L t = M +1 m=1 L tm . Note that we have M + 1 terms since we add the end symbol? end to the ground-truth transcripts, which needs to be predicted by the network as well. During training we use teacher forcing <ref type="bibr" target="#b37">[38]</ref>, i.e., we do not sample from the predicted action probabilities to feed the decoder, but we use the ground truth action labels? m .</p><p>Given the probability scores a m and the hidden state of the decoder, we use another fully connected MLP with two layers to predict m , which corresponds to the logarithm of the relative length of the segment. Notice that the parameters of this MLP are not updated based on the transcript prediction loss, but based on the mutual consistency loss L ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Mutual Consistency Loss</head><p>Using the frame classification and the segment generation branches, two representations Y and S for the action segmentation are produced. However, so far we have only defined a loss for the predicted transcript A of the segment representation S, which we compare directly with the ground-truth transcript?, but not for the predicted lengths of the actions L and the framewise class probabilities Y . We therefore propose the mutual consistency (MuCon) loss, which enforces that the two representations match each other and are mutually consistent. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the mutual consistency loss takes the ground-truth transcript A, the predicted segment lengths L, and framewise class probabilities Y as input. Since the loss is used to train both branches, it needs to be differentiable with respect to L and Y .</p><p>In principle, there are two choices: either we map Y to S or S to Y . The first approach, however, is not practical since it would need to detect consistent segments within Y . While this could be done using Viterbi decoding, it would make our network as expensive as segmentation through alignment approaches. Our goal, however, is to propose an approach that is fast and accurate. An explicit mapping from S to Y followed by the computation of a framewise loss, however, is also inefficient. We therefore combine the mapping and loss computation. Furthermore, we use? instead of A since we have already a loss that enforces that A is close to?. Like teacher forcing, it also stabilizes the training since it guides the mutual consistency loss by? from the beginning of the training when A is still noisy. In our experiments, we show that this reduces the standard deviation over different runs.</p><p>The computation of the mutual consistency loss is illustrated in <ref type="figure" target="#fig_2">Figure 3b</ref>. We start with the estimated relative log length m for each segment s m . The relative log length m is then converted into the absolute length m and for each segment we compute its absolute starting position p m within the video. This step is described in Section 4.5.1. Given m and p m , for each segment we generate a mask w m using the differentiable mask generation module, which is described in Section 4.5. As mentioned before, for each segment we use the ground-truth action label? m instead of the estimated class probabilities.</p><p>Finally, we can compare the consistency of the predicted frame-wise class probabilities Y with each segment defined by the mask w m and the label? m . To this end, we first compute the average of Y for each segment based on the mask w m :</p><formula xml:id="formula_0">g(Y, w m ) = T t=1 y t w m [t] m (1) where g(Y, w m ) ? R N , w m [t]</formula><p>is the value of the mask at frame t, and m is the absolute length of the segment. Then, we compute the cross-entropy for each segment:</p><formula xml:id="formula_1">L ?m (Y, w m ,? m ) = ? log e g(Y,wm)[?m] N n=1 e g(Y,wm)[n]<label>(2)</label></formula><p>where we use the softmax function to normalize the class probabilities per segment and the ground-truth label? m . Since we normalize per segment using the softmax function, we use the estimates of Y before the softmax layer of the frame classification branch in <ref type="bibr" target="#b0">(1)</ref>. The final mutual consistency loss L ? is defined as the sum of all segment losses: The mutual consistency loss <ref type="formula" target="#formula_2">(3)</ref> is a differentiable function of the masks w m and the frame-wise class probabilities Y . Due to the definition of the differentiable mask generation, which we will describe in the following section, the masks are differentiable with respect to the estimated segment lengths L. This means that the gradients of the mutual consistency loss are backpropagated through both branches as it is required to train the network.</p><formula xml:id="formula_2">L ? = M m=1 L ?m (Y, w m ,? m ).<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Differentiable Mask Generation</head><p>In order to compute the mutual consistency loss, we need to generate the masks w m that act like gating functions that only allow information from the predicted temporal regions to pass through. The masks w m are functions of the predicted absolute length m and predicted starting position p m of each segment such that</p><formula xml:id="formula_3">w m [t] 1 p m ? t ? p m + m 0 otherwise , t ? [1 . . . T ]. (4)</formula><p>Examples of generated masks are shown in <ref type="figure" target="#fig_3">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Localization</head><p>In order to obtain the predicted absolute length m and the predicted starting position p m for each segment, we first calculate the absolute length values L 1:M = ( 1 , . . . , M ) for a video with T frames such that M m=1 m = T , i.e., the absolute lengths sum up to be equal to the length of the video. Having the absolute length m of each segment, we can also compute the absolute starting position p m for each segment:</p><formula xml:id="formula_4">m = T e m M k=1 e k , p 1 = 0, p m = m?1 k=1 k .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Temporal Transformation</head><p>For generating the masks, we transform a reference template tensor U ? [0, 1] J to w m ? [0, 1] T where J is a canonical value equal to 100. The reference template tensor can be of any shape and we evaluate three shapes, namely box, bell, and trapezoid which are depicted in <ref type="figure" target="#fig_3">Figure 4</ref>. By adjusting the absolute lengths m and starting positions p m , it is also possible to introduce an overlap for the masks.</p><p>We transform U to w m by scaling and translating it using m and p m , respectively. Therefore, we use a 1D affine transformation matrix T ? such that</p><formula xml:id="formula_5">i u [t] = T ? (t) = [? 0 ? 1 ] t 1 ?t ? [1, . . . , T ]<label>(6)</label></formula><p>where i u [t] are the sampling points over U . As a result of the affine transformation <ref type="formula" target="#formula_5">(6)</ref>, the element indices of the sampling points in the template i u [t] can be outside the valid range of [1, . . . , J]. In such cases, the indices will be ignored in the sampling process <ref type="bibr" target="#b7">(8)</ref>. The affine transformation parameters are</p><formula xml:id="formula_6">? 0 = J m , ? 1 = ?Jp m m<label>(7)</label></formula><p>where ? 0 scales the reference template U to the estimated length m and ? 1 translates it to the estimated position p m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Temporal Sampling</head><p>To perform the aforementioned temporal transformation, we should sample from U using the sampling points i u [t] and produce the sampled mask w m . Each index i u [t] refers to the element in U where a sampling kernel must be applied to get the value at the corresponding element w m [t] in the output mask. Similar to <ref type="bibr" target="#b38">[39]</ref>, we perform this operation as follows</p><formula xml:id="formula_7">w m [t] = J j=1 U [j]?(i u [t] ? j) ?t ? [1, . . . , T ]<label>(8)</label></formula><p>where ? is the sampling kernel. Since we use a linear kernel, it can be written as </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.4">Backpropagation</head><p>To be able to backpropagate through the generated masks w m , we define the gradients with respect to the sampling indices i u [t] as</p><formula xml:id="formula_8">?w m [t] ?i u [t] = J j=1 U j ? ? ? ? ? 0 |i u [t] ? j| ? 1 1 j ? 1 &lt; i u [t] ? j ?1 j &lt; i u [t] &lt; j + 1 .<label>(10)</label></formula><p>Since the sampling indices i u [t] are a function of the predicted lengths m , the loss gradients are backpropagated to the segment generation branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Regularization</head><p>To prevent degenerate solutions, i.e., solutions where the lengths of some segments are large and the length of the other segments are almost zero, we add a regularization term for the predicted relative log lengths L:</p><formula xml:id="formula_9">L = M m=1 max(0, ? m ? w) + max(0, m ? w).<label>(11)</label></formula><p>The proposed length regularizer adds a penalty if m &lt; ?w or m &gt; w. Note that the relative log lengths can be negative and are later converted into the absolute length as described in Section 4.5.1.</p><p>We also use the smoothing loss introduced in <ref type="bibr" target="#b8">[9]</ref> for the frame classification branch:</p><formula xml:id="formula_10">L s = 1 T N t,n? 2 t,n ,<label>(12)</label></formula><p>? t,n = min(?, | log y t,n ? log y t?1,n |),</p><p>with ? = 4 as in <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Fully Supervised and Mixed Training</head><p>Although our approach is designed for weakly supervised action segmentation, we can easily adapt it to a setting where all or some videos of the training set are fully annotated, i.e., they are annotated not by the transcript? but by the framewise labels? .</p><p>In this case, we have two additional losses for our network shown in <ref type="figure" target="#fig_1">Figure 2</ref>. We use the cross-entropy loss for the frame classification branch to compare the predicted class probabilities Y with the framewise labels? , and we use the mean squared error loss for the segment generation branch to compare the estimated relative log lengths L with the ground truth segment lengths. To this end, we convert the absolute ground-truth lengths into relative log lengths.</p><p>Although the mutual consistency loss is only necessary in case of weakly supervised learning or in a setting where only a subset of the videos is fully annotated, we show that using the mutual consistency loss also improves the accuracy when the network is trained in a fully supervised way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Inference</head><p>As discussed at the beginning of Section 4, we can either use the frame classification branch, which predicts the framewise class probabilities Y , or the segment generation branch, which predicts the segments S. In the later case, we start with the start symbol a start and the decoder generates new segments until the special end symbol a end is predicted as illustrated in <ref type="figure" target="#fig_2">Figure 3a</ref>. The estimated relative log segment lengths are then converted into absolute lengths as described in Section 4.5.1. The two approaches for inference are denoted by MuCon-Y and MuCon-S, respectively. However, as it is shown in <ref type="figure" target="#fig_0">Figure 1</ref>, we achieve the highest accuracy at a small increase in inference time if we use the predictions of both branches. We denote this approach by MuCon-full. For simplicity, we use L instead of L to denote the estimated absolute lengths of the segments in this section. In order to fuse both predictions, we keep the inferred transcript A, but reestimate the lengths of the segments L using Y :</p><formula xml:id="formula_12">L * = argmax L p(L|Y, A, L).<label>(14)</label></formula><p>Similar to <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b39">[40]</ref>, we can factorize the term p(L|Y, A, L) yielding</p><formula xml:id="formula_13">L * = argmax L T t=1 y t [a ?(t,L) ] M m=1 P m (? m )<label>(15)</label></formula><p>where P m (? m ) denotes a Poisson distribution with expected mean m , which corresponds to the absolute segment length that has been estimated by the segment generation branch. Depending onL, the segment number changes for a frame t and it is denoted by ?(t, L). Although L * is obtained by dynamic programming as described in <ref type="bibr" target="#b13">[14]</ref>, we do not need to optimize over all possible transcripts as in <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>. While <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref> align each transcript of the training set to the test video and take the training transcript that best aligns to the test video, our approach infers the transcript A directly from the test video and only aligns A to the test video. MuCon-full is therefore still much faster than <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref> as shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We evaluate our approach for two tasks, namely action segmentation as described in Section 3 and action alignment. In contrast to action segmentation, the transcripts are also given for the test videos in case of action alignment. Besides of the weakly supervised setting, we also evaluate the approach when it is trained fully supervised or in a mixed setting where some videos are fully annotated and the other videos are only weakly annotated by transcripts. Before we evaluate the approach, we discuss the evaluation protocols and further implementation details 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation Protocols and Datasets</head><p>We evaluate our method on two popular datasets, the Breakfast dataset <ref type="bibr" target="#b17">[18]</ref> and the Hollywood extended dataset <ref type="bibr" target="#b26">[27]</ref>. The Breakfast dataset contains more than 1.7k videos of different cooking activities. The videos contain 10 different types of breakfast activities such as prepare cereal or prepare coffee which consists of 48 different fine-grained actions. In our experiments, we follow the 4 train/test splits provided with the dataset and report the average. The Hollywood extended dataset contains 937 video sequences taken from Hollywood movies. The videos contain 16 different action classes. We follow the train/test split strategy of <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>. The main performance metric used for the action segmentation task is the mean over frames (MoF) accuracy <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. We also report the F1 score that has been used by fully supervised approaches <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b23">[24]</ref>. For action segmentation, we also directly evaluate the performance of the predicted transcripts, which is measured by the matching score or edit distance <ref type="bibr" target="#b40">[41]</ref>. For action alignment, we use the intersection over detection (IoD) metric as in <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b26">[27]</ref>. The metrics are described in the Appendix A. If not otherwise specified, we report the average and standard deviation over 5 runs with different random seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details</head><p>We train the entire network end-to-end after initializing it with Gaussian random weights. In each iteration, we use only a single video, i.e., the batch size is 1. We use a single group norm <ref type="bibr" target="#b41">[42]</ref> layer with 32 groups after the final layer of the temporal backbone f t . The dimensionality of the shared latent video representation Z is D = 128. Unless otherwise stated, we apply temporal max pooling with kernel size 2 1. Source code is available at: github.com/yassersouri/MuCon after the convolutional layers 1, 2, 4, and 8 for all experiments on the Breakfast dataset. For the experiments on the Hollywood Extended dataset, we omit the last temporal max pooling after the convolutional layer 8 since the videos in the Hollywood extended dataset are relatively short. The size of the hidden states of the bidirectional LSTM encoder and the LSTM decoder in our segment generation module are set to 128. We also employ an input embedding for the LSTM decoder of size 128 with 0.25 dropout.</p><p>In the weakly supervised setting, the training loss is defined as L = L ? + L t + ?L + ?L s . We use ? = ? = 0.1 unless otherwise specified. Since most of the frames in the Hollywood extended dataset are annotated by background, which increases the possibility of degenerate solutions, we set ? to 1 for the Hollywood extended dataset. We use w = 2 in (11). The initial learning rate is set to 0.01 and is lowered by a factor of 10 after 70 epochs for the Breakfast dataset and after 60 epochs for the Hollywood extended dataset. We train our network for 150 epochs for all weakly supervised experiments. In the fully supervised and mixed supervision experiments, we train for 110 epochs since convergence is faster with full supervision.</p><p>As input features for the Breakfast and Hollywood extended datasets, we use RGB+flow I3D <ref type="bibr" target="#b1">[2]</ref> features extracted from a network that was pretrained on the Kinetics400 dataset <ref type="bibr" target="#b42">[43]</ref>. These features have been previously used for fully supervised approaches <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b23">[24]</ref>. A recent study found that current weakly supervised approaches perform better with IDT features than I3D features <ref type="bibr" target="#b43">[44]</ref>. We therefore report the results for the features where the corresponding methods perform best, i.e., I3D for <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b23">[24]</ref> and IDT for <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Experiments</head><p>In this section, we quantitatively examine different components in our method. For each metric, We report the average and standard deviation over 5 runs on split 1 of the Breakfast dataset <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Transcript Prediction</head><p>We first analyze how well the proposed approach predicts the transcripts, which is measured by the matching score. If we only use the segment generation branch, the network achieves a matching score of 0.729 as shown in row 4 of <ref type="table" target="#tab_2">Table 1a</ref>. If we add the frame classification branch and the mutual consistency loss L ? , the matching score increases to 0.774. This shows that the mutual consistency loss also improves the transcript prediction.</p><p>In the first four rows of <ref type="table" target="#tab_2">Table 1a</ref>, we furthermore evaluate the impact of the dilation factors for the temporal backbone f t . If we replace the temporal backbone by a single 1-d convolution, the matching score decreases from 0.729 to 0.691. If we use as in the proposed network 11 layers with 1-d convolutions but with fixed dilation factor of 1, the matching score does not significantly increase since it does not substantially increase the receptive field. Only when we linearly or exponentially increase the dilation factors for each layer, we observe an improvement of the matching score. (b) Impact of mask generation: As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, different shapes with or without overlap can be used for the mask generation. We report the MoF accuracy, matching score, and F1 score.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regularizer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Mask Generation Settings</head><p>As described in Section 4.5, different shapes of mask templates can be used for the mutual consistency loss. We evaluated three shapes, namely box, bell, and trapezoid, and added an optional overlap of 10% as depicted in <ref type="figure" target="#fig_3">Figure 4</ref>. The results reported in <ref type="table" target="#tab_2">Table 1b</ref> show that the shape of the mask has little impact on the quality of the predicted transcript, but that the bell shape has the worst performance. In general, having hard boundaries for the masks without any overlap performs best. For all other experiments, we therefore use the box shape without any overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Effect of Regularizers</head><p>As described in Section 4.6, we also use two additional regularizers during training, namely the length loss L and the smoothing loss L s . We evaluate the impact of each term in <ref type="table" target="#tab_2">Table 1c</ref> by removing L , L s , or both. Without any regularizer, the MoF accuracy decreases from 49.0 to 47.4 and F1@50 from 40.2 to 39.7. While adding only the smoothing loss does not result in an improvement, the length regularizer and the combination of both improves MoF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Different Variants for Inference</head><p>As described in Section 4.8, we have three options for inference. We can use the frame classification branch, the segment generation branch, or both. The three approaches are denoted by MuCon-Y, MuCon-S, and MuCon-full, respectively. The results in <ref type="table" target="#tab_2">Table 1d</ref> show that fusing the two branches improves the MoF accuracy compared to MuCon-S by more than 5% and F1@50 by more than 6% at a small increase in inference time as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Since MuCon-Y predicts only frame-wise labels and not segments, the F1 scores are very low for MuCon-Y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.5">Effect of Teacher Forcing</head><p>As described in Sections 4.3 and 4.4, we use the ground truth transcript while predicting the transcript and calculating the MuCon loss in order to stabilize the training at the beginning. In <ref type="table" target="#tab_2">Table 1e</ref>, we evaluate the impact of teacher forcing where we also include a setting that uses teacher forcing only for the first 70 epochs of training. The results show that teacher forcing does not impact the average accuracy but it reduces the standard deviation, which is an indicator of a stable training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Statistical Analysis</head><p>As the standard deviations in <ref type="table" target="#tab_2">Table 1</ref> show, the results vary for different random seeds. In order to provide a thorough comparison with other weakly supervised action segmentation approaches, we use a statistical test to measure if the differences in accuracy are significant or not. To this  <ref type="table">TABLE 2</ref> Comparison of accuracy, training, and inference time. We report the average and standard deviation of MoF and matching score for 5 runs on the entire Breakfast dataset <ref type="bibr" target="#b17">[18]</ref>. For ISBA and MuCon-Y, the matching score is not calculated as they predict frame-wise labels and not the transcript. Training and inference time is measured as wall clock time. The training time is measured for training on split 1 of the Breakfast dataset <ref type="bibr" target="#b17">[18]</ref>. The inference time is measured as the average inference time for a video from the test set of split 1.   <ref type="table">Table 2</ref>. Note that the test is symmetrical. If the p-value is smaller than 0.05 the difference is considered as significant (indicated with ); otherwise the difference is not considered as significant (indicated with ).</p><p>end, we performed five training/inference iterations on the entire Breakfast dataset. Besides of our approach (MuCon), we included CDFL <ref type="bibr" target="#b16">[17]</ref>, NNV <ref type="bibr" target="#b13">[14]</ref>, and ISBA <ref type="bibr" target="#b14">[15]</ref> for which the official open-source implementation is available. We use the hyperparameters as they were provided by the authors for this dataset. We use the Friedman statistical test <ref type="bibr" target="#b44">[45]</ref> with the dataset split as the blocking factor. We selected the Friedman test as it does not make any restrictive assumption on the underlying distribution of the data. While we report the average and standard deviation of the MoF accuracy and matching score in <ref type="table">Table 2</ref>, the results of the Friedman test for the MoF accuracy and the matching score are shown in Tables 3a and 3b, respectively. We observe that MuCon and CDFL are both significantly better than ISBA and NNV. While the average MoF accuracy and the average matching score of MuCon are higher than the ones of CDFL, the difference is only statistically significant for the matching score. This shows that MuCon is on par with CDFL in terms of MoF accuracy, but significantly better in terms of matching score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Training and Inference Time</head><p>Besides of the accuracy, we also compare the training and inference time of our approach with CDFL <ref type="bibr" target="#b16">[17]</ref>, NNV <ref type="bibr" target="#b13">[14]</ref>, and ISBA <ref type="bibr" target="#b14">[15]</ref>. For a fair comparison, we always used the same hardware. For training, we used a machine with an Nvidia GeForce GTX 1080Ti GPU, 188 GB of RAM, and an Intel(R) Xeon(R) Gold 5120 (2.20GHz) CPU. For testing, we used a machine with an Nvidia GeForce GTX Titan X GPU, 32 GB of RAM, and an Intel(R) Core(TM) i7-4930K (3.40GHz) CPU. We only calculate the wall time for training and deactivated for all methods any unnecessary operations like saving intermediate results. Since we used pre-computed features for all experiments, the time measurement includes the time to load the features but not the time to compute the features. In average, computing the features takes 92 seconds per video for the Breakfast dataset where 72.5 seconds are spent for calculating the optical flow.</p><p>The results are reported in <ref type="table">Table 2</ref>. We observe that our approach is 14 times faster to train and 20 times faster during inference compared to the state-of-the-art approach CDFL <ref type="bibr" target="#b16">[17]</ref>. As mentioned before, our approach does not perform any Viterbi decoding during training, which makes it faster during training. Also and most importantly, our approach only performs one Viterbi decoding step for the estimated transcript during inference as compared to CDFL and NNV which need to optimize over all possible transcripts. This shows that our approach offers by far the best trade-off between accuracy and runtime as it is also illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>In order to avoid the full alignment of all transcripts of the training set to a test video, beam search can be used. Beam search limits the maximum number of hypotheses and removes hypotheses with low probability at an early stage. The smaller the beam size, i.e., the number of hypotheses, is, the faster is the inference. This, however, comes at the cost of reducing the accuracy. In order to evaluate how</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>MoF -Action Segmentation IoD -Action Alignment ECTC <ref type="bibr" target="#b11">[12]</ref> 27.7 45.0 HMM/RNN <ref type="bibr" target="#b12">[13]</ref> 33  <ref type="table">TABLE 4</ref> Comparison to the state-of-the-art. The highest score is indicated by a bold font and the second highest is underlined. * indicates that the reported metric is averaged over 5 runs.</p><p>much runtime reduction can be achieved by beam search, we evaluate CDFL with beam search. <ref type="figure" target="#fig_5">Figure 5</ref> shows the MoF accuracy and inference time for different beam sizes. In contrast to <ref type="figure" target="#fig_0">Figure 1</ref>, we report the accuracy for only one run and only split 1 of the Breakfast dataset. We furthermore plot the inference time of the entire test set in log scale for better visualization. We observe that beam search reduces the inference time of CDFL by multiple orders of magnitude but also its accuracy. When we compare a setting where CDFL with beam search is as fast as MuCon, we observe that MuCon achieves a much higher accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Further Comparisons</head><p>Although a statistical test is the golden standard for comparing different methods, this is only possible if the results of multiple runs are available. We therefore also compare our method with state-of-the-art methods based on the reported numbers on the Breakfast dataset <ref type="bibr" target="#b17">[18]</ref> in <ref type="table">Table 4a</ref> and on the Hollywood extended dataset <ref type="bibr" target="#b26">[27]</ref> in <ref type="table">Table 4b</ref> for weakly supervised action segmentation and alignment. Our approach outperforms all other methods except of CDFL for both tasks and datasets. As already discussed in Sections 5.4 and 5.5, our approach achieves either an accuracy that is comparable to CDFL or significantly better. Furthermore, our approach is 20 times faster during inference and provides thus a much better trade-off between accuracy and runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.1">Fully Supervised</head><p>As mentioned in Section 4.7, we can apply our approach to the fully supervised setting as well. For comparison, we use the same metrics as in <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b23">[24]</ref> namely MoF (termed accuracy in <ref type="bibr" target="#b8">[9]</ref>), Edit which measures predicted transcript similarity (similar to the matching score), and F1 score at different overlaps. We first evaluate whether the mutual consistency loss L ? also improves the accuracy in a fully supervised setting. For this ablation experiment, we use split 1 of the Breakfast dataset. As it is shown in <ref type="table" target="#tab_8">Table 5</ref>, the proposed mutual consistency loss improves the accuracy for all metrics. This shows that the mutual consistency loss is not only useful for weakly supervised learning, but also for fully supervised learning.</p><p>We furthermore compare our approach to other fully supervised action segmentation approaches in <ref type="table" target="#tab_9">Table 6</ref>. Although our approach was designed for weakly supervised   learning, it outperforms the state-of-the-art for most metrics. Only for the MoF accuracy, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b23">[24]</ref> perform better, but these networks use multiple stages and thus more layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.2">Mixed Supervision</head><p>Since our network can be trained in a weakly as well in a fully supervised setting, we can train the network also in a mixed setting where a small percentage of the videos is fully annotated and the remaining videos are only weakly annotated by transcripts. As before, we report the average and standard deviation over 5 runs where we randomly sample the videos with frame-wise annotations for each run. The results for a varying percentage of fully annotated videos are reported in <ref type="table" target="#tab_11">Table 7</ref> and visualized in <ref type="figure" target="#fig_6">Figure 6</ref>. In case of 0%, the setting corresponds to weakly supervised learning and 100% corresponds to fully supervised learning. The difference in accuracy between the weakly and fully supervised cases is about 15%. While there is no significant improvement if only 1-2% of the videos are fully annotated, the accuracy increases when at least 5% of the videos are fully annotated. Having 10% of the videos fully annotated, the accuracy gap between the mixed setting and the fully supervised setting is already reduced by about 50%.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Qualitative Evaluation</head><p>We provide some qualitative results for four different test videos of the Breakfast dataset in <ref type="figure">Figure 7</ref>. <ref type="figure">Figure 7a</ref> shows a video where NNV, CDFL, and MuCon perform well. Only NNV hallucinates the action take cup, which is not present in the video. The hallucination of actions occurs due to the alignment of the transcripts of the training set to the test video. In this example, there is a very high uncertainty among the class probabilities estimated by NNV at the beginning of the video. Since the probabilities for take cup are low but slightly higher than for spoon powder or background, the transcript take cup -spoon powder -pour milk -stir milk achieves a higher alignment score than the correct transcript spoon powder -pour milk -stir milk. <ref type="figure">Figures 7b and 7c</ref> show examples where also CDFL hallucinates actions that are plausible based on the transcripts but that do not occur in the video. In contrast, MuCon does not suffer from hallucinating actions since it infers the transcript directly from the test video and it does not search the training transcript that best aligns to the test video. <ref type="figure">Figure 7d</ref> visualizes a failure case where all approaches fail to infer the take plate action at the beginning of the video. In this example, CDFL provides a better estimate than MuCon. <ref type="figure" target="#fig_8">Figure 8</ref> shows qualitative examples for comparing the different types of supervision, namely weak, mixed (10%), and full supervision. <ref type="figure" target="#fig_8">Figure 8a</ref> shows the result for a test video where MuCon is able to infer the actions correctly using weak, mixed, or full supervision. However, the action boundaries are more accurately estimated if MuCon is trained with more supervision. <ref type="figure" target="#fig_8">Figure 8b</ref> shows a very difficult video where even the fully supervised approach makes a mistake. background -pour cereals -pour milk -take bowl -stir cereals GT MuCon NNV CDFL (c) background -pour coffee -pour milk -take cup -add teabag -pour water -spoon sugar GT MuCon NNV CDFL (d) background -take plate -take knife -cut orange -squeeze orange -take glass -pour juice <ref type="figure">Fig. 7</ref>. Qualitative examples for weakly supervised action segmentation on the Breakfast dataset. Each figure visualizes a different video from the test set of split 1. We compare the results from MuCon, NNV, and CDFL with the ground truth (GT). Each row shows the result for the entire duration of a video and the colored segments show when the actions occur in the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We proposed a novel approach for weakly supervised action segmentation from transcripts. It consists of a two-branch network that predicts two representations for action segmentation. To train the network, we introduced a new mutual consistency loss (MuCon) that enforces these two representations to be consistent during training. Using MuCon and a transcript prediction loss, we can train our network end-to-end without the need for additional steps. We show that the proposed network with the mutual consistency loss achieves an accuracy that is either on par or better than the state-of-the-art. At the same time, it is much faster during inference and offers the best trade-off between accuracy and inference time. Furthermore, we have shown that the mutual consistency loss increases the accuracy even for fully supervised learning and that the network can be applied to a mixed setting where a few videos are fully annotated and the other videos are only weakly annotated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A METRICS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Mean over Frames Accuracy (MoF)</head><p>The mean over frames accuracy <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> is defined as the number of frames with correctly predicted labels divided by the total number of frames. Given a set of predictions Y (j) and ground truth labels? (j) , where j = 1, . . . , V and V is the number of videos in the test set, the MoF is defined as</p><formula xml:id="formula_14">MoF = V j=1 T (j) t=1 I(Y (j) t =? (j) t ) V j=1 T (j)<label>(16)</label></formula><p>where T (j) is the length of the jth video and I is the indicator function. MoF-BG is similar to MoF, but it only considers the ground truth frames which are not annotated as background (BG).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Matching Score</head><p>The matching score <ref type="bibr" target="#b40">[41]</ref> measures the similarity of the predicted transcript compared to the ground truth transcript:</p><p>Matching Score = 2 ? number of matches</p><formula xml:id="formula_15">|A| + |?|<label>(17)</label></formula><p>where |A| is the length of the predicted transcript and |?| is the length of the ground truth transcript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Intersection Over Detection (IoD)</head><p>IoD is only defined for the alignment task since the transcript and number of segments are known for this task. This provides a one to one matching between the predicted and ground truth segments. The IoD is then computed by the average intersection of a ground truth segment with an associated predicted segment divided by the length of the predicted segment, i.e., |S ??|/|S|. We use the same definition and code as <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Edit and F1 Score</head><p>Edit and F1 scores are calculated using the same definition and code as <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b23">[24]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Average inference time per video (seconds) vs. average mean over frames (MoF) accuracy (%) for weakly supervised action segmentation approaches on the Breakfast dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Our proposed network consists of three subnetworks (gray). The temporal backbone ft embeds the input features in the hidden representation Z which is used for two branches. While the frame classification branch fc predicts framewise class probabilities Y for action segmentation, the segment generation branch fs predicts the segment representation S for action segmentation. We train our network using two loss functions. While the transcript prediction loss Lt enforces that the predicted transcript A matches the ground-truth transcript?, our proposed mutual consistency (MuCon) loss L? enforces that the two representations are consistent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Visualization of the segment generation branch and the loss functions. (a) Segment generation branch fs and the transcript prediction loss Lt.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Examples of different masks for three consecutive action segments. The top row shows regular masks with different shapes while the bottom row shows masks generated with added 10% overlap. The left, middle and right figures depict box, bell, and trapezoid shaped masks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>U</head><label></label><figDesc>[j] max(0, 1?|i u [t]?j|) ?t ? [1, . . . , T ]. (9)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Using beam search to reduce the inference time of CDFL. The blue squares show the performance of CDFL with different values for the beam size. The x-axis is the total inference time for split 1 of the Breakfast dataset in log scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>MoF accuracy (%) for training with mixed supervision. The x-axis denotes the percentage of videos that are fully annotated. The average accuracy for 5 runs on split 1 of the Breakfast dataset is reported.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>background -spoon powder -pour milk -stir milk -take cup</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>background -pour oil -crack egg -fry egg -add salt and pepper -put egg to plate Qualitative examples for different levels of supervision on the Breakfast dataset. Each figure visualizes a different video from the test set of split 1. GT visualizes the ground truth segmentation and Weak, Mixed, and Full visualize the output of MuCon trained with weak, mixed (10%), or full supervision, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The first four rows show the matching score of the segment generation branch for different dilation factors used for the temporal backbone. In these settings, the mutual consistency loss L? is not used. The last row shows the proposed setting with the mutual consistency loss.</figDesc><table><row><cell>ft</cell><cell>L?</cell><cell>Mat Score</cell><cell>Shape</cell><cell cols="2">Overlap MoF</cell><cell>Mat Score</cell><cell>F1@{10,25,50}</cell></row><row><cell>single 1-d conv</cell><cell></cell><cell>0.691?0.010</cell><cell>box</cell><cell></cell><cell cols="2">49.0?2.0 0.774?0.006</cell><cell>67.9 59.5 40.2</cell></row><row><cell>fixed dilation</cell><cell></cell><cell>0.696?0.021</cell><cell>bell</cell><cell></cell><cell cols="2">43.6?2.3 0.777?0.009</cell><cell>65.1 55.5 35.4</cell></row><row><cell>increasing dilation, linear</cell><cell></cell><cell>0.727?0.019</cell><cell>trapezoid</cell><cell></cell><cell cols="2">48.7?1.8 0.773?0.006</cell><cell>67.3 59.0 39.4</cell></row><row><cell>increasing dilation, exponential</cell><cell></cell><cell>0.729?0.015</cell><cell>box</cell><cell>10%</cell><cell cols="2">48.8?1.5 0.787?0.006</cell><cell>68.3 59.6 39.8</cell></row><row><cell>increasing dilation, exponential</cell><cell></cell><cell>0.774?0.006</cell><cell>bell</cell><cell>10%</cell><cell cols="2">43.5?0.8 0.779?0.007</cell><cell>64.8 55.3 34.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell>trapezoid</cell><cell>10%</cell><cell cols="2">48.0?2.7 0.780?0.009</cell><cell>67.7 59.1 39.5</cell></row></table><note>(a) Transcript prediction accuracy:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The first three rows denote settings where the length regularizer L for the segment generation branch, the smoothing loss Ls for the frame classification branch, or both are omitted.</figDesc><table><row><cell cols="4">MoF 47.4?1.1 0.787?0.009 Mat Score 47.5?1.7 0.780?0.005 48.3?0.9 0.777?0.011 49.0?2.0 0.774?0.006 (c) Impact of regularizers: Inference variant F1@{10,25,50} none 68.1 59.3 39.7 Ls 67.8 59.0 39.5 L 67.8 59.0 40.3 L + Ls 67.9 59.5 40.2 MuCon-Y MuCon-S MuCon-full (d) Impact of fusing both branches: While MuCon-Y and MoF F1@{10,25,50} 44.7?1.4 28.1 22.6 13.3 43.6?1.2 65.6 57.2 33.8 49.0?2.0 67.9 59.5 40.2</cell></row><row><cell></cell><cell></cell><cell cols="2">MuCon-S use either the frame classification branch or the</cell></row><row><cell></cell><cell></cell><cell cols="2">segment generation branch for inference, MuCon-full fuses</cell></row><row><cell></cell><cell></cell><cell cols="2">both branches for inference.</cell></row><row><cell>Teacher forcing</cell><cell>MoF</cell><cell>Mat Score</cell><cell>F1@{10,25,50}</cell></row><row><cell>None</cell><cell cols="2">48.7?2.9 0.779?0.009</cell><cell>68.2 59.5 40.3</cell></row><row><cell>70 epochs</cell><cell cols="2">48.9?2.1 0.783?0.009</cell><cell>68.2 59.5 39.8</cell></row><row><cell>All epochs</cell><cell cols="2">49.0?2.0 0.774?0.006</cell><cell>67.9 59.5 40.2</cell></row></table><note>(e) Impact of teacher forcing: The first row denotes a setting where teacher forcing is not used for training. The second row denotes a setting where teacher forcing is used only for the first 70 epochs. The last row denotes a setting where teacher forcing is used for all epochs.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1</head><label>1</label><figDesc>Ablation experiments on split 1 of the Breakfast dataset. We report the average and standard deviation over 5 runs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3</head><label>3</label><figDesc>Friedman statistical test results. The top row and the leftmost column indicate different approaches and their MoF accuracy and matching score from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 5</head><label>5</label><figDesc>Effect of the MuCon loss on the accuracy in the fully supervised setting. The results are reported for split 1 of the Breakfast dataset.</figDesc><table><row><cell>Breakfast</cell><cell></cell><cell>F1@{10,25,50}</cell><cell></cell><cell>Edit</cell><cell>MoF</cell></row><row><cell>ED-TCN [10]*</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>43.3</cell></row><row><cell>HTK [28]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>50.7</cell></row><row><cell>TCFPN [15]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>52.0</cell></row><row><cell>HTK(64) [8]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>56.3</cell></row><row><cell>GRU [13]*</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>60.6</cell></row><row><cell>GRU+length prior [29]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>61.3</cell></row><row><cell>MS-TCN [9]</cell><cell>52.6</cell><cell>48.1</cell><cell>37.9</cell><cell>61.7</cell><cell>66.3</cell></row><row><cell>MS-TCN++ [24]</cell><cell>64.1</cell><cell>58.6</cell><cell>45.9</cell><cell>65.6</cell><cell>67.6</cell></row></table><note>MuCon 73.2?0.4 66.1?0.5 48.4?0.6 76.3?0.5 62.8?1.0</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 6</head><label>6</label><figDesc>Comparison with the state-of-the-art for fully supervised action segmentation on the Breakfast dataset. (* obtained from<ref type="bibr" target="#b14">[15]</ref>).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 7</head><label>7</label><figDesc>Training with mixed supervision. The average MoF accuracy and standard deviation for 5 runs on split 1 of the Breakfast dataset is reported.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The work has been partially funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) -GA 1927/4-2 (FOR 2535 Anticipating Human Behavior) and the ERC Starting Grant ARCA (677650).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spatio-temporal channel correlation networks for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yousefzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yousefzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Temporal 3d convnets using temporal transition layer,&quot; in CVPR Workshops</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large scale holistic video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">An end-to-end generative framework for video segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ms-tcn: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abu</forename><surname>Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Connectionist temporal modeling for weakly supervised action labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weakly supervised action learning with rnn based fine-to-coarse modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neuralnetworkviterbi: A framework for weakly supervised video learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Weakly-supervised action segmentation with iterative soft boundary assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">D 3 TW: Discriminative differentiable dynamic time warping for weakly supervised action alignment and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weakly supervised energy-based learning for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goal-directed human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Temporal segmentation and activity classification from first-person sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Spriggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Temporal localization of actions with actoms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2782" to="2795" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast saliency based pooling of fisher encoded dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV THUMOS Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Segmental spatiotemporal cnns for fine-grained action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Abufarha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<title level="m">MS-TCN++: Multi-Stage Temporal Convolutional Network for Action Segmentation</title>
		<imprint>
			<publisher>PAMI</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Temporal segmentation of egocentric videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Poleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Personal-locationbased temporal segmentation of egocentric videos for lifelogging applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Battiato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weakly supervised action labeling in videos under ordering constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of actions from transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="78" to="89" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Hybrid RNN-HMM Approach for Weakly Supervised Temporal Action Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="765" to="779" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Sequence-to-segment networks for segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>SSW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A multistream bi-directional recurrent neural network for fine-grained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="270" to="280" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">edit-distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lambert</surname></persName>
		</author>
		<ptr target="https://github.com/belambert/edit-distance" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">On Evaluating Weakly Supervised Action Segmentation Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Souri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Minciullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The use of ranks to avoid the assumption of normality implicit in the analysis of variance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">200</biblScope>
			<biblScope unit="page" from="675" to="701" />
			<date type="published" when="1937" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
