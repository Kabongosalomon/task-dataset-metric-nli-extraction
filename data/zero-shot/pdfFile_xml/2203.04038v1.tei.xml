<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gait Recognition with Mask-based Regularization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanfu</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Southern University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beibei</forename><surname>Lin</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Beijing Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunli</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Beijing Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">Q</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Shiqi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>2?</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Southern University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Gait Recognition with Mask-based Regularization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Gait Recognition</term>
					<term>Regularization</term>
					<term>Network Generalization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most gait recognition methods exploit spatial-temporal representations from static appearances and dynamic walking patterns. However, we observe that many part-based methods neglect representations at boundaries. In addition, the phenomenon of overfitting on training data is relatively common in gait recognition, which is perhaps due to insufficient data and low-informative gait silhouettes. Motivated by these observations, we propose a novel mask-based regularization method named ReverseMask. By injecting perturbation on the feature map, the proposed regularization method helps convolutional architecture learn the discriminative representations and enhances generalization. Also, we design an Inception-like ReverseMask Block, which has three branches composed of a global branch, a feature dropping branch, and a feature scaling branch. Precisely, the dropping branch can extract fine-grained representations when partial activations are zero-outed. Meanwhile, the scaling branch randomly scales the feature map, keeping structural information of activations and preventing overfitting. The plug-and-play Inception-like ReverseMask block is simple and effective to generalize networks, and it also improves the performance of many state-of-theart methods. Extensive experiments demonstrate that the ReverseMask regularization help baseline achieves higher accuracy and better generalization. Moreover, the baseline with Inception-like Block significantly outperforms state-of-the-art methods on the two most popular datasets, CASIA-B and OUMVLP. The source code will be released.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Gait recognition <ref type="bibr" target="#b39">[31]</ref> utilizes appearance and walking patterns as clues to identify people from images sequences. Gait recognition can achieve perception-free human identification at a distance, which is hardly achievable by other biometrics such as the face, fingerprint, iris. Nevertheless, gait recognition is still facing several challenges such as pose <ref type="bibr" target="#b32">[24]</ref>, carrying and clothing <ref type="bibr" target="#b48">[40]</ref>, ageing <ref type="bibr" target="#b27">[19]</ref>, illumination, and occlusions.</p><p>(a) (b) (c) (d) (e) (f) <ref type="figure">Fig. 1</ref>. Visualization <ref type="bibr" target="#b54">[46]</ref> of activation for different methods on CASIA-B. Left: (a) input silhouettes. (b) activation visualization of GaitSet <ref type="bibr" target="#b12">[4]</ref>. (c) GaitSet with our proposed ReverseMask regularization method. Right: (d) input silhouettes. (e) activation visualization of GaitGL <ref type="bibr" target="#b35">[27]</ref>. (f) GaitGL with ReverseMask regularization, respectively. GaitSet concentrates on the head and foot regions. GaitGL distributes the attention on different moving parts of a human body, but the noncontinuous attention reveals that information is missing at the horizontal boundaries.</p><p>Recently, several studies <ref type="bibr" target="#b41">[33,</ref><ref type="bibr" target="#b47">39,</ref><ref type="bibr" target="#b46">38,</ref><ref type="bibr" target="#b50">42,</ref><ref type="bibr" target="#b34">26]</ref> achieved impressive performance to facilitate gait recognition using the deep convolutional neural networks. Notably, most appearance-based gait recognition models only adopt a limited number of stacked layers (refers to depth). In contrast, other visual recognition tasks <ref type="bibr" target="#b11">[3,</ref><ref type="bibr" target="#b21">13]</ref> have already greatly benefited from very deep models. However, such deep architecture is inferior to shadow networks in gait recognition. To our best knowledge, the preference to apply shallow models is mainly caused by two aspects: i) The task of gait recognition has less data for training. For example, Sports-1M and YouTube-8M <ref type="bibr" target="#b28">[20,</ref><ref type="bibr" target="#b9">1]</ref> contain millions of action videos, while the largest cross-view gait dataset <ref type="bibr" target="#b43">[35]</ref> provides 0.1 million gait sequences for training. ii) Silhouettes provide less information than information from the RGB modality. Therefore, we think that insufficient training data can easily lead many deep networks <ref type="bibr" target="#b12">[4,</ref><ref type="bibr" target="#b23">15,</ref><ref type="bibr" target="#b52">44]</ref> to risk overfitting the salient characteristics. The overfitting phenomenon can be glimpsed from the activations visualization of classical GaitSet <ref type="bibr" target="#b12">[4]</ref> as shown in <ref type="figure">Fig. 1 (b)</ref>. The learned representations easily focus on the most discriminative pattern, but it leads to poor generalization performance on validation. Unexpectedly, this problem has not been indicated well yet in the previous literature.</p><p>By partitioning the holistic human body into many horizontal parts, the part-based methods <ref type="bibr" target="#b16">[8,</ref><ref type="bibr" target="#b35">27,</ref><ref type="bibr" target="#b50">42]</ref> leverage partial features to prevent the issue of overfitting on salient representations. Nonetheless, the learned representations by part-based methods are noncontinuous and distributed sparsely as shown in <ref type="figure">Fig. 1 (e)</ref>. The conventional part-based methods employ hard boundary partition, where spatial clues of each part can only concentrate on inner partial regions, neglecting inter-part correlation. This phenomenon of noncontinuous representations refers to boundary isolation in this paper. Therefore, it is necessary to prevent overfitting and boundary, furthermore improving the generalization ability and performance of deep networks for robust gait recognition.</p><p>To address the overfitting issue, various data augmentation and regularization methods <ref type="bibr" target="#b42">[34,</ref><ref type="bibr" target="#b13">5]</ref> have been proposed, such as input-level random erasing <ref type="bibr" target="#b53">[45]</ref> and feature-level DropBlock <ref type="bibr" target="#b19">[11]</ref>. The principle of these methods is to inject noise into raw input or feature, producing extra data so that convolutional networks do not overfit the training data. We argue that the main drawback of various erasing-based methods is that it only zero-out features. Besides, it also can apply perturbation by scaling activations. The scaling features extents to bring about more noise to prevent network overfitting. Moreover, it is also perfectly suitable for appearance-based gait recognition because the scaling regularization supervises the network to look for structural evidence of gait for simply silhouettes. For the boundary issue, we analyze this isolation is mainly caused by manual partition like GaitPart <ref type="bibr" target="#b16">[8]</ref>, where such convolutional layers can only capture internal representations but ignore semantic information at the boundary. Therefore, it is straightforward to consider generating random partition to avoid neglecting regional representations during training to overcome the boundary issue.</p><p>The aforementioned intuitions inspires us to address both overfitting and boundary problems by introducing a mask-based regularization method. In this work, we propose a novel regularization method called ReverseMask, with a corresponding Inception-like ReverseMask Block. Specifically, the Inception-like ReverseMask Block has a parallel architecture consisting of three branches, which are global branch, dropping branch, and scaling branch. Within both dropping branch and scaling branch, the novel ReverseMask layer is plugging as a regularizer where receiving features from the previous layer and then producing a pair of features with perturbation. Specifically, the ReverseMask layer zero-outs partial features for the dropping branch. Therefore, the dropping branch effectively captures fine-grained representations since convolutional filters must look at informative regions to fit the erased feature. For the scaling branch, Reverse-Mask randomly scales the value of activations so that the perturbation forces a convolutional filter to learn the structural characteristics of gait silhouettes.</p><p>In our experiments, adding Inception-like ReverseMask Block to GaitSet shows its regularizing convolutional networks as shown in <ref type="figure">Fig. 1</ref> as well as improving generalization performance in cross-view gait recognition under clothing from 74.8% to 76.3%. Besides, Inception-like ReverseMask Block relieves the phenomena of boundary isolation of part-based methods and improves cross-view gait recognition accuracy as well. In summary, the contributions of this work are listed as follows:</p><p>-The novel ReverseMask layer is superior to regular DropBlock regularization in our experiments. The ReverseMask provides much more stable regularization and speeds up the training.</p><p>-We propose a novel Inception-like ReverseMask Block with the scaling branch, which helps to capture structural gait representations. In particular, Inceptionlike ReverseMask Block can be flexibly embedded into the most recent gait frameworks, improving the discriminativeness of feature representations and the generalization performance of models. -The network with proposed regularization method outperforms start-of-theart methods on two popular benchmarks: CASIA-B <ref type="bibr" target="#b48">[40]</ref> and OUMVLP <ref type="bibr" target="#b43">[35]</ref>.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Gait Recognition</head><p>Holistic-based recognition. To extract holistic gait features, template-based methods <ref type="bibr" target="#b41">[33,</ref><ref type="bibr" target="#b47">39]</ref> utilize convolutional neural networks directly on gait templates like Gait Energy Images <ref type="bibr" target="#b20">[12]</ref> (GEI) in early. Wu et al. <ref type="bibr" target="#b47">[39]</ref> propose three types of architecture to recognize the most discriminative changes of gait features and provide many comprehensive experiments on cross-view gait recognition performance. Nevertheless, template-based methods lose temporal and fine-grained spatial information. In contrast, many sequence-based methods <ref type="bibr" target="#b12">[4,</ref><ref type="bibr" target="#b23">15,</ref><ref type="bibr" target="#b51">43]</ref> conduct feature extractors on each frame to capture detailed spatial clues across frames. For example, Chao et al. <ref type="bibr" target="#b12">[4]</ref> regard a gait as a set of independent frames, then aggregate and fuse the extracted set-level feature by the proposed set pooling unit. However, the gait recognition methods based on holistic representations tend to focus on the most representative salient patterns, leading to distinguishing the subjects trickily.</p><p>Part-based recognition. The part-based methods <ref type="bibr" target="#b16">[8,</ref><ref type="bibr" target="#b35">27,</ref><ref type="bibr" target="#b50">42]</ref> extensively exploit fine-grained spatial cues from multiple parts for local representation learning. The part-based methods can extract from different types of local regions i.e. patch <ref type="bibr" target="#b38">[30]</ref>, body components <ref type="bibr" target="#b36">[28,</ref><ref type="bibr" target="#b32">24]</ref>, vertical or horizontal bins <ref type="bibr" target="#b17">[9]</ref>, and attentive regions <ref type="bibr" target="#b30">[22]</ref>. GaitPart <ref type="bibr" target="#b16">[8]</ref> introduces the Focal Convolution Layer to enhance the part-level spatial features by splitting the input feature map into several parts horizontally. Horizontal Pooling is widely used in many approaches <ref type="bibr" target="#b23">[15,</ref><ref type="bibr" target="#b24">16,</ref><ref type="bibr" target="#b35">27]</ref> since GaitSet <ref type="bibr" target="#b12">[4]</ref> adopts the Horizontal Pyramid Pooling <ref type="bibr" target="#b18">[10]</ref> from person reidentification. Specifically, Horizontal Pyramid Pooling separates feature maps into hierarchical strips with multi-scales hierarchy, improving spatial discriminative ability. Most works <ref type="bibr" target="#b35">[27,</ref><ref type="bibr" target="#b16">8,</ref><ref type="bibr" target="#b50">42]</ref> are based on pre-defined uniform partition leading to the problem of boundary. Zhang et al. <ref type="bibr" target="#b50">[42]</ref> propose a method based on the learned partition, but boundary isolation still exists. Recently, GaitGL <ref type="bibr" target="#b35">[27]</ref> combines both global and partial features, gaining more robust spatial representation. Besides, it also introduces 3D ConvNets to learn spatial-temporal integrated features for gait recognition. However, no matter learned or hand-crafted partition-based methods have introduced the boundary problem, neglecting the spatial information at the foundation.</p><formula xml:id="formula_0">x t x t ! = " ? ( )~( ) # = " ?(1 ? ) $%" ! = ! + # $%" # = ( ? ) ! +( ? ) # $%" = ! ! = " ? ReverseMask DropBlock + + Fig. 2.</formula><p>Illustration of our proposed ReverseMask. Given two random variable, Re-verseMask introduces perturbation on networks by reducing values of feature map on selected regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Erasing Images or Activations</head><p>Image based erasing <ref type="bibr" target="#b14">[6,</ref><ref type="bibr" target="#b53">45]</ref> is widely adopted as a data augmentation technique. In recent years, cutout <ref type="bibr" target="#b14">[6]</ref> has been demonstrated that masking out partial feature maps can improve generalization of convolutional neural networks and achieve better performance in many tasks such as object detection <ref type="bibr" target="#b10">[2]</ref> and person re-identification <ref type="bibr" target="#b53">[45]</ref>.</p><p>Feature based erasing <ref type="bibr" target="#b13">[5,</ref><ref type="bibr" target="#b19">11]</ref> is an alternative regularization technique that is implemented by using zero-masking directly on the feature map. Dropout <ref type="bibr" target="#b42">[34]</ref> is effective to prevent overfitting, but it designs initially for fully-connected layer. While the mechanism of dropout also brings many successful works on convolutional neural networks, such as SpatialDropout <ref type="bibr" target="#b45">[37]</ref> and DropBlock <ref type="bibr" target="#b19">[11]</ref>. Our method drops identical, randomly selected regions of convolutional features for sequences in a batch, which has been proved effective in previous literature <ref type="bibr" target="#b13">[5]</ref>. However, our work presents a scaling mechanism where parts of convolutional features are multiplied by a random ratio <ref type="bibr" target="#b44">[36]</ref>. Feature maps are multiplied by a random ratio rather than zero, presenting novel structural representations for robust gait recognition. Furthermore, we also argue that such structural representations are ignored, while many methods <ref type="bibr" target="#b16">[8,</ref><ref type="bibr" target="#b33">25]</ref> only proposed to capture local representations over coarse pre-defined parts <ref type="bibr" target="#b35">[27,</ref><ref type="bibr" target="#b17">9]</ref>.</p><formula xml:id="formula_1">N x T x C x H x W N x P x C Stage 1 Stage 2 Stage 3 TP &amp; HP &amp;FC &amp; BN N x class FC Conv + TP replace with + Conv (0,1) ( , ) (a) Plain (d) Inception-like RM Block RMFE RMFE x x y=F(x) Conv ? !"!#$ = ? !%&amp;'$(! + ? )*( = (? ! , ? " , ? # ) (b) DB Block (c) RM Block DropBlock x y=F(x) Conv x y=F(x) RMFE ( , ) Fig. 3.</formula><p>The framework of our proposed method. Top: a simplified model <ref type="bibr" target="#b35">[27]</ref> as our baseline model; Bottom: Four replaceable blocks. TP, FC, HP and BN mean temporal pooling aggregation layer <ref type="bibr" target="#b35">[27,</ref><ref type="bibr" target="#b12">4]</ref>, fully connected layer, horizontal pooling layer <ref type="bibr" target="#b18">[10]</ref>, and batch normalization layer <ref type="bibr" target="#b26">[18]</ref>, respectively. Conv indicates a convolutional layer followed by Leaky ReLU <ref type="bibr" target="#b37">[29]</ref>. RM is our proposed ReverseMask layer, and it receive a pair variable to inject noise on feature map. While RMFE is the feature extraction layer specifically designed for ReverseMask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our approach</head><p>In this work, we propose a novel regularization method named ReverseMask, which can help relieve convolutional architecture overfitting on training sets. In this section, we first formulate the simple yet effective ReverseMask (Sec. 3.1).</p><p>Then the detailed definitions of other corresponding components are followed (Sec. 3.2). Finally, two variants of building block are introduced (Sec. 3.3) for making ReverseMask as easy-to-use as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ReverseMask Layer</head><p>ReverseMask is a simple regularization method similar to DropBlock. Its main difference from DropBlock is significant. (1) ReverseMask is a general maskbased regularizer, which produces a pair of masked features instead of only one masked feature. (2) ReverseMask introduces perturbation on networks by reducing values of feature maps on selected regions. Illustrations of ReverseMask and DropBlock are shown in <ref type="figure">Fig. 2</ref>. Notably, there are only two parameters for ReverseMask, which are reg prob and p. reg prob determines the probability of changing activations undergoing ReverseMask. p controls the area ratio of Masked generation. During the training stage, we first randomly sample mask on the given feature map X l ? R b l ?n l ?c l ?h l ?w l at the lth layer. Then its paired mask can be determined by reversing the mask matrix. The process can be defined as</p><formula xml:id="formula_2">M m i,j ? Bernoulli(p) (1) M p i,j = 1 ? M m i,j<label>(2)</label></formula><p>where M m ? R h l ?w l and M p ? R h l ?w l refer to sampled mask and its paired mask. Notably, such a pair of masks is temporally synchronized, and we only conduct experiments with shared masks across different feature channels for simplicity and fair comparison with part-based methods. Then, applying the paired of reversal masks on given features, two masked features can be obtained</p><formula xml:id="formula_3">X m l = X l ? M m (3) X r l = X l ? M r<label>(4)</label></formula><p>It is worth noticing that these masked features are identical to features produced by DropBlock, but ReverseMask has a pair of masked features. Finally, we introduce perturbation on activations for regularization with a pair of randomly generated values ?, ? ? U nif orm(0, 1) (5)</p><formula xml:id="formula_4">X m lout = ?X m l + ?X r l (6) X p lout = (1 ? ?)X m l + (1 ? ?)X r l<label>(7)</label></formula><p>where two random variables ? and ? satisfy the uniform distribution to perturb activations. X m lout ? R b l ?n l ?c l ?h l ?w l and X p lout ? R b l ?n l ?c l ?h l ?w l are obtained as input of further feature learning. Therefore, our proposed ReverseMask is a more general DropBlock, generating zero-masked features and scaling features to regularize the network. Beyond feature-level erasing methods, our proposed mask-based scaling regularization method is able to capture structural information from gait silhouettes. At the same time, erasing-based regularizer tends to perform better on fine-grained representation learning.</p><p>Mask Sampling. In the experiments, we have studied many mask sampling strategies, such as masking independent random units as shown in <ref type="figure">Fig. 2</ref>, and masking continuous regions. The detailed analysis refers to Sec. 4.3.</p><p>Setting the value of reg prob. In our implements, reg prob = 1 is constant for fair comparison with part-based methods. Taking GaitGL as an example, it can be seen as mask-based with special dropping out certain regions of activations.</p><p>Setting the value of p. We investigate the area ratio for the masked region in the experiments. p is applied to 0.5 as the optimal ratio, which means half of the feature map tends to be perturbed.</p><p>Setting the value of ? and ?. The ReverseMask is a flexible and general regularization method. By setting the different values of ? and ?, the networks regularized by ReverseMask could perform extremely differently in representation learning. We found that ReverseMask enables a network to capture fine-grained representations, like conventional part-based methods doing. The ReverseMask resembles DropBlock when ? = 1, ? = 0. When ?, ? are sampled from the uniform distribution, information about the structure can still be sent to further extraction. In summary, there are two variants of ReverseMask according to the value of ?, ?. We illustrate these two variants of ReverseMask in different colors as shown in figure 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ReverseMask Feature Extraction</head><p>The conventional part-based methods utilize a shared convolutional layer to extract local representations. While GaitPart <ref type="bibr" target="#b16">[8]</ref> utilized two-dimensional filters, and GaitGL <ref type="bibr" target="#b35">[27]</ref> adopted three-dimensional convolutions which enhanced spatiotemporal feature learning. In other words, focal convolutions apply to each part with shared parameters. The receptive field of the focal convolution layer is restricted, which leads to the issue of boundary. To alleviate such boundary weakness, our proposed ReverseMask randomly generates a pair of zero-masking features when ? = 1, ? = 0. Therefore, the convolutions can still capture the fine-grained representations without boundary neglect. In our implements, we use identical 3D convolution to GaitGL <ref type="bibr" target="#b35">[27]</ref> for a fair comparison with part-based methods, which can design as</p><formula xml:id="formula_5">F l = W (X m lout ) + W (X p lout )<label>(8)</label></formula><p>where W (?) means a 3D convolution operation, and X m lout and X p lout are the paired feature generated by Eqn. 6 and Eqn. 7. As we see, the ReverseMask Feature Extraction layer remains a full feature map for further layers, which brings about many advantages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Variants of Building Block</head><p>Plain ReverseMask Block is a plain-like block similar to plain architecture as shown in <ref type="figure">Fig. 2</ref>. The Plain ReverseMask Block is general. It can resemble DropBlock-like regularization when ? = 1, ? = 0, which refers to dropping Plain-RMB for short. Also, it resembles scaling regularization when ?, ? sample from the uniform distribution, which refers to scaling Plain-RMB for short. In our experiments, both scaling and dropping Plain-RMB are superior to DropBlock.</p><p>Inception-like ReverseMask Block is designed for two reasons: (1) It is necessary to design such multi-branches architecture to establish a fair comparison with GaitGL. (2) The ReverseMask with different settings changes the distribution of the training set. Therefore such Inception-like ReverseMask Block can take advantage of multiple branches. As shown in <ref type="figure">Fig.3(d)</ref>, three representations learned from each branch are then aggregated by feature fusion module, which denotes as</p><formula xml:id="formula_6">F = F g + F d + F s F ? R b l ?n l ?c out l ?h l ?w l<label>(9)</label></formula><p>where F g , F d , and F s represent features obtained from global, dropping and scaling branch, respectively. In addition, this strategy is used in every stage, excluding the last one. The final representations are concatenated in the final stage , which is commonly used in <ref type="bibr" target="#b35">[27,</ref><ref type="bibr" target="#b24">16,</ref><ref type="bibr" target="#b23">15]</ref>. The fusion module is represented as:</p><formula xml:id="formula_7">F = concat(F g , F d , F s ) F ? R b l ?n l ?c out l ?3h l ?w l<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The proposed method is evaluated on two popular public datasets: CASIA-B and OU-MVLP. CASIA-B is easy to evaluate the robustness to different variations, and OU-MVLP is the largest public gait dataset. Implementation details, results, comparisons, ablation study, and analysis are presented in the following part of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Settings</head><p>Datasets. CASIA-B <ref type="bibr" target="#b48">[40]</ref> consists of 124 subjects with ten sequences under 11 views, resulting in 124?10?11 = 13640 gait sequences. Each subject walked ten times in three conditions, i.e. six in normal (NM), two with a bag (BG), two with a coat (CL). To compare fairly, we follow the experiment protocol in <ref type="bibr" target="#b12">[4]</ref>   <ref type="bibr" target="#b12">[4]</ref> also. That means 5153 subjects in the training set and the other 5154 subjects in the test set. In the test phase, sequence NM#-01 is put into the gallery set, and sequence NM#-00 is in the probe set.</p><p>Implementation details: (1) Human body silhouettes are aligned, cropped and resized to 64 ? 44 by the preprocessing method <ref type="bibr" target="#b43">[35]</ref>. The sequence length is 30 frames in the training phase, while the whole sequence is used in the test phase.</p><p>(2) The separate Batch All (BA+) triplet loss <ref type="bibr" target="#b22">[14]</ref> is applied to train our model. The batch size (p,k) is set up as <ref type="bibr" target="#b16">(8,</ref><ref type="bibr" target="#b24">16)</ref> for CASIA-B and <ref type="bibr" target="#b40">(32,</ref><ref type="bibr" target="#b16">8)</ref> for OU-MVLP, respectively. Margin m for triplet loss is set to 0.2. (3) The pooling parameter p is set to 6.5 for Generalized-Mean pooling <ref type="bibr" target="#b40">[32]</ref>. (4) The iteration is set to 60K, 80K, and 80K for ST, MT, and LT, respectively, in CASIA-B training. In OU-MVLP training, the iteration is increased to 210K since there is more data in OU-MVLP. (5) Adam <ref type="bibr" target="#b29">[21]</ref> is employed as the optimizer, and the initial learning rate is 1e ? 4 with weight decay 5e ? 4. The learning rate reduces to 1e ? 5 after 70K iterations for MT and LT, while the learning rate changes to 1e ? 5 after 150K, then 5e ? 6 after 200K iterations for OU-MLVP. (6) For OU-MVLP, we doubled channel sizes of all convolution layers since OU-MVLP has more data than in CASIA-B.</p><p>Evaluation metric Rank-1 accuracy excluding identical-view sequences is taken as the same evaluation metric used in <ref type="bibr" target="#b12">[4]</ref> and some other state-of-the-art methods. The metric has been widely employed to evaluate the performance of crossview gait recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparisons with State-of-the-Art</head><p>Results on CASIA-B. Tab. 1 shows our performance on CASIA-B, with three training configurations. With the large-scale configuration, we obtain 97.7%, 95.3%, and 86.0% on rank-1 accuracy under NM, BG, and CL, respectively. The performance of our proposed method surpasses the classical method GaitSet <ref type="bibr" target="#b12">[4]</ref> by a large margin. Compared with two typical part-based methods, GaitPart <ref type="bibr" target="#b16">[8]</ref> and GaitGL <ref type="bibr" target="#b35">[27]</ref>, our model outperforms GaitPart by 7.3% under the most challenging condition of clothing (CL). At the same time, it outperforms the significant accuracy of GaitGL with 2.4% under CL. The comparison with part-based methods replied on the coarse partition demonstrated Inception-like Reverse-Mask Block's effectiveness in enhancing the model by integrating structural and fine-grained representations. A recent part-based method, 3DLocal <ref type="bibr" target="#b25">[17]</ref>, models the dynamic motion information on learned partition differently from previous pre-defined parts. However, the result illustrates that our model outperforms 3DLocal under all walking conditions significantly, which shows that random partition with simpleness can compete with adaptive region localization. We analyze the boosting performance coming from two aspects: (1) The proposed ReverseMask regularization injects noise into the feature map, which helps to prevent models from overfitting training data. Therefore, a better generalization performance is obtained in the test dataset. (2) random mask sampling considers full local representations from silhouettes, while the conventional partbased methods neglect characteristics around the gap between horizontal stripes. Therefore, the mask-based model should be superior to the previous part-based method depending on fixed horizontal stripes. Results on OU-MVLP. Tab. 2 lists the rank-1 accuracy of our model and other state-of-the-art methods on OU-MVLP. Our method achieves the best performance of 90.9% on cross-view gait recognition. It noticed that 3DLocal obtains equal accuracy to our method when the invalid probe sequences are included, but it only demonstrates its performance of 96.5% if the invalid probe sequences are excluded. Our method achieves 97.5% rank-1 accuracy, a much better result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation study</head><p>Masking strategies. We study five variants of mask sampling strategies. The illustration and implements are detailed in supplementary materials. Those variants of sampling strategies achieve comparable performance, while different sampling strategies impact a lot on performance in other visual recognition tasks. We analyze that other visual tasks can take advantage of RGB modality, while silhouette-based gait recognition tends to capture static features and motion patterns from low-informative data.  Impact of masked area ratio p. As <ref type="figure" target="#fig_0">Fig. 4</ref> shows, we found the stacked structure of DropBlock with the harsh setting of feature dropping can result in performance degradation. To our best knowledge, these results can be explained in two ways. (1) The harsh regularization changes data distribution dramatically; <ref type="bibr" target="#b10">(2)</ref> The feature dropping in the shallow layer provides the incomplete feature map for the following layer, obstructing the following convolutions to learn representations.</p><p>Benefit of ReverseMask. According to the analysis of performance degradation when harshly dropping feature map, we design ReverseMask Feature Extraction, which compensates the incomplete feature by its reversal masked feature. Albeit, the ReverseMask is simple, our experiments in <ref type="figure" target="#fig_0">Fig. 4(b)</ref> show that it is effective to alleviate the issue of performance degradation led by conventional DropBlock.</p><p>Mask-based vs. Part-based. Our proposed mask-based regularizer can resemble part-based when the mask is constant instead of randomly generated.</p><p>To illustrate the transition from mask-based model to conventional part-based model, we demonstrate such transition in the supplementary materials. The results in Tab. 3 indicate that the mask-based model can outperform the partbased model by utilizing the information at the boundary, which is neglected by part-based methods. Feature scaling vs. Feature dropping. As shown in <ref type="figure" target="#fig_0">Fig.4(a)</ref>, the curve reveals two significant phenomenon: (1) Although the trends of performance are the same for both scaling and dropping regularization, the model performs robust in the condition of clothing setting when dropping regularization is applied. <ref type="formula" target="#formula_2">(2)</ref> The model with scaling regularization obtains better accuracy on both normal and bag-carrying conditions. In the literature <ref type="bibr" target="#b31">[23,</ref><ref type="bibr" target="#b51">43]</ref>, gait structural representations contribute to distinguish subjects especially for normal and bag-carrying conditions, since the appearance information are relatively complete in these two conditions. It conducts that feature scaling regularizes model to learn structural representations which is robust on the conditions of normal wearing and bagcarrying, and feature dropping regularization enforces the networks to look at fine-grained features which is robust on the clothing condition.</p><p>Variants of Building Block. Two variants of blocks are built for evaluating proposed ReverseMask regularization method. As mentioned previously, Plain ReverseMask Block although has not improved performance to baseline model, but it helps model alleviate the drawback of DropBlock and prevents network from performance degradation. Besides, Scaling Plain ReverseMask Block achieves better performance than Dropping Plain ReverseMask Block. Notably, the most significant improvement is obtained when Inception-like ReverseMask Block is used to aggregate global, structural, and local representations. In addition, we think that the Plain ReverseMask Block is still with potential superiority to baseline model since the reg prob set to one. In other words, it means this regularization configuration only supervises model to fix scaling data without any original silhouettes or features.</p><p>Extend to other methods. The proposed Inception-like ReverseMask Block is not only effective, but also plug-and-play. The ReverseMask regularization can generalize to the majority of gait methods, We study the extensive ReverseMask to three representative models. In Tab.4, all models gain performance improvement after integrating ReverseMask regularizer to enhance discriminativeness of representations. We can observe that the model based on the Conv3D can gain relatively higher improvements. We analysis it is because Conv3D enhances models ability by superiority spatio-temporal representations learning. Comparison to other regularization techniques. We compare Reverse-Mask to random erasing and DropBlock, which are two commonly used regularization techniques. In Tab. 5, Inception-like ReverseMask Block has better performance than not only feature-level but also input-level erasing regularization methods. Besides, we train baseline model with Inception-like ReverseMask Block and random erasing, and it achieves the best performance which shows our proposed feature-level regularization method can complementary to other regularization techniques. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel mask-based regularization method called Re-verseMask for obtaining better generalization performance, while it also vanishes the problems of overfitting and boundary existing in previous methods. To generalize the ReverseMask to the majority of works, we present the Inception-like ReverseMask Block which is able to capture more discriminative representations. In particular, feature scaling branch tends to extract structural information from silhouettes appearance, and feature dropping effectively utilizes local representations. Extensive experiments verify that the proposed regularization method achieves appealing performance on the CASIA-B and OU-MVLP. The Reverse-Mask feature is potentially extended to other tasks as an effective regularizer for better generalization of model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(Fig. 4 .</head><label>4</label><figDesc>b) DropBlock (DB) vs. ReverseMask (RM) area ratio p area ratio p (a) DropBlock (DB) vs. Scaling DropBlock (SDB) Performance comparison with different area ratio for sampling selected masking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>MT configuration split CASIA-B to a training set with 62 subjects and a test set with 62 subjects. LT configuration has 74 subjects for training and 50 subjects for testing. Each subject's first four sequences (NM#01-NM#04) are put into the gallery set in the test phase. The remaining two sequences of NM, BG, and CL are in three different probe sets respectively to evaluate the robustness to different variations.OU-MVLP<ref type="bibr" target="#b43">[35]</ref> was created by Osaka University and is the largest public gait database. It contains 10307 subjects, and each subject walks twice under 14 views. So, there are 2 ? 14 = 28 sequences for each subject. In our experiments, we follow the same protocol used in</figDesc><table /><note>which is widely employed by many other methods. Our experiments are conducted on three different configurations: Small-scale Training (ST), Medium-scale Training (MT), and Large-scale Training (LT). CASIA-B is split into a training set with 24 subjects and a test set with 100 subjects in ST configuration.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Rank-1 accuracy (%) on CASIA-B under all view angles, different settings, and conditions, excluding identical-view cases. ? 18 ? 36 ? 54 ? 72 ? 90 ? 108 ? 126 ? 144 ? 162 ? 180 ? Mean</figDesc><table><row><cell>Gallery NM#1-4</cell><cell>0 ? -180 ?</cell></row><row><cell>Probe</cell><cell>0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Rank-1 accuracy (%) on OUMVLP dataset under different view angles, excluding identical-view cases. The top eight rows and bottom six rows show the results with and without invalid probe sequences, respectively. ? 15 ? 30 ? 45 ? 60 ? 75 ? 90 ? 180 ? 195 ? 210 ? 225 ? 240 ? 255 ? 270 ?</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Probe View</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="16">Method 0 GaitSet [4] 79.3 87.9 90.0 90.1 88.0 88.7 87.7 81.8 86.5 89.0 89.2 87.2 87.6 86.2 87.1 Mean</cell></row><row><cell cols="16">GaitPart [8] 82.6 88.9 90.8 91.0 89.7 89.9 89.5 85.2 88.1 90.0 90.1 89.0 89.1 88.2 88.7</cell></row><row><cell>GLN [15]</cell><cell cols="15">83.8 90.0 91.0 91.2 90.3 90.0 89.4 85.3 89.1 90.5 90.6 89.6 89.3 88.5 89.2</cell></row><row><cell cols="16">GaitKMM [41] 56.2 73.7 81.4 82.0 78.4 78.0 76.5 60.2 72.0 79.8 80.2 76.7 76.3 73.9 74.7</cell></row><row><cell cols="16">GaitGL [27] 84.9 90.2 91.1 91.5 91.1 90.8 90.3 88.5 88.6 90.3 90.4 89.6 89.5 88.8 89.7</cell></row><row><cell cols="16">CSTL [16] 87.1 91.0 91.5 91.8 90.6 90.8 90.6 89.4 90.2 90.5 90.7 89.8 90.0 89.4 90.2</cell></row><row><cell cols="16">3DLocal [17] 86.1 91.2 92.6 92.9 92.2 91.3 91.1 86.9 90.8 92.2 92.3 91.3 91.1 90.2 90.9</cell></row><row><cell>Ours</cell><cell cols="15">87.9 91.5 91.7 92.0 92.0 91.6 91.3 90.7 90.3 90.9 91.1 90.8 90.5 90.2 90.9</cell></row><row><cell cols="16">GaitSet [4] 84.5 93.3 96.7 96.6 93.5 95.3 94.2 87.0 92.5 96.0 96.0 93.0 94.3 92.7 93.3</cell></row><row><cell cols="16">GaitPart [8] 88.0 94.7 97.7 97.6 95.5 96.6 96.2 90.6 94.2 97.2 97.1 95.1 96.0 95.0 95.1</cell></row><row><cell>GLN [15]</cell><cell cols="15">89.3 95.8 97.9 97.8 96.0 96.7 96.1 90.7 95.3 97.7 97.5 95.7 96.2 95.3 95.6</cell></row><row><cell cols="16">GaitGL [27] 90.5 96.1 98.0 98.1 97.0 97.6 97.1 94.2 94.9 97.4 97.4 95.7 96.5 95.7 96.2</cell></row><row><cell>3DLocal [17]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>96.5</cell></row><row><cell>Ours</cell><cell cols="15">93.7 97.5 98.6 98.8 98.0 98.5 98.2 96.5 96.7 98.2 98.1 97.1 97.6 97.2 97.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Gait recognition performance reported in rank-1 accuracy on CASIA-B.</figDesc><table><row><cell>Model</cell><cell>Dim</cell><cell>NM BG CL Mean</cell></row><row><cell>Baseline [27]</cell><cell>4096</cell><cell>97.0 93.9 83.3 91.4</cell></row><row><cell>Baseline + part-based [27]</cell><cell cols="2">4096 ? 2 97.4 94.5 83.8 92.0</cell></row><row><cell cols="3">Baseline + Inception RMB(w/o dropping branch) 4096 ? 2 97.6 94.8 85.1 92.5</cell></row><row><cell cols="3">Baseline + Inception RMB(w/o scaling branch) 4096 ? 2 97.5 94.7 85.2 92.5</cell></row><row><cell>Baseline + Inception RMB(w/o global branch)</cell><cell cols="2">4096 ? 2 97.6 95.0 85.1 92.6</cell></row><row><cell>Baseline + Inception RMB</cell><cell cols="2">4096 ? 3 97.7 95.3 86.0 93.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Effectiveness of ReverseMask regularization. To be noticed that the results of GaitSet are reproduced by<ref type="bibr" target="#b15">[7]</ref>.</figDesc><table><row><cell>Model</cell><cell>NM</cell><cell>BG</cell><cell>CL</cell><cell>Mean</cell></row><row><cell>OpenGait</cell><cell>96.3</cell><cell>92.2</cell><cell>77.6</cell><cell>88.7</cell></row><row><cell cols="5">OpenGait + Inception-like ReverseMask Block 97.0 (0.7) 92.2 (0.0) 79.4 (1.8) 89.5 (0.8)</cell></row><row><cell>GaitSet</cell><cell>95.9</cell><cell>91.3</cell><cell>74.8</cell><cell>87.3</cell></row><row><cell cols="5">GaitSet + Inception-like ReverseMask Block 96.1 (0.2) 91.3 (0.0) 76.3 (1.5) 87.9 (0.6)</cell></row><row><cell>Baseline</cell><cell>97.0</cell><cell>93.9</cell><cell>83.3</cell><cell>91.4</cell></row><row><cell cols="5">Baseline + Inception-like ReverseMask Block 97.7 (0.7) 95.3 (1.4) 86.0 (2.7) 93.0 (1.6)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Performance of different regularization techniques. RMB donates Reverse-Mask Block for short. While Scaling DropBlock is a DropBlock-like regularization, Scaling DropBlock scales feature map rather than dropping. Inception RMB(reg prob=1) + Random Erasing(reg prob=0.5) 98.1 96.0 86.9 93.7</figDesc><table><row><cell>Model</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaitgl</surname></persName>
		</author>
		<idno>27] 77.0 87.8 93.9 92.7 83.9 78.7 84.7 91.5 92.5 89.3 74.4 86.0</idno>
		<imprint/>
	</monogr>
	<note>Ours 78.1 89.2 95.3 92.6 83.9 79.7 85.2 91.8 93.2 89.6 73.9 86.6</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaitgl</surname></persName>
		</author>
		<idno>27] 68.1 81.2 87.7 84.9 76.3 70.5 76.1 84</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaitgl</surname></persName>
		</author>
		<idno>27] 46.9 58.7 66.6 65.4 58.3 54.1 59.5 62.7 61.3 57.1 40.6 57.4 Ours 50.2 65.4 70.8 69.0 63.0 58.0 63.3 67.6 66.2 61.6 43.2 61.7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaitgl</surname></persName>
		</author>
		<idno>27] 93.9</idno>
		<imprint/>
	</monogr>
	<note>6 98.8 97.3 95.2 92.7 95.6 98.1 98.5 96.5 91.2 95.9</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaitgl</surname></persName>
		</author>
		<idno>27] 88.5 95.1 95.9</idno>
		<imprint/>
	</monogr>
	<note>2 91.5 85.4 89.0 95.4 97.4 94.3 86.3 92.1 Ours 89.6 95.5 96.8 95.5 92.2 87.0 90.9 95.5 98.2 94.6 87.1 93.0</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaitgl</surname></persName>
		</author>
		<idno>27] 70.7 83.2 87.1 84.7 78.2 71.3 78.0 83.7 83.6 77.1 63.1 78.3 Ours 73.1 85.9 90.6 88.4 80.6 75.5 81.5 86.5 87.4 81.4 66.5 81.6</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaitgl</surname></persName>
		</author>
		<idno>27] 96.0 98.3 99.0 97.9 96.9 95.4 97.0 98.9 99.3 98.8 94.0 97.4 3DLocal [17] 96.0 99.0 99.5 98.9</idno>
		<imprint/>
	</monogr>
	<note>1 94.2 96.3 99.0 98.8 98.5 95.2 97.5 Ours 96.5 98.4 99.2 98.0 97.1 95.5 97.4 99.2 99.3 99.1 95.0 97</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaitpart</surname></persName>
		</author>
		<idno>8] 70.7 85.5 86.9 83.3 77.1 72.5 76.9 82.2 83.8 80.2 66.5 78.7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaitgl</surname></persName>
		</author>
		<idno>27] 76.6 90.0 90.3 87.1 84.5 79.0 84.1 87.0 87.3 84.4 69.5 83.6 3DLocal [17] 78.2 90.2 92.0 87.1 83.0 76.8 83.1</idno>
		<imprint>
			<biblScope unit="page">86</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<title level="m">Youtube-8m: A large-scale video classification benchmark. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Liao</surname></persName>
		</author>
		<title level="m">Yolov4: Optimal speed and accuracy of object detection. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Gaitset: Regarding gait as a set for cross-view gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Batch dropblock network for person re-identification and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">Opengait: A strong baseline and bag of tricks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Gaitpart: Temporal part-based model for gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">New area based metrics for gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prugel-Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Audio-and Video-Based Biometric Person Authentication</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Horizontal pyramid matching for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dropblock: A regularization method for convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Individual recognition using gait energy image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhanu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Masked autoencoders are scalable vision learners</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person reidentification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Gait lateral network: Learning discriminative and compact representations for gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Context-sensitive temporal feature learning for gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<title level="m">3d local convolutional neural networks for gait recognition</title>
		<imprint>
			<publisher>ICCV</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The ou-isir gait database comprising the large population dataset and performance evaluation of gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Iwama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okumura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>TIFS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Largescale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">End-to-end model-based gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Pose-based temporal-spatial network (ptsn) for gait recognition with carrying and clothing variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CCBR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Gaitmask: Mask-based model for gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Gait recognition with multiple-temporal-scale 3d convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ACM MM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Gait recognition via effective global-local feature representation and local temporal aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Toward understanding the limits of gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Malave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osuntogun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sudhakar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometric Technology for Human Identification</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<title level="m">Rectifier nonlinearities improve neural network acoustic models</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">ICML</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cloth invariant gait recognition using pooled segmented statistical features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nandy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Human identification based on gait</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Fine-tuning cnn image retrieval with no human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Geinet: Viewinvariant gait recognition using a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shiraga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Muramatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Echigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICB</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multi-view large population gait dataset and its performance evaluation for cross-view gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takemura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Muramatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Echigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCVA</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Beyond dropout: Feature map distortion to regularize deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Multi-view gait recognition using 3d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICIP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">A comprehensive study on crossview gait based human identification with deep cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">A framework for evaluating the effect of view angle, clothing and carrying condition on gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>ICPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Cross-view gait recognition with deep universal linear embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">A comprehensive study on gait biometrics using a joint cnn-based method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>PR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">On learning disentangled representations for gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Gait recognition via disentangled representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Atoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Omni-scale feature learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
