<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TFPose: Direct Human Pose Estimation with Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weian</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide ? Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongtao</forename><surname>Ge</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide ? Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide ? Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide ? Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide ? Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide ? Alibaba Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TFPose: Direct Human Pose Estimation with Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code is available at: https://git.io/AdelaiDet</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a human pose estimation framework that solves the task in the regression-based fashion. Unlike previous regression-based methods, which often fall behind those state-of-the-art methods, we formulate the pose estimation task into a sequence prediction problem that can effectively be solved by transformers. Our framework is simple and direct, bypassing the drawbacks of the heatmapbased pose estimation. Moreover, with the attention mechanism in transformers, our proposed framework is able to adaptively attend to the features most relevant to the target keypoints, which largely overcomes the feature misalignment issue of previous regression-based methods and considerably improves the performance. Importantly, our framework can inherently take advantages of the structured relationship between keypoints. Experiments on the MS-COCO and MPII datasets demonstrate that our method can significantly improve the state-of-the-art of regressionbased pose estimation and perform comparably with the best heatmap-based pose estimation methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human pose estimation requires the computer to obtain the human keypoints of interest in an input image and plays an important role in many computer vision tasks such as human behavior understanding.</p><p>Existing mainstream methods solving the task can be generally categorized into heatmap-based ( <ref type="figure" target="#fig_0">Figure 1 top)</ref> and regression-based methods ( <ref type="figure" target="#fig_0">Figure. 1 bottom)</ref>. Heatmapbased methods often first predict a heatmap or a classification score map with fully convolutional networks (FCNs), and then the body joints are located by the peak's locations in the heatmap or the score map. Most pose estimation methods are heatmap-based because it has relatively higher accuracy. However, the heatmap-based methods may suffer the following issues. 1) A post-processing (e.g., the "taking-maximum" operation) is needed. The * First two authors contributed equally. CS is the corresponding author. post-processing might not be differentiable, making the framework not end-to-end trainable. 2) The resolution of heatmaps predicted by the FCNs is usually lower than the resolution of the input image. The reduced resolution results in a quantization error and limits the precision of the keypoint's localization. This quantization error might be solved by shifting the output coordinates according to the value of the pixels near the peak, but it makes the framework much more complicated and introduces more hyperparameters.</p><formula xml:id="formula_0">?? (X 1 ,Y 1 ) (X 2 ,Y 2 ) (X K-1 ,Y K-1 ) (X K ,Y K ) ?? (X 1 ,Y 1 ) (X 2 ,Y 2 ) (X K-1 ,Y K-1 ) (X K ,Y K ) (a) Heatmap-based method ?? (X 1 ,Y 1 ) (X 2 ,Y 2 ) (X K-1 ,Y K-1 ) (X K ,Y K ) ?? (X 1 ,Y 1 ) (X 2 ,Y 2 ) (X K-1 ,Y K-1 ) (X K ,Y K ) (b) Regression-based method</formula><p>3) The ground truth heatmaps need to be manually designed and heuristically tuned, which might cause many noises and ambiguities contained in the ground-truth maps, as show in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>In contrast, the regression-based methods usually directly map the input image to the coordinates of body joints with a FC (fully-connected) prediction layer, eliminating the need for heatmaps. The pipeline of regression-based methods is much more straightforward than heatmap-based methods as in principle pose estimation is a kind of regression tasks such as object detection. Moreover, the regssionbased method can bypass the aforementioned drawbacks of heatmap-based methods, thus being more promising.</p><p>However, there are only a few research works focusing on regression-based methods because regression-based methods often have inferior performance to heatmap-based methods. The reasons may be four-fold. First, in order to reduce the network parameters in the FC layer, in the Deep-Pose <ref type="bibr" target="#b45">[46]</ref>, a global average pooling is applied to reduce the feature map resolution before the FC layer. This global average pooling destroys the spatial structure of the convolutional feature maps, and significantly deteriorates the performance. Next, as shown in DirectPose <ref type="bibr" target="#b43">[44]</ref> and SPM <ref type="bibr" target="#b34">[35]</ref>, in regression-based methods, the convolutional features and predictions are misaligned, which results in low localization precsion of the keypoints. Moreover, regression-based methods only regress the coordinates of body joints and does not take account of the structured dependency between these keypoints <ref type="bibr" target="#b40">[41]</ref>.</p><p>Recently, we have witnessed the rise of vision transformers <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b53">54]</ref>. The transformers are originally designed for the sequence-to-sequence tasks, which inspires us to formulate the single person pose estimation to the problem of predicting K-length sequential coordinates, where K is the number of body joints for one person. This leads to a simple and novel regression-based pose estimation framework, termed TFPose (i.e., Transformer-based Pose Estimation). As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, taking as inputs the feature maps of CNNs, the transformer sequentially predict K coordinates. TFPose can bypass the aforementioned difficulties of regression-based methods. First, it does not need the global average pooling as in DeepPose <ref type="bibr" target="#b45">[46]</ref>. Second, due to the multi-head attention mechanism, our method can avoid the feature misalignment between the convolutional features and predictions. Third, since we predict the keypoints in the sequential way, the transformer can naturally capture the structured dependency between the keypoints, resulting in improved performance.</p><p>We summarize the main contributions as follows.</p><p>? TFPose is the first transformer-based pose estimation framework. Our proposed framework adapts to the simple and straightforward regression-based methods, which is end-to-end trainable and can overcome many drawbacks of the heatmap-based methods.</p><p>? Moreover, our TFPose can naturally learn to exploit the structured dependency between the keypoints without heuristic designs, e.g., in <ref type="bibr" target="#b40">[41]</ref>. This results in improved performance and better interpretability.</p><p>? TFPose achieves greatly advance the state-of-the-art of regression-based methods, making the regressionbased methods comparable to the state-of-the-art heatmap-based ones. For example, we improve the previously best regression-based method Sun et al. <ref type="bibr" target="#b41">[42]</ref> by 4.4% AP on the COCO keypoint detection task, and Aiden et al. <ref type="bibr" target="#b33">[34]</ref> by 0.9% PCK on the MPII benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Transformers in computer vision. After being proposed in <ref type="bibr" target="#b46">[47]</ref>, Transformers have achieved significant progress in NLP (Natural Language Processing) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref>. Recently, Transformers have also attracted much attention in computer vision community. For basic image classification task, ViT <ref type="bibr" target="#b14">[15]</ref> apply a pure Transformer to sequential image patches. Expect for image classification, vision Transformer is also widely applied to object detection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b53">54]</ref>, segmentation <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref>, pose estimation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28]</ref>, low-level vision task <ref type="bibr" target="#b7">[8]</ref>. More details, we refer to <ref type="bibr" target="#b15">[16]</ref>. Specially, DETR <ref type="bibr" target="#b5">[6]</ref> and Deformable DETR <ref type="bibr" target="#b53">[54]</ref> formulate the object detection task to predict a box set so that object detection model can be trained end-to-end; the Transformer applications in both 3D Hand Pose Estimation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> and 3D human pose estimation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> show that Transformer is suitable for modeling human pose.</p><p>Heatmap-based 2D pose estimation. Heatmap-based 2D pose estimation methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b50">51]</ref> perform the state-of-the-art accuracy in 2D human pose estimation. Recently, most work, including both top-down and bottom up, are heatmap-based methods. <ref type="bibr" target="#b32">[33]</ref> firstly propose hourglass-style framework and hourglass-style framework also be widely applied, such as, <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref>. <ref type="bibr" target="#b39">[40]</ref> propose a novel network architecture for heatmap-based 2D pose estimation and achieve a excellent performance. <ref type="bibr" target="#b9">[10]</ref> propose a new bottom-up method achieve impressive performance in CrowdPose dataset <ref type="bibr" target="#b24">[25]</ref> and improved by <ref type="bibr" target="#b30">[31]</ref>. <ref type="bibr" target="#b3">[4]</ref> propose a efficient network achieving the the-state-of-art performance in COCO keypoint detection dataset <ref type="bibr" target="#b28">[29]</ref>. However, <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b43">44]</ref> argue that heatmap-based methods cannot be trained end-toend, due to the "taking-maximum" operation. Recently, the noise and ambiguity in the ground truth heatmap are found by <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b40">41]</ref>. <ref type="bibr" target="#b20">[21]</ref> finds the heatmap data processing applied by most previous work is biased and proposed an new unbiased data processing method.</p><p>Regression-based 2D pose estimation. 2D human pose estiamtion is naturally a regression problem <ref type="bibr" target="#b41">[42]</ref>. However, regression based methods are not accurate as well as heatmap-based methods, thus there are just a few works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46]</ref> for it. Apart from that, although some methods, such as G-RMI <ref type="bibr" target="#b35">[36]</ref>, apply regression method to reduce the quantization errors casued by heatmap, they are essentially heatmap-based methods. There are some work point out the reason of the bad performance of regressionbased method. Directpose <ref type="bibr" target="#b43">[44]</ref> points out the feature misalignment issue and propose a mechanism to align the feature and the predictions; <ref type="bibr" target="#b40">[41]</ref> indicates regression-based method cannot learn the structure-aware information well and proposal a hand-design model for pose estimation to Then, we pass the output embedding of the decoder to a multi-layer feed forward network that predicts final keypoint coordinates.</p><p>force regression-based method learn the structure-aware information better; Sun et al. <ref type="bibr" target="#b41">[42]</ref> propose integral regression, which shares the merits of both heatmap representation and regression approaches, to avoid non-differentiable postprocessing and quantization error issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">TFPose Architecture</head><p>This work focus on the single pose estimation task. Following previous works, we first apply a person detector to obtain the bounding boxes of persons. Then, according to the detected boxes, each person is cropped from the input image. We denote the cropped image by I ? R h?w?3 , where h, w is the height and the width of the image, respectively. With the cropped image with a single person, the previous heatmap-based methods apply a convolutional neural network F to the patch to predict keypoint heatmaps H ? R h?w?k (H k for k th joint) of this person, where k is the number of the predicted keypoint. Formally, we have H = F(I).</p><p>(1)</p><p>Each pixel of H represents the probability that the body joints locate at this pixel. To obtain the joints' coordinates J ? R 2?k (J k for k th joint), those methods usually use the "taking-maximum" operation to obtain the locations with peak activations. Formally, let p be the spatial locations on H, and it can be formulated as</p><formula xml:id="formula_1">J k = arg max p (H k (p)).<label>(2)</label></formula><p>Note that in the heatmap-based methods, the localization precision of p is up to the resolution of H, which is often much lower than the resolution of the input and thus causes the quantization errors. Moreover, the arg max operation here is not differential, making the pipeline not end-to-end trainable. In TFPose, we instead treat J as a K-length se-quence and directly map the input I to the body joints' coordinates J. Formally,</p><formula xml:id="formula_2">J = F(I),<label>(3)</label></formula><p>where F is composed of three main components: a standard CNN backbone to extract multi-level feature representations, a feature encoder to capture and fuse multi-level features and a coarse-to-fine decoder to generate the a sequence of keypoint coordinates. It is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Note that our TFPose is fully differentiable and the localization precision is not limited by the resolution of the feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Transformer Encoder</head><p>As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, the backbone extracts multi-level features of the input image. The multi-level feature maps represents the level embeddings depicting which level a feature vector comes from. E P represents the pixel embedding depicting the spatial location of a feature vector on the feature maps. We use F E 0 to denote F0 with position embedding. Following <ref type="bibr" target="#b53">[54]</ref>, both F0 and F E 0 are the inputs of the transformer.</p><formula xml:id="formula_3">+ + + + ?? Flatten Position embedding C2 C3 C4 C5 E p E l 2 E l 3 E l 4 E l 5 ?? Flatten F 0 F E 0</formula><p>are denoted by C 2 , C 3 , C 4 and C 5 , respectively, whose strides are 4, 8, 16 and 32, respectively. We separately apply a 1 ? 1 convolution to these feature maps so that they have the same number of the output channels. These feature maps are flatten and concatenated together, which results in the input F 0 ? R n?c to the first encoder in the transformer , where n is the number of the pixel in the F 0 . Here, we use F i denotes the output to the i-th encoder in the transformer. Following <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b53">54]</ref>, F 0 is added with the positional embeddings and we denote F 0 with the positional embeddings by F E 0 . The details of the positional embeddings will be discussed in Section 3.2. Afterwards, both F 0 and F E 0 are sent to the transformer to compute the memory M ? R n?c . With the memory M , a query matrix Q ? R K?c will be used in the transformer decoder to obtain the K body joints' coordinates J ? R K?2 .</p><p>We follow Deformable DETR <ref type="bibr" target="#b53">[54]</ref> to design the encoder in our transformer. As mentioned before, before F 0 is taken as inputs, each feature vector of F 0 is added with the positional embeddings. Following Deformable DETR, we use both level embedding E L l ? R 1?c and pixel position embeddings E P ? R n?c . The former encodes the level where the feature vector comes from, and the latter is the feature vector's spatial location on the feature maps. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, all the feature vectors from level l are added with E L l and then the feature vectors are added with their pixel position embeddings E p , where E p is the 2-D cosine positional embeddings corresponding to the 2-D location of the feature vector on the feature maps.</p><p>In TFPose, we use N E = 6 encoder layers. For e th encoder layer, as shown in <ref type="figure" target="#fig_4">Figure 4</ref>, the previous encoder layer's outputs will be taken as the input of this layer. Following Deformable DETR, we also compute the pixel-topixel attention between the output vectors of each encoder layer (denoted by 'p2p attention'). After N E transformer encoder layers are applied, we can obtain the memory M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Transformer Decoder</head><p>In the decoder, we aim to decode the desired keypoint coordinates from the memory M . As mentioned before, we use a query matrix Q ? R K?c to achieve this. Q is essentially an extra learnable matrix, which is jointly updated with the model parameters during training and each row of which corresponds to a keypoint. In TFPose, we have N D transformer decoder layers. As shown in <ref type="figure" target="#fig_4">Figure 4</ref>, each decoder layer takes as input the memory M and the outputs of the previous decoder layer Q d?1 ? R K?c . The first layer takes as inputs M and the matrix Q. Similarly, Q d?1 is added with the positional embeddings. The result is denoted by Q E d?1 . The Q d?1 and Q E d?1 will be sent to the query-toquery attention module (denoted as 'q2q attention'), which aims to model the dependency between human body joints. The q2q attention module use Q d?1 , Q E d?1 and Q E d?1 as val-ues, queries and keys, respectively. Later, the output of the q2q attention module and M used to compute the pixel-toquery attention (denoted as 'p2q attention') with the value being the former and query being the latter. Then, an MLP will be applied to the output of p2q attention the output of the decoder Q d . The keypoint coordinates are predicted by applying an MLP with output channels being 2 to each row of Q d . Instead of simply predicting the keypoint coordinates in the final decoder layer, inspired by <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b53">54]</ref>, we require all the decoder layers to predict the keypoint coordinates. Specifically, we let the first decoder layer directly predict the target coordinates. Then, every other decoder layer refines the predictions of its previous decoder layer by predicting refinements ?? d ? R K?2 . In that way, the keypoint coordinates can be progressively refined. Formally, let? d?1 be the keypoint coordinates predicted by the (d ? 1)-th decoder layer, the predictions of the d-th decoder layer ar?</p><formula xml:id="formula_4">y d = ?(? ?1 (? d?1 ) + ?? d ),<label>(4)</label></formula><p>where ? and ? ?1 denote the sigmoid and inverse sigmoid function, respectively.? 0 is a randomly-initialized matrix and jointly updated with model parameters during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Targets and Loss Functions</head><p>The loss functions of TFPose consist of two parts. The first part is the L 1 regression loss. Let y ? R K?2 be the ground-truth coordinates. The regression loss is formulated as,  </p><formula xml:id="formula_5">L reg = N D d=1 ||y ?? d ||,<label>(5)</label></formula><p>where N D is the number of the decoders, and every decoder layer is supervised with the target keypoint coordinates. The second part is an auxiliary loss L aux . Following DirectPose <ref type="bibr" target="#b43">[44]</ref>, we use the auxiliary heatmap learning during training 1 , which can result in better performance. In order to use the heatmap leanrning, we gather the feature vectors that were C 5 from M and reshape these vectors into the original spatial shape. The result is denoted by M C5 ? R (h/32)?(w/32)?c . Similar to simple baseline <ref type="bibr" target="#b50">[51]</ref>, we apply 3? deconvolution to M C5 to upsample the feature maps by 8 and generate the heatmap H ? R (h/4)?(w/4)?K . Then, we compute the mean square error (MSE) loss between the predicted and ground-truth heatmaps. The ground-truth heatmaps are generated by following <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b51">52]</ref>. Formally, the auxiliary loss function is</p><formula xml:id="formula_6">L aux = ||H ??|| 2 ,<label>(6)</label></formula><p>We sum the two loss functions to obtain the final overall loss</p><formula xml:id="formula_7">L overall = L reg + ?L aux ,<label>(7)</label></formula><p>where ? is a constant and used to balance the two losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details.</head><p>Datasets. We conduct a number of ablation experiments on two mainstream pose estimation datasets. Our experiments are mainly conducted on COCO2017 Keypoint Detection <ref type="bibr" target="#b49">[50]</ref> benchmark, which contains about 250K person instances with 17 keypoints. Following common settings, we use the same person detector in Simple Baseline <ref type="bibr" target="#b51">[52]</ref> for COCO evaluation. We report results on the val set for ablation studies and compare with other state-of-the-art methods on the test-dev set. The Average Precision (AP) based on Object Keypoint Similarity (OKS) is employed as the evaluation metric.</p><p>Besides COCO dataset, we also report results on MPII dataset <ref type="bibr" target="#b0">[1]</ref>. MPII is a popular benchmark for single person 2D pose estimation, which has 25K images. In total, there are 29K annotated poses for training, and another 7K poses for testing. The Percentage of Correct Keypoints (PCK) metric is used for evaluation. Model settings. Unless specified, ResNet-18 <ref type="bibr" target="#b17">[18]</ref> is used as the backbone in ablation study. The size of input image is 256 ? 192 or 384 ? 288. The weights pre-trained on Ima-geNet <ref type="bibr" target="#b12">[13]</ref> are used to initialize the ResNet backbone. The <ref type="bibr" target="#b0">1</ref> The heatmap branch is removed in inference.  rest parts of our network are initialized with random parameters. For the Transformer, we adopt Defermable Attention Module proposed in <ref type="bibr" target="#b53">[54]</ref> and the same hyper-parameters are used.</p><p>Training. All the models are optimized by AdamW <ref type="bibr" target="#b29">[30]</ref> with a base learning rate of 4 ? 10 ?3 . ? 1 and ? 2 are set to 0.9 and 0.999. Weight decay is set to 10 ?4 . ? is set to 50 by default for balancing the regression loss and auxiliary loss. Unless specified, all the experiments use a cosine learning schedule with base learning rate 4 ? 10 ?3 . Learning rate of the Transformers and the linear projections for predicting keypoints offsets is decreased by a factor of 10. For data augmentation, random rotation ([?40 ? , 40 ? ]), random scale ([0.5, 1.5]), flipping and half body data augmentation <ref type="bibr" target="#b49">[50]</ref> are applied. For auxiliary loss, we follow Unbiased Data Processing (UDP) <ref type="bibr" target="#b20">[21]</ref> to generate unbiased ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>Query-to-query attention. In the proposed TFPose, queryto-query attention is designed to capture structure-aware information cross all the keypoints. Unlike <ref type="bibr" target="#b40">[41]</ref> which uses a hand-design method to explicitly force the model to learn the structure-aware information, query-to-query attention models human body structure implicitly. To study the effect of query-to-query attention, we report the results of removing the query-to-query attention in all decoder layers. As shown in <ref type="table">Table 1</ref>, the proposed query-to-query attention improve the performance by 1.3% AP with only 0.  <ref type="table">Table 3</ref> -Ablation study of different numbers of decoder layers on COCO val2017 set. ND is the number of decoder layers used for refining the location of key-points. "GFLOPs" indicates the computational cost. In this experiment, we set the number of transformer encoder layers: NE = 6.</p><p>of the input features and the number of decoder layers in Transformer decoder. As shown in <ref type="table">Table 2</ref>, Transformers with 256-channel feature maps is 1.3% AP higher than 128-channels ones. Moreover, we change the number of decoder layers. As shown in <ref type="table">Table 3</ref>, the performance grows at the first three layers and saturates at the fourth decoder layer. Auxiliary loss. As shown in previous works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b53">54]</ref>, the transformer modules may converge slower. To mitigate this issue, we adopt the deformable attention module proposed in <ref type="bibr" target="#b53">[54]</ref>. Apart from that, we propose an auxiliary loss to accelerate the convergence speed of TFPose. Here, we investigate the effect of the auxiliary loss. In this experiment, the first model is only supervised by regression loss; the second model is supervised by both regression loss and auxiliary loss. As shown in <ref type="figure" target="#fig_6">Figure 5</ref> and <ref type="table">Table 4</ref>, the auxiliary loss can significantly accelerates the convergence speed of TF-Pose and boost the performance by a large margin (+2.3% AP).  <ref type="table">Table 4</ref> -Ablation study of effectiveness of auxiliary loss on COCO val2017 set. "aux" indicts whether using auxiliary loss. In this experiment, we set the number of transformer encoder layers: NE = 6, and transformer decoder layers: ND = 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discussions on TFPose</head><p>Visualization of sampling keypoints. To study how the Deformable Attention Module locate the body joints, we visualize the sampling locations of the module on the feature maps C 3 . In Deformable Attention Module, there are 8 attention heads and every head will sample 4 points on every feature map. So for the C 3 feature map, there are 32 sampling points. As shown in <ref type="figure" target="#fig_8">Figure 7</ref>, the sampling points (red dot) are all densely located nearby the ground truth (yellow circle). This visualization shows that TFPose can address the feature mis-alignment issue in a sense, and supervises the CNN with dense pixel information.</p><p>Visualization of query-to-query attention. To further study how the query-to-query self-attention module works, we visualize the attention weights of the query-to-query self-attention. As shown in <ref type="figure" target="#fig_7">Figure 6</ref>, there are two obvious patterns of attention: the first attention pattern is that the symmetric joints (e.g. left shoulder and right shoulder) are more likely to attend to each other, and the second attention pattern is that the adjacent joints (e.g. eyes, nose, and mouth) are more likely to attend to each other.</p><p>To have a better understanding of this attention pattern, we also visualize the attention graph between each keypoint according to the attention maps in the supplementary. This attention pattern suggests that TFPose can employ the context and structured relationship between the body joints to locate and classify the types of body joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with State-of-the-art Methods</head><p>In this section, we compare TFPose with previous stateof-the-art 2D pose estimation methods on COCO val split, COCO test-dev split and MPII <ref type="bibr" target="#b0">[1]</ref>. We compare these method in terms of both accuracy and computational cost. The results of our proposed TFPose and other state-of-theart methods are listed in <ref type="table">Table 5</ref>, <ref type="table">Table 6 and Table 7</ref>. Results on COCO val. set. As shown in <ref type="table">Table 5</ref>, with similar computational cost, TFPose with 4 encoder layers and ResNet-50 surpass the previous regression-based method DeepPose with ResNet-101 (70.5% AP vs. 56.0% AP) by a large margin and even has much better performance than DeepPose with ResNet-152 (70.5% AP vs. 58.3% AP). Besides, TFPose also outperform many heatmap-based methods, for example, 8-stage Hourglass <ref type="bibr" target="#b32">[33]</ref>(70.5% AP vs.   67.1% AP), CPN <ref type="bibr" target="#b8">[9]</ref>(70.5% AP vs. 69.4% AP) by a large margin. It is also important to note that TFPose with 4 encoder layers and ResNet-50 outperforms the strong baseline SimpleBaseline <ref type="bibr" target="#b51">[52]</ref> with ResNet-50 (70.5% AP vs. 70.4% AP) with lower computational cost (7.68 GFLOPs vs. 8.9 GFLOPs).</p><p>Results on COCO test-dev set. As shown in <ref type="table">Table 6</ref>, TF-Pose achieves the best result among regression-based methods. Especially, TFPose with 6 encoder layers and ResNet-50 achieves 70.9% AP, which is higher than the Int. Reg. <ref type="bibr" target="#b41">[42]</ref> (67.8% AP), and our computational cost is lower than Results on MPII test set. On the MPII benchmark, TF-Pose also achieves the best results among the regressionbased methods. As shown in <ref type="table">Table 7</ref>, TFPose with ResNet-50 is higher than the method proposed by Aiden et al. <ref type="bibr" target="#b33">[34]</ref> (90.4% vs. 89.5%) with the same backbone. TFPose is also comparable to heatmap-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed a novel pose estimation framework named TFPose built upon Transformers, which largely im-proves the performance of the regression-based pose estimation and bypasses the drawbacks of heatmap-based methods such as the non-differentiable post-processing and quantization error. We have shown that with the attention mechanism, TFPose can naturally capture the structured relationship between the body joints, resulting in improved performance. Extensive experiments on the MS-COCO and MPII benchmarks show that TFPose can achieve state-ofthe-art performance among regression-based methods and is comparable to the best heatmap-based methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Qualitative Results of TFPose</head><p>We show more qualitative results in <ref type="figure" target="#fig_0">Figure 10</ref>. TFPose works reliably under various challenging cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Visualization of Transformer Attentions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Query-to-query Attention</head><p>We observe two obvious query-to-query attention patterns in different decoder layers, termed symmetric pattern and adjacent pattern, respectively. Both patterns exist in all decoder layers, we illustrate them separately for convenience. For symmetric pattern, <ref type="figure">Figure 8</ref> demonstrates that the correlation between all pairs of symmetric joints in the third decoder layer. For adjacent pattern, <ref type="figure">Figure 9</ref> explicitly shows that adjacent joints attend to each other in the second decoder layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Multi-scale Deformable Attention</head><p>We visualize the learned multi-scale deformable attention modules for better understanding. As shown in <ref type="figure" target="#fig_0">Figure 11</ref> and <ref type="figure" target="#fig_0">Figure 12</ref>, the visualization indicates that TF-Pose looks at context information surround the ground truth joint. More concretely, the sampling points near the ground truth joint have higher attention weight (denoted as red), while the sampling points far from the ground truth joint own lower attention weight (denoted as blue).  <ref type="figure">Figure 8</ref> -The pattern of symmetric joints. As shown in the right graph, left shoulder and right shoulder are symmetric joints and they attend to each other. The same pattern can be found in other body joints including left elbow and right elbow, left hip and right hip etc.  <ref type="figure">Figure 9</ref> -The pattern of adjacent joints. As shown in the right graph, left shoulder attend to its adjacent joints including right shoulder, left elbow, and head. The same pattern can be found in other body joints, e.g.., elbow and wrist.  <ref type="figure" target="#fig_0">Figure 11</ref> -Visualization of right shoulder's pixel-to-query attention in the last decoder layer. For readability, we draw the sampling points and attention weights from C3 feature map in different pictures. Each sampling point is marked as a filled circle whose color indicates its corresponding weight. The ground truth joint is shown as yellow cross marker. <ref type="figure" target="#fig_0">Figure 12</ref> -Visualization of right knee's pixel-to-query attention in the last decoder layer. For readability, we draw the sampling points and attention weights from C3 feature map in different pictures. Each sampling point is marked as a filled circle whose color indicates its corresponding weight. The ground truth joint is shown as yellow cross marker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Low High</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 -</head><label>1</label><figDesc>Comparison of mainstream pose estimation pipelines. (a) Heatmap-based methods. (b) Regression-based methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 -</head><label>2</label><figDesc>Overall pipeline of TFPose. The model directly predicts a sequence of keypoint coordinates in parallel by combining a common CNN with a transformer architecture. A transformer decoder takes as input a fix number of keypoint queries and encoder output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 -</head><label>3</label><figDesc>Positional encoding. This figure illustrates the positional embeddings to the input F0 of the transformer. E l i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 -</head><label>4</label><figDesc>Transformer architecture. During training, deconvolution modules are used to upsamle transformer encoder output (MC 5 ) for for auxiliary loss. During testing, only output Transformer decoder. 'Norm' represent normalization; (Xi, Yi) represent the coordinate for i th keypoint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 -</head><label>5</label><figDesc>Convergence curves of TFPose superivsed by different kinds of losses on COCO val2017 set. "wo aux loss" indicates only regression loss is employed. "with aux loss" indicates both regression loss and auxiliary loss are employed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 -</head><label>6</label><figDesc>Visualization of the attention weights of the q2q attention. We average the attention maps over the whole COCO 2017val dataset. The left map is the attention weights of the second decoder layer. The right map is the attention weights of the third decoder layer. 'L' means the joints are in the left. 'R' means the joints are in the right. The horizontal axis and the vertical axis represent the input query and key of the attention module, respectively. Multi-head attention computes the attention weights between each pair of the queries and keys. The query attends more to the key with a higher attention weight.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 -</head><label>7</label><figDesc>Visualisation of the sampling point on feature map. There are 17 queries for 17 keypoints. We visualize 12 body joints queries (not including facial joints). Each image correspond to a body joints. Red dot represent the sampling point; yellow circle represent the ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 -</head><label>10</label><figDesc>Qualitative results of TFPose with ResNet-50 on COCO2017 val set (single-model and singe-scale testing). The joints in upper body are represented by green and the joints in lower body are represented by blue. Low High</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 -Table 2 -</head><label>12</label><figDesc>The effect of query-to-query attention in decoder layers on COCO val2017 set. "q2q" indicates whether add queryto-query attention in the decoder. In this experiment, we set the number of transformer encoder layers: NE = 0, and transformer decoder layers: ND = 6. As shown in the table, decoder with query-to-query attention have better performance. The effect of the number of channels of the input features to the Transformer encoder on COCO val2017 set. C is the number of channels. In this experiment, we set the number of transformer encoder layers: NE = 0, and transformer decoder layers: ND = 6.ND GFLOPs AP kp AP kp</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>50</cell><cell>AP kp 75</cell><cell>AP kp M</cell><cell>AP kp L</cell></row><row><cell></cell><cell>3.51</cell><cell>63.2</cell><cell>85.1</cell><cell>69.9</cell><cell>60.3</cell><cell>70.2</cell></row><row><cell></cell><cell>3.61</cell><cell>64.5</cell><cell>85.2</cell><cell>71.2</cell><cell>61.5</cell><cell>71.5</cell></row><row><cell>C</cell><cell cols="3">GFLOPs AP kp AP kp 50</cell><cell>AP kp 75</cell><cell>AP kp M</cell><cell>AP kp L</cell></row><row><cell>128</cell><cell>2.28</cell><cell>63.2</cell><cell>85.0</cell><cell>69.8</cell><cell>60.6</cell><cell>69.8</cell></row><row><cell>256</cell><cell>3.61</cell><cell>64.5</cell><cell>85.2</cell><cell>71.2</cell><cell>61.5</cell><cell>71.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>50</cell><cell>AP kp 75</cell><cell>AP kp M</cell><cell>AP kp L</cell></row><row><cell>1</cell><cell>6.32</cell><cell>65.7</cell><cell>86.3</cell><cell>73.4</cell><cell>63.0</cell><cell>72.4</cell></row><row><cell>2</cell><cell>6.41</cell><cell>66.9</cell><cell>86.5</cell><cell>74.0</cell><cell>64.2</cell><cell>73.8</cell></row><row><cell>3</cell><cell>6.50</cell><cell>67.1</cell><cell>86.6</cell><cell>74.2</cell><cell>64.5</cell><cell>73.9</cell></row><row><cell>4</cell><cell>6.59</cell><cell>67.2</cell><cell>86.6</cell><cell>74.2</cell><cell>64.6</cell><cell>74.0</cell></row><row><cell>5</cell><cell>6.68</cell><cell>67.2</cell><cell>86.6</cell><cell>74.2</cell><cell>64.6</cell><cell>74.1</cell></row><row><cell>6</cell><cell>6.77</cell><cell>67.2</cell><cell>86.6</cell><cell>74.2</cell><cell>64.6</cell><cell>74.1</cell></row></table><note>1 GFLOPs more computational cost. Configurations of Transformer decoder. Here we study the effect of width and depth of the decoder. Specifically, we conduct experiments by varying the number of channelsq2q GFLOPs AP kp AP kp</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>aux GFLOPs AP kp AP kp</figDesc><table><row><cell></cell><cell></cell><cell>50</cell><cell>AP kp 75</cell><cell>AP kp M</cell><cell>AP kp L</cell></row><row><cell>6.76</cell><cell>67.2</cell><cell>86.6</cell><cell>74.2</cell><cell>64.6</cell><cell>74.1</cell></row><row><cell>6.76</cell><cell>69.5</cell><cell>87.5</cell><cell>76.5</cell><cell>66.1</cell><cell>77.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 -Table 6 -</head><label>56</label><figDesc>Comparisons with previous works on the COCO val split. For CPN, the results in the brackets are with online hard keypoints mining. All the reported methods use person detectors with similar performance. Specifically, Hourglass and CPN use the person detector with 55.3% AP on COCO. Others use the person detector with 56.4% AP. DeepPose is implemented by the mmpose<ref type="bibr" target="#b11">[12]</ref>. Flipping test is applied for all model. ND represents the number of encoder layers. Comparisons with state-of-the-art methods on COCO test-dev set. ? and ? denote flipping and multi-sacle testing, respectively. Input size and the GFLOPs are shown for the single person pose estimation methods. 'ResNet-Ince.' represent the ResNet inception. The Simple baseline(ResNet-50) is tested with the official code. ND represents the number of encoder layers.</figDesc><table><row><cell>the Int. Reg. (9.15 GFLOPs vs. 11.0 GFLOPs). More-</cell></row><row><cell>over, with the same bacbone ResNet-50, our TFPose even</cell></row><row><cell>achieves better performance than the strong heatmap-based</cell></row><row><cell>method SimpleBaseline (70.5% vs. 70.0% AP) with less</cell></row><row><cell>computational complexity (7.7 GFLOPS vs. 8.9 GFLOPS).</cell></row><row><cell>Additionally, the results of TFPose are also close to the</cell></row><row><cell>best reported pose estimation results. For exmaple, the per-</cell></row><row><cell>formance of TFPose (72.2% AP) is close to the ResNet-</cell></row><row><cell>Inception based CPN(72.1% AP) and ResNet-152 based</cell></row><row><cell>SimpleBaseline (73.7% AP). Note that they use much larger</cell></row><row><cell>backbones than ours.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table 7-MPII human pose test set PCKh accuracies. For our model, the number of encoder layers ND is set to 6.</figDesc><table><row><cell>Method</cell><cell>Head</cell><cell>Sho.</cell><cell>Elb.</cell><cell>Wri.</cell><cell>Hip</cell><cell>Knee</cell><cell>Ank.</cell><cell>Total</cell></row><row><cell></cell><cell></cell><cell cols="2">Heatmap-based methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pishchulin et al. [37]</cell><cell>74.3</cell><cell>49.0</cell><cell>40.8</cell><cell>34.1</cell><cell>36.5</cell><cell>34.4</cell><cell>35.2</cell><cell>44.1</cell></row><row><cell>Tompson et al. [45]</cell><cell>95.8</cell><cell>90.3</cell><cell>80.5</cell><cell>74.3</cell><cell>77.6</cell><cell>69.7</cell><cell>62.8</cell><cell>79.6</cell></row><row><cell>Hu et al. [19]</cell><cell>95.0</cell><cell>91.6</cell><cell>83.0</cell><cell>76.6</cell><cell>81.9</cell><cell>74.5</cell><cell>69.5</cell><cell>82.4</cell></row><row><cell>Lifshitz et al. [27]</cell><cell>97.8</cell><cell>93.3</cell><cell>85.7</cell><cell>80.4</cell><cell>85.3</cell><cell>76.6</cell><cell>70.2</cell><cell>85.0</cell></row><row><cell>Raf et al. [38]</cell><cell>97.2</cell><cell>93.9</cell><cell>86.4</cell><cell>81.3</cell><cell>86.8</cell><cell>80.6</cell><cell>73.4</cell><cell>86.3</cell></row><row><cell>Bulat et al. [3]</cell><cell>97.9</cell><cell>95.1</cell><cell>89.9</cell><cell>85.3</cell><cell>89.4</cell><cell>85.7</cell><cell>81.7</cell><cell>89.7</cell></row><row><cell>Chu et al. [11]</cell><cell>98.5</cell><cell>96.3</cell><cell>91.9</cell><cell>88.1</cell><cell>90.6</cell><cell>88.0</cell><cell>85.0</cell><cell>91.5</cell></row><row><cell>Ke et al. [24]</cell><cell>98.5</cell><cell>96.8</cell><cell>92.7</cell><cell>88.4</cell><cell>90.6</cell><cell>89.3</cell><cell>86.3</cell><cell>92.1</cell></row><row><cell>Tang et al. [43]</cell><cell>98.4</cell><cell>96.9</cell><cell>92.6</cell><cell>88.7</cell><cell>91.8</cell><cell>89.4</cell><cell>86.2</cell><cell>92.3</cell></row><row><cell>Zhang et al. [53]</cell><cell>98.6</cell><cell>97.0</cell><cell>92.8</cell><cell>88.8</cell><cell>91.7</cell><cell>89.8</cell><cell>86.6</cell><cell>92.5</cell></row><row><cell></cell><cell cols="3">Regression-based methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Carreira et al. [7]</cell><cell>95.7</cell><cell>91.7</cell><cell>81.7</cell><cell>72.4</cell><cell>82.8</cell><cell>73.2</cell><cell>66.4</cell><cell>81.3</cell></row><row><cell>Sun et al. [41]</cell><cell>97.5</cell><cell>94.3</cell><cell>87.0</cell><cell>81.2</cell><cell>86.5</cell><cell>78.5</cell><cell>75.4</cell><cell>86.4</cell></row><row><cell>Aiden et al. (ResNet-50) [34]</cell><cell>97.8</cell><cell>96.0</cell><cell>90.0</cell><cell>84.3</cell><cell>89.8</cell><cell>85.2</cell><cell>79.7</cell><cell>89.5</cell></row><row><cell>Ours (ResNet-50)</cell><cell>98.0</cell><cell>95.9</cell><cell>91.0</cell><cell>86.0</cell><cell>89.8</cell><cell>86.6</cell><cell>82.6</cell><cell>90.4</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank Alibaba Group for the donation of GPU cloud computing resources.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inform. Process. Syst</title>
		<meeting>Adv. Neural Inform. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="717" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning delicate local representations for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxiong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binyi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erjin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="455" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Openpose: realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gines</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="172" to="186" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00364</idno>
		<title level="m">Pre-trained image processing transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="5386" to="5395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1831" to="1840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Openmmlab pose estimation toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mmpose</forename><surname>Contributors</surname></persName>
		</author>
		<idno>2020. 8</idno>
		<ptr target="https://github.com/open-mmlab/mmpose" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12556</idno>
		<title level="m">A survey on visual transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down reasoning with hierarchical rectified gaussians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5600" to="5609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Facial landmarks detection by self-iterative regression based landmarks-attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The devil is in the details: Delving into unbiased data processing for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="5700" to="5709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Handtransformer: Non-autoregressive structured modeling for 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="17" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hot-net: Non-autoregressive transformer for 3d handobject pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3136" to="3145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-scale structure-aware network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lipeng</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ching</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Crowdpose: Efficient crowded scenes pose estimation and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10863" to="10872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binyi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuming</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00148</idno>
		<title level="m">Rethinking on multi-stage networks for human pose estimation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Human pose estimation using deep consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ita</forename><surname>Lifshitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="246" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">End-to-end human pose and mesh reconstruction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09760</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Rethinking the heatmap regression for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxiong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erjin</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15175</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05424</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiden</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Prendergast</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07372</idno>
		<title level="m">Numerical coordinate regression with convolutional neural networks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Single-stage multi-person pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="6951" to="6960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Chris Bregler, and Kevin Murphy. Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nori</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="4903" to="4911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Strong appearance and expressive spatial models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3487" to="3494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An efficient convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umer</forename><surname>Rafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Kostrikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brit. Mach. Vis. Conf., volume</title>
		<meeting>Brit. Mach. Vis. Conf., volume</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deeply learned compositional models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Directpose: Direct end-to-end multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07451</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2984</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inform. Process. Syst</title>
		<meeting>Adv. Neural Inform. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">MaX-DeepLab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<idno>2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<idno>2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mscoco keypoints challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binyi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuming</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Human pose estimation with spatial contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01760</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deformable DETR: Deformable Transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
