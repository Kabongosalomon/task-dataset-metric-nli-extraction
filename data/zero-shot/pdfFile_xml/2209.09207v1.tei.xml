<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Table Detection in the Wild: A Novel Diverse Table Detection Dataset and Method</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinal</forename><surname>Haloi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Subex AI Labs</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Shekhar</surname></persName>
							<email>shashank.shekhar@subex.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Subex AI Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Siddhant</roleName><forename type="first">Nikhil</forename><surname>Fande</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Subex AI Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swaroop</forename><surname>Dash</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Subex AI Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><forename type="middle">G</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Subex AI Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Table Detection in the Wild: A Novel Diverse Table Detection Dataset and Method</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent deep learning approaches in table detection achieved outstanding performance and proved to be effective in identifying document layouts. Currently, available table detection benchmarks have many limitations, including the lack of samples diversity, simple table structure, the lack of training cases, and samples quality. In this paper, we introduce a diverse largescale dataset for table detection with more than seven thousand samples containing a wide variety of table structures collected from many diverse sources. In addition to that, we also present baseline results using a convolutional neural network-based method to detect table structure in documents. Experimental results show the superiority of applying convolutional deep learning methods over classical computer vision-based methods. The introduction of this diverse table detection dataset will enable the community to develop high throughput deep learning methods for understanding document layout and tabular data processing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head> <ref type="table" target="#tab_1">Table detection</ref> <p>is one of the crucial tasks in document layout analysis and table data understanding. Especially for extracting data from scanned documents table detection plays a significant role.</p><p>The recent development of applying deep learning in computer vision-related tasks enabled the researcher to develop a state-of-the-art document layout analysis system <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Deep learning technique such as convolutional neural network is widely used in segmentation <ref type="bibr" target="#b2">[3]</ref>, classification <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> and object detection task <ref type="bibr" target="#b5">[6]</ref>. Building a high-performance convolutional model requires a large dataset resembling the target problem. A good dataset has the attributes of good samples diversity, high-quality ground truths, readable samples resolution, and a lot of training cases. As we have observed, all the current table detection datasets have various limiting aspects.</p><p>Firstly, the quality of samples available in these datasets is not adequate, which makes it hard for the trained model to generalize well in the high-quality sample cases. The structural details of tables are important in the detection and low-quality samples lack clarity in capturing minute details of tables structure. <ref type="bibr">Hence</ref> As another contribution, we also provide baseline results using a deep learning-based model and classical selective search method. Additionally, we also provide a novel method to compute the diversity of the samples of a dataset by leveraging latent representation computed using a pre-trained encoder.</p><p>The presented dataset can help the researchers to develop a novel table detection method to understand and map document layouts for information extraction. This dataset will make it easy for the community to apply data-dependent algorithms such as deep convolutional neural networks for the task of layout and table detection.</p><p>Our experimental results using the presented deep learning method show promising results in detecting complex table structures. The baseline results were able to outperform the classical computer vision-based methods for detecting tables asserting the advantages of a data-driven deep neural network.</p><p>The rest of the paper is organized as follows: Section 2 explores the current table detection methods and benchmarks. Section 3 introduces the presented dataset, its properties, and evaluation metrics. Section 4 presents the deep convolutional layer-based RetinaNet method and selective search-based detection method for table detection in the wild images. Section 5 shows the experimental results obtained by classical computer vision methods alongside the proposed deep learning method on our benchmark, and section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we will go through the publicly available table detection datasets and the recent advancements of deep learning in this domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Table Detection Datasets</head><p>TableBank <ref type="bibr" target="#b6">[7]</ref> dataset is prepared from word and latex documents available on the internet. This dataset contains 417K labeled table images. This dataset does not contain any tables from scanned pdf documents. Another limitation is the diversity of the samples, this dataset was collected from '.docx' format documents and scientific articles from the arxiv.org website.</p><p>Marmot <ref type="bibr" target="#b7">[8]</ref> dataset is consisting of 2000 pdf pages with tables. Most of these examples were collected from research papers.</p><p>ICDAR 2013 <ref type="bibr" target="#b8">[9]</ref> dataset released as part of 2013 competition for both table detection and its structural analysis. It contains 128 samples collected mostly from US and EU government sources.</p><p>UNLV <ref type="bibr" target="#b9">[10]</ref> dataset contains 427 samples collected from scanned documents. These samples were sourced from magazines, newspapers, corporate reports, business letters, etc.</p><p>DeepFigures <ref type="bibr" target="#b10">[11]</ref> is a large dataset consisting of figures and tables samples. This dataset is collected from latex and XML sources. It has 1.4M induced tables, but it does not contain any scanned documents, also the intra-samples diversity of this dataset is limited.</p><p>To summarize our dataset has many advantages over the existing public datasets as follows:</p><p>? Samples include scanned and searchable documents. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Table Detection Methods</head><p>Both classical computer vision and deep learning-based methods are used to solve the table detection problem. Recently convolutional detection-based methods achieved stateof-the-art results on publicly available datasets such as <ref type="table" target="#tab_1">Table-Bank, ICDAR, Marmot, etc.</ref> Li et al. <ref type="bibr" target="#b6">[7]</ref> proposed a Faster-RCNN <ref type="bibr" target="#b11">[12]</ref> based convolutional detection method and reported a performance of 0.93 F1 score in the TableBank dataset. They have used Resnet-101 and Resnet-152 very deep neural networks as feature extractor backbone.</p><p>TableNet <ref type="bibr" target="#b12">[13]</ref> is another deep learning method formulate table detection as region segmentation problem and uses FCN architecture <ref type="bibr" target="#b14">[14]</ref> to segment table region. It uses VGG-16 <ref type="bibr" target="#b15">[15]</ref> as feature extractor for the FCN model. TableNet has reported a performance of 0.95 F1 score on the iCDAR dataset.</p><p>Another Faster-RCNN based approach proposed by Gilani et al. <ref type="bibr" target="#b16">[16]</ref> reported state-of-the-art performance on the UNLV dataset.</p><p>Schreiber et al. <ref type="bibr" target="#b17">[17]</ref> uses a deep learning-based method to detect tables and identify table structure by detecting rows, columns, and table cells. It also uses the Faster-RCNN method with VGG-16 as the feature extractor backbone.</p><p>Classical feature engineering approaches were also used to detect tables in scanned and searchable documents. Kasar et al. <ref type="bibr" target="#b18">[18]</ref> used an SVM classifier on features extracted using horizontal and vertical lines information to predict if a sample has a table or not. Silva et al. <ref type="bibr" target="#b19">[19]</ref> uses Hidden Markov model to detect tables from documents. Based on the MXY tree data structure Cesarini et al. <ref type="bibr" target="#b20">[20]</ref> presented a hierarchical representation for locating tables in document images.</p><p>Despite the successes of applying deep learning models on various datasets, these methods still lack generalization performance on out-of-domain samples due to the constrained nature of the available public datasets. Applying these models to production scenarios is still a work in progress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE DATASET</head><p>In this section, we introduce the metadata details and the evaluation criteria of STDW table detection dataset. Dataset can be accessed in this link: https://github.com/subex/STDW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The STDW Table Detection Dataset</head><p>Data Sources: To prepare the dataset, we have utilized the resources available on the internet. Documents containing tables were collected from various sources to ensure intradataset samples diversity. Sources such as electronic component datasheets, material safety data sheets, product safety data sheets, billing invoices, research papers, finance reports, books, etc are used for gathering the samples. Samples contain English, German, Japanese, Hindi, and many other languages capturing diverse scripts. <ref type="figure" target="#fig_1">Figure 2</ref> shows some sample images from the collected dataset.</p><p>Data Modalities: Both scanned and searchable documents with tables are included in the dataset. Scanned documents include RGB and grayscale samples. Collected samples have Labelling: All samples in the dataset are labeled manually using the GUI-based Labelme <ref type="bibr" target="#b21">[21]</ref> annotation tool. Bounding box-based standard detection problem's labeling method is used to label all the images. For each of the image bounding boxes, coordinates are stored in an XML file as shown in the Listing 1 following the PASCAL VOC <ref type="bibr" target="#b22">[22]</ref> annotation format. For each bounding box, top left (xmin, ymin) and bottomright (xmax, ymax) coordinates are stored. <ref type="figure" target="#fig_2">Figure 3</ref> shows a sample image with a bounding box marked on the image showing the position respective coordinates.</p><p>Data Statistics: <ref type="table" target="#tab_1">Table I</ref> depicts the STDW dataset samples statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Benchmark Evaluations</head><p>We define criteria for two types of table detection evaluation metrics to unify reported results on this dataset. Work derived using this benchmark dataset may use these metrics for fair comparisons. We report the performance of the baseline models on these two metrics.</p><p>IoU-Intersection over union: Overlap between two bounding boxes is measured using Intersection Over Union (IoU) also known as Jaccard Index. We use this metric to measure the extent of the correctness of the predicted bounding box with the ground truth bounding box. Eq 1 shows the mathematical formulation on how to calculate IoU between two bounding boxes B1 and B2.</p><formula xml:id="formula_0">IoU = |B1 ? B2| |B1 ? B2|<label>(1)</label></formula><p>AP-Average Precision: Average Precision (AP) summarizes a precision-recall curve as the weighted mean of the precisions obtained at the different thresholds. The increase in recall from the previous threshold has used a weight. Eq 2 shows the mathematical formulation on how to compute AP using a precision-recall curve, where P n and R n are the precision and recall at the nth threshold.</p><formula xml:id="formula_1">AP = n (R n ? R n?1 )P n<label>(2)</label></formula><p>AP metric is calculated at different IoU thresholds to get <ref type="figure">Fig. 4</ref>. RetinaNet based method for Table Detection. RetinaNet uses two task-specific heads, one for predicting objects labels and the other for predicting object bounding boxes. RetinaNet is a fully convolutional model and it uses upsampling layers and skip connections to build the input layers for the task-specific heads.   the best combination of IoU and AP for the object detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHODS</head><p>In this section, we detail a data-driven deep learning method and a classical feature-based method to locate the table in images. We formulate the problem as a classic object detection problem, where the object of consideration will be a tabular structure present in the images.</p><p>Deep learning-based object detection methods are proven to be very effective. We use RetinaNet <ref type="bibr" target="#b23">[23]</ref> object detection method as baseline for this dataset. RetinaNet achieved stateof-the-art performance on the COCO <ref type="bibr" target="#b24">[24]</ref> dataset. We also use a selective search <ref type="bibr" target="#b25">[25]</ref> model for object detection to report baseline results using classical computer vision.</p><p>In this section, we introduce the deep convolutional object detection method RetinaNet and the selective search-based objection detection method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. RetinaNet for Table Detection</head><p>RetinaNet is a one-stage, dense objective detection method designed using convolutional layers. It is consists of a backbone network and two task-specific heads. The backbone network is designed to model input image features using convolutional layers. The backbone network is a stack of multiple skip connection-based convolutional blocks. Taskspecific heads are designed to get classification scores and the coordinates for each of the anchor bounding boxes. <ref type="figure">Figure 4</ref> shows a high-level depiction of the RetinaNet. RetinaNet addresses the extreme foreground-background class imbalance problem observed in the object detector training by introducing a sample complexity-based weighted cross-entropy loss called focal loss. Focal loss enforces the trainer to focus on hard examples by assigning higher weights. Cross entropy loss for binary classification is defined as shown in the Eq 3</p><formula xml:id="formula_2">CE(p, y) = ?log(p) if y = 1 ?log(1 ? p) otherwise<label>(3)</label></formula><p>To handle the large class imbalance problem focal loss adds a modulating factor (1 ? p t ) ? to the original binary cross entropy loss function. Focal loss is defined as shown in the Eq 4, where ? ? 0 is a tuneable parameter.  <ref type="table" target="#tab_1">TABLE II  COMPARISONS BETWEEN THE STDW DATASET AND SOME OF THE OTHER PUBLICLY AVAILABLE DATASETS FOR TABLE DETECTION. OUR DATASET   PROVIDES THE MOST DIVERSE, HIGH-</ref></p><formula xml:id="formula_3">QUALITY SAMPLES. F L(p t ) = ?(1 ? p) ? log(p t )<label>(4)</label></formula><p>In this work we use a ?-balanced variant of the focal loss as shown in the Eq 5</p><formula xml:id="formula_4">F L(p t ) = ?? t (1 ? p) ? log(p t )<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Selective Search</head><p>Selective search is a method to detect all possible object bounding boxes in an input image. Selective search is combined with classical image-level features and a support vector machine to detect objects. It uses a hierarchical grouping algorithm to find all possible bounding boxes with an object, those bounding boxes can be overlapping. Selective search assigns objectness score to bounding boxes denoting the probability of object presence inside the bounding box. A bounding box with a high objectness score might contain the object of interest. For computing features of bounding boxes bag-of-words <ref type="bibr" target="#b26">[26]</ref> with color-SIFT descriptors <ref type="bibr" target="#b27">[27]</ref> is used. For the classification of bounding boxes, a support vector machine with a histogram intersection kernel is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Dataset Diversity</head><p>We estimate diversity based on the dataset's sample variations in the latent space. The spread of the dataset in the latent space is defined as diversity. The larger the spread of the samples, the higher is the diversity. To calculate the latent representation of each of the samples we use an ImageNet <ref type="bibr" target="#b28">[28]</ref> pre-trained VGG-16 <ref type="bibr" target="#b15">[15]</ref> model's block5 pool layer. VGG-16 is a very deep convolutional model used for image-related problems. The latent representation is of dimension 25088 and computed on input image resized to <ref type="bibr">[224,</ref><ref type="bibr">224,</ref><ref type="bibr" target="#b2">3]</ref>. Algorithm 1 provides the full procedure for diversity computation using a deep convolutional encoder model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In our experiments, we evaluate the classical computer vision method and convolutional RetinaNet model on our dataset.</p><p>We have used Tensorflow <ref type="bibr" target="#b29">[29]</ref> and Datum <ref type="bibr" target="#b30">[30]</ref> for deep learning experiments described in this work. Tensorflow is used to build and train the RetinaNet model and Datum is used to prepare and load the dataset for training. Batch normalization <ref type="bibr" target="#b31">[31]</ref> is used to reduce covariate shift and achieve faster convergence. Also, we have used the Nesterov momentum   <ref type="table" target="#tab_1">Table III</ref>. For selective search, we use the official Matlab code provided by the author.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Baseline results for STDW</head><p>We provide baseline results for table detection on the STDW dataset using two methods. In one method we use the deep learning-based RetinaNet model and in the other method, we use a selective search-based object detection approach. For baseline results, IOU and AP are computed on the test set. We trained a RetinaNet model with XX feature extractor for detecting tables from input images. The hyperparameter set used in the RetinaNet experiment is shown in <ref type="table" target="#tab_1">Table IV</ref>. For a selective search-based approach we use a visual codebook of size 4000 and 4 levels spatial pyramid using a 1x1, 2x2, 3x3 and 4x4 division. This setting results in a total feature vector of length 360000. <ref type="table" target="#tab_1">Table III</ref> shows the baseline results obtained using the deep learning-based RetinaNet model and classical selective searchbased approach. RetinaNet performed significantly higher than   that of the selective search method. <ref type="figure">Figure 6</ref> shows the variation of average precision with respect to intersection over the union of the predicted bounding boxes with that of the ground truth boxes. We report the AP metric value at the IoU threshold of 0.5. Results for a few images are visualized by drawing the predicted bounding boxes on the top of the original images as shown in <ref type="figure" target="#fig_6">Figure 5</ref>. The baseline model predicted the locations of the tables correctly on the test images, though the predicted bounding boxes are not exactly precise, further investigation on the build building side is required to improve the results.</p><p>Additionally, we have also computed the diversity metric for the STDW dataset and also for the TableBank, Marmot and ICDAR 2013 datasets. From <ref type="table" target="#tab_1">Table II</ref> diversity column we can see that the diversity of the proposed STDW dataset is more than the other open source datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>A large-scale table detection dataset is introduced in this paper. Out dataset includes 7K image samples collected from various open sources. Compared to the current datasets for this task, the STDW dataset is very diverse and contains high-quality manually annotated samples. The large scale and diverse nature of the data enable us to apply data-driven deep learning methods for table detection problems and achieve better performance. The provided baseline experimental results show the superiority of the data-driven deep learning approach over classical features-driven computer vision approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>High diversity of samples, capturing a wide range of table designs. ? High resolution of samples. ? Biggest dataset with manual annotation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Dataset samples images with diverse examples one or more tables in them. Samples resolution various from 500 * 500 * 3 to 5000 * 5000 * 3, capturing a wide range of image qualities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>An example image with bounding boxes annotation. We denote topleft corner as (xmin, ymin) and bottom-right corner as (xmax, ymax)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Listing 1 .</head><label>1</label><figDesc>An example XML file content showing the annotation for a single bounding box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>&lt;</head><label></label><figDesc>a n n o t a t i o n&gt; &lt; o b j e c t&gt; &lt;name&gt;T a b l e&lt; / name&gt; &lt;p o s e&gt;U n s p e c i f i e d&lt; / p o s e&gt; &lt; t r u n c a t e d&gt;0&lt; / t r u n c a t e d&gt; &lt; d i f f i c u l t&gt;0&lt; / d i f f i c u l t&gt; &lt;bndbox&gt; &lt;xmin&gt;541&lt; / xmin&gt; &lt;ymin&gt;970&lt; / ymin&gt; &lt;xmax&gt;4060&lt; / xmax&gt; &lt;ymax&gt;2766&lt; / ymax&gt; &lt; / bndbox&gt; &lt; / o b j e c t&gt; . . . &lt; o b j e c t&gt; . . . &lt; / o b j e c t&gt; &lt; / a n n o t a t i o n&gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1 7 :</head><label>17</label><figDesc>Dataset Diversity Metric 1: Initialize VGG-16 model with ImageNet weights. 2: f eatures ? [] 3: // Compute samples latent representations 4: for each image image f ilepath do 5: // Read, resize and normalize image 6: image np = read image(image f ilepath) image np = resize and normalize(image np) eature = V GG16(image np) 10: f eatures ? f eatures ? f eature 11: // Compute standard deviation across features dimension 12: f eatures std ? compute std(f eatures, axis = 1) 13: // Compute L2 norm 14: diversity = f eatures std 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Table Detection results using the baseline RetinaNet model on images from the Subex TDW daatset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, samples resolution and quality are important in the training detection model. Secondly, the diversity of samples is very limited. Dataset samples should capture all possible variations of table structures starting from a simple table to complex nested table structures including grid and non-grid variations. Deep learning model generalization performance improves with diverse discriminative samples as it learns the underlying pattern present in the dataset. Fig. 1. Table detection in documents. An example of tables in a datasheet document. Finally, the most crucial is the number of available samples in the dataset. Dataset's cardinality has a direct impact on the overall performance. Advanced deep learning model depends on a large dataset, a limited number of samples prevent us from applying complex very deep networks for table detection. A deep learning model built with limited data suffers from overfitting and poor inference time results. To address these problems, we have gathered a new largescale benchmark dataset for table detection. The presented dataset consists of 7K image samples from diverse sources capturing a wide variety of table occurrences. We have collected table samples from scanned documents, word documents, and searchable pdf documents. Collected samples were checked for quality standards by removing blurry, noisy, and low-quality samples from the final dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I STDW</head><label>I</label><figDesc>DATASET SAMPLES STATISTICS. IT INCLUDES BOTH SCANNED AND SEARCHABLE/NATIVE-DIGITAL DOCUMENTS,</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III PERFORMANCE</head><label>III</label><figDesc>OF CLASSICAL AND DEEP LEARNING METHODS ON STDW optimizer with a constant learning rate. RetinaNet model is trained using a GPU accelerator for 30 epochs to reach the baseline performance as reported in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table</head><label></label><figDesc>Detection results using the baseline RetinaNet model on images from the Subex TDW daatset.</figDesc><table><row><cell>Hyperparameter</cell><cell>Value</cell></row><row><cell>Base learning rate</cell><cell>0.0004</cell></row><row><cell>Optimizer</cell><cell>SGD</cell></row><row><cell>Momentum</cell><cell>0.9</cell></row><row><cell>Batch size</cell><cell>1</cell></row><row><cell>Buffer size</cell><cell>20 * batch size</cell></row><row><cell>Epochs</cell><cell>30</cell></row><row><cell>Learning rate schedule</cell><cell>Constant</cell></row><row><cell>Loss</cell><cell>Focal loss</cell></row><row><cell>Feature extractor</cell><cell>Resnet-50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV HYPERPARAMETERS</head><label>IV</label><figDesc>USED FOR TRAINING THE BASELINE RETINANET MODEL ON STDW.Fig. 6. IoU versus AP plot on the STDW test set using the RetinaNet baseline model.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>This research was carried out at Subex AI Labs. We gratefully acknowledge the support of team members in verifying the dataset annotation quality.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multi-task handwritten document layout analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quir?s</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08852</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast cnn-based document layout analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augusto</forename><surname>Borges Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M. Palhares</forename><surname>Viana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1173" to="1180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Rethinking convolutional semantic segmentation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haloi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.07991</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep learning: Generalization requires deep compositional feature space design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haloi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01983</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.01949</idno>
		<title level="m">Tablebank: A benchmark dataset for table detection and recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dataset, ground-truth and performance metrics for table detection evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 10th IAPR International Workshop on Document Analysis Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="445" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Icdar 2013 table competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>G?bel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 12th International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1449" to="1453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An open approach towards the benchmarking of table structure recognition systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kieninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th IAPR International Workshop on Document Analysis Systems</title>
		<meeting>the 9th IAPR International Workshop on Document Analysis Systems</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extracting scientific figures with distantly supervised neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ammar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM/IEEE on joint conference on digital libraries</title>
		<meeting>the 18th ACM/IEEE on joint conference on digital libraries</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="223" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tablenet: Deep learning model for end-to-end table detection and tabular data extraction from scanned document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Paliwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="128" to="133" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Table detection using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Qasim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 14th IAPR international conference on document analysis and recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="771" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepdesrt: Deep learning for detection and structure recognition of tables in document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 14th IAPR international conference on document analysis and recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1162" to="1167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to detect tables in scanned document images using line information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kasar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barlas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chatelain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 12th International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1185" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning rich hidden markov models in document analysis: Table location</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 10th International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="843" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Trainable table location in document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cesarini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marinai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Soda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Object recognition supported by user interaction for service robots</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="236" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Labelme: Image polygonal annotation with python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GitHub repository</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge 2012 (voc2012) development kit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis, Statistical Modelling and Computational Learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visual categorization with bags of keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on statistical learning in computer vision, ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evaluating color descriptors for object and scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1582" to="1596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Datum: A system for tfrecord dataset management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shekhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GitHub repository</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
