<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding The Robustness in Vision Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
						</author>
						<title level="a" type="main">Understanding The Robustness in Vision Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent studies show that Vision Transformers (ViTs) exhibit strong robustness against various corruptions. Although this property is partly attributed to the self-attention mechanism, there is still a lack of systematic understanding. In this paper, we examine the role of self-attention in learning robust representations. Our study is motivated by the intriguing properties of the emerging visual grouping in Vision Transformers, which indicates that self-attention may promote robustness through improved mid-level representations. We further propose a family of fully attentional networks (FANs) that strengthen this capability by incorporating an attentional channel processing design. We validate the design comprehensively on various hierarchical backbones. Our model achieves a state-of-the-art 87.1% accuracy and 35.8% mCE on ImageNet-1k and ImageNet-C with 76.8M parameters. We also demonstrate state-of-the-art accuracy and robustness in two downstream tasks: semantic segmentation and object detection. Code will be available at https://github.com/NVlabs/FAN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent advances in visual recognition are marked by the rise of Vision Transformers (ViTs) <ref type="bibr" target="#b8">(Dosovitskiy et al., 2020)</ref> as state-of-the-art models. Unlike ConvNets <ref type="bibr" target="#b21">(LeCun et al., 1989;</ref><ref type="bibr" target="#b19">Krizhevsky et al., 2012</ref>) that use a "sliding window" strategy to process visual inputs, the initial ViTs feature a design that mimics the Transformers in natural language processing -An input image is first divided into a sequence of * Work done during an internship at NVIDIA. ? Work partially done during the affiliation with NUS.  . Retention rate is defined as robust accuracy / clean accuracy. Left to right in bottom row: input image contaminated by corruption (snow) and the visualized clusters. Visualization is conducted on the output features (tokens) of the second last layers. All models are pretrained on ImageNet-1K. Input size is set to 448 ? 448 following <ref type="bibr" target="#b3">(Caron et al., 2021)</ref>. patches (tokens), followed by self-attention (SA) <ref type="bibr" target="#b40">(Vaswani et al., 2017)</ref> layers to aggregate the tokens and produce their representations. Since introduction, ViTs have achieved good performance in many visual recognition tasks.</p><p>Unlike ConvNets, ViTs incorporate the modeling of nonlocal relations using self-attention, giving it an advantage in several ways. An important one is the robustness against various corruptions. Unlike standard recognition tasks on clean images, several works show that ViTs consistently outperform ConvNets by significant margins on corruption robustness <ref type="bibr" target="#b0">(Bai et al., 2021;</ref><ref type="bibr" target="#b53">Zhu et al., 2021;</ref><ref type="bibr">Paul &amp; Chen, 2022;</ref><ref type="bibr">Naseer et al., 2021)</ref>. The strong robustness in ViTs is partly attributed to their self-attention designs, but this hypothesis is recently challenged by an emerging work ConvNeXt <ref type="bibr">(Liu et al., 2022)</ref>, where a net-  work constructed from standard ConvNet modules without self-attention competes favorably against ViTs in generalization and robustness. This raises an interesting question on the actual role of self-attention in robust generalization.</p><p>Our approach: In this paper, we aim to find an answer to the above question. Our journey begins with the intriguing observation that meaningful segmentation of objects naturally emerge in ViTs during image classification <ref type="bibr" target="#b3">(Caron et al., 2021)</ref>. This motivates us to wonder whether selfattention promotes improved mid-level representations (and thus robustness) via visual grouping -a hypothesis that echoes the odyssey of early computer vision (U.C. Berkeley). As a further examination, we analyze the output tokens from each ViT layer using spectral clustering <ref type="bibr" target="#b27">(Ng et al., 2002)</ref>, where the significant 1 eigenvalues of the affinity matrix correspond to the main cluster components. Our study shows an interesting correlation between the number of significant eigenvalues and the perturbation from input corruptions: both of them decrease significantly over midlevel layers, which indicates the symbiosis of grouping and robustness over these layers.</p><p>To understand the underlying reason for the grouping phenomenon, we interpret SA from the perspective of information bottleneck (IB) <ref type="bibr" target="#b36">(Tishby et al., 2000;</ref><ref type="bibr" target="#b35">Tishby &amp; Zaslavsky, 2015)</ref>, a compression process that "squeezes out" unimportant information by minimizing the mutual information between the latent feature representation and the target class labels, while maximizing mutual information between the latent features and the input raw data. We show that under mild assumptions, self-attention can be written as an iterative optimization step of the IB objective. This partly explains the emerging grouping phenomenon since IB is known to promote clustered codes. 1 eigenvalues are larger than a predefined threshold .</p><p>As shown in <ref type="figure" target="#fig_1">Fig.2 (a)</ref>, previous Vision Transformers often adopt a multi-head attention design, followed by an MLP block to aggregate the information from multiple separate heads. Since different heads tend to focus on different components of objects, the multi-head attention design essentially forms a mixture of information bottlenecks. As a result, how to aggregate the information from different heads matters. We aim to come up with an aggregation design that strengthens the symbiosis of grouping and robustness. As shown in <ref type="figure" target="#fig_1">Fig.2 (b)</ref>, we propose a novel attentional channel processing design which promotes channel selection through reweighting. Unlike the static convolution operations in the MLP block, the attentional design is dynamic and content-dependent, leading to more compositional and robust representations. The proposed module results in a new family of Transformer backbone, coined Fully Attentional Networks (FANs) after their designs.</p><p>Our contributions can be summarized as follows:</p><p>? Instead of focusing on empirical studies, this work provides an explanatory framework that unifies the trinity of grouping, information bottleneck and robust generalization in Vision Transfomrers. ? We also conduct extensive experiments in semantic segmentation and object detection. We show that the significant gain in robustness from our proposed design is transferrable to these downstream tasks.</p><p>Our study indicates the non-trivial benefit of attention representations in robust generalization, and is in line with the recent line of research observing the intriguing robustness in ViTs. We hope our observations and discussions can lead to a better understanding of the representation learning in ViTs and encourage the community to go beyond standard recognition tasks on clean images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Fully Attentional Networks</head><p>In this section, we examine some emerging properties in ViTs and interpret these properties from an information bottleneck perspective. We then present the proposed Fully Attentional Networks (FANs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Preliminaries on Vision Transformers</head><p>A standard ViT first divides an input image into n patches uniformly and encodes each patch into a token embedding x i ? R d , i = 1, . . . , n. Then, all these tokens are fed into a stack of transformer blocks. Each transformer block leverages self-attention for token mixing and MLPs for channelwise feature transformation. The architecture of a transformer block is illustrated in the left of <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Token mixing. Vision transformers leverage self-attention to aggregate global information. Suppose the input token embedding tensor is X = [x 1 , . . . , x n ] ? R d?n , SA applies linear transformation with parameters W K , W Q , W V to embed them into the key K = W K X ? R d?n , query Q = W Q X ? R d?n and value V = W V X ? R d?n respectively. The SA module then computes the attention matrix and aggregates the token features as follows:</p><formula xml:id="formula_0">Z = SA(X) = Softmax Q K ? d V WL,<label>(1)</label></formula><p>where W L ? R d?d is a linear transformation and Z = [z 1 , . . . , z n ] is the aggregated token features and ? d is a scaling factor. The output of the SA is then normalized and fed into the MLP to generate the input to the next block.</p><p>Channel processing. Most ViTs adopt an MLP block to transform the input tokens into features Z:</p><formula xml:id="formula_1">Z = MLP(Z).</formula><p>(2)</p><p>The block contains two Linear layers and a GELU layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Intriguing Properties of Self-Attention</head><p>We begin with the observation that meaningful clusters emerge on ViT's token features z. We examine such phenomenon using spectral clustering <ref type="bibr" target="#b27">(Ng et al., 2002)</ref>, where the token affinity matrix is defined as S ij = z i z j . Since the number of major clusters can be estimated by the multiplicity of significant eigenvalues (Zelnik-Manor &amp; Perona, 2004) of S, we plot the number of (in)significant eigenvalues across different ViT-S blocks <ref type="figure" target="#fig_2">(Figure 3 (a)</ref>). We observe that by feeding Gaussian noise x ? N (0, 1), the resulting perturbation (measured the by normalized feature norm) decreases rapidly together with the number of significant eigenvalues. Such observation indicates the symbiosis of grouping and improved robustness over middle blocks.</p><p>We additionally visualize the same plot for FAN-S-ViT in <ref type="figure" target="#fig_2">Figure 3</ref> (b) where similar trend holds even more obviously. The noise decay of ViT and FAN is further compared to ResNet-50 in <ref type="figure" target="#fig_2">Figure 3</ref> (c). We observe that: 1) the robustness of ResNet-50 tends to improve upon downsampling but plateaus over regular convolution blocks.</p><p>2) The final noise decay of ResNet-50 less significant. Finally, we visualize the grouped tokens obtained at different blocks in <ref type="figure" target="#fig_3">Figure 4</ref>, which demonstrates the process of visual grouping by gradually squeezing out unimportant components. Additional visualizations on different features (tokens) from different backbones are provided in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">An Information Bottleneck Perspective</head><p>The emergence of clusters and its symbiosis with robustness in Vision Transformers draw our attention to early pioneer works in visual grouping (U.C. <ref type="bibr" target="#b39">Berkeley;</ref><ref type="bibr" target="#b1">Buhmann et al., 1999)</ref>. In some sense, visual grouping can also be regarded as some form of lossy compression <ref type="bibr" target="#b46">(Yang et al., 2008)</ref>. We thus present the following explanatory framework from an information bottleneck perspective.</p><p>Given a distribution X ? N (X , ) with X being the observed noisy input and X the target clean code, IB seeks a mapping f (Z|X) such that Z contains the relevant information in X for predicting X . This goal is formulated as the following information-theoretic optimization problem:</p><formula xml:id="formula_2">f * IB (Z|X) = arg min f (Z|X) I(X, Z) ? I(Z, X ),<label>(3)</label></formula><p>Here the first term compresses the information and the second term encourages to maintain the relevant information.</p><p>In the case of an SA block, Z = [z 1 , . . . , z n ] ? R d?n denote the output features and X = [x 1 , . . . , x n ] ? R d?n the input. Assuming i is the data point index, we have:</p><p>Proposition 2.1. Under mild assumptions, the iterative step to optimize the objective in Eqn.</p><p>(3) can be written as:</p><formula xml:id="formula_3">zc = n i=1 log[nc/n] n det ? exp ? c ? ?1 x i 1/2 n c=1 exp ? c ? ?1 x i 1/2 xi,<label>(4)</label></formula><p>or in matrix form:</p><formula xml:id="formula_4">Z = Softmax(Q K/d)V ,<label>(5)</label></formula><formula xml:id="formula_5">with V = [x 1 , . . . , x N ] log[nc/n] n det ? , K = [? 1 , . . . , ? N ] = W K X, Q = ? ?1 [x 1 , . . . ,</formula><p>x N ] and d = 1/2. Here n c , ? and W K are learnable variables.</p><p>Remark. We defer the proof to the appendix. The above proposition establishes an interesting connection between the vanilla self-attention (1) and IB (3), by showing that SA aggregates similar inputs x i into representations Z with cluster structures. Self-attention updates the token features following an IB principle, where the key matrix K stores the temporary cluster center features ? c and the input features x are clustered to them via soft association (softmax). The new cluster center features z are output as the updated token  <ref type="formula">)</ref> show that the number of zero eigenvalues increases as the model goes deeper, which indicates the emerging grouping of tokens. Given the input Gaussian noise, its magnitude similarly decays over more self-attention blocks. Such a phenomenon is not observed in the ResNet-50 model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Block8</head><p>Block9 Block10</p><p>BLock 6 Block7 Input  <ref type="table" target="#tab_3">Table 1</ref>. The cluster visualizations are generated by applying spectral clustering on token features from each FAN block.</p><p>features. The stacked SA modules in ViTs can be broadly regarded as an iterative repeat of this optimization which promotes grouping and noise filtering.</p><p>Multi-head Self-attention (MHSA). Many current Vision Transformer architectures adopt an MHSA design where each head tends to focus on different object components. In some sense, MHSA can be interpreted as a mixture of information bottlenecks. We are interested in the relation between the number of heads versus the robustness under a fixed total number of channels. As shown in <ref type="figure" target="#fig_5">Figure 7</ref>, having more heads leads to improved expressivity and robustness. But the reduced channel number per head also causes decreased clean accuracy. The best trade-off is achieved with 32 channels per head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Fully Attentional Networks</head><p>With the above mixture of IBs interpretation, we intend to design a channel processing module that strengthens robust representation through the aggregation across different heads. Our design is driven by two main aspects: 1) To promote more compositional representation, it is desirable to introduce channel reweighting since some heads or channels do capture more significant information than the others. 2) The reweighting mechanism should involve more spatially holistic consideration of each channel to leverage the promoted grouping information, instead of making "very local" channel aggregation decisions.</p><p>A starting point towards the above goals is to introduce a channel self-attention design similar to <ref type="bibr">XCiT (El-Nouby et al., 2021)</ref>. As shown in <ref type="figure" target="#fig_4">Figure 5</ref> (a), the channel attention (CA) module adopts a self-attention design which moves the MLP block into the self-attention block, followed by matrix multiplication with the D ?D channel attention matrix from the channel attention branch.</p><p>Attentional feature transformation. A FAN block introduces the following channel attention (CA) to perform feature transformation which is formulated as:</p><formula xml:id="formula_6">CA(Z) = Softmax (W Q Z)(W K Z) ? n MLP(Z),<label>(6)</label></formula><p>Here W Q ? R d?d and W K ? R d?d are linear transformation parameters. Different from SA, CA computes the attention matrix along the channel dimension instead of the token dimension (recall Z ? R d?n ), which leverages the feature covariance (after linear transformation W Q , W K ) for feature transformation. Strongly correlated feature channels with larger correlation values will be aggregated while outlier features with low correlation values will be isolated. This aids the model in filtering out irrelevant information.</p><p>With the help of CA, the model can filter irrelevant features and thus form more precise token clustering for the foreground and background tokens. We will give a more formal description on such effects in the following section.</p><p>We will verify the improved robustness from CA over existing ViT models in the rest of the paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Efficient Channel Self-attention</head><p>There are two limits of applying the conventional selfattention calculation mechanism along the channel dimension. The first one is the computational overhead. The computational complexity of CA introduced in Eqn 6 is quadratically proportional to D 2 , where D is the channel dimension. For modern pyramid model designs <ref type="bibr" target="#b22">Liu et al., 2021)</ref>, the channel dimension becomes larger and larger at the top stages. Consequently, direct applying CA can cause a large computational overhead. The second one is the low parameter efficiency. In conventional SA module, the attention distribution of the attention weights is sharpened via a Softmax operation. Consequently, only a partial of the channels could contribute to the representation learning as most of the channels are diminished by being multiplied with a small attention weights. To overcome these, we explore a novel self-attention like mechanism that is equipped with both the high computational efficiency and parameter efficiency. Specifically, two major modifications are proposed. First, instead of calculating the co-relation matrix between the tokens features, we first generate a token prototype, Z, Z ? R n?1 , by averaging over the channel dimension. Intuitively, Z aggregates all the channel information for each spatial positions represented by tokens. Thus, it is informative to calculate the co-relation matrix between the token features and token prototype Z, resulting in learn complexity with respect to the channel dimension. Secondly, instead of applying a Softmax function, we use a Sigmoid function for normalizing the attention weights and then multiply it with the token features instead of using MatMul to aggregate channel information. Intuitively, we do not force the channel to select only a few of the "important" token features but re-weighting each channel based on the spatial co-relation. Indeed, the channel features are typically considered as independent. A channel with large value should not restrain the importance of other channels. By incorporating those two design concepts, we propose a novel channel self-attention and it is calculated via Eqn. <ref type="formula">(7)</ref>: <ref type="formula">(7)</ref> Here, ? denotes the Softmax operation along the token dimension and Z denotes the token prototype (Z ? R 1?N ).We use sigmoid as the Norm. The detailed block architecture design is also shown in <ref type="figure" target="#fig_4">Figure 5</ref>. We verify that the novel efficient channel self-attention takes consumes less computational cost while improve the performance significantly.</p><formula xml:id="formula_7">ECA(Z) = Norm (W Q ?(Z)) ?(Z) ? n MLP(Z),</formula><p>The detailed results will be shown in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiment Results &amp; Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experiment details</head><p>Datasets and evaluation metrics. We verify the model robustness on Imagenet-C (IN-C), Cityscape-C and COCO-C without extra corruption related fine-tuning. The suffix '-C' denotes the corrupted images based on the original dataset with the same manner proposed in <ref type="bibr" target="#b14">(Hendrycks &amp; Dietterich, 2019)</ref>. To test the generalization to other types of out-of-distribution (OOD) scenarios, we also evaluate the accuracy on ImageNet-A (Hendrycks et al., 2021) (IN-A) and ImageNet-R (IN-R) <ref type="bibr" target="#b14">(Hendrycks &amp; Dietterich, 2019)</ref>.</p><p>In the experiments, we evaluate the performance with both the clean accuracy on ImageNet-1K (IN-1K) and the robustness accuracy on these out-of-distribution benchmarks.</p><p>To quantify the resilience of a model against corruptions, we propose to calibrate with the clean accuracy. We use retention rate (Ret R) as the robustness metric, defined as R = Robust Acc. Clean Acc. = IN-C IN-1K . We also report the mean corruption error (mCE) following <ref type="bibr" target="#b14">(Hendrycks &amp; Dietterich, 2019)</ref>. For more details, please refer to Appendix A.2. For Cityscapes, we take the average mIoU for three severity levels for the noise category, following the practice in SegFormer . For all the rest of the datasets, we take the average of all five severity levels.</p><p>Model selection. We design four different model sizes (Tiny, Small, Base and large) for our FAN models, abbreviated as '-T', '-S', '-B' and '-L' respectively. Their detailed configurations are shown in <ref type="table" target="#tab_3">Table 1</ref>. For ablation study, we use ResNet-50 as a representative model for CNNs and ViT-S as a representative model for the conventional vision transformers. ResNet-50 and ViT-S have similar model sizes and computation budget as FAN-S. When comparing with SOTA models, we take the most recent vision transformer and CNN models as baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Analysis</head><p>In this section, we present a series of ablation studies to analyze the contribution of self-attention in model robustness. Since multiple advanced training recipes have been recently introduced, we first investigate their effects in improving model robustness. We then compare ViTs and CNNs with exactly the same training recipes to exclude factors other than architecture design that might affect model robustness.</p><p>Effects of advanced training tricks. We empirically evaluate how different training recipes could be used to improve the robustness, with the results reported in <ref type="table" target="#tab_4">Table 2</ref>. Interestingly, it is observed that widely used tricks such as knowledge distillation (KD) and large dataset pretraining do improve the absolute accuracy. However, they do not significantly reduce the performance degradation when transferred to ImageNet-C. The main improvement comes from the advanced training recipe such as the CutMix and RandAugmentation adopted in DeiT training recipe. In the following comparison, we use the ViT-S trained with DeiT recipe and increased block number with reduced channel dimension, denoted as ViT-S * . In addition, to make fair comparison, we first apply those advanced training techniques to reproduce the ResNet-50 performance. Adding new training recipes to CNNs. We make a step by step empirical study on how the robustness of ResNet-50 model changes when adding advanced tricks. We examine three design choices: training recipe, attention mechanism and down-sampling methods. For the training recipe, we adopt the same one as used in training the above ViT-S model. We use Squeeze-and-Excite (SE) attention <ref type="bibr" target="#b17">(Hu et al., 2018)</ref> and apply it along the channel dimension for the feature output of each block. We also investigate different downsampling strategies, i.e., average pooling (ResNet-50 default) and strided convolution. The results are reported in <ref type="table" target="#tab_5">Table 3</ref>. As can be seen, adding attention (Squeeze-and-Excite (SE) attention) and using more advanced training recipe do improve the robustness of ResNet-50 significantly. We take the best-performing ResNet-50 with all these tricks, denoted as ResNet-50 * , for the following comparison.</p><p>Advantages of ViTs over CNNs on robustness. To make fair comparison, we use all the above validated training tricks to train the ViT-S and ResNet-50 to their best perfor- mance. Specifically, ResNet-50 * is trained with DeiT recipe, SE and strided convolution; ViT-S * is also trained with DeiT recipe and has 12 blocks with 384 embedding dimension for matching the model size as ResNet-50. Results in <ref type="table" target="#tab_6">Table  4</ref> show that even with the same training recipe, ViTs still outperform ResNet-50 in robustness. These results indicate that the improved robustness in ViTs may come from their architectural advantages with self-attention. This motivates us to further improve the architecture of ViTs by leveraging self-attention more broadly to further strengthen the model's robustness. Difference among ViT, SWIN-ViT and ConvNeXt. Very recent CNN models has shown superiority of the robustness over the recent state-of-the-art transformer based models SWIN transformer. We here interpret this from the view of information bottleneck. As explained in Sec. 2.3, the SA module is forming an IB to select essential tokens. As SWIN transformer deploys a window based local self-attention mechanism, it forces the model to select information from a predefined window area. Such a local window IB forces each window to select tokens from a local constrained features. Intuitively, when a selected window contains no essential information, a local SA is forced to select some key tokens and thus resulting a set of sub-optimal clusters. Thus, the robustness of SWIN transformer is worse than the recent SOTA CNN model ConvNeXt. However, as shown in <ref type="table" target="#tab_7">Table  5</ref>, DeiT achieve better robustness with 24.1% less number of parameters, compared to ConvNeXt model. We thus argue that transformers with global SA module are still more robust than the state-of-the-art ConvNeXt model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Fully Attentional Networks</head><p>In this subsection, we investigate how the new FAN architecture improves the model's robustness among different architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impacts of efficient channel attention</head><p>We first ablate the impacts of different forms of channel attentions in terms of GPU memory consumption, clean image accuracy and robustness. The results are shown in <ref type="table" target="#tab_8">Table 6</ref>. Compared to the original self-attention module, SE attention consumes less memory and achieve comparable clean image accuracy and model robustness. By taking the spatial relationship into consideration, our proposed CSA produces the best model robustness with comparable memory consumption to the SE attention. This is possibly because their local attention hinders global clustering and the IB-based information extraction, as detailed in Section 3.2. The drop in robustness can be effectively remedied by using the FAN block. By adding the ECA to the feature transformation of SWIN models, we obtain the FAN-SWIN, a new FAN model whose spatial self-attention is augmented by the shifted window attention in SWIN. As shown in <ref type="table" target="#tab_9">Table 7</ref>, adding FAN block improves the accuracy on ImageNet-C by 5%. Such a significant improvement shows that our proposed CSA does have significant effectiveness on improving the model robustness. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FAN-Hybrid.</head><p>From the clustering process as presented in <ref type="figure" target="#fig_2">Figure 3</ref>, we find that the clustering mainly emerges at the top stages of the FAN model, implying the bottom stages to focus on extracting local visual patterns. Motivated by this, we propose to use convolution blocks for the bottom two stages with down-sampling and then append FAN blocks to the output of the convolutional stages. Each stage includes 3 convolutional blocks. This gives the FAN-Hybrid model. In particular, we use the ConvNeXt <ref type="bibr">(Liu et al., 2022)</ref>, a very recent CNN model, to build the early stages of our hybrid model. As shown in <ref type="table" target="#tab_9">Table 7</ref>, we find original ConvNeXt exhibits strong robustness than SWIN transformer, but performs less robust than FAN-ViT and FAN-Swin models. However, the FAN-Hybrid achieves comparable robustness as FAN-ViT and FAN-SWIN and presents higher accuracy for both clean and corrupted datasets, implying FAN can also effectively strengthen the robustness of a CNN-based model. Similar to FAN-SWIN, FAN-Hybrid enjoys efficiency for processing large-resolution inputs and dense prediction tasks, making it favorable for downstream tasks. Thus, for all downstream tasks, we use FAN-Hybrid model to compare with other state-of-the-art models. More details on the FAN-Hybrid and FAN-SWIN architecture can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Comparison to SOTAs on various tasks</head><p>In this subsection, we evaluate the robustness of FAN with other SOTA methods against common corruptions on different downstream tasks, including image classification (ImageNet-C), semantic segmentation (Cityscapes-C) and object detection (COCO-C). Additionally, we evaluate the robustness of FAN on various other robustness benchmarks including ImageNet-A and ImageNet-R to further show its non-trivial improvements in robustness.</p><p>Robustness in image classification. We first compare the robustness of FAN with other SOTA models by directly applying them (pre-trained on ImageNet-1K) to the ImageNet-C dataset <ref type="bibr" target="#b14">(Hendrycks &amp; Dietterich, 2019)</ref> without any finetuning. We divide all the models into three groups according to their model size for fair comparison. The results are shown in <ref type="table" target="#tab_10">Table 8</ref> and the detailed results are summarized in <ref type="table" target="#tab_3">Table 12</ref>. From the results, one can clearly observe that all the transformer-based models show stronger robustness than CNN-based models. Under all the models sizes, our proposed FAN models surpass all other models significantly. They offer strong robustness to all the types of corruptions. Notably, FANs perform excellently robust for bad weather conditions and digital noises, making them very suitable for vision applications in mobile phones and self-driving cars.</p><p>We also evaluate the zero-shot robustness of the Swin transformer and the recent ConvNeXt. Both of them demonstrate weaker robustness than the transformers with global selfattention. However, adding FAN to them improves their robustness, enabling the resulted FAN-SWIN and FAN-Hybrid variants to inherit both high applicability for downstream tasks and strong robustness to corruptions. We will use FAN-Hybrid variants in the applications of segmentation and detection.</p><p>Robustness in semantic segmentation. We further eval-Impulse Noise</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Segformer-B2 FAN-S-H</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet-50 Impulse Noise</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Segformer-B2 FAN-S-H</head><p>ResNet-50 <ref type="figure">Figure 6</ref>. Segmentation visualization on corrupted images with impulse noise (severity 3) and snow (severity 3). We select the recent state-of-the-art SegFormer model  as a strong baseline. FAN-S-H denotes our hybrid model. Under comparable model size and computation, FAN achieve significantly improved segmentation results over ResNet-50 and SegFormer-B2 model. A video demo is available via external players and in <ref type="figure">Figure 8</ref> in the appendix. uate robustness of our proposed FAN model for the segmentation task. We use the Cityscapes-C for evaluation, which expands the Cityscapes validation set with 16 types of natural corruptions. We compare our model to variants of DeeplabV3+ and latest SOTA models. The results are summarized in <ref type="table" target="#tab_11">Table 9</ref> and by category results are summarized in <ref type="table" target="#tab_3">Table 13</ref>. Our model significantly outperforms previous models. FAN-S-Hybrid surpasses the latest SegFormer-a transformer based segmentation model-by 6.8% mIoU with comparable model size. The results indicate strong robustness of FAN. Robustness in object detection. We also evaluate the robustness of FAN for the detection task on COCO-C dataset, an extension of COCO generated similarly as Cityscapes-C. The results are summarized in <ref type="table" target="#tab_3">Table 10</ref>  Robustness against out-of-distribution. The FAN encourages token features to form clusters and implicitly selects the informative features, which would benefit generaliza- tion performance of the model. To verify this, we directly test our ImageNet-1K trained models for evaluating their robustness, in particular for out-of-distribution samples, on ImageNet-A and ImageNet-R. The experiment results are summarized in <ref type="table" target="#tab_3">Table 11</ref>. Among these models, ResNet-50 <ref type="bibr">(Liu et al.)</ref> presents weakest generalization ability while the recent ConvNeXt substantially improves the generalization performance of CNNs. The transformer-based models, Swin and RVT performs comparably well as ConvNeXt and much better than ResNet-50. Our proposed FANs outperform all these models significantly, implying the fully-attentional architecture aids generalization ability of the learned representations as the irrelevant features are effectively processed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Works</head><p>Vision Transformers <ref type="bibr" target="#b40">(Vaswani et al., 2017)</ref> are a family of transformer-based architectures on computer vision tasks. Unlike CNNs relying on certain inductive biases (e.g., locality and translation invariance), ViTs perform the global interactions among visual tokens via self-attention, thus having less inductive bias about the input image data. Such designs have offered significant performance improvement on various vision tasks including image classification <ref type="bibr" target="#b8">(Dosovitskiy et al., 2020;</ref><ref type="bibr">Yuan et al., 2021;</ref><ref type="bibr" target="#b54">Zhou et al., 2021a;</ref>, object detection <ref type="bibr" target="#b2">(Carion et al., 2020;</ref><ref type="bibr">Zhu et al., 2020;</ref><ref type="bibr" target="#b7">Dai et al., 2021;</ref><ref type="bibr" target="#b52">Zheng et al., 2020)</ref> and segmentation <ref type="bibr" target="#b22">Liu et al., 2021;</ref><ref type="bibr" target="#b52">Zheng et al., 2020)</ref>. The success of vision transformers for vision tasks triggers broad debates and studies on the advantages of self-attention versus convolutions <ref type="bibr">(Raghu et al., 2021;</ref><ref type="bibr" target="#b34">Tang et al., 2021)</ref>. Compared to convolutions, an important advantage is the robustness against observable corruptions. Several works <ref type="bibr" target="#b0">(Bai et al., 2021;</ref><ref type="bibr" target="#b53">Zhu et al., 2021;</ref><ref type="bibr">Paul &amp; Chen, 2022;</ref><ref type="bibr">Naseer et al., 2021)</ref> have empirically shown that the robustness of ViTs against corruption consistently outperforms ConvNets by significant margins. However, how the key component (i.e. self-attention) contributes to the robustness is under-explored. In contrast, our work conducts empirical studies to reveal intriguing properties (i.e., token grouping and noise absorbing) of self-attention for robustness and presents a novel fully attentional architecture design to further improve the robustness.</p><p>There exists a large body of work on improving robustness of deep learning models in the context of adversarial examples by developing robust training algorithms <ref type="bibr" target="#b20">(Kurakin et al., 2016;</ref><ref type="bibr" target="#b32">Shao et al., 2021)</ref>, which differs from the scope of our work. In this work, we focus the zero-shot robustness to the natural corruptions and mainly study improving model's robustness from the model architecture perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we verified self-attention as a contributor of the improved robustness in vision transformers. Our study shows that self-attention promotes naturally formed clusters in tokens, which exhibits interesting relation to the extensive early studies in vision grouping prior to deep learning. We also established an explanatory framework from the perspective of information bottleneck to explain these properties of self-attention. To push the boundary of robust representation learning with self-attention, we introduced a family of fully-attentional network (FAN) architectures, where self-attention is leveraged in both token mixing and channel processing. FAN models demonstrate significantly improved robustness over their CNN and ViT counterparts.</p><p>Our work provides a new angle towards understanding the working mechanism of vision transformers, showing the po-tential of inductive biases going beyond convolutions. Our work can benefit wide real-world applications, especially safety-critical ones such as autonomous driving.</p><p>Zhu, C., Ping, W., Xiao, C., Shoeybi, M., Goldstein, T., Anandkumar, A., and Catanzaro, B. Long-short transformer: Efficient transformers for language and vision. In NeurIPS, 2021.</p><p>Zhu, X., Su, W., Lu, L., Li, B., Wang, X., and Dai, J. Deformable detr: Deformable transformers for end-to-end object detection. ICLR, 2020.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supplementary Details</head><p>A.1. Proof on the relationship between the Information Bottleneck and Self-Attention Given a distribution X ? N (X , ) with X being the observed noisy input and X the target clean code, IB seeks a mapping f (Z|X) such that Z contains the relevant information in X for predicting X . This goal is formulated as the following information-theoretic optimization problem q * IB (z|x) = arg min q(z|x)</p><formula xml:id="formula_8">I(X, Z) ? ?I(Z, X ),<label>(8)</label></formula><p>subject to the Markov constraint Z ? X ? X . ? is a free parameter that trades-off the information compression by the first term and the relevant information maintaining by the second.</p><p>The information bottleneck approach can be applied for solving unsupervised clustering problems. Here we choose X to be the data point with index i that will be clustered into clusters with indices c.</p><p>As mentioned above, we assume the following data distribution:</p><formula xml:id="formula_9">p(x|i) ? exp ? 1 2 2 x ? x i 2 ,<label>(9)</label></formula><p>where s is a smoothing parameter. We assume the marginal to be p(i) = 1 N , where N is the number of data points. Using the above notations, the t-th step in the iterative IB for clustering is formulated as</p><formula xml:id="formula_10">q (t) (c|i) = log q (t?1) (c) K(x, ?) exp ?? KL[p(x|i)|q (t?1) (x|c)] , q (t) (c) = n (t) c N , q (t) (x|c) = 1 n (t) c i?S (t) c p(x|i).<label>(10)</label></formula><p>Here K(x, ?) is the normalizing factor and S c denotes the set of indices of data points assigned to cluster c.</p><p>We choose to replace q(x|c) with a Gaussian approximation g(x|c) = N (x|? c , ? c ) and assume is sufficiently small. Then,</p><formula xml:id="formula_11">KL[p(x|i)|g(x|c)] ? (? c ? x i ) ? ?1 c (? c ? x i ) + log det ? c + B,<label>(11)</label></formula><p>where B denotes terms not dependent on the assignment of data points to clusters and thus irrelevant for the objective. Thus the above cluster update can be written as:</p><formula xml:id="formula_12">q (t) (c|i) = log q (t?1) (c) det ? c exp ?(? c ? x i ) ? ?1 c (? c ? x i ) Z(x, ?) = log q (t?1) (c) det ? c exp ?(? c ? x i ) ? ?1 c (? c ? x i ) c exp ?(? c ? x i ) ? ?1 c (? c ? x i ) .<label>(12)</label></formula><p>The next step is to update ? c to minimize the KL-divergence between g(x|c) and p(x|c):</p><formula xml:id="formula_13">KL[q(x|c)|g(x|c)] = ? q(x|c) log g(x|c)dx ? H[q(x|c)] = ? 1 n c i?Sc N (x; x i , 2 ) log g(x|c)dx ? H[q(x|c)] ? ? 1 n c i?Sc log g(x i |c) ? H[q(x|c)]<label>(13)</label></formula><p>Minimizing the above w.r.t. ? c gives:</p><formula xml:id="formula_14">? (t) c = 1 N N i=1 q(c|i)x i = N i=1 log q (t?1) (c) N det ? c exp ?(? c ? x i ) ? ?1 c (? c ? x i ) c exp ?(? c ? x i ) ? ?1 c (? c ? x i ) x i .<label>(14)</label></formula><p>By properly re-arranging the above terms and writing them into a compact matrix form, the relationship between the IB approach and self-attention would become clearer. Assume ? c = ? is shared across all the clusters. Assume ? c are normalized w.r.t. ? ?1 c , i.e., ? c ? ?1 c ? c = 1. <ref type="bibr">. .</ref> . , x N ]. Define d = 1/2. Then the above update (15) can be written as:</p><formula xml:id="formula_15">? (t) c = N i=1 log[n c /N ] N det ? exp ? c ? ?1 xi 1/2 c exp ? c ? ?1 xi 1/2 x i .<label>(15)</label></formula><formula xml:id="formula_16">Define Z = [? (t) 1 ; . . . ; ? (t) N ], V = [x 1 , . . . , x N ]W V , K = [? (t?1) 1 , . . . , ? (t?1) N ], Q = ? ?1 [x 1 ,</formula><formula xml:id="formula_17">Z = Softmax Q K d V.<label>(16)</label></formula><p>Here the softmax normalization is applied along the row direction. Thus we conclude the proof for Proposition 2.1.</p><p>Proposition 2.1 can be proved by following the above road map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Implementation details</head><p>Architecture implementation When comparing to other state-of-the-art methods, we add in a depthwise convolution layer in the MLP block following the practice in previous methods <ref type="bibr">El-Nouby et al., 2021)</ref>. For the shortcut connection, we multiply the residual path with a learnable parameter to stabilize the training, following the same practice in <ref type="bibr" target="#b38">(Touvron et al., 2021b)</ref>.</p><p>ImageNet classification For all the experiments and ablation studies, the models are pretrained on ImageNet-1K if not specified additionally. The training recipes follow the one used in <ref type="bibr" target="#b37">(Touvron et al., 2021a)</ref> for both the baseline model and our proposed FAN model family. Specifically, we train FAN for 300 epochs using AdamW with a learning rate of 2e-3. We use 5 epochs to linearly warmup the model. We adopt a cosine decaying schedule afterward. We use a batch size of 2048 and a weight decay of 0.05. We adopt the same data augmentation schemes as <ref type="bibr" target="#b37">(Touvron et al., 2021a)</ref> including Mixup, Cutmix, RandAugment, and Random Erasing. We use Exponential Moving Average (EMA) to speed up the model convergence in a similar manner as timm library <ref type="bibr" target="#b43">(Wightman, 2019)</ref>. For the image classification tasks, we also include two class attention blocks at the top layers as proposed by Touvron et al..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic segmentation and object detection</head><p>For FAN-ViT, we follow the same decoder proposed in semantic transformer (SETR) <ref type="bibr" target="#b53">(Zheng et al., 2021)</ref> and the same training setting used in Segformer . For object detection, we finetune the faster RCNN <ref type="bibr" target="#b30">(Ren et al., 2015)</ref> with 2x multi-scale training. The resolution of the training image is randomly selected from 640?640 to 896 ? 896. We use a deterministic image resolution of size 896? 896 for testing.</p><p>For FAN-Swin and FAN-Hybrid, We finetune Mask R-CNN <ref type="bibr" target="#b11">(He et al., 2017)</ref> on the COCO dataset. Following Swin Transformer <ref type="bibr" target="#b22">(Liu et al., 2021)</ref>, we use multi-scale training, AdamW optimizer, and 3x schedule. The codes are developed using MMSegmentation <ref type="bibr" target="#b6">(Contributors, 2020)</ref> and MMDetection <ref type="bibr" target="#b4">(Chen et al., 2019)</ref> toolbox.</p><p>Corruption dataset preparation For ImageNet-C, we directly download it from the mirror image provided by Hendrycks &amp; Dietterich. For Cityscape-C and COCO-C, we follow Kamann &amp; Rother and generate 16 algorithmically generated corruptions from noise, blur, weather and digital categories.</p><p>Evaluation metrics For ImageNet-C, we use retentaion as a main metric to measure the robustness of the model which is defined as ImageNet-C Acc. ImageNet Clean Acc . It measures how much accuracy can be reserved when evaluated on ImageNet-C dataset. When comparing with other models, we also report the mean corruption error (mCE) in the same manner defined in the ImageNet-C paper <ref type="bibr" target="#b14">(Hendrycks &amp; Dietterich, 2019)</ref>. The evaluation code is based on timm library <ref type="bibr" target="#b43">(Wightman, 2019)</ref>. For semantic segmentation and object detection, we load the ImageNet-1k pretrained weights and finetune on Cityscpaes and COCO clean image dataset. Then we directly evaluate the performance on Cityscapes-C and COCO-C. We report semantic segmentation performance using mean Intersection over Union (mIoU) and object detection performance using mean average precision (mAP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Impact of head numbers</head><p>A.4. Detailed benchmark results on corrupted images on classification, segmentation and detection</p><p>The by category robustness of selected models and FAN models are shown in Tab. 12, Tab. 13 and Tab. 14 respectively. As shown, the strong robustness of FAN is transferrable to all downstreaming tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Architecture details of FAN-Swin and FAN-Hybrid</head><p>For FAN-Swin architecture, we follow the same macro architecture design by only replacing the conventional self-attention module with the efficient window shift self-attention in the same manner as proposed in the Swin transformer <ref type="bibr" target="#b22">(Liu et al., 2021)</ref>. For the FAN-Hybrid architecture, we use three convolutional building blocks for each stage in the same architecture as proposed in ConvNeXt <ref type="bibr">(Liu et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6. Feature clustering and visualizations</head><p>To cluster the token features, we first normalize the tokens taken from the second last block's output with a SoftMax function. We then calculate a self-correlation matrix based on the normalized tokens and use it as the affinity matrix for spectral clustering. <ref type="figure">Figure 9</ref> provides more visualization on clustering results of token features from our FAN, ViT and CNN models. The visualization on Cityscape is shown in <ref type="figure">Figure 8</ref>.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Main results on ImageNet-C (top figure) and clustering visualization (bottom row)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Comparison between conventional ViT block and the proposed FAN block. (a) ViT block: Input tokens are first aggregated by self-attention, followed by a linear projection and an MLP is appended to the self attention block for feature transformation. (b) FAN block: both token self-attention and channel attention are applied, which makes the entire network fully attentional. The linear projection layer after the channel attention is removed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Analysis on the grouping of tokens and noise decay. (a) and (b) shows the # of insignificant (zero) eigenvalues and the noise input decay of ViT-S and FAN-S respectively; (c) shows the comparison of noise norm across different blocks in FAN-S, ViT-S and ResNet-50. Plots shown in (a) and (b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Clustering visualization for different blocks. The visualization is based on our proposed FAN-S model as detailed in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Comparison among channel attention designs. (a) CA: a channel self attention design similar to XCiT (El-Nouby et al., 2021), but differently applied on the output of the MLP block. (b) The proposed efficient channel attention (ECA).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Impacts of head number on model robustness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .Figure 9 .</head><label>89</label><figDesc>12. Coomparison of model robustness on ImageNet-C (%). FAN shows stronger robustness than other models under all the image corruption settings. 'ResNet-50 * ' denotes our reproduced results with the same training and augmentation recipes for fair comparison. Visualization on Cityscapes. A video demonstration is available with external player Understanding The Robustness in Vision Transformers clustering visualization. Our FAN model provides much clearer clusters that feature important regions of foreground objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1 Institute of Data Science, National University of Singapore 2 NVIDIA 3 The University of Hong Kong 4 ASU 5 Caltech 6 ByteDance. Correspondence to: Zhiding Yu &lt;zhid-ingy@nvidia.com&gt;. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Details and abbreviations of different FAN variants. Model #Blocks Channel Dim. #Heads Param. FLOPs</figDesc><table><row><cell>FAN-T</cell><cell>12</cell><cell>192</cell><cell>4</cell><cell>7.3M 1.4G</cell></row><row><cell>FAN-S</cell><cell>12</cell><cell>384</cell><cell>8</cell><cell>28.3M 5.3G</cell></row><row><cell>FAN-B</cell><cell>18</cell><cell>448</cell><cell>8</cell><cell>54.0M 10.4G</cell></row><row><cell>FAN-L</cell><cell>24</cell><cell>480</cell><cell>10</cell><cell>80.5M 15.8G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Impacts of various performance improvement tricks on model robustness (%).</figDesc><table><row><cell>Model</cell><cell cols="3">IN-1K IN-C Retention mCE (?)</cell></row><row><cell>ViT-S</cell><cell>77.9 54.2</cell><cell>70</cell><cell>63.5</cell></row><row><cell>+ DeiT Recipe</cell><cell>79.3 57.1</cell><cell>72</cell><cell>57.1</cell></row><row><cell cols="2">+ #Blocks (8 ?12) 79.9 58.0</cell><cell>72</cell><cell>56.2</cell></row><row><cell>+ KD</cell><cell>81.3 59.6</cell><cell>73</cell><cell>54.0</cell></row><row><cell>+ IN22K w/o KD</cell><cell>81.8 59.7</cell><cell>73</cell><cell>54.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Robustness of ResNet-50 with various performance improvement tricks (%).</figDesc><table><row><cell>Model</cell><cell cols="3">IN-1K IN-C Retention mCE (?)</cell></row><row><cell>ResNet-50</cell><cell>76.0 38.8</cell><cell>51</cell><cell>76.7</cell></row><row><cell cols="2">+ DeiT Recipe 79.0 43.9</cell><cell>46</cell><cell>69.7</cell></row><row><cell>+ SE</cell><cell>79.8 50.1</cell><cell>63</cell><cell>63.1</cell></row><row><cell cols="2">+ Strided Conv 80.2 52.1</cell><cell>65</cell><cell>61.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Robustness</figDesc><table><row><cell></cell><cell cols="4">comparison between ResNet-50 and ViT-S</cell></row><row><cell>(%).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="4">Param IN-1K IN-C Retention mCE (?)</cell></row><row><cell cols="2">ResNet-50  *  25M</cell><cell>80.2 52.1</cell><cell>65</cell><cell>61.6</cell></row><row><cell>ViT-S  *</cell><cell>22M</cell><cell>79.9 58.0</cell><cell>72</cell><cell>56.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Robustness comparison among Swin, ConvNeXt, DeiT and FAN. The mIoU of ConvNeXt, DeiT, Swin and Seg-Former models are our reproduced results.</figDesc><table><row><cell>Model</cell><cell>Param.</cell><cell cols="3">ImageNet Clean Corrupt Reten. Clean Corrupt Reten. Cityscapes</cell></row><row><cell cols="3">ConvNeXt (Liu et al.) 29M 82.1 59.1</cell><cell>72.0 79.0 54.2</cell><cell>68.6</cell></row><row><cell>SWIN (Liu et al.)</cell><cell cols="2">28M 81.3 55.4</cell><cell>68.1 78.0 47.3</cell><cell>61.7</cell></row><row><cell cols="3">DeiT-S (Touvron et al.) 22M 79.9 58.1</cell><cell>72.7 76.0 55.4</cell><cell>72.9</cell></row><row><cell cols="3">FAN-Hybrid-S (Ours) 26M 83.5 64.7</cell><cell>78.2 81.5 66.4</cell><cell>81.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Effects</figDesc><table><row><cell cols="5">of different channel attentions on model robustness</cell></row><row><cell>(%).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="4">Mem.(M) IN-1K IN-C Retention mCE (?)</cell></row><row><cell>FAN-ViT-S-SA</cell><cell>235</cell><cell>81.3 61.7</cell><cell>76</cell><cell>51.4</cell></row><row><cell>FAN-ViT-S-SE</cell><cell>126</cell><cell>81.2 62.0</cell><cell>76</cell><cell>50.0</cell></row><row><cell>FAN-ViT-S-ECA</cell><cell>127</cell><cell>82.5 64.6</cell><cell>78</cell><cell>47.7</cell></row></table><note>FAN-ViT &amp; FAN-Swin. Using the FAN block to replace the conventional transformer block forms the FAN-ViT. FAN-ViT significantly enhances the robustness. However, compared to ViT, the robustness of Swin architecture (Liu et al., 2021) (which uses shifted window attention) drops.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 .</head><label>7</label><figDesc>Effects of architectural changes on model robustness (%).</figDesc><table><row><cell>Model</cell><cell cols="3">IN-1K IN-C Retention mCE (?)</cell></row><row><cell>ViT-S  *</cell><cell>79.9 58.1</cell><cell>73</cell><cell>56.2</cell></row><row><cell>+ FAN</cell><cell>81.3 61.7</cell><cell>76</cell><cell>51.4</cell></row><row><cell>Swin-T</cell><cell>81.4 55.4</cell><cell>68</cell><cell>59.6</cell></row><row><cell>+ FAN</cell><cell>81.9 59.4</cell><cell>73</cell><cell>54.5</cell></row><row><cell cols="2">ConvNeXt-T 82.1 59.1</cell><cell>72</cell><cell>54.8</cell></row><row><cell>+ FAN</cell><cell>82.5 60.8</cell><cell>74</cell><cell>53.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>Main results on image classification. FAN models show improved performance in both clean accuracy and robustness than other models. ? denotes models are pretrained on ImageNet-22K.</figDesc><table><row><cell>Model</cell><cell cols="3">Param./FLOPs IN-1K IN-C Retention</cell></row><row><cell>ResNet18 (He et al.)</cell><cell>11M/1.8G</cell><cell cols="2">69.9 32.7 46.8%</cell></row><row><cell>MBV2 (Sandler et al.)</cell><cell>4M/0.4G</cell><cell cols="2">73.0 35.0 47.9%</cell></row><row><cell>EffiNet-B0 (Tan &amp; Le)</cell><cell>5M/0.4G</cell><cell cols="2">77.5 41.1 53.0%</cell></row><row><cell>PVTV2-B0 (Wang et al.)</cell><cell>3M/0.6G</cell><cell cols="2">70.5 36.2 51.3%</cell></row><row><cell>PVTV2-B1 (Wang et al.)</cell><cell>13M/2.1G</cell><cell cols="2">78.7 51.7 65.7%</cell></row><row><cell>FAN-T-ViT</cell><cell>7M/1.3G</cell><cell cols="2">79.2 57.5 72.6%</cell></row><row><cell>FAN-T-Hybrid</cell><cell>7M/3.5G</cell><cell cols="2">80.1 57.4 71.7%</cell></row><row><cell>ResNet50 (He et al.)</cell><cell>25M/4.1G</cell><cell>79</cell><cell>50.6 64.1%</cell></row><row><cell>DeiT-S (Touvron et al.)</cell><cell>22M/4.6G</cell><cell cols="2">79.9 58.1 72.7%</cell></row><row><cell>Swin-T (Liu et al.)</cell><cell>28M/4.5G</cell><cell cols="2">81.3 55.4 68.1%</cell></row><row><cell>ConvNeXt-T (Liu et al.)</cell><cell>29M/4.5G</cell><cell cols="2">82.1 59.1 71.9%</cell></row><row><cell>FAN-S-ViT</cell><cell>28M/5.3G</cell><cell cols="2">82.9 64.5 77.8%</cell></row><row><cell>FAN-S-Hybrid</cell><cell>26M/6.7G</cell><cell cols="2">83.5 64.7 77.5%</cell></row><row><cell>Swin-S(Liu et al.)</cell><cell>50M/8.7G</cell><cell cols="2">83.0 60.4 72.8%</cell></row><row><cell>ConvNeXt-S (Liu et al.)</cell><cell>50M/8.7G</cell><cell cols="2">83.1 61.7 74.2%</cell></row><row><cell>FAN-B-ViT</cell><cell>54M/10.4G</cell><cell cols="2">83.6 67.0 80.1%</cell></row><row><cell>FAN-B-Hybrid</cell><cell>50M/11.3G</cell><cell cols="2">83.9 66.4 79.1%</cell></row><row><cell>FAN-B-Hybrid  ?</cell><cell>50M/11.3G</cell><cell cols="2">85.6 70.5 82.4%</cell></row><row><cell>DeiT-B (Touvron et al.)</cell><cell>89M/17.6G</cell><cell cols="2">81.8 62.7 76.7%</cell></row><row><cell>Swin-B (Liu et al.)</cell><cell>88M/15.4G</cell><cell cols="2">83.5 60.4 72.3%</cell></row><row><cell>ConvNeXt-B (Liu et al.)</cell><cell>89M/15.4G</cell><cell cols="2">83.8 61.7 73.6%</cell></row><row><cell>FAN-L-ViT</cell><cell>81M/15.8G</cell><cell cols="2">83.9 67.7 80.7%</cell></row><row><cell>FAN-L-Hybrid</cell><cell>77M/16.9G</cell><cell cols="2">84.3 68.3 81.0%</cell></row><row><cell>FAN-L-Hybrid  ?</cell><cell>77M/16.9G</cell><cell cols="2">86.5 73.6 85.1%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 .</head><label>9</label><figDesc>Main results on semantic segmentation.</figDesc><table><row><cell>'R-' and 'X-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 .</head><label>10</label><figDesc>Main results on object detection. FAN shows stronger clean accuracy and robustness than other models. ' ?' denotes the accuracy pretrained on ImageNet-22K.</figDesc><table><row><cell>Model</cell><cell cols="4">Encoder Size COCO COCO-C Retention</cell></row><row><cell></cell><cell>Mask R-CNN</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet-50 (He et al., 2016)</cell><cell>25.4M</cell><cell>39.9</cell><cell>21.3</cell><cell>53.3%</cell></row><row><cell>ResNet101 ((He et al., 2016))</cell><cell>44.1M</cell><cell>41.8</cell><cell>23.3</cell><cell>55.7%</cell></row><row><cell>DeiT-S (Touvron et al., 2021a)</cell><cell>22.1M</cell><cell>40.0</cell><cell>26.9</cell><cell>67.3%</cell></row><row><cell>Swin-T (Liu et al., 2021)</cell><cell>28.0M</cell><cell>46.0</cell><cell>29.3</cell><cell>63.7%</cell></row><row><cell>FAN-T-Hybrid</cell><cell>7.4M</cell><cell>45.8</cell><cell>29.7</cell><cell>64.8%</cell></row><row><cell>FAN-S-Hybrid</cell><cell>26.3M</cell><cell>49.1</cell><cell>35.5</cell><cell>72.3%</cell></row><row><cell cols="3">Cascade Mask R-CNN</cell><cell></cell><cell></cell></row><row><cell>FAN-T-Hybrid</cell><cell>7.4M</cell><cell>50.2</cell><cell>33.1</cell><cell>65.9%</cell></row><row><cell>FAN-S-Hybrid</cell><cell>26.3M</cell><cell>53.3</cell><cell>38.7</cell><cell>72.6%</cell></row><row><cell>FAN-L-Hybrid</cell><cell>76.8M</cell><cell>54.1</cell><cell>40.6</cell><cell>75.0%</cell></row><row><cell>FAN-L-Hybrid  ?</cell><cell>76.8M</cell><cell>55.1</cell><cell>42.0</cell><cell>76.2%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 .</head><label>11</label><figDesc>Main results on out-of-distribution generalization. FAN models show improved generalization across all datasets. ' ? ' denotes results with finetuning on 384 ? 384 image resolution. IN-C is measured by mCE (?). All metrics are scaled by (%).</figDesc><table><row><cell>Model</cell><cell cols="3">Params (M) Clean IN-A IN-R IN-C</cell></row><row><cell></cell><cell cols="2">ImgNet-1k pretrain</cell><cell></cell></row><row><cell>XCiT-S12 (El-Nouby et al.)</cell><cell>26.3</cell><cell>81.9</cell><cell>25.0 45.5 51.5</cell></row><row><cell>XCiT-S24 (El-Nouby et al.)</cell><cell>47.7</cell><cell>82.6</cell><cell>27.8 45.5 49.4</cell></row><row><cell>RVT-S* (Mao et al.)</cell><cell>23.3</cell><cell>81.9</cell><cell>25.7 47.7 51.4</cell></row><row><cell>RVT-B* (Mao et al.)</cell><cell>91.8</cell><cell>82.6</cell><cell>28.5 48.7 46.8</cell></row><row><cell>Swin-T (Liu et al.)</cell><cell>28.3</cell><cell>81.2</cell><cell>21.6 41.3 59.6</cell></row><row><cell>Swin-S (Liu et al.)</cell><cell>50</cell><cell>83.4</cell><cell>35.8 46.6 52.7</cell></row><row><cell>Swin-B (Liu et al.)</cell><cell>87.8</cell><cell>83.4</cell><cell>35.8 64.2 54.4</cell></row><row><cell>ConvNeXt-T (Liu et al.)</cell><cell>28.6</cell><cell>82.1</cell><cell>24.2 47.2 53.2</cell></row><row><cell>ConvNeXt-S (Liu et al.)</cell><cell>50.2</cell><cell>82.1</cell><cell>31.2 49.5 51.2</cell></row><row><cell>ConvNeXt-B (Liu et al.)</cell><cell>88.6</cell><cell>83.8</cell><cell>36.7 51.3 46.8</cell></row><row><cell>MAE-ViT-B (He et al.)</cell><cell>86</cell><cell>83.6</cell><cell>35.9 48.3 51.7</cell></row><row><cell>MAE-ViT-L (He et al.)</cell><cell>307</cell><cell>85.9</cell><cell>57.1 59.9 41.8</cell></row><row><cell>FAN-S-ViT (Ours)</cell><cell>28.0</cell><cell>82.5</cell><cell>29.1 50.4 47.7</cell></row><row><cell>FAN-B-ViT (Ours)</cell><cell>54.0</cell><cell>83.6</cell><cell>35.4 51.8 44.4</cell></row><row><cell>FAN-L-ViT (Ours)</cell><cell>80.5</cell><cell>83.9</cell><cell>37.2 53.1 43.3</cell></row><row><cell>FAN-S-Hybrid (Ours)</cell><cell>26.0</cell><cell>83.6</cell><cell>33.9 50.7 47.8</cell></row><row><cell>FAN-B-Hybrid (Ours)</cell><cell>50.0</cell><cell>83.9</cell><cell>39.6 52.9 45.2</cell></row><row><cell>FAN-L-Hybrid (Ours)</cell><cell>76.8</cell><cell>84.3</cell><cell>41.8 53.2 43.0</cell></row><row><cell></cell><cell cols="2">ImgNet-22k pretrain</cell><cell></cell></row><row><cell>ConvNeXt-B  ? (Liu et al.)</cell><cell>88.6</cell><cell>86.8</cell><cell>62.3 64.9 43.1</cell></row><row><cell>FAN-L-Hybrid (Ours)</cell><cell>76.8</cell><cell>86.5</cell><cell>60.7 64.3 35.8</cell></row><row><cell>FAN-L-Hybrid  ? (Ours)</cell><cell>76.8</cell><cell>87.1</cell><cell>74.5 71.1 36.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Are transformers more robust than cnns? In NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Image recognition: Visual grouping, recognition, and learning. Proceedings of the National Academy of Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="14203" to="14204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joulin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9650" to="9660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Mmdetection: Open mmlab detection toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Mmsegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Contributors</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Up-detr: Unsupervised pre-training for object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1601" to="1610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Xcit: Cross-covariance image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Natural adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15262" to="15271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rethinking spatial dimensions of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11936" to="11945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Benchmarking the robustness of semantic segmentation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kamann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8828" to="8838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>A convnet for the 2020s. CVPR, 2022</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards robust vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Intriguing properties of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ranasinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vision transformers are robust learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Do vision transformers see like convolutional neural networks? NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15670</idno>
		<title level="m">On the adversarial robustness of visual transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Sparse mlp for image recognition: Is self-attention really necessary?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.05422</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning and the information bottleneck principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Information Theory Workshop (ITW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
		<title level="m">The information bottleneck method. physics/0004057</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Going deeper with image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="32" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Reorganization: Grouping, contour detection, segmentation, ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">C</forename><surname>Berkeley</surname></persName>
		</author>
		<ptr target="https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polosukhin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">I. Attention is all you need. NeurIPS</title>
		<imprint>
			<biblScope unit="page">30</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised segmentation of natural images via lossy data compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="212" to="225" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Self-tuning spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Icnet for realtime semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="405" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">End-toend object detection with adaptive clustering transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torr</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepvit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11886</idno>
		<title level="m">Towards deeper vision transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Refining self-attention for vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Refiner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03714</idno>
	</analytic>
	<monogr>
		<title level="m">2021b. Model Param. Average Blur Noise Digital Weather Motion Defoc Glass Gauss Gauss Impul Shot Speck Contr Satur JPEG Pixel Bright</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Snow Fog Frost Mobile Setting (&lt; 10M</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-B * (</forename><surname>Vit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dosovitskiy</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">88</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<title level="m">Hybrid-IN22K (Ours)</title>
		<imprint>
			<biblScope unit="page">50</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-B-In22k ;</forename><surname>Swin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">88</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The mIoUs of compared CNN models are adopted from (Kamann &amp; Rother, 2020). The mIoU of ConvNeXt, DeiT, Swin and SegFormer models are our reproduced results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Model Average Blur Noise Digital Weather Motion Defoc Glass Gauss Gauss Impul Shot Speck Bright Contr Satur JPEG Snow Spatt Fog Frost</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Table 13. Comparison of Model Robustness on Cityscapes-C (%)</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Comparison of model robustness on COCO-C (%). FAN shows stronger robustness than other models. Model Average Blur Noise Digital Weather Motion Defoc Glass Gauss Gauss Impul Shot Speck Bright</title>
	</analytic>
	<monogr>
		<title level="j">Contr Satur JPEG Snow Spatter Fog Frost ResNet</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<publisher>Faster-RCNN</publisher>
		</imprint>
	</monogr>
	<note>Table 14. Ren et al.</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<title level="m">Hybrid-IN22k (Ours)</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
