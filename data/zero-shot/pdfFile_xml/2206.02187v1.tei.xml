<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">M2FNet: Multi-modal Fusion Network for Emotion Recognition in Conversation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><surname>Chudasama</surname></persName>
							<email>vishal.chudasama1@sony.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Media Analysis Group</orgName>
								<orgName type="institution">Sony Research India</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Purbayan</forename><surname>Kar</surname></persName>
							<email>purbayan.kar@sony.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Media Analysis Group</orgName>
								<orgName type="institution">Sony Research India</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Gudmalwar</surname></persName>
							<email>ashish.gudmalwar@sony.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Media Analysis Group</orgName>
								<orgName type="institution">Sony Research India</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nirmesh</forename><surname>Shah</surname></persName>
							<email>nirmesh.shah@sony.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Media Analysis Group</orgName>
								<orgName type="institution">Sony Research India</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Wasnik</surname></persName>
							<email>pankaj.wasnik@sony.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Media Analysis Group</orgName>
								<orgName type="institution">Sony Research India</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoyuki</forename><surname>Onoe</surname></persName>
							<email>naoyuki.onoe@sony.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Media Analysis Group</orgName>
								<orgName type="institution">Sony Research India</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">M2FNet: Multi-modal Fusion Network for Emotion Recognition in Conversation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Emotion Recognition in Conversations (ERC) is crucial in developing sympathetic human-machine interaction. In conversational videos, emotion can be present in multiple modalities, i.e., audio, video, and transcript. However, due to the inherent characteristics of these modalities, multi-modal ERC has always been considered a challenging undertaking. Existing ERC research focuses mainly on using text information in a discussion, ignoring the other two modalities. We anticipate that emotion recognition accuracy can be improved by employing a multi-modal approach. Thus, in this study, we propose a Multi-modal Fusion Network (M2FNet) that extracts emotion-relevant features from visual, audio, and text modality. It employs a multi-head attention-based fusion mechanism to combine emotion-rich latent representations of the input data. We introduce a new feature extractor to extract latent features from the audio and visual modality. The proposed feature extractor is trained with a novel adaptive marginbased triplet loss function to learn emotion-relevant features from the audio and visual data. In the domain of ERC, the existing methods perform well on one benchmark dataset but not on others. Our results show that the proposed M2FNet architecture outperforms all other methods in terms of weighted average F1 score on well-known MELD and IEMOCAP datasets and sets a new state-of-theart performance in ERC.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Emotions are the unseen mental states that are linked to thoughts and feelings <ref type="bibr" target="#b22">[23]</ref>. In the absence of physiological indications, they could only be detected by human actions such as textual utterances, visual gestures, and acoustic signals. Emotion Recognition in Conversations (ERC) seeks to recognize the human emotions in conversations depending on their textual, visual, and acoustic cues. Recently, ERC * Pankaj Wasnik is the corresponding author. <ref type="figure">Figure 1</ref>. Multi-modal data as input has become an essential task in multimedia content analysis and moderation. It is a prominent trait to understand the nature of the interaction between users and the content. It has applications in various tasks, namely, AI interviews, personalized dialog systems, sentiment analysis, and understanding the user's perception of the content from the platforms like YouTube, Facebook, and Twitter <ref type="bibr" target="#b22">[23]</ref>.</p><p>In literature, we can see that many state-of-the-art methods adopt text-based processing to perform robust ERC <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref>, such methods do not take into consideration the vast amount of information present in the acoustic and visual modalities. Since the ERC data mainly consists of all three modalities, i.e., text, visual, and acoustic, we hypothesize that the robust fusion of these modalities can improve the performance and robustness of the existing systems. A sample of emotional expressions in three different modalities is presented in <ref type="figure">Figure 1</ref> where the ERC system takes each modality as input and predicts the associated emotion.</p><p>In this paper, we propose a multi-modal fusion network (M2FNet) that takes advantage of the multi-modal nature of real-world media content by introducing a novel multi-head fusion attention layer. This layer combines features from different modalities to generate rich emotion-relevant representations by mapping the information from acoustic and visual features to the latent space of the textual features. In addition, we propose a new feature extractor model to ex- tract the deeper features from the audio and visual contents. Here, we introduce a new adaptive margin-based triplet loss function, which helps the proposed extractor to learn representations more effectively. Additionally, we propose a dual network inspired from <ref type="bibr" target="#b12">[13]</ref> to combine the emotional content from the scene by taking into account the multiple people present in it. Furthermore, from the literature, we can see that state-of-the-art ERC methods perform well on one benchmark dataset, for example, IEMOCAP <ref type="bibr" target="#b1">[2]</ref> while their performance degrades on more complex datasets like MELD <ref type="bibr" target="#b21">[22]</ref>. This motivates us to propose a robust multimodal ERC system.</p><p>In order to verify the robustness of the proposed network, one experiment is carried out to compare its performance with existing text-based and multi-modal ERC methods. This comparison is visualized in <ref type="figure" target="#fig_0">Figure 2</ref> where results are given in terms of weighted average F1 score on MELD <ref type="bibr" target="#b1">[2]</ref> and IEMOCAP <ref type="bibr" target="#b21">[22]</ref> datasets. Here, it can be observed that the proposed M2FNet model obtains a higher weighted average F1 score than other models. Followings are our major contributions:</p><p>? A novel multi-modal fusion network called M2FNet is proposed for emotion recognition in conversation.</p><p>? A multi-head attention-based fusion layer is introduced, which aids the proposed system to combine latent representations of the different inputs.</p><p>? To extract deeper relevant features from audio and visual modality utterances, we introduce a new feature extractor model.</p><p>? In the feature extractor model, we propose a new adaptive margin-based triplet loss function that helps the proposed model to learn emotion-relevant features.</p><p>? To take advantage of the scene's emotional content, we also present a weighted face model that considers multiple people present in the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Emotion recognition in conversations (ERC) is different from traditional emotion recognition. Rather than treating emotions as static states, ERC involves emotional dynamics of a conversation, in which the context plays a vital role. Prior works on ERC mainly which use text, and audio features are proposed in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref>. In the past few years, datasets with visual, acoustic and textual cues have been made publicly available <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref>. On these datasets, several deep learning methods are applied to recognize emotion. These techniques can be classified based on the type of data; either they merely utilize text or use multi-modal data (i.e. text, visual and audio).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Text-based methods</head><p>With the advent of the Transformer <ref type="bibr" target="#b31">[32]</ref>, the focus on text-based methods has recently increased. Due to the vast amount of information present in text data, current methods approach ERC as a purely text-based problem. Li et al. <ref type="bibr" target="#b15">[16]</ref> use BERT <ref type="bibr" target="#b4">[5]</ref> to encode the individual sentences and then uses a dialog level network for multitask learning on auxiliary tasks to generate better latent representations of the dialog as a whole. Furthermore, Li et al. <ref type="bibr" target="#b13">[14]</ref> build on this by incorporating transformer at the dialog end. Jiangnan et al. <ref type="bibr" target="#b14">[15]</ref> took this one step further by using the contextual representations from a BERT and transformer dialog network by designing three types of masks and utilizing them in three independent transformer blocks. The three designed masks learn the conventional context, Intra-Speaker, and Inter-Speaker dependency.</p><p>In <ref type="bibr" target="#b5">[6]</ref>, Ghosal et al. incorporates different elements of commonsense such as mental states, events, and causal relations to learn interactions between interlocutors participating in a conversation. Authors in <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b27">[28]</ref> use Graph Neural networks to encode inter utterance and inter speaker relationships. Kim et al. <ref type="bibr" target="#b10">[11]</ref> model contextual information by simply prepending speaker names to utterances and inserting separation tokens between the utterances in a dialogue. To generate contextualized utterance representations, Wang et al. <ref type="bibr" target="#b32">[33]</ref> uses LSTM-based encoders to capture self and inter-speaker dependency of interlocutors. A directed acyclic graph (DAG) based ERC was introduced by Shen et al. in <ref type="bibr" target="#b26">[27]</ref> which is an attempt to combine the strengths of conventional graph-based and recurrence-based neural networks. In <ref type="bibr" target="#b37">[38]</ref>, Zhu et al. propose a new model in which the transformer model fuses the topical and commonsense information to predict the emotion label. Recently, Song et al. <ref type="bibr" target="#b28">[29]</ref> proposed the EmotionFlow model, which encodes the user's utterances via concatenating the context with an auxiliary question, and then, a random field is applied to capture the sequential information at the emotion level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-modal Methods</head><p>Prior literature on using previous utterances to provide context with respect to the utterance in hand has set the benchmark for dyadic conversations. In <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref>, authors use previous utterances of both parties in a dyadic conversation and the contextual information from the same to predict the emotional state of any given utterance. Majumder et al. <ref type="bibr" target="#b17">[18]</ref> build on this by separately modeling the uni-modal contextual information and then using a hierarchical tri-modal feature level fusion for obtaining a rich feature representation of the utterance. DialogueRNN <ref type="bibr" target="#b18">[19]</ref> tracks the contextual information of each speaker and the global state as separate entities. It uses the global and speaker's emotional context to produce accurate predictions. Zhang et al. <ref type="bibr" target="#b35">[36]</ref> introduced a model called ConGCN, which uses Graph Convolution networks on both audio and Text utterance features to model Speaker-Utterance and Utterance-Utterance relationships concurrently in a single network. Mao et al. <ref type="bibr" target="#b19">[20]</ref> investigate the differentiated multi-modal emotional behaviors from the intra-and intermodal perspectives. On a similar dataset, the CMU-Mosei methods like Loshchilov et al <ref type="bibr" target="#b2">[3]</ref> and Tsai et al <ref type="bibr" target="#b30">[31]</ref> use multi-head attention based fusion <ref type="bibr" target="#b31">[32]</ref> for multi-modal emotion recognition.</p><p>In most of the previous work, methods do not consider distinct facial features that play a significant role in determining the emotional context of the conversation. They use the frames as a whole entity but do not extract the essential part of the frame (i.e., face). Additionally, most of these methods do not have an active fusion strategy other than simple concatenation to take advantage of the wealth of the information present in the form of visual and acoustic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Framework</head><p>This section first introduces the problem statement and then provide details of the architecture design of the proposed framework briefly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Statement</head><p>A dialog consists of k number of utterances (U ) along with their respective labels (Y ) arranged together with respect to time where each utterance is accompanied by it's respective video clip, speech segment and text transcript. Mathematically, a dialog for k number of utterances can be formulated as follows:</p><formula xml:id="formula_0">{U, Y } = {{x i =&lt; x i t , x i a , x i v &gt;, y i }|i ? [1, k]},<label>(1)</label></formula><p>Here, x i denotes the i th utterance made up of corresponding x t (text), x a (audio) and x v (visual) component, while y i indicates respective i th utterance's emotion label. The proposed network takes this data as input and assigns the right emotion to any given utterance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-modal Fusion Network: M2FNet</head><p>We propose a hierarchical framework called Multimodal Fusion network (i.e., M2FNet) which is illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. The network is designed based on two levels of feature extraction:</p><p>? Utterance level feature extraction ? Dialog level feature extraction Initially, the features are extracted by the utterance level module independently. Then, at the dialog level extraction network, the model learns to predict the right emotion for each utterance by using the contextual information from the dialog as a whole. In the subsequent subsections, we briefly discuss the design steps occurring in both utterance and dialog level feature extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Utterance level feature extraction</head><p>From <ref type="figure" target="#fig_1">Figure 3</ref>, one can see that there are k number of utterances and each utterance is made up of x t (text), x a (audio) and x v (visual) content. In this step, the features from each modality are extracted for each utterance separately before passing to the dialog level feature extraction network. Each modality's input signals are passed through their corresponding feature extractors for generating their embeddings.</p><p>Text: In order to provide deeper inter utterance context, the text modality data (i.e., x t ) are passed through the Text Feature Extractor module. Here, we employ a modified RoBERTa model (? M ?RoBERT a ) proposed by Kim et al. <ref type="bibr" target="#b10">[11]</ref> as feature extractor. Every utterance's x t is accompanied by its preceding and next utterance text separated by the separator token &lt; S &gt;. The modified RoBERTa model is fine-tuned on this transcript and the respective utterance's labels. The last layer activations {F IT : F 1,T , F 2,T ......F k,T } obtained from the modified RoBERTa model by passing the utterance's text data can be represented by:</p><formula xml:id="formula_1">F i IT = ? M ?RoBERT a (x i t ) | i ? [1, k], ?F IT ? R k?D T .</formula><p>(2) Here, F i IT denotes i th utterance's embeddings and D T denotes the size of the embeddings of text utterance.</p><p>Audio: On the audio end, we introduce a new feature extractor model. The network design of the proposed feature extractor module is discussed briefly in Subsection 3.3. Initially, the audio contents are transformed into 2D Mel Spectrogram in RGB format and then passed through the feature extractor model. Here, the audio signal is first processed via different augmentation techniques like time warping and Additive White Gaussian Noise (AWGN) noise. Then the augmented signals are transformed into the corresponding Mel Spectrograms <ref type="bibr" target="#b23">[24]</ref>. For computing the Mel Spectrogram, the Short Time Fourier transform (STFT) is used with the frame length of 400 samples (25 ms) and hop length of 160 samples (10ms). We also use 128 Mel filter banks to generate the Mel Spectrogram <ref type="bibr" target="#b29">[30]</ref>.</p><p>The proposed extractor takes the Mel Spectrograms (i.e., x a ) as input and generate the corresponding feature embeddings {F IA : F 1,A , F 2,A ......F k,A }. The functionality of the proposed audio feature extractor module can be mathematically expressed as,</p><formula xml:id="formula_2">F i IA = ? AF E (x i a ) | i ? [1, k], ?F IA ? R k?D A (3)</formula><p>where, F i IA is the i th utterance's embeddings and D A indicates the size of embeddings of audio utterance and the ? AF E denotes the function of introduced audio feature extractor module.</p><p>Visual: In order to extract rich emotion-relevant features from the visual signal, we propose a dual network inspired from <ref type="bibr" target="#b12">[13]</ref> that exploit not only human facial expression but also context information in a joint and boosting manner. For both tasks, we use our proposed extractor model (as discussed in Subsection 3.3) and train on the CASIA webface database <ref type="bibr" target="#b34">[35]</ref> to extract the deeper features from the visual image. Following are the details of steps involved in the dual network:</p><p>? To encode the context information of the scene as a whole our trained feature extractor model is performed on 15 successive frames of the utterance clip. Following which the features are max pooled over the frame axis to obtain the scene embeddings of the utterance.</p><p>? To extract facial emotion-related features from the same 15 successive frames of the utterance clip, we propose a weighted Face Model (i.e., as visualized in <ref type="figure" target="#fig_1">Figure 3</ref>) which works as follows:</p><p>-Given a frame, it is passed through a Multi-task Cascaded Convolutional Network (MTCNN) <ref type="bibr" target="#b36">[37]</ref> to detect the faces present in the frame. This returns the bounding box of each face along with its confidence. Then each of the respective faces is passed through our trained feature extractor model to obtain emotion-relevant features of each respective face. Now, the areas of each bounding box accompanying the faces are normalized to bring their values between 0 and 1. Following this, a weighted sum is performed using the features of each face and their respective normalized areas to obtain the facial emotion feature of a frame.</p><p>-The same process is followed for each of the 15 frames, and similarly, upon extraction, the features are max-pooled over the frame axis to obtain the facial features of the utterance.</p><p>Upon extraction of features from each network, the scene embeddings are concatenated with the facial features to obtain a more comprehensive representation of the visual data in an utterance. Finally, this visual feature extractor's output {F IV : F 1,V , F 2,V ......F k,V } can be formulated as:</p><formula xml:id="formula_3">F i IV = ? W F (x i v ) | i ? [1, k], ?F IV ? R k?D V<label>(4)</label></formula><p>where, F i IA is the i th utterance's embeddings and ? W F denotes the function operation of weighted face model while D V is the size of the feature embedding of the visual utterance. Finally, these embeddings (i.e., text, acoustic and visual) are then sent to the dialog level feature extractor as an input to learn the correct prediction of emotions for each utterance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Dialog level feature extraction</head><p>The design diagram at dialog level feature extraction of the proposed M2FNet model is illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. Each modality embeddings (i.e., F IT , F IA and F IV ) are passed through their corresponding network with a variable stack of transformer encoders <ref type="bibr" target="#b31">[32]</ref> to learn the inter utterance context. The number of transformer encoders for text, audio, and visual modality is denoted by N T , N A , and N V , respectively. We also employ a local skip connection between each encoder to prevent the model from ignoring the lower-level features. The corresponding feature maps obtained from the encoders can be mathematically expressed as:</p><formula xml:id="formula_4">F i T = T r N T (...(T r N2 (T r N1 (F i IT )))), F i A = T r N A (...(T r N2 (T r N1 (F i IA )))), F i V = T r N V (...(T r N2 (T r N1 (F i IV )))), where, i ? [1, k].<label>(5)</label></formula><p>Here, T r is the operation function of the transformer encoder.</p><p>The corresponding feature maps associated with the text, visual and audio (i.e., F T , F V , F A ) are passed to a novel Multi-Head Attention Fusion module that helps the network in incorporating visual and acoustic information. The network architecture of the attention fusion module is also depicted in <ref type="figure" target="#fig_1">Figure 3</ref>. Here, the text features F T are used as input to fusion module as Query (Q) and Value (V) for the multi-head attention operation, and then the visual F V and acoustic F A features for the dialog are used as Key (K) in order to modulate the attention given to each utterance at any time-step. Hence, each individual modality is now mapped to the text vector space, and the respective features are concatenated and passed to a fully connected layer which outputs a vector ? R k?D T . The output of the fusion layer is passed through the next fusion layer along with the previous F A and F V feature maps. Here, the m number of multi-head attention fusion layers stacked together in order to generate the final feature outcome (F f usion ) as demonstrated below:</p><formula xml:id="formula_5">F i f usion1 = ? 1 (F i A , F i T , F i V ), F i f usionm = ? m (...(? 2 (F i A , F i f usion1 , F i V ))), where, i ? [1, k].<label>(6)</label></formula><p>In the above equation, ? indicates the learning function of the proposed Multi-Head Attention Fusion layer. The main difference in our Fusion strategy compared to the previous work using Multi-Head Attention is that our strategy involves changing the key across modalities while keeping the Query and Value the same to better modulate inter utterance attention and incorporate inter-modal information.</p><p>The feature outcome of the last multi-head attention fusion layer (i.e., F f usionm ) is concatenate with visual and acoustic feature maps (i.e., F V and F A ) as expressed below:</p><formula xml:id="formula_6">F i f inal = Concate(F i f usionm , F i A , F i V )<label>(7)</label></formula><p>Finally, we append two fully connected layers (FC) which generates the desired predicted output (i.e., Y i p =&lt; y i 1 , y i 2 , ..., y i p &gt;, where, i ? [1, k]) of our proposed system. </p><formula xml:id="formula_7">y i p = F C(F C(F i f inal )).<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature Extractor Module</head><p>In order to fetch deep features from audio and visual contents, we propose a new feature extractor model and the same is illustrated in <ref type="figure" target="#fig_2">Figures 4</ref>. The proposed extractor is designed based on triplet network to leverage the importance of triplet loss function <ref type="bibr" target="#b24">[25]</ref>. Initially, the anchor, positive and negative samples have been generated as suggested in <ref type="bibr" target="#b24">[25]</ref> for audio and visual modalities. Then these samples are passed through encoder network followed by projector module. Here, we use standard ResNet18 <ref type="bibr" target="#b8">[9]</ref> as a backbone of the encoder network while the projector consists a linear fully connected layer which project the embedding of encoder network to desired representations (i.e.,</p><formula xml:id="formula_8">Z = [z 1 , ...z N ] ? R N ?d ) composed of N representations with dimension d.</formula><p>The proposed extractor model is trained using weighted combination of three loss function i.e., adaptive margin triplet loss (i.e., L AM T ), covariance loss (i.e., L Cov ) and variance loss (i.e., L V ar ) functions. It can be expressed as follow:</p><formula xml:id="formula_9">L F E = ? 1 ? L AM T + ? 2 ? L Cov + ? 3 ? L V ar ,<label>(9)</label></formula><p>where, ? 1 , ? 2 and ? 3 are weighting factors that controls the distribution of different loss functions.</p><p>In <ref type="bibr" target="#b24">[25]</ref>, authors design the triplet loss function used to learn good representations of faces based on anchor, positive and negative samples. Here, authors have used a fixed margin value in their triplet loss function that helps to separate out the representations of positive and negative samples. However, in some cases where the positive or negative samples have the same distance with the anchor or the positive sample is only a bit closer to the anchor than the negative sample, the loss would be zero, and there would be no correction even though it should still be pulling the positive sample closer and pushing the negative sample away from the anchor. To overcome this issue, we propose a triplet loss function based on a adaptive margin value. This can be mathematically written as In addition, we also utilize the variance loss function proposed by Bardes et al. <ref type="bibr" target="#b0">[1]</ref> which helps the proposed model to tackle the mode collapse issue. Mathematically, the variance loss function can be presented as,</p><formula xml:id="formula_10">L AM T = D a,</formula><formula xml:id="formula_11">L V ar = 3 k=1 L V ar (Z k ); Z k = Z a , Z p , Z n L V ar (Z k ) = 1 d d j=1 1 ? V ar(Z :,j ) +<label>(12)</label></formula><p>Here, V ar(Z) (i.e., 1</p><formula xml:id="formula_12">N ?1 N i=1 (Z i ??) 2 )</formula><p>denotes the variance obtained from the corresponding representations, while? is mean of the corresponding representation.</p><p>To decorrelate the different dimensions of the representations, we adopt the covariance loss function <ref type="bibr" target="#b0">[1]</ref> and the the same can be expressed mathematically for representation as</p><formula xml:id="formula_13">L Cov = 3 k=1 L Cov (Z k ); Z k = Z a , Z p , Z n L Cov (Z k ) = 1 d i =j Cov(Z k ) T i,j ,<label>(13)</label></formula><p>where, Cov(Z) = 1 </p><formula xml:id="formula_14">N ?1 N i=1 (Z i ??)(Z i ??) T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Analysis and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>For fair comparison with state-of-the-art methods, we evaluate our proposed network (M2FNet) on Interactive Emotional Dyadic Motion Capture (IEMOCAP) <ref type="bibr" target="#b1">[2]</ref>, and Multimodal EmotionLines Dataset (MELD) <ref type="bibr" target="#b21">[22]</ref> benchmark datasets. The statistics of these datasets are reported in <ref type="table" target="#tab_0">Table 1</ref> and details are provided in Section 1 of the supplementary material. Both IEMOCAP and MELD are multimodal datasets with textual, visual, and acoustic data.</p><p>MELD: The MELD <ref type="bibr" target="#b21">[22]</ref> is a multimodal and multiparty dataset containing more than 1,400 conversations and 13,000 utterances from the Friends TV series. The utterances are annotated with one of the seven emotion labels (anger, disgust, sadness, joy, surprise, fear, and neutral). We use the pre-defined train/val split provided in the MELD dataset. The details of this dataset are given in <ref type="table" target="#tab_0">Table 1</ref>. IEMOCAP: The IEMOCAP database <ref type="bibr" target="#b1">[2]</ref> is an acted, multimodal and multi-speaker database consisting of videos of dyadic sessions having approximately 12 hours of audiovisual data with text transcriptions. Each video contains a single conversation, which is segmented into multiple utterances. Each utterance is annotated with one of six emotion labels, i.e., happy, sad, neutral, angry, excited, and frustrated. The database statistics are given in <ref type="table" target="#tab_0">Table 1</ref>. We randomly select 10% of training conversations as evaluation split for computing the hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training Setups and Hyper-parameter Tuning</head><p>All experiments are carried out using a single NVIDIA GeForce RTX 3090 card. We adopt AdamW <ref type="bibr" target="#b16">[17]</ref> as the optimizer with an initial learning rate 5e-4 with L 2 weight decay ranges between 5e-4 and 5e-5. Dropout is used with a rate between 0.4 and 0.5. The number of encoder layers in each modality's encoder (i.e., N T , N A , N V ) is tuned using a greedy scheme and set to 1 and 5 for MELD and IEMOCAP validation datasets, respectively. The number of multi-head attention fusion layers is set to 5 (i.e., = m) for both dataset. The proposed M2FNet framework is trained using the categorical cross-entropy on each utterances softmax output for each of M dialogs and their k utterances. Section 2 of the supplementary material provides training and validation performance details.</p><formula xml:id="formula_15">Loss = ? 1 M ? k ? M i=1 k j=1 C t=1 y i,j,t ? log(y i,j,t p ).<label>(14)</label></formula><p>To extract deeper features from audio and visual contents, we introduce a new feature extractor model. Here, ResNet18 <ref type="bibr" target="#b8">[9]</ref> is used as encoder module while the projector module consists a fully connected layer which projects the embeddings of encoder network to desired representations (i.e., Z). Here we set the number of representations as Z = 300. For audio task, the extractor model is trained on Mel Spectrograms obtained from the corresponding audio  signals while in case of visual feature extraction, it is trained on well-known CASIA webface database <ref type="bibr" target="#b34">[35]</ref>. Here, the extractor model is trained using the loss function mentioned in Equation no. 9 in which the weighting factors ? 1 , ? 2 and ? 3 are set to 20, 5 and 1, respectively. The proposed extractor model is trained upto 60 and 100 epochs for audio and visual task, respectively using Adam optimizer with learning rate of 1e-4 and decay rate of 1e-6. We mainly employ weighted average F1 score as evaluation metric due to its suitability to test with imbalance dataset. Additionally, we present our results in terms of classification accuracy to evaluate the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation studies</head><p>To better understand the contribution of different modules in the proposed M2FNet model, we have conducted several ablation studies on both IEMOCAP and MELD datasets. The corresponding results are compared in terms of accuracy and weighted average F1 scores for MELD and IEMOCAP testing datasets.</p><p>To validate the impact of each modality, we train the pro- posed network with and/or without Text, Video, and Audio as input and without using the proposed Fusion module.</p><p>From <ref type="table" target="#tab_1">Table 2</ref>, one can observe that concatenation of multimodal input with all three modalities obtain higher accuracy and weighted average F1 score than other scenarios such as using only one or two modalities. Furthermore, our fusion mechanism helps to enhance the accuracy by 2.53% and 0.57% for IEMOCAP and MELD datasets, respectively. However, in the case of weighted average F1 score, it obtains slightly inferior performance for the MELD dataset while improving it by 2.74% for the IEMOCAP dataset. On the visual end, we utilize scene and weighted face embeddings. For understanding the importance of both embeddings, two more experiments have been carried out where the proposed network with individual embedding has been trained, and the corresponding results are given in <ref type="table">Table 3</ref>. From the table, it can be observed here that the weighted face model or the scene encoding network on their own do not improve the results; however, when the network can access both, it significantly improves the results. This shows that both the context from the scene and the people in the scene are equally important for emotion recognition.</p><p>We also observe the effect of transformer encoders in the proposed framework. In these experiments, we set same number of transformer encoders (i.e., N A = N V = N T ) for each modality and observe its effect for different numbers. The corresponding results are presented in <ref type="table" target="#tab_2">Table 4</ref> where it is observed that N A = N V = N T = 1 gives best performance for MELD dataset while the N A = N T = N V = 5 setting helps the proposed framework to obtain higher performance for IEMOCAP testing dataset.</p><p>In the proposed model, we have set m = 5 number of Multi-Head Attention Fusion modules. To validate this, we train the proposed model with different numbers of the Multi-Head Attention Fusion modules and observe the corresponding accuracy and weighted average F1 score. This analysis is demonstrated in <ref type="table" target="#tab_3">Table 5</ref> where one can observe that the proposed model with five Multi-Head Attention Fusion modules (i.e., m = 5) obtains higher quantitative measures on both datasets.  <ref type="table">Table 6</ref>. Quantitative comparison with text-based state-of-the-art methods in terms of weighted average F1 score. <ref type="figure" target="#fig_5">Figure 5</ref> presents the performance of our model in terms of the weighted average F1 score of different emotions. Here, we can see that our model has obtained the highest F1 score of 82.11% for the Sad emotion of the IEMOCAP dataset. Similarly, for MELD dataset, it obtained highest score for Neutral emotion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Model Performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparative Analysis</head><p>To validate the robustness of the proposed network, we compare our proposed network with state-of-the-art textbased ERC systems in terms of weighted average F1 score and the same is presented in <ref type="table">Table 6</ref>. Here, one can notice that the proposed network has a state-of-the-art performance by obtaining superior quantitative results than previous methods on both datasets (i.e., 0.21% higher than previous best EmotionFlow [29] model on MELD dataset while 1.83% higher than that of the previous best DAG-ERC <ref type="bibr" target="#b26">[27]</ref> model on IEMOCAP testing dataset).</p><p>When compared with the existing multi-modal methods, our proposed M2FNet network shows a substantial im- provement compared to state-of-the-art multi-modal ERC methods. The obtained results are presented in <ref type="table" target="#tab_4">Table 7</ref> where one can notice that the proposed M2FNet model obtains 2.19% and 2.71% higher accuracy and weighted average F1 score on MELD dataset than that of previous best performance. Similarly, it set 0.77% and 0.63% higher accuracy and weighted average F1 sore than that of previous best DialogueTRM model <ref type="bibr" target="#b19">[20]</ref> on IEMOCAP testing dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitations</head><p>Our model sometimes gets confused and miss-classifies similar or close emotions such as Frustration and Anger, Happy and Excited. We can also observe that for highly imbalance data, our model misclassifies many emotions as the emotion with higher number of data samples. For example, many emotions are overwhelmingly predicted as Neutral for MELD dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a robust multi-modal fusion network called M2FNet for the task of Emotion Recognition in Conversation. In M2FNet, we propose a multi-head fusion attention module that helps the network to extract rich features from multiple modalities. A new feature extractor model is introduced in the proposed design to learn the audio and visual features effectively. Here, a new adaptive margin triplet loss function is introduced, which helps the extractor module to learn representations effectively. A new weighted face model is proposed in our framework to learn the rich facial features. Detailed analysis shows that encoding both scene and face-related information is essential for emotion recognition. Similarly, we observed that multi-modal fusion is necessary to leverage information from multiple modalities present in an utterance. Finally, our experiments validate the robustness of the proposed network quantitatively on both benchmark datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Quantitative analysis on MELD and IEMOCAP datasets in terms of weighted average F1 score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Network design of the proposed framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Network design of the proposed Extractor network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>distance based similarity metric between representations of anchor and positive, anchor and negative, positive and negative samples, respectively. m AM is the adaptive margin which is calculated based on similarity and dissimilarity measures asm AM = m sim AM + m dissim AM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>indicates the covariance matrix of corresponding representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Predictions made by the network on the MELD and IEMOCAP test sets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Statistics of the testing benchmark datasets: MELD and IEMOCAP</figDesc><table><row><cell>Statistics</cell><cell cols="2">MELD</cell><cell cols="2">IEMOCAP</cell></row><row><cell>Splitting</cell><cell cols="4"># Dialog # Utterance # Dialog # Utterance</cell></row><row><cell>Train</cell><cell>1098</cell><cell>9989</cell><cell>100</cell><cell>4778</cell></row><row><cell>Dev</cell><cell>114</cell><cell>1109</cell><cell>20</cell><cell>980</cell></row><row><cell>Test</cell><cell>280</cell><cell>2610</cell><cell>31</cell><cell>1622</cell></row><row><cell>No. of Classes</cell><cell>7</cell><cell></cell><cell>6</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation Studies based comparison to validate the impact of each modality.</figDesc><table><row><cell>Models</cell><cell>Remarks</cell><cell cols="2">IEMOCAP Accuracy Weighted Average F1</cell><cell cols="2">MELD Weighted Accuracy Average F1</cell></row><row><cell>Only Audio</cell><cell>-</cell><cell>26.56</cell><cell>21.79</cell><cell>49.04</cell><cell>39.63</cell></row><row><cell>Only Visual</cell><cell>-</cell><cell>20.39</cell><cell>13.10</cell><cell>45.63</cell><cell>32.44</cell></row><row><cell>Visual + Audio</cell><cell>Concat</cell><cell>35.12</cell><cell>31.35</cell><cell>48.35</cell><cell>35.74</cell></row><row><cell>Only Text</cell><cell>-</cell><cell>66.30</cell><cell>66.20</cell><cell>67.24</cell><cell>66.23</cell></row><row><cell>Text + Audio</cell><cell>Concat</cell><cell>66.52</cell><cell>66.48</cell><cell>67.80</cell><cell>66.32</cell></row><row><cell>Text + Visual</cell><cell>Concat</cell><cell>66.64</cell><cell>66.67</cell><cell>67.81</cell><cell>66.35</cell></row><row><cell>Text + Visual + Audio</cell><cell>Concat</cell><cell>67.16</cell><cell>67.12</cell><cell>67.28</cell><cell>66.81</cell></row><row><cell>Text + Visual + Audio</cell><cell>Fusion</cell><cell>69.69</cell><cell>69.86</cell><cell>67.85</cell><cell>66.71</cell></row><row><cell cols="6">Table 3. Ablation Studies based comparison to validate the impact</cell></row><row><cell>of dual network.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Models</cell><cell cols="3">IEMOCAP Accuracy Weighted Average F1</cell><cell cols="2">MELD Weighted Accuracy Average F1</cell></row><row><cell>Scene Embeddings</cell><cell></cell><cell>67.65</cell><cell>67.70</cell><cell>65.23</cell><cell>64.23</cell></row><row><cell cols="2">Weighted Face Embeddings</cell><cell>67.32</cell><cell>67.28</cell><cell>66.37</cell><cell>65.67</cell></row><row><cell>Proposed</cell><cell></cell><cell>69.69</cell><cell>69.86</cell><cell>67.85</cell><cell>66.71</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Ablation Studies based comparison to validate the impact of no. of transformer encoders.</figDesc><table><row><cell>No. of</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Transformer Encoders</cell><cell cols="2">IEMOCAP</cell><cell cols="2">MELD</cell></row><row><cell>(i.e., N A = N T = N V )</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Accuracy</cell><cell>Weighted Average F1</cell><cell>Accuracy</cell><cell>Weighted Average F1</cell></row><row><cell>1</cell><cell>69.32</cell><cell>69.36</cell><cell>67.85</cell><cell>66.71</cell></row><row><cell>2</cell><cell>69.07</cell><cell>69.21</cell><cell>67.16</cell><cell>65.82</cell></row><row><cell>3</cell><cell>69.32</cell><cell>69.44</cell><cell>66.86</cell><cell>66.06</cell></row><row><cell>4</cell><cell>69.13</cell><cell>69.22</cell><cell>67.20</cell><cell>66.06</cell></row><row><cell>5</cell><cell>69.69</cell><cell>69.86</cell><cell>67.47</cell><cell>66.38</cell></row><row><cell>6</cell><cell>69.13</cell><cell>69.26</cell><cell>67.24</cell><cell>65.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Ablation Studies based comparison to validate the impact of multi-head attention fusion layers.</figDesc><table><row><cell>No. of Attention Fusion Layers</cell><cell cols="2">IEMOCAP</cell><cell cols="2">MELD</cell></row><row><cell></cell><cell>Accuracy</cell><cell>Weighted Average F1</cell><cell>Accuracy</cell><cell>Weighted Average F1</cell></row><row><cell>1</cell><cell>67.22</cell><cell>67.34</cell><cell>66.63</cell><cell>66.11</cell></row><row><cell>2</cell><cell>68.08</cell><cell>68.19</cell><cell>67.47</cell><cell>66.50</cell></row><row><cell>3</cell><cell>69.13</cell><cell>69.25</cell><cell>67.47</cell><cell>66.65</cell></row><row><cell>4</cell><cell>69.01</cell><cell>68.95</cell><cell>67.59</cell><cell>66.51</cell></row><row><cell>5</cell><cell>69.69</cell><cell>69.86</cell><cell>67.85</cell><cell>66.71</cell></row><row><cell>6</cell><cell>68.95</cell><cell>69.08</cell><cell>66.82</cell><cell>65.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>Quantitative comparison with multimodal-based state-ofthe-art methods on MELD and IEMOCAP datasets. Here, top two performances are highlighted with bold font texts.</figDesc><table><row><cell>Name of Model</cell><cell cols="2">MELD</cell><cell cols="2">IEMOCAP</cell></row><row><cell></cell><cell>Accuracy</cell><cell>Weighted Average F1</cell><cell>Accuracy</cell><cell>Weighted Average F1</cell></row><row><cell>BC-LSTM-Att [21]</cell><cell>57.50</cell><cell>56.44</cell><cell>56.32</cell><cell>56.19</cell></row><row><cell>DialogRNN [19]</cell><cell>59.54</cell><cell>57.03</cell><cell>63.40</cell><cell>62.75</cell></row><row><cell>ConGCN [36]</cell><cell>-</cell><cell>57.40</cell><cell>64.18</cell><cell>64.18</cell></row><row><cell>Xie at al. [34]</cell><cell>65.00</cell><cell>64.00</cell><cell>-</cell><cell>-</cell></row><row><cell>DialogueTRM [20]</cell><cell>65.66</cell><cell>63.55</cell><cell>68.92</cell><cell>69.23</cell></row><row><cell>M2FNet</cell><cell>67.85</cell><cell>66.71</cell><cell>69.69</cell><cell>69.86</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Vicreg: Variance-invariance-covariance regularization for selfsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Bardes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04906</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Iemocap: interactive emotional dyadic motion capture database. Language Resources and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murtaza</forename><surname>Bulut</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="335" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A transformer-based joint-encoding for emotion recognition and sentiment analysis. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Benoit</forename><surname>Delbrouck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">No?</forename><surname>Tits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Brousmiche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Dupont</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Real-life emotions detection with lexical and paralinguistic cues on human-human call center dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurence</forename><surname>Devillers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurence</forename><surname>Vidrascu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth International Conference on Spoken Language Processing</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">COSMIC: commonsense knowledge for emotion identification in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepanway</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">F</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<idno>abs/2010.02795</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepanway</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niyati</forename><surname>Chhaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.11540</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Conversational memory network for emotion recognition in dyadic dialogue videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference</title>
		<meeting>the conference</meeting>
		<imprint>
			<publisher>NIH Public Access</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page">2122</biblScope>
		</imprint>
	</monogr>
	<note>Meeting,</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dou</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingwei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Huai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dialoguecrn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.01978</idno>
		<title level="m">Contextual reasoning networks for emotion recognition in conversations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Emoberta: Speaker-aware emotion recognition in conversation with roberta</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taewoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piek</forename><surname>Vossen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.12009</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Toward detecting emotions in spoken dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Chul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shrikanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on speech and audio processing</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="293" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Context-aware emotion recognition networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10143" to="10152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">HiTrans: A transformer-based context-and speakersensitive model for emotion detection in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijiang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12" />
			<biblScope unit="page" from="4190" to="4200" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A hierarchical transformer with speaker modeling for emotion recognition in conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyi</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.14781</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multi-task learning with auxiliary speaker identification for conversational emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijiang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.01478</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Alexander Gelbukh, Erik Cambria, and Soujanya Poria. Multimodal sentiment analysis using hierarchical fusion with context modeling. Knowledge-based systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page" from="124" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Dialoguernn: An attentive RNN for emotion detection in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">F</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<idno>abs/1811.00405</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Dialoguetrm: Exploring the intra-and inter-modal emotional behaviors in the conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhao</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiguo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.07637</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Context-dependent sentiment analysis in user-generated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Meld: A multimodal multi-party dataset for emotion recognition in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02508</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Emotion recognition in conversation: Research challenges, datasets, and recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="100943" to="100953" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Theory and applications of digital speech processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Schafer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Prentice Hall Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhou</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixian</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.08695</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Directed acyclic graph network for conversational emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhou</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Quan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.12907</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Summarize before aggregate: A globalto-local heterogeneous graph inference network for conversational emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongming</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhuang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12" />
			<biblScope unit="page" from="4153" to="4163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Emotionflow: Capture the dialogue level emotion transitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangjun</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longtao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Accepted in ICASSP</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A scale for the measurement of the psychological magnitude pitch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><forename type="middle">Smith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Volkmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwin Broomell</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of the acoustical society of america</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="185" to="190" />
			<date type="published" when="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multimodal transformer for unaligned multimodal language sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference. Association for Computational Linguistics. Meeting</title>
		<meeting>the conference. Association for Computational Linguistics. Meeting</meeting>
		<imprint>
			<publisher>NIH Public Access</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page">6558</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Contextualized emotion recognition in conversation as sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="186" to="195" />
		</imprint>
	</monogr>
	<note>1st virtual meeting</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robust multimodal emotion recognition from conversation with transformer-based crossmodality fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baijun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariia</forename><surname>Sidulova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung Hyuk</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">4913</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<title level="m">Learning face representation from scratch</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Modeling both contextand speaker-sensitive dependence for emotion detection in multi-speaker conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangqing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changlong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoushan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</meeting>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Topic-driven and knowledge-aware transformer for dialogue emotion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Pergola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.01071</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
