<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000. ?DARTS: Model Uncertainty-Aware Differentiable Architecture Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biswadeep</forename><surname>Chakraborty</surname></persName>
							<email>biswadeep@gatech.edu.</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<postCode>30332</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Graduate Student Member, IEEE</roleName><forename type="first">Saibal</forename><surname>Mukhopadhyay</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<postCode>30332</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biswadeep</forename><surname>Chakraborty</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<postCode>30332</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000. ?DARTS: Model Uncertainty-Aware Differentiable Architecture Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ACCESS.2017.DOI</idno>
					<note>Corresponding author:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>INDEX TERMS Uncertainty Estimation</term>
					<term>Neural Architecture Search</term>
					<term>Monte Carlo Dropout</term>
					<term>DARTS</term>
					<term>ImageNet</term>
					<term>Image classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a Model Uncertainty-aware Differentiable ARchiTecture Search (?DARTS) that optimizes neural networks to simultaneously achieve high accuracy and low uncertainty. We introduce concrete dropout within DARTS cells and include a Monte-Carlo regularizer within the training loss to optimize the concrete dropout probabilities. A predictive variance term is introduced in the validation loss to enable searching for architecture with minimal model uncertainty. The experiments on CIFAR10, CIFAR100, SVHN, and ImageNet verify the effectiveness of ?DARTS in improving accuracy and reducing uncertainty compared to existing DARTS methods. Moreover, the final architecture obtained from ?DARTS shows higher robustness to noise at the input image and model parameters compared to the architecture obtained from existing DARTS methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Uncertainty estimation of neural networks is a critical challenge for many practical application <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. We can approximate the uncertainty of a neural network by incorporating Monte-Carlo dropout <ref type="bibr" target="#b1">[2]</ref>. <ref type="bibr">Gal et al. used</ref> concrete dropout as a continuous relaxation of dropout's discrete masks to improve accuracy and provide better uncertainty calibration <ref type="bibr" target="#b2">[3]</ref>. For a given training dataset, the model uncertainty of a network depends on its architecture. However, no prior work aims to optimize network architectures to reduce model uncertainty.</p><p>We approach this problem by exploring the neural architecture search (NAS) <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>. Liu et al. have proposed a differentiable architecture search (DARTS) <ref type="bibr" target="#b5">[6]</ref> that uses continuous relaxation of the architecture representation allowing an efficient architecture search using gradient descent, thereby improving the efficiency of NAS. The DARTS perform a bi-level optimization problem where an outer loop searches over architectures using a validation loss (L valid ) and the inner loop optimizes parameters (weights) for each architecture using a training loss (L train ).</p><p>We advance the baseline DARTS framework by introducing: (a) concrete dropout <ref type="bibr" target="#b2">[3]</ref> layers within DARTS cell to enable a well-calibrated uncertainty estimation; (b) a Monte-Carlo dropout based regularizer <ref type="bibr" target="#b1">[2]</ref> within L train to optimize dropout probabilities; and (c) a predictive variance term in L valid to search for architecture with minimal model uncertainty. The proposed architecture search is defined as Model Uncertainty-aware Differentiable ARchiTecture Search (?DARTS). This paper makes the following key contributions:</p><p>? We develop the ?DARTS framework and training process to improve accuracy and simultaneously reduce model uncertainty of neural network. <ref type="bibr">?</ref> We show that the architecture search process in ?DARTS converges to a flatter minima compared to the standard DARTS method. <ref type="bibr">?</ref> We show that the final architecture obtained via ?DARTS converges to a flatter minima during model training compared to the model obtained using the standard DARTS method. <ref type="bibr">?</ref> We test the final DNN models obtained from architecture search methods on the CIFAR10, CIFAR100, SVHN, and ImageNet datasets. We show that the ?DARTS method improves the accuracy and uncertainty of the final DNN model found using the archi-tecture search. <ref type="bibr">?</ref> We also showed that the ?DARTS method has better performance when subjected to input noise and generalizes well when tested with parameter noise This paper aims to find the architecture which not only maximizes the accuracy but also minimizes the predictive uncertainty of the model. We do so using the predictive variance as the regularizer of the bi-level objective function of the DARTS architecture search method. This new architecture search method finds architecture with lower model uncertainty and better generalizability as it induces an implicit regularization on the Hessian of the loss function, leading to more generalizable solutions for the architecture search process.</p><p>The rest of the paper is organized as follows: Section II discusses the background and related works to this paper, Section III discusses the theoretical description of the novel Model Uncertainty Aware DARTS methodology proposed in this paper, while Section IV revolves around the implementation details about the architecture search baselines for comparison with the proposed ?DARTS method. Section V deals with the experiments undertaken and the results obtained thereby while finally, Section VI summarizes and discusses the conclusions we arrived at from the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND AND RELATED WORKS A. UNCERTAINTY ESTIMATION USING CONCRETE DROPOUT.</head><p>Considering the likelihood function of the models to be a multivariate normal distribution given as p (y * |f ? (x * )) = N (y * ; f ? (x * ) , ?), we can get an approximate estimate of the expected value of the variational predictive distribution by sampling T sets of weights? t (t = 1, . . . , T ) from the variational dropout distribution <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>:</p><formula xml:id="formula_0">E q ? (y * |x * ) [y * ] ? 1 T ? f? t (x * )<label>(1)</label></formula><p>where f? t (x * ) denotes a forward pass through the model with the weights which are sampled? t , thus effectively, performing T forward passes through the network f with dropout and this process is known as Monte Carlo (MC) dropout.</p><p>For multiclass classification tasks with softmax likelihood p (y * |x * , ?) = ? (f ? (x * )) , we can approximated the variational predictive distribution as:</p><formula xml:id="formula_1">q ? (y * |x * ) ? 1 T t ? f? t (x * )<label>(2)</label></formula><p>B. DIFFERENTIABLE ARCHITECTURE SEARCH. <ref type="bibr">Liu et al. presented</ref> DARTS to perform a one-shot neural architectural search. The DARTS method <ref type="bibr" target="#b5">[6]</ref> is based on the principle of continuous relaxation of the architecture representation, allowing efficient architectural space search using a gradient descent approach. DARTS formulates the architecture search as a differentiable problem, thus overcoming the scalability challenges faced by other RLbased NAS methods. The DARTS optimization procedure is defined as a bi-level optimization problem where L val is the outer objective and L train is the inner objective as:</p><formula xml:id="formula_2">min ? L val (?, w * (?)) s.t. w * (?) = arg min w L train (?, w)<label>(3)</label></formula><p>where the validation loss function L val determines the architecture parameters ?(outer variables) and the training loss L train optimizes the network weights w (inner variables).</p><p>The computational graph is learned in one go in the DARTS-based architecture search process. At the end of the search phase, the connections and their associated operations are pruned, keeping the ones that have the highest magnitude of their related architecture weights multipliers. In their paper, Noy et al. <ref type="bibr" target="#b33">[34]</ref> showed that the harsh pruning in DARTS, which occurs only once at the end of the search phase, is sub-optimal and that a gradual pruning of connections can improve both the search efficiency and accuracy. On the other hand, Bi et al. <ref type="bibr" target="#b34">[35]</ref> solve the problem of searching in a complex search space by starting with a complete super-network and gradually pruning out weak operators. Though these methods help in finding efficient architectures, these methods neither estimate nor try to minimize the uncertainty of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Improving DARTS Search Space</head><p>Many different works have worked on improving the shortcomings of the DARTS methodologies. For example, in the paper, <ref type="bibr" target="#b30">[31]</ref>, the authors pointed out that despite the large gap between the architecture depths in search and evaluation scenarios, the DARTS method report lower accuracy in evaluating the searched architecture or when transferring to another architecture. The authors gradually increased the depth of the searched architectures to address this issue.</p><p>Alternatively, in the paper <ref type="bibr" target="#b31">[32]</ref>, the authors addressed the issue of large memory and computing overheads in jointly training a super-network and searching for an optimal architecture. The authors proposed an approach to sample a small part of the super-network to reduce the redundancy in exploring the network space, thereby performing a more efficient search without comprising the performance. Again, Dong et al. <ref type="bibr" target="#b32">[33]</ref> represented the search space as a DAG to reduce the search time of the architecture search process. This paper uses the DARTS search space and methodology as the baseline. However, our method could very well be extended to perform for any such methodologies mentioned above and search spaces when the aim is to find an architecture in the search space which not only improves the accuracy but also minimizes the uncertainty while making it more robust to noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Improving Robustness and Generalizability of DARTS:</head><p>Arber et. al. <ref type="bibr" target="#b6">[7]</ref> and Chen et.al <ref type="bibr" target="#b29">[30]</ref> more stabilized neural architecture search methods. SmoothDARTS (SDARTS) <ref type="bibr">VOLUME 4, 2016</ref> use a perturbation-based regularisation to smooth the loss landscape and improve the generalizability. Empirical results have shown that a generalization performance of the architecture found by DARTS improves with a lower eigenvalue of the Hessian matrix of the validation loss for the architectural parameters (? 2 ? L DARTS val ). RobustDARTS <ref type="bibr" target="#b6">[7]</ref> has been proposed to improve robustness by (i) computing the Hessian and stopping DARTS early to limit the eigenvalues (converge to a flatter minima) and (ii) using an L2 regularization term in the training loss (L train ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. NEURAL ARCHITECTURE DISTRIBUTION SEARCH:</head><p>Ardywibowo et al. showed that searching for a distribution of architectures that performs well on a given task allows us to identify standard building blocks among all uncertaintyaware architectures <ref type="bibr" target="#b28">[29]</ref>. With this formulation, the authors optimized a stochastic out-of-distribution detection (OoD) objective and constructed an ensemble of models to perform OoD detection. However, the work concentrates on the detection of out-of-distribution uncertainty by optimizing the Widely Applicable Information Criterion (WAIC), a penalized likelihood score used as the OoD detection criterion. However, they do not discuss the model uncertainty that arises from the generalization error and mainly focus on just the out-of-distribution uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. CONTRIBUTION OF THIS WORK.</head><p>The prior works on NAS and DARTS do not focus on minimizing uncertainty. Therefore, the key contribution of this paper is an architecture search method that can simultaneously maximize accuracy and reduce model uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MODEL UNCERTAINTY AWARE DARTS</head><p>We propose ?DARTS, a neural architecture search method to find the optimal architecture that simultaneously improves the accuracy and reduces the model uncertainty while also giving a tighter estimate of the model uncertainty using the concrete dropout layers. We formulate the ?DARTS bi-level optimization problem as:</p><formula xml:id="formula_3">min ? L DARTS val (?, w * (?)) + Var model p(y|x) (?, w * (?)) s.t. w * (?) = arg min w L DARTS train (?, w) + L MC (?) (4) where, L DARTS MC (?)</formula><p>is the Monte Carlo dropout loss and Var model p(y|x) (?, w * (?)) is the predictive variance. As pointed out by Zela et al. <ref type="bibr" target="#b6">[7]</ref>, it can be seen that increasing the inner objective regularization helps to control the largest eigenvalue and more strongly allows it to find solutions with the smaller Hessian spectrum and better generalization properties. Again, we observe the implicit regularization effect on the outer objective, which reduces the overfitting of the architectural parameters. This is further discussed in Appendix A. Compared to the optimization problem for DARTS (see 3), we make the following key updates: We know that the validation loss, L val , is used for the architecture search process, and we want our neural architecture search method to search for the architecture which also has the least predictive uncertainty along with high accuracy. Hence, we add the predictive variance term to the validation loss term and the new validation loss function is given as L DARTS val (?, w * (?)) + Var model p(y|x) (?, w * (?)), where w * is the optimal set of weights found by optimizing the training loss function in the other bi-level optimization problem. The predictive variance is estimated as the variance of the T Monte Carlo samples on the network as:</p><formula xml:id="formula_4">Var model p(y|x) (y) = ? model = D 1 T T t=1 (y t ??) 2<label>(5)</label></formula><p>where {y l } T t=1 is a set of T sampled outputs for weights instances ? l ? q(?; ?) and y = 1/T t y t .</p><p>Again, we added a Monte-Carlo loss function in the training loss function since the training loss determines the weights of a particular architecture. To get well-calibrated uncertainty estimates, adapting the dropout probability as a variational parameter to the data at hand is necessary. So, we use the concrete dropout layers and add the Monte Carlo loss to calibrate the dropout probabilities. As shown in <ref type="bibr" target="#b2">[3]</ref>, the optimization objective that follows from the variational interpretation can be written as</p><formula xml:id="formula_5">L MC (?) = 1 N KL (q ? (?) p(?)) ? L ?DARTS train (?, w) = ? 1 M i?S log p (y i |f ? (x i )) + 1 N KL (q ? (?) p(?))<label>(6)</label></formula><p>where ? is the parameters to optimize, N is the number of data points, S is a random set of M data points, f ? (x i ) is the neural network's output on input x i when evaluated with weight matrices realisation ?, and p (y i | f ? (x i )) is the model's likelihood, e.g. a Gaussian with mean f ? (x i ) . The KL term KL (q ? (?) p(?)) is a "regularization" term which ensures that the approximate posterior q ? (?) does not deviate too far from the prior distribution p(?). The total derivative of L val w.r.t. ? evaluated on (?, w * (?)) would be:</p><formula xml:id="formula_6">d d? (L val + Var (y * )) = ? ? (L val + Var (y * )) ?? w (L val + Var (y * )) ? 2 w L train + ? 2 w L MC (?) ?1 ? 2 ?,w (L train + L MC (?))<label>(7)</label></formula><p>where ? ? = ? ?? , ? w = ? ?w and ? 2 ?,w = ? 2 ???w . In general, computing the inverse of the Hessian is not possible considering the high dimensionality of the model parameters w. Thus, we use gradient-based iterative algorithms to find the optimal w * . However, to avoid repeated training of each architecture which is computationally very expensive, we approximate w * (?) by updating the current model parameters w using a single gradient descent similar to the approximation step done in <ref type="bibr" target="#b5">[6]</ref>:</p><formula xml:id="formula_7">w * (?) ? w ? ?? w [L train (?, w) + L MC (?)] (8) ? ?w * ?? (?) = ??? 2 ?,w [L train (?, w) + L MC (?)]</formula><p>where ? is the learning rate for the virtual gradient step, DARTS takes with respect to the model weights w. Therefore, we obtain:</p><formula xml:id="formula_8">d d? (L val + Var (y * )) = ? ? L val (?, w * ) + ? ? Var (y * ) ? ?? w [L val (?, w * ) + Var (y * )]? 2 ?,w (L train (?, w * ) + L MC (?))<label>(9)</label></formula><p>where the inverse Hessian ? 2 w L ?1 train is replaced by the learning rate ?.</p><p>The final output using the ?DARTS method gives us the optimal architecture, which has the maximum accuracy and the minimum uncertainty in the architecture search space. However, there are some benefits to including the predictive variance term in the validation loss and the Monte Carlo dropout loss. We hypothesize a two-fold benefit from this modified loss function structure.</p><p>Firstly, the predictive variance term acts as a regularizer to the validation loss function. It makes the neural architecture search method more robust and resilient to input or parameter noise. In this paper, we shall prove this hypothesis empirically by computing the largest eigenvalue of the Hessian of the validation loss to indicate the flatness of the loss minima of the architecture search process. Also, analytical proof of this is given in Appendix B, considering a simplified linear model.</p><p>Similarly, the Monte Carlo dropout loss added to the training loss function acts as another regularizer. Hence, the final architecture obtained from the neural architecture search method also gives better performance under noise perturbation. Similarly, we also prove this empirically by computing the largest eigenvalue of the Hessian of the training loss to indicate that the architecture converges to a flat minima for the final architecture we got from the neural architecture search process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. IMPLEMENTATION OF BASELINES AND ?DARTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DARTS:</head><p>We implemented the standard DARTS method <ref type="bibr" target="#b5">[6]</ref> with the search space constituting of the following operations: O : 3 ? 3, 5 ? 5 separable convolutions (sep conv), 3 ? 3, 5 ? 5 dilated separable convolutions (dil conv), 3 ? 3 max pooling, 3 ? 3 average pooling, identity, and zero ( <ref type="figure" target="#fig_0">Fig. 1</ref>). All the operations in the method are of unit stride length, wherever applicable, and paddings are added to the convolved feature maps to preserve their spatial resolution. For the convolution operations, we use the order ReLU/Conc Drop(ReLu)-Conv-BN as done in <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. The output node of the convolutional cells is the depthwise concatenation of all the intermediary nodes except the input nodes. The network is then formed by stacking multiple cells following the same principle used in <ref type="bibr" target="#b5">[6]</ref>. We used the implementation of DARTS used in <ref type="bibr" target="#b5">[6]</ref> for our results and ran it for 50 epochs for each searching and training algorithm.</p><p>DARTS with Concrete Dropout (DARTS-CD). The final architecture obtained from DARTS cannot be directly used for uncertainty estimation. We modify the optimal architecture from the standard DARTS to enable uncertainty estimation. We include a Concrete dropout <ref type="bibr" target="#b2">[3]</ref> layer in all the layers of the final architecture, and the DARTS-CD is obtained by adding a Monte Carlo dropout loss to the training loss function L train to calculate the optimal dropout probabilities <ref type="bibr" target="#b2">[3]</ref>. We use the re-trained final architecture to estimate accuracy and uncertainty using multiple MC samples.</p><p>RobustDARTS with Concrete Dropout (RDARTS-CD). We implemented the RobustDARTS (RDARTS) model using the L2 regularization in the training loss, and the early stopping mechanism <ref type="bibr" target="#b6">[7]</ref>. The RDARTS generates a final architecture. Like DARTS-CD, concrete dropout layers are included in each of the layers in RDARTS for uncertainty estimation. The modified architecture is re-trained by adding Monte-Carlo dropout loss in the loss function. The modified final architecture from RDARTS calculates the model uncertainty and accuracy.</p><p>Progressive Differentiable Architecture Search with Concrete Dropout (P-DARTS-CD) We also implemented the Progressive Differentiable Architecture Search (P-DARTS) <ref type="bibr" target="#b30">[31]</ref>. P-DARTS is an efficient algorithm that gradually increases the depth of the searched architectures while training. P-DARTS solves the problems of heavier computational overheads and weaker search stability using VOLUME 4, 2016 search space approximation and regularization, respectively. In this paper, we added Concrete Dropout after each layer of the P-DARTS searched architecture to get an uncertainty estimate using the Monte Carlo Dropout methodology. The modified network is trained with added Monte Carlo dropout loss. The modified final architecture from P-DARTS is then used to calculate the accuracy and model uncertainty.</p><p>PC-DARTS-CD Partially-Connected DARTS (PC-DARTS) <ref type="bibr" target="#b31">[32]</ref> performs a more efficient search without comprising the performance by sampling a small part of the super-network to reduce the redundancy in exploring the network space. Similar to the previously mentioned architectures, we add Concrete Dropout layers to the final architecture from the PC-DARTS method and, using the Monte Carlo Dropout loss in the loss function, get an estimate of the uncertainty of the model.</p><p>GOLD-NAS-CD Gradual One-Level Differentiable Neural Architecture Search (GOLD-NAS) introduces a variable resource constraint to one-level optimization so that the weak operators are gradually pruned out from the supernetwork. Similar to the other methods, we add additional Concrete Dropout layers after each layer in the final architecture to get the uncertainty estimate of the model.</p><p>ASAP-CD Architecture Search, Anneal and Prune (ASAP) <ref type="bibr" target="#b33">[34]</ref> uses a differentiable search space that allows the annealing of architecture weights while gradually pruning inferior operations. In this way, the search converges to a single output network continuously. Similar to the other methods, we add Concrete Dropout layers after each layer in the final architecture obtained in the paper to get the uncertainty estimate of the model.</p><p>?DARTS. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the overall architecture of the proposed ?DARTS method, including the internal details of a cell. We include the following operations in O: 3?3, 5?5 separable convolutions (sep conv), 3?3, 5?5 dilated separable convolutions (dil conv), 3?3 max pooling, 3?3 average pooling, identity, and zero. The key difference between an ?DARTS cell compared to the standard DARTS cell is that </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. TRAINING CONDITIONS</head><p>The experiments performed in this paper were performed on a single NVIDIA GeForce GTX 1080 Ti Graphics Card. To get a fair comparison between the models, the experiments performed in this paper keep the training parameters constant for all the models. We trained all the methods for 50 epochs for the architecture search process with a batch size of 32 and a learning rate of 0.05. After getting the final architecture, we train the final model for 50 epochs with a batch size of 64 and a learning rate of 0.025 to get the best results of the model obtained. <ref type="table" target="#tab_0">Table 1</ref> summarizes the hyperparameters used in the architecture search.</p><p>The experimental results of the paper are divided into the following subsections:</p><p>? Comparative analysis of the architecture search methods, including ablation studies, robustness, convergence, and run-time. ? Comparative analysis of the final architecture, including flatness of the loss surface and testing errors.</p><p>? Comparative analysis of the final DNN models, including accuracy and uncertainty comparisons and tolerance to input and parameter noise.</p><p>In the rest of this paper, we primarily train the models under the same training conditions mentioned above for uniformity and evaluate them to get the accuracy and uncertainty estimates. For reference, we used the vanilla models (without adding concrete dropout layers), as implemented in the original papers, and compared them with the accuracy and training condition of ?DARTS as implemented in this paper. The results are shown in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. ANALYSIS OF THE ARCHITECTURE SEARCH METHODS</head><p>The searched architecture using the ?DARTS architecture search method is shown in <ref type="figure" target="#fig_2">Fig. 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Ablation Studies</head><p>In this section, we perform an ablation study for the two different cases: 1. removing the Monte Carlo loss function L MC (?) from the training loss L DARTS val 2. removing the prediction variance term Var model p(y|x) (?, w * (?)) from the validation loss term L DARTS train . Role of Var model p(y|x) (?, w * (?)): Including L MC (?) results in calibrated uncertainty values in the inner loop of the DARTS optimization loop. However, if the predictive variance term is removed, the neural architecture search method is not optimized for the uncertainty values. Removing the predictive variance loss term but not the MC loss is equivalent to the DARTS-CD architecture search method. We simulate this ablation study on the CIFAR 10 dataset and compare the performance with the case where we include the predictive variance term in the training loss function. We plot the variation of the predictive uncertainty with increasing epochs for ?DARTS and DARTS-CD in <ref type="figure" target="#fig_3">Fig. 4</ref>.</p><p>In this paper, we search for an architecture that not only maximizes accuracy but simultaneously minimizes the model uncertainty. We accomplish this by using the predictive variance as a regularizer of the loss function as given in Eq. 4. To verify that adding the predictive variance term helps in searching for architectures with lower predictive uncertainty, we plotted the evolution of the predictive uncertainty of different architecture search results with respect to ?DARTS. The results are shown in <ref type="figure" target="#fig_4">Fig.  5</ref>. We see that the model uncertainty (measured using the predictive variance) remains almost constant for models like DARTS-CD and RDARTS-CD, which steadily decreases for the ?DARTS architecture search method, which uses the predictive variance as a regularizer of the Cross-Entropy loss function.</p><p>Role of L MC (?): We see that on removing the Monte Carlo dropout loss term from the training loss L DARTS val , the dropout probabilities in each of the layers are not updated, and hence the uncertainty is not calibrated. Therefore, we get a wrongful estimate of the uncertainty of the model. In <ref type="figure" target="#fig_5">Fig. 6</ref> we show this by repeating the same experiment with different values of dropout probabilities and show that each of the dropout probabilities gives rise to a different uncertainty estimate as discussed in <ref type="bibr" target="#b2">[3]</ref>. We also note that if we remove the Monte Carlo loss function from the architecture search process while keeping the predictive variance term, we will get an architecture optimized for an uncalibrated noise. Hence, the optimal architecture found using this method may not be the best architecture to minimize the noise.</p><p>2) Robustness.</p><p>We empirically evaluate the robustness of the architecture search methods by estimating the largest eigenvalue of the Hessian of the validation loss function (L valid ) for each method. <ref type="table" target="#tab_1">Table 2</ref> shows that the ?DARTS method has a smaller value of the largest eigenvalue of the Hessian of the validation loss compared to the other methods making it more robust. We plot the evolution of the largest eigenvalue of the Hessian of the validation loss in <ref type="figure" target="#fig_6">Fig. 7</ref>. We see that while for the standard DARTS method, the largest eigenvalue keeps increasing for increasing epochs. However, the largest eigenvalue for the Hessian of the validation loss function of the ?DARTS method increases very slowly and is much lower than the other methods discussed. The RDARTS-CD can generate similar robustness but requires the implementation of early stopping. The empirical analysis verifies that including predictive variance Var model p(y|x) (?, w * (?)) in the validation loss (L valid ) improves robustness of the architecture search method (an analytical proof of this property is given in the Appendix assuming a linear model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Convergence and Runtime Analysis</head><p>In this section, we discuss the run times and the convergence of the architecture search method. <ref type="figure">Fig. 8</ref> shows the validation error of the search models for each epoch for the CIFAR 10, CIFAR 100, SVHN, and the ImageNet datasets. ?DARTS shows a faster convergence (or lower validation loss) than other architectures. <ref type="table" target="#tab_2">Table 3</ref> shows that the runtime of ?DARTS is similar to DARTS, DARTS-CD, and RDARTS-CD.        <ref type="table" target="#tab_3">Table 4</ref> shows that ?DARTS has a lower mean and standard deviation of the testing error compared to DARTS, DARTS-CD, and RDARTS-CD. Hence, we empirically verify that using the Monte-Carlo dropout loss as a regularizer in the training loss in ?DARTS, instead of using an L2 regularizer as done in RobustDARTS <ref type="bibr" target="#b6">[7]</ref>, improves the robustness of the final architecture. These results prove that the ?DARTS architecture search method converges to a flatter minima for each iteration of the bilevel optimization. Importance of MC Dropout Loss within Bi-level Optimization. Also, it is to be noted that in the DARTS-CD method, we are manually adding the Concrete Dropout layers after the architecture search process. So, the comparative analysis of the DARTS-CD and the ?DARTS shows us that just adding the concrete dropout without solving the bi-level optimization problem does not provide us with the optimal solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. COMPARATIVE ANALYSIS OF THE FINAL MODELS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Accuracy and Uncertainty</head><p>We compare the performance of ?DARTS considering CI-FAR10 , CIFAR100 <ref type="bibr" target="#b18">[19]</ref>, SVHN <ref type="bibr" target="#b19">[20]</ref> and ImageNet <ref type="bibr" target="#b20">[21]</ref> datasets. All experiments are performed using PyTorch <ref type="bibr" target="#b17">[18]</ref> based models. For each dataset, we estimate the accuracy and uncertainty of the final architectures obtained from ?DARTS with the following models: (1) architectures obtained from standard DARTS, (2) architectures obtained from the other baseline architecture search methods, and (3) existing deep networks for image classification with Concrete dropout. We calculate the model uncertainty by the number of Monte Carlo passes of the network and getting the predictive variance (see equation <ref type="formula" target="#formula_4">(5)</ref>).</p><p>As examples of image classification network, we consider MobileNetv2 <ref type="bibr" target="#b13">[14]</ref> , VGG16 <ref type="bibr" target="#b14">[15]</ref>, ResNet20 <ref type="bibr" target="#b15">[16]</ref>, Efficient-Net <ref type="bibr" target="#b16">[17]</ref>. All networks were implemented from the source code. We used the standard models for each of them and implemented them in PyTorch. We also added a Concrete dropout-based ReLu layer instead of the ReLu layer in the original implementation of each of the models. An MC dropout loss is also added to the loss function to calculate the optimal dropout probabilities to get a tighter bound on the uncertainty estimate of the model. The modified models were re-trained, and uncertainty estimations were performed considering multiple MC samples.</p><p>We obtained the confidence interval of the performances of the architectures searched using the architecture search methods by calculating the mean and standard deviation of the model accuracy. We did this by re-training and reevaluating the final searched model 10 times with different initializations. The observed mean and variance of the accuracy of the models and the mean of their uncertainty are shown in <ref type="table" target="#tab_4">Table 5</ref>. The table shows that the ?DARTS outperforms the standard differentiable architecture search methods like DARTS-CD, RDARTS-CD, P-DARTS-CD, PC-DARTS-CD, GOLD-NAS-CD, and ASAP-CD. The architecture obtained with ?DARTS also outperforms standard non-NAS-based architectures like MobileNetv2, VGG16, ResNet20, and EfficientNet-B0. It is to be noted here that all of the baseline models shown in <ref type="table">Table V</ref> are equipped with concrete dropout to calculate the uncertainty of the VOLUME 4, 2016 model. We see that under these training conditions, the CIFAR-10 results of ?DARTS are much higher (96.22%) than the standard DARTS (94.32%). A similar trend can be seen for ImageNet (74.64% for ?DARTS compared to 70.63% for DARTS). This shows that the proposed ?DARTS method can achieve good accuracy and uncertainty scores with minimal training (50 epochs) compared to the other baselines. A reason for the better performance of the model on the ImageNet dataset in comparison to CIFAR10 might be attributed to the better regularization of the loss functions obtained due to the addition of the predictive variance and the Monte Carlo loss functions help in a more efficient search for architecture. Also, adding the regularization terms helps stabilize the ?DARTS search by implicitly regularizing the Hessian of the loss functions. We see that the implicit regularization leads to a smaller dominant eigenvalue of the Hessian, which serves as a proxy for the sharpness and thus leads to a more generalizable architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Input Noise Tolerance of Final DNN Models</head><p>We study the accuracy and uncertainty of the final DNN models obtained from various architecture search methods under Gaussian noise to the input images. We used the CIFAR10, CIFAR100, and SVHN datasets and compared varying Signal-to-Noise Ratio (SNR). We do not re-train the models with noisy images. Instead, noisy images are only applied during inference [ <ref type="table" target="#tab_5">Table 6</ref>]. The added predictive variance term and the Monte-Carlo loss terms act as a regularizer, improving the performance compared to the standard methods. We observe that the ?DARTS process has higher accuracy and lower uncertainty under noise than DNN models from other architecture search methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Tolerance of Final DNN Models to Noisy Parameters</head><p>We test the architecture's performance when the model parameters are perturbed by a small gaussian noise ? N (0, 1).</p><p>We hypothesize that a more stable neural architecture will lower the testing error and uncertainty variance. <ref type="table">Table 7</ref> shows that the DNN model generated by the ?DARTS method gives the least variance. Hence, we conclude that the ?DARTS final architecture is stable and can handle noise perturbations very well. Comparing the results of <ref type="table" target="#tab_4">Tables 5 and 7</ref>, we see that with the inclusion of the parameter noise, though the accuracy for all the models fall and the uncertainty increases, the change in the accuracy and uncertainty for the ?DARTS model [e.g. CIFAR10:</p><p>?Mean Accuracy = 2.61, ?Variance = 0.045] is lower than the change in other networks. Hence, we can conclude that the ?DARTS model is more resilient to parameter noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this paper, we proposed a novel method, referred to as ?DARTS, to search for a neural architecture that simultaneously improves the accuracy and the uncertainty of the final neural network architecture. ?DARTS uses concrete dropout layers within the cells of a neural architecture search framework, and adds predictive variance and Monte-Carlo dropout losses as regularizers in the validation and training losses respectively. We experimentally demonstrate that ?DARTS improves the performance of the neural architecture search and the final architecture found using the architecture search by showing that the optimization problem converges to a flat minima. We also empirically show that the final architecture is stable when perturbed with input and parameter noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. IMPLICIT REGULARIZATION OF THE HESSIAN</head><p>The addition of the regularizer to the loss function of the ?DARTS architecture search method induces implicit regularization on the Hessian matrix. It has been empirically pointed out that the dominant eigenvalue of ? 2 A L val (w, A) (spectral norm of Hessian) is highly correlated with the generalization quality of DARTS solutions <ref type="bibr" target="#b6">[7]</ref>. In standard DARTS training, the Hessian norm grows exponentially leading to bad test performance. Chen et al. <ref type="bibr" target="#b29">[30]</ref> plot this Hessian norm during the training procedure and found that DARTS with perturbation-based regularization consistently reduce the Hessian norms during the training procedure. They also showed the spectral norm of Hessian is corre- lated with the solution quality, and that perturbation-based regularization of the DARTS search strategy can implicitly control the Hessian norm.</p><p>The update of w in ?DARTS can thus implicitly control the trace norm of the Hessian of the loss function. If the matrix is close to positive semi-definite, this is approximately regularizing the (positive) eigenvalues of ? 2 ? L ?DARTS val (w, ?). Therefore, ?DARTS reduces the Hessian norm through its training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. COMPARISON OF STABILITY USING LARGEST EIGENVALUES</head><p>In <ref type="bibr" target="#b6">[7]</ref>, the authors empirically showed the instability of the DARTS method is related to the norm of Hessian ? 2 ? L valid . Chen et al. <ref type="bibr" target="#b29">[30]</ref> verified this by plotting the validation accuracy landscape of the DARTS and showed it is extremely sharp and thus, even a small perturbation in ? can drastically change the validation accuracy.</p><p>Here we prove that the eigenvalue of the Hessian of validation loss in ?DARTS (? 2 ? L ?DARTS valid ) is lower than the eigenvalue of the Hessian of validation loss in DARTS (? 2 ? L DARTS valid ). Lemma 6.1: The largest eigenvalue of the Hessian of the validation loss of DARTS is given by,</p><formula xml:id="formula_9">? max (? 2 ? L DARTS valid ) = max ? i N ? max ( N i=1 x i x T i ), where, ? i = ? x T i ? 1 ? ? x T i ?<label>(10)</label></formula><p>Proof. We consider x i is the input vector, ? is the weight matrix of architectural parameter, and y i is the vector denoting the labels of the classes. For simplicity, we consider a linear model x T i ? for this proof. Considering the cross entropy loss as the validation loss (L valid ) we obtain:</p><formula xml:id="formula_10">L DARTS valid = ? 1 N N i=1 y i log ? x T i ? + (1 ? y i ) log 1 ? ? x T i ?<label>(11)</label></formula><formula xml:id="formula_11">? 2 ? L DARTS valid = 1 N N i=1 ? x T i ? 1 ? ? x T i ? x i x T i = 1 N N i=1 ? d x i x T i (12) VOLUME 4, 2016</formula><p>where, ?(.) is the sigmoid function, ? d = ? x T i ? 1 ? ? x T i ? for each i. We know the Rayleigh quotient of ? 2 ? L DARTS valid is given by:</p><formula xml:id="formula_12">R DARTS (z) = z T ? 2 ? L DARTS valid z = z T 1 N N i=1 ? d x i x T i z = 1 N z T N i=1 ? d x i x T i z<label>(13)</label></formula><p>where, z is any unit-length vector. Assuming maximum of ? d is represented as ? max d , we obtain:</p><formula xml:id="formula_13">R DARTS (z) ? ? max d N z T N i=1 x i x T i z = ? max d N R Mx (z) where, R M (z) is the Raleigh quotient of the matrix M x = N i=1</formula><p>x i x T i as given by:</p><formula xml:id="formula_14">R M (z) = z T N i=1 x i x T i z = N i=1 z T x i x T i z = N i=1 z T x i 2 ? 0 (14)</formula><p>Since, the maximum eigenvalue of a symmetric matrix (A) is equal to the maximum value of its Raleigh quotient (? max (A) = R max A )), we obtain:</p><formula xml:id="formula_15">? max (? 2 ? L DARTS valid ) = R max DARTS = ? max d N R max M = ? max d N ? max N i=1 x i x T i<label>(15)</label></formula><p>Corollary. The validation loss function of the standard DARTS (L DARTS valid ) method is convex in nature. Proof. We know that, the smallest eigenvalue of any square symmetric matrix is given as the minimum of the Rayleigh quotient of that matrix. Since ? d ? 0, from equations 13 and 14, we obtain R DARTS (z) ? 0. Hence, the smallest possible eigenvalue of ? 2 ? L DARTS valid must be zero or positive which implies that the validation loss function is convex. Lemma 6.2: The largest eigenvalue of the Hessian of the validation loss of ?DARTS is given by,</p><formula xml:id="formula_16">? max (? 2 ? L ?DARTS valid ) ? ? max ud N ? max N i=1 x i x T i where, ? ud = ? ? x 2 (1 ? ? ? x 2 ) + 4? 2 ? x 2 (1 ? ? x 2 ) ? 8? 2 (? x 2 ) 2 (1 ? ? x 2 ) + 2? x 2 (1 ? ? x 2 ) ? 2( ? ? x 2 (1 ? ? ? x 2 ) + ? ? x 2 ) ? x 2 = ?(?x i x T i ? T ) and ? x = ?(x T i ?)</formula><p>Proof. To estimate ? 2 ? L ?DARTS valid , we add the predictive variance term to the DARTS validation loss.</p><formula xml:id="formula_17">Var model p(y|x) (?) = E((x T i ?) 2 ) ? E(x T i ?) 2 = 1 T T i=1 ?(?x i x T i ? T ) ? ( 1 T T i=1 ?(x T i ?)) 2<label>(16)</label></formula><formula xml:id="formula_18">? 2 ? L ?DARTS valid = ? 2 ? L DARTS valid + ? 2 ? Var model p(y|x) (?) = 1 N N i=1 ? x (1 ? ? x ) x i x T i + 1 T T i=1 {[? x 2 (1 ? ? x 2 ) ? 2? 2 x 2 (1 ? ? x 2 )]4? T ?x i x T i + ? x 2 (1 ? ? x 2 )2x i x T i } ? 2 T 2 [ T i=1 ? x (1 ? ? x ) T i=1 ? x (1 ? ? x )]x i x T i ? 2 T 2 [ T i=1 ? x T i=1 ? x (1 ? ? x ) ? 2(? x ) 2 (1 ? ? x )]x i x T i<label>(17)</label></formula><p>Without loss of generality, we consider the case when T = N . Therefore, the Raleigh quotients of ? 2 ? L ?DARTS valid can be computed as:</p><formula xml:id="formula_19">R ?DARTS (z) = z T ? 2 ? L ?DARTS valid z = z T 1 N N i=1 ? x (1 ? ? x ) + ? x 2 (1 ? ? x 2 ) ? 2? 2 x 2 (1 ? ? x 2 ) 4? T ? + 2? x 2 (1 ? ? x 2 ) x i x T i ? 2 N 2 N i=1 ? x (1 ? ? x ) N i=1 ? x (1 ? ? x ) + N i=1 ? x N i=1 ? x (1 ? ? x ) ? 2(? x ) 2 (1 ? ? x ) x i x T i z</formula><p>Since N is large, we note that</p><formula xml:id="formula_20">N i=1 ? x (1 ? ? x ) ? 1; N i=1 ? x (1 ? ? x ) ? 2(? x ) 2 (1 ? ? x ) ? 1.</formula><p>Thus, we can say that</p><formula xml:id="formula_21">(i) N i=1 ? x (1 ? ? x ) N i=1 ? x (1 ? ? x ) ? N i=1 ? x (1 ? ? x ) (18) (ii) N i=1 ? x N i=1 ? x (1 ? ? x ) ? 2(? x ) 2 (1 ? ? x ) ? N i=1 ? x<label>(19)</label></formula><p>We take R ?DARTS (z) = z T 1</p><formula xml:id="formula_22">N N i=1 ? j x i x T i z,</formula><p>where, ? j is the polynomial in ? x , ? x 2 , and ?. Also, since ? x is convex, using Jensen's inequality (? 2 x ? ? x 2 ) we get:</p><formula xml:id="formula_23">? j ? ? ? x 2 (1 ? ? ? x 2 ) + 4? T ?? x 2 (1 ? ? x 2 ) ? 8? T ?(? x 2 ) 2 (1 ? ? x 2 ) + 2? x 2 (1 ? ? x 2 ) ? 2 ? ? x 2 (1 ? ? ? x 2 ) + ? ? x 2<label>(20)</label></formula><p>Assuming, maximum value of ? ud and ? j are given by ? max ud and ? max j , respectively, we obtain:</p><formula xml:id="formula_24">R ?DARTS (z) ? ? max j N z T N i=1 x i x T i z ? ? max ud N z T N i=1 x i x T i z<label>(21)</label></formula><p>Using the relation between maximum eigenvalue and Ralyeigh quotient, similar to the Lemma 6.1, the largest eigenvalue of the ?DARTS method is given as:</p><formula xml:id="formula_25">? max (? 2 ? L DARTS valid ) = R max ?DARTS ? ? max ud N ? max N i=1 x i x T i<label>(22)</label></formula><p>Lemma 6.3: If ? max (? 2 ? L DARTS valid ) and ? max (? 2 ? L ?DARTS valid ) be the maximum eigenvalues of the Hessian of the validation loss for DARTS and the ?DARTS, respectively, then</p><formula xml:id="formula_26">? max (? 2 ? L ?DARTS valid ) ? ? max (? 2 ? L DARTS valid )</formula><p>Proof. We represent ? d as: ? d = p(1 ? p), where p = ? x T i ? ? [0, 1] due to the properties of the sigmoid function. Moreover, since ? is the weights in the neural network we have ? &lt; 1 =? ? T ? &lt; 1. Hence, by maximizing ? i with respect to p we obtain:? max . We now maximize ? j with respect to q. Finally, noting ? T ? &lt; 1, we observe that ? ud ? 0. Thus the function becomes: ? max ud &lt; 0 &lt; 0.25 = ? max d . From equation <ref type="formula" target="#formula_0">(14)</ref> we note that R max M ? 0. Therefore, using equations <ref type="bibr" target="#b14">(15)</ref>, <ref type="bibr" target="#b21">(22)</ref>, and the bound on ? max ud , we get</p><formula xml:id="formula_27">? max (? 2 ? L ?DARTS valid ) ? ? max (? 2 ? L DARTS valid )<label>(23)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. COMPARISON OF ACCURACY OF UNMODIFIED MODELS</head><p>We demonstrate the comparative performance of the ?DARTS method with respect to models searched using other architecture search methods with the training condition for each model as reported in the original paper. Note that the original NAS and DARTS papers only reported accuracy of the final models (not uncertainty). The results are shown in <ref type="table">Table C</ref>. <ref type="bibr" target="#b0">1</ref>. We see that the ?DARTS, trained with only 50 epochs gives comparable performance to other networks trained for a much higher number of epochs and with a much higher batch size. This result further showcases the robustness and easy trainability of ?DARTS.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Flowchart of the ?DARTS method</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Summary of Baselines and ?DARTS. XDARTS-CD refers to the baseline architecture search models viz., P-DARTS-CD, PC-DARTS-CD, RobustDARTS-CD, GOLD-NAS-CD, ASAP-CD described in Section IV</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Figure showing the normal and reduction cell representations searched using the ?DARTS algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Plot showing the mean and variance for the variation of model uncertainty, measured using the predictive variance for different architecture search methods with increasing epochs for CIFAR 10 dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Plot showing the variation of predictive variance for different architecture search methods with increasing epochs for CIFAR 10 dataset (Note that the y-axis is represented in the logarithmic scale)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Plot showing uncertainty estimates for different values of dropout probability</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Plot showing largest eigenvalue of the Hessian of the validation loss function for increasing epochs for CIFAR 10 dataset C. COMPARATIVE ANALYSIS OF THE FINAL ARCHITECTURE Performance of Final Architectures. We compare the performance of the final architectures obtained from the architecture search methods by computing the largest eigenvalue of the Hessian of the training loss function L train . Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>d = 0. 25</head><label>25</label><figDesc>We now represent ? ud = ? q(1 ? ? q) + 4? T ?[q(1 ? q) ? 2q 2 (1 ? q)] + 2q(1 ? q) ? 2( ? q(1 ? ? q) + ? q}, where q = ? x 2 = ?(?x i x T i ? T ) ? [0, 1]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 .</head><label>1</label><figDesc>Table showing the hyperparameters of the architecture search process</figDesc><table><row><cell cols="2">Hyperparameters Values</cell></row><row><cell>Learning Rate</cell><cell>0.025</cell></row><row><cell>Weight Decay</cell><cell>0.0243</cell></row><row><cell>Epochs</cell><cell>50</cell></row><row><cell>Batch Size</cell><cell>16</cell></row><row><cell>No. of cells</cell><cell>14</cell></row></table><note>each cell has an option to have a Concrete dropout layer. The Concrete dropout layer, if included, enables computation of the uncertainty values of the model. Wherever applicable, the operations are of unit stride, and also we use padding in the convolved feature maps. ?DARTS also includes a Concrete dropout in the final softmax layer of the model. The comprehensive details for all the methods described above are shown in Fig. 2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 .</head><label>2</label><figDesc>Robustness of Architecture Search Methods (Largest Eigenvalues of the ? 2 w L valid )</figDesc><table><row><cell>Benchmark</cell><cell cols="3">DARTS RDARTS-CD DARTS-CD</cell><cell>?DARTS (This Work)</cell></row><row><cell>CIFAR10</cell><cell>0.396</cell><cell>0.110</cell><cell>0.142</cell><cell>0.097</cell></row><row><cell>CIFAR100</cell><cell>0.773</cell><cell>0.504</cell><cell>0.578</cell><cell>0.492</cell></row><row><cell>SVHN</cell><cell>0.202</cell><cell>0.045</cell><cell>0.079</cell><cell>0.021</cell></row><row><cell>ImageNet</cell><cell>0.202</cell><cell>0.045</cell><cell>0.079</cell><cell>0.021</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3 .</head><label>3</label><figDesc>Run-time in GPU hours for different architecture search methods</figDesc><table><row><cell>Dataset</cell><cell>CIFAR10</cell><cell>CIFAR100</cell><cell cols="2">SVHN ImageNet</cell></row><row><cell>DARTS</cell><cell>9.3</cell><cell>37.5</cell><cell>5.1</cell><cell>102.3</cell></row><row><cell>RDARTS-CD</cell><cell>9.7</cell><cell>37.6</cell><cell>5.9</cell><cell>111.7</cell></row><row><cell>DARTS-CD</cell><cell>9.4</cell><cell>37.5</cell><cell>5.2</cell><cell>104.9</cell></row><row><cell>?DARTS</cell><cell>9.6</cell><cell>37.7</cell><cell>5.3</cell><cell>109.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4 .</head><label>4</label><figDesc>Performance of Final Architectures Convergence of Architecture Search methods for CIFAR10 dataset 4 shows that the final architecture obtained from ?DARTS has a lower eigenvalue of the training loss function indicating convergence to a flatter minima. We further evaluate robustness considering the mean and standard deviation of errors by training the final architecture multiple times and testing it on the test dataset.</figDesc><table><row><cell>Benchmark</cell><cell>DARTS</cell><cell>RDARTS-CD (Ours)</cell><cell>DARTS-CD (Ours)</cell><cell>?DARTS (This Work)</cell></row><row><cell></cell><cell cols="3">Largest Eigenvalues of the ? 2 w L training</cell><cell></cell></row><row><cell>CIFAR10</cell><cell>0.217</cell><cell>0.093</cell><cell>0.102</cell><cell>0.089</cell></row><row><cell>CIFAR100</cell><cell>0.545</cell><cell>0.419</cell><cell>0.503</cell><cell>0.395</cell></row><row><cell>SVHN</cell><cell>0.176</cell><cell>0.039</cell><cell>0.061</cell><cell>0.017</cell></row><row><cell>ImageNet</cell><cell>0.176</cell><cell>0.039</cell><cell>0.061</cell><cell>0.017</cell></row><row><cell></cell><cell cols="3">Testing Errors (Mean ? Standard Deviation)</cell><cell></cell></row><row><cell>CIFAR10</cell><cell>5.68 ? 0.27</cell><cell>5.11 ? 0.21</cell><cell>5.71 ? 0.24</cell><cell>3.78 ? 0.20</cell></row><row><cell>CIFAR100</cell><cell>24.63 ? 0.81</cell><cell>22.67 ? 0.66</cell><cell>25.19 ? 0.74</cell><cell>20.02 ? 0.40</cell></row><row><cell>SVHN</cell><cell>4.67 ? 0.19</cell><cell>3.93 ? 0.15</cell><cell>4.48 ? 0.17</cell><cell>2.12 ? 0.127</cell></row><row><cell>ImageNet</cell><cell>29.37 ? 5.03</cell><cell>27.14 ? 3.97</cell><cell>28.71 ? 4.98</cell><cell>25.36 ? 3.92</cell></row></table><note>Fig. 8.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5 .</head><label>5</label><figDesc>Comparison of Accuracy and Uncertainty of Final Models from Different Architecture Search Processes (All models are trained for 50 epochs and batch size of 64)</figDesc><table><row><cell>Model</cell><cell>FLOPS(M)</cell><cell cols="2">CIFAR10</cell><cell cols="2">CIFAR100</cell><cell cols="2">SVHN</cell><cell cols="2">ImageNet</cell></row><row><cell></cell><cell></cell><cell>Accuracy</cell><cell>Uncertainty</cell><cell>Accuracy</cell><cell>Uncertainty</cell><cell>Accuracy</cell><cell>Uncertainty</cell><cell>Accuracy</cell><cell>Uncertainty</cell></row><row><cell>MobileNetv2 [14]</cell><cell>569</cell><cell>88.79 ? 0.38</cell><cell>0.915</cell><cell>68.54 ?0.56</cell><cell>1.266</cell><cell>93.67 ?0.22</cell><cell>0.247</cell><cell>68.83 ? 3.99</cell><cell>1.853</cell></row><row><cell>VGG16 [15]</cell><cell>160</cell><cell>84.11 ? 0.41</cell><cell>1.034</cell><cell>71.19 ? 0.52</cell><cell>1.482</cell><cell>93.15 ? 0.21</cell><cell>0.344</cell><cell>62.46 ? 2.11</cell><cell>1.897</cell></row><row><cell>ResNet20 [16]</cell><cell>320</cell><cell>86.47 ? 0.37</cell><cell>0.895</cell><cell>75.22 ? 0.46</cell><cell>1.246</cell><cell>93.74 ? 0.16</cell><cell>0.238</cell><cell>65.11 ? 2.02</cell><cell>1.816</cell></row><row><cell>EfficientNet-B0 [17]</cell><cell>390</cell><cell>89.01 ? 0.47</cell><cell>1.059</cell><cell>76.02 ? 0.55</cell><cell>1.215</cell><cell>94.87 ? 0.24</cell><cell>0.291</cell><cell>70.83 ? 4.15</cell><cell>1.893</cell></row><row><cell>DARTS [6]</cell><cell>595</cell><cell>94.32 ? 0.27</cell><cell>N/A</cell><cell>75.37 ? 0.81</cell><cell>N/A</cell><cell>95.33 ? 0.19</cell><cell>N/A</cell><cell>70.63 ? 5.03</cell><cell>NA</cell></row><row><cell>P-DARTS-CD</cell><cell>532</cell><cell>95.32 ? 0.78</cell><cell>0.496</cell><cell>78.94 ? 0.89</cell><cell>0.883</cell><cell>97.79 ? 0.33</cell><cell>0.266</cell><cell>72.13 ? 4.07</cell><cell>1.854</cell></row><row><cell>PC-DARTS-CD</cell><cell>557</cell><cell>96.47 ? 0.81</cell><cell>0.463</cell><cell>80.01 ? 0.95</cell><cell>0.964</cell><cell>96.82 ? 0.36</cell><cell>0.279</cell><cell>72.81 ? 4.12</cell><cell>1.896</cell></row><row><cell>GOLD-NAS-A-CD</cell><cell>278</cell><cell>95.06 ? 1.03</cell><cell>0.412</cell><cell>78.97 ? 1.15</cell><cell>0.992</cell><cell>96.13 ? 0.52</cell><cell>0.333</cell><cell>69.56 ? 4.95</cell><cell>2.038</cell></row><row><cell>GOLD-NAS-I-CD</cell><cell>463</cell><cell>93.21 ? 0.91</cell><cell>0.519</cell><cell>79.15 ? 1.06</cell><cell>1.003</cell><cell>96.71 ? 0.47</cell><cell>0.303</cell><cell>72.21 ? 4.13</cell><cell>1.755</cell></row><row><cell>ASAP-CD</cell><cell>573</cell><cell>95.72 ? 1.15</cell><cell>0.481</cell><cell>79.27 ? 1.11</cell><cell>0.992</cell><cell>98.01 ? 0.63</cell><cell>0.484</cell><cell>73.96 ? 4.28</cell><cell>1.812</cell></row><row><cell>DARTS-CD</cell><cell>598</cell><cell>94.29 ? 0.24</cell><cell>0.474</cell><cell>74.81 ? 0.74</cell><cell>1.097</cell><cell>95.52 ? 0.17</cell><cell>0.195</cell><cell>71.29 ? 4.98</cell><cell>1.712</cell></row><row><cell>RDARTS-CD</cell><cell>605</cell><cell>94.89 ? 0.21</cell><cell>0.396</cell><cell>77.34 ? 0.66</cell><cell>0.815</cell><cell>96.07 ? 0.15</cell><cell>0.189</cell><cell>72.86 ? 3.97</cell><cell>1.684</cell></row><row><cell>?DARTS (This Work)</cell><cell>602</cell><cell>96.22 ? 0.20</cell><cell>0.162</cell><cell>79.98 ? 0.40</cell><cell>0.561</cell><cell>97.88 ? 0.11</cell><cell>0.127</cell><cell>74.64 ? 3.92</cell><cell>1.523</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 6 .</head><label>6</label><figDesc>Accuracy and Uncertainty of Different DNN Models under Input Noise</figDesc><table><row><cell>Model | SNR</cell><cell cols="2">40dB</cell><cell></cell><cell>30dB</cell><cell></cell><cell>20dB</cell></row><row><cell></cell><cell cols="2">Accuracy Uncertainty</cell><cell>Accuracy</cell><cell>Uncertainty</cell><cell>Accuracy</cell><cell>Uncertainty</cell></row><row><cell cols="7">Accuracy and Uncertainty of Different Network Architectures under Input Noise for CIFAR 10</cell></row><row><cell>DARTS</cell><cell>88.38</cell><cell>N/A</cell><cell>82.69</cell><cell>N/A</cell><cell>40.87</cell><cell>NA</cell></row><row><cell>DARTS-CD</cell><cell>89.16</cell><cell>0.574</cell><cell>82.17</cell><cell>0.913</cell><cell>44.11</cell><cell>1.502</cell></row><row><cell>RDARTS-CD</cell><cell>90.88</cell><cell>0.488</cell><cell>82.75</cell><cell>0.899</cell><cell>44.29</cell><cell>1.479</cell></row><row><cell>?DARTS</cell><cell>92.04</cell><cell>0.198</cell><cell>84.16</cell><cell>0.397</cell><cell>49.34</cell><cell>1.206</cell></row><row><cell cols="7">Accuracy and Uncertainty of Different Network Architectures under Input Noise for CIFAR 100</cell></row><row><cell>DARTS</cell><cell>76.21</cell><cell>N/A</cell><cell>71.37</cell><cell>N/A</cell><cell>37.17</cell><cell>NA</cell></row><row><cell>DARTS-CD</cell><cell>76.89</cell><cell>0.632</cell><cell>71.04</cell><cell>1.174</cell><cell>38.41</cell><cell>2.047</cell></row><row><cell>RDARTS-CD</cell><cell>77.39</cell><cell>0.599</cell><cell>72.58</cell><cell>1.007</cell><cell>39.16</cell><cell>1.992</cell></row><row><cell>?DARTS</cell><cell>78.54</cell><cell>0.483</cell><cell>75.82</cell><cell>0.397</cell><cell>46.22</cell><cell>1.632</cell></row><row><cell cols="7">Accuracy and Uncertainty of Different Network Architectures under Input Noise for SVHN</cell></row><row><cell>DARTS</cell><cell>94.77</cell><cell>N/A</cell><cell>89.19</cell><cell>N/A</cell><cell>70.22</cell><cell>NA</cell></row><row><cell>DARTS-CD</cell><cell>94.98</cell><cell>0.148</cell><cell>90.33</cell><cell>0.396</cell><cell>71.68</cell><cell>0.835</cell></row><row><cell>RDARTS-CD</cell><cell>95.03</cell><cell>0.106</cell><cell>91.00</cell><cell>0.313</cell><cell>72.15</cell><cell>0.794</cell></row><row><cell>?DARTS</cell><cell>97.34</cell><cell>0.095</cell><cell>92.3</cell><cell>0.199</cell><cell>75.45</cell><cell>0.552</cell></row><row><cell cols="3">TABLE 7. Performance of Final DNN under Parameter noise</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">CIFAR 10</cell><cell cols="2">CIFAR 100</cell><cell cols="2">SVHN</cell></row><row><cell></cell><cell>Accuracy</cell><cell>Uncertainty</cell><cell>Accuracy</cell><cell>Uncertainty</cell><cell>Accuracy</cell><cell>Uncertainty</cell></row><row><cell>DARTS</cell><cell>90.17 ? 0.298</cell><cell>N/A</cell><cell>74.13 ? 0.785</cell><cell>N/A</cell><cell>93.19 ? 0.211</cell><cell>N/A</cell></row><row><cell>DARTS-CD</cell><cell>91.86 ? 0.297</cell><cell>0.312</cell><cell>74.68 ? 0.587</cell><cell>0.638</cell><cell>93.91 ? 0.206</cell><cell>0.201</cell></row><row><cell>RDARTS-CD</cell><cell>92.78 ? 0.29</cell><cell>0.217</cell><cell>75.76 ? 0.591</cell><cell>0.411</cell><cell>94.11 ? 0.210</cell><cell>0.109</cell></row><row><cell>?DARTS</cell><cell>94.12 ? 0.25</cell><cell>0.134</cell><cell>79.89 ? 0.491</cell><cell>0.398</cell><cell>95.14 ? 0.17</cell><cell>0.074</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 8 .</head><label>8</label><figDesc>*TABLE C.1: Comparison of Reported Test Errors of Models for Different Architecture Search Processes (The test errors are the values reported in the original papers)</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="2">CIFAR 10</cell><cell></cell><cell></cell><cell>ImageNet</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Params(M)</cell><cell>Accuracy</cell><cell cols="2">Epochs Batch Size</cell><cell>Params(M)</cell><cell cols="3">Accuracy Epochs Batch Size</cell></row><row><cell>P-DARTS [31]</cell><cell>10.5</cell><cell>2.25</cell><cell>250</cell><cell>1024</cell><cell>5.4</cell><cell>24.1</cell><cell>250</cell><cell>1024</cell></row><row><cell>PC-DARTS [32]</cell><cell>3.6</cell><cell>2.57</cell><cell>600</cell><cell>128</cell><cell>5.3</cell><cell>24.2</cell><cell>250</cell><cell>1024</cell></row><row><cell>SNAS [36]</cell><cell>2.8</cell><cell>2.85</cell><cell>600</cell><cell>96</cell><cell>4.3</cell><cell>27.3</cell><cell>250</cell><cell>96</cell></row><row><cell>GOLD-NAS-A [35]</cell><cell>1.58</cell><cell>2.93</cell><cell>200</cell><cell>96</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell>GOLD-NAS-Z [35]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>6.4</cell><cell>23.9</cell><cell>250</cell><cell>1024</cell></row><row><cell>ASAP [34]</cell><cell>6.0</cell><cell>1.68</cell><cell>1500</cell><cell>128</cell><cell>6.0</cell><cell>24.4</cell><cell>250</cell><cell>64</cell></row><row><cell>DARTS [6]</cell><cell>3.3</cell><cell>2.76</cell><cell>600</cell><cell>96</cell><cell>4.7</cell><cell>26.7</cell><cell>250</cell><cell>128</cell></row><row><cell>RDARTS [7]</cell><cell>3.1</cell><cell>2.97</cell><cell>50</cell><cell>64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell>?DARTS (This Work)</cell><cell>4.1</cell><cell>3.78</cell><cell>50</cell><cell>32</cell><cell>5.3</cell><cell>25.36</cell><cell>50</cell><cell>64</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">   VOLUME 4, 2016   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">   VOLUME 4, 2016   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">   VOLUME 4, 2016   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">   VOLUME 4, 2016   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">   VOLUME 4, 2016   </note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">University Of Cambridge</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02142</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Concrete dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3581" to="3590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Aging evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference On Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055.</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Understanding and robustifying differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Arber Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Marrakchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09656</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5574" to="5584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02680</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An overview of bilevel optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Colson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marcotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Savard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals Of Operations Research</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="page" from="235" to="256" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Designing neural network architectures using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02167</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Jmlr</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mo-bilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The IEEE Conference On Computer Vision And Pattern Recognition</title>
		<meeting>Of The IEEE Conference On Computer Vision And Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The IEEE Conference On Computer Vision And Pattern Recognition</title>
		<meeting>Of The IEEE Conference On Computer Vision And Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">&amp; Others PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">&amp; Others Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference On Computer Vision And Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sharp minima can generalize for deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The 34th International Conference On Machine Learning</title>
		<meeting>Of The 34th International Conference On Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1019" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Parameter space noise for exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Asfour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01905</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Scalable deep multi-agent reinforcement learning via observation embedding and parameter noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="54615" to="54622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Evolution strategies as a scalable alternative to reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03864</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The effects of adding noise during backpropagation training on a generalization performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="643" to="674" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Unique Properties of Flat Minima in Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mulayoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">NADS: Neural Architecture Distribution Search for Uncertainty Awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ardywibowo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boluki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Stabilizing Differentiable Architecture Search via Perturbation-based Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05283</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Progressive differentiable architecture search: Bridging the depth gap between search and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The IEEE International Conference On Computer Vision</title>
		<meeting>Of The IEEE International Conference On Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1294" to="1303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Pc-darts: Partial channel connections for memory-efficient differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05737</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The IEEE Conference On Computer Vision And Pattern Recognition</title>
		<meeting>Of The IEEE Conference On Computer Vision And Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nayman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Doveh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Asap</surname></persName>
		</author>
		<title level="m">Architecture search, anneal and prune. International Conference On Artificial Intelligence And Statistics</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="493" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Gold-nas: Gradual, one-level</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03331</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">differentiable. ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">SNAS: stochastic neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09926.</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">He is currently pursuing the Ph.D. degree in Electrical and</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biswadeep</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graduate Student Member, IEEE) received the B.E. degree in Electronics and Telecommunication engineering from Jadavpur University</title>
		<meeting><address><addrLine>Kolkata, India; Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>Computer Engineering with the Georgia Institute of Technology ; National University of</orgName>
		</respStmt>
	</monogr>
	<note>under the supervision of Prof. S. Mukhopadhyay. Before starting his Ph.D., he worked as a Research Assistant at the Singtel</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">His current research interests lies in unsupervised learning methods using spiking neural networks for spatio-temporal prediction and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Singapore</forename><surname>Lab</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">He was a Research Staff Member with the IBM Thomas J</title>
	</analytic>
	<monogr>
		<title level="m">2000, and the Ph.D. degree in electrical and computer engineering from Purdue University</title>
		<meeting><address><addrLine>Kolkata, India; West Lafayette, IN, USA, in; Yorktown Heights, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Watson Research Center</publisher>
			<date type="published" when="2006-08" />
		</imprint>
		<respStmt>
			<orgName>SAIBAL MUKHOPADHYAY (Fellow, IEEE) received the B.E. degree in Electronics and Telecommunication engineering from Jadavpur University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">He has authored or coauthored over 200 articles in refereed journals and conferences, and holds five U.S. patents. His research interests include design of energyefficient, intelligent, and secure systems in nanometer technologies. He was a recipient of the Office of Naval Research Young Investigator Award, in 2012, the National Science Foundation CAREER Award</title>
	</analytic>
	<monogr>
		<title level="m">the SRC Inventor Recognition Award in 2008, the SRC Technical Excellence Award, in 2005, and the IBM Ph.D. Fellowship Award, for years</title>
		<meeting><address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>Pettit Professor with the School of Electrical and Computer Engineering, Georgia Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note>2011, the IBM Faculty Partnership Award</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
