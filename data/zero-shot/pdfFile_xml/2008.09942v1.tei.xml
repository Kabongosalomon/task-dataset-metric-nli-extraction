<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Few-Shot Image Classification via Contrastive Self-Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyi</forename><surname>Li</surname></persName>
							<email>lijianyi1488@stu.xjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communications</orgName>
								<orgName type="institution">Xian Jiaotong University Xi&apos;an</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guizhong</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information and Communications</orgName>
								<address>
									<addrLine>Xian Jiaotong University Xi&apos;an</addrLine>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Few-Shot Image Classification via Contrastive Self-Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Index Terms-few shot learning, contrastive self-supervised learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most previous few-shot learning algorithms are based on meta-training with fake few-shot tasks as training samples, where large labeled base classes are required. The trained model is also limited by the type of tasks. In this paper we propose a new paradigm of unsupervised few-shot learning to repair the deficiencies. We solve the few-shot tasks in two phases: meta-training a transferable feature extractor via contrastive self-supervised learning and training a classifier using graphaggregation, self-distillation and manifold augmentation. Once meta-trained, the model can be used in any type of tasks with a task-dependent classifier training. Our method achieves stateof-the-art performance in a variety of established few-shot tasks on the standard few-shot visual classication dataset, with an 8-28% increase compared to the available unsupervised few-shot learning methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Most previous few-shot learning algorithms are based on meta-training with fake few-shot tasks as training samples, where large labeled base classes are required. The trained model is also limited by the type of tasks. In this paper we propose a new paradigm of unsupervised few-shot learning to repair the deficiencies. We solve the few-shot tasks in two phases: meta-training a transferable feature extractor via contrastive self-supervised learning and training a classifier using graphaggregation, self-distillation and manifold augmentation. Once meta-trained, the model can be used in any type of tasks with a task-dependent classifier training. Our method achieves stateof-the-art performance in a variety of established few-shot tasks on the standard few-shot visual classication dataset, with an 8-28% increase compared to the available unsupervised few-shot learning methods.</p><p>Index Terms-few shot learning, contrastive self-supervised learning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In recent years deep learning has made major advances in computer vision areas such as image recognition, video object detection and tracking. A deep neural network needs a large amount of labeled data to fit its parameters whereas it is laborious to label so many examples by human annotators. Thus the problem of learning with few labeled samples called few-shot learning has been paid more and more attention. Fewshot learning is described as a classification task set in N -way and k -shot, which means to distinguish N categories, each of which has k (quite small) labeled samples. The model predict classes for new examples only depending on k labeled data. The annotated data is called the support set, and the new data belonging to the N categories is called query set.</p><p>People have proposed varieties of few-shot methods, all of which rely on meta-training assisted with base classes. The universal approach is to use the base classes to construct fake few-shot tasks for training the network first, with the purpose of enabling the network an ability to accomplish real fewshot tasks through simulating the process of carrying out the fake tasks. This is called the meta-training stage with tasks as samples. Next, use the trained network to complete real few-shot tasks of novel classes, and calculate the classification accuracy on the query set in the tasks to evaluate the algorithm, which is usually called the meta-testing. The whole procedure is shown in <ref type="figure" target="#fig_1">Fig.1</ref>. <ref type="figure" target="#fig_1">Fig. 1</ref>. The universal method used in supervised few-shot learning, which consists of meta-training and meta-testing. In the meta-training, the training sample is actually a mimic few-shot task comprised of some labeled data chosen from base classes. And in the meta-testing the model will solve a real task with few labeled data and an unlabeled query set chosen from novel classes. We show a model trained for solving 3-way 1-shot tasks in this figure.</p><p>Few-shot learning algorithms could be classified into three categories. The first <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref> is based on metric learning, which consists of three steps of feature extraction, distance measure, and prediction, relying on effective metric design and reducing the cross entropy loss in meta training to improve classification accuracy. The second are the teacher-student network based methods including <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b7">[8]</ref>. The teacher network guides the student network to solve the few-shot tasks in terms of parameter initialization, parameter update and other aspects. The algorithm enables the teacher network to obtain the ability to instruct the student network via meta-training. The third category such as <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b9">[10]</ref> is based on the transduction, which propagates the label of the support data to the queries through a specific graph, thereby obtaining the predicted class of the query set. The algorithm optimizes the accuracy of propagation label through meta-training.</p><p>The meta-training determines the models performance in the few-shot learning algorithm. However it brings two obvious drawbacks. First, the meta-training requires a large number of labeled auxiliary examples (base classes). Those algorithms can not work without adequate labeled samples. Second, the meta-training phase uses tasks as training samples. Therefore, a task type decided by the values of N and k needs to be certain before meta-training to ensure that the number of images contained by each mimic few shot task (i.e. a metatraining sample) is consistent during training. The meta-trained network can only be used to solve few-shot tasks with the same type as the meta-training samples, and it performs worse in other types of tasks. However, in reality we need to solve various types of few-shot tasks, and it is unreasonable to metatrain the network from scratch in order to solve a few shot task with new type.</p><p>In order to solve these two problems, we propose a new paradigm of few shot learning based on contrastive selfsupervised learning (CSSL-FSL). Specifically, our method abandons the meta-training phase, which takes the fake fewshot tasks as samples, and uses instead two new phases: the meta-training via self-supervised learning directly using a single image as a training sample, and the training of a classification network. In the first phase, a comparative selfsupervised learning method is used to obtain a feature extractor with good generalization ability using unlabeled images. In the second phase, our method solve real few-shot tasks. The metatrained feature extractor is used to extract features from all the images in the current task, and a feature aggregation is carried out, based on a specific graph defined by the current task so that the information of the query set can interact with that of the support set. We use the aggregated support set features to train a fully connected neural classification network. The classifier can predict classes of the query set after training.</p><p>Furthermore, we demonstrate that the self-distillation <ref type="bibr" target="#b10">[11]</ref> and the manifold data augmentation are helpful for training the classification networks. Self-distillation is a special form of knowledge distillation used to promote classification accuracy. Manifold augmentation is used in our method to expand the training dataset by combining data at the semantic level.</p><p>Our key contributions can be summarized as follows:</p><p>? We propose CSSL-FSL, a new paradigm of unsupervised few-shot learning. By adopting the methodology of contrastive self-supervised learning, the two problems intrinsic in the existing meta-training paradigm are solved simultaneously. Thus our method does not require a large number of labeled samples for training. In addition, the meta-trained model can carry out different types of real few-shot tasks. ? We leverage a graph with a trainable network parameter to aggregate features of samples in few-shot tasks to obtain more discriminating ones, which is similar to SGC <ref type="bibr" target="#b11">[12]</ref>. ? We propose to use the manifold augmentation and selfdistillation technologies to alleviate the lack of labeled samples in the phase of training the classification network. ? Adequate experiments demonstrate that our method reaches state-of-the-art accuracy on miniImageNet , a standardized benchmark in few-shot learning.</p><p>The paper is organized as follows. In II, we introduce the related works. Our methodology is described in III. In IV, experimental results on the standard vision dataset are shown in comparison with the proposed works. Finally, a conclusion is drawn in V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>In this section we aim to show the three types of supervised few-shot learning algorithms proposed in previous years. In addition, we introduce some unsupervised few-shot learning methods presented recently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Metric Based Methods</head><p>The core of metric learning is to extract features from the support set and query set, then obtain the class prototypes using the support set, and predict classes of queries via the nearest neighbor algorithm and attention mechanism. Through meta-training, a metric based method obtains a feature extractor that facilitates completing the classification task based on distance measurement. Matching Networks <ref type="bibr" target="#b0">[1]</ref> used LSTM to extract full context embeddings from images and applied attention mechanism to classify. Prototypical Networks <ref type="bibr" target="#b1">[2]</ref> proposed to use Euclidean distance to better measure the similarity between features, and use prototypes of each class to classify queries. Relation Network <ref type="bibr" target="#b2">[3]</ref> used a neural network to replace the traditional distance metric, and directly output the queries categories via an end-to-end network. DC-IMP <ref type="bibr" target="#b12">[13]</ref> introduced dense classification and leverage implanting to bring metric learning the task dependency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Teacher-Student Based Methods</head><p>A Teacher-Student based method has two networks, called the teacher network and the student network. The student network is in charge of fulfilling the few-shot task, and the teacher network provide guidance on how to fulfil that task. A Teacher-Student based method ensures the teacher network possessing excellent guidance ability through meta-training, so that when facing real few-shot tasks, the teacher network can perform task-dependent guidance. In <ref type="bibr" target="#b7">[8]</ref>, the teacher network guides on how the student networks parameters update. When the student network updates, it does not obey the standard gradient descent, but uses the teacher networks output as update values on the parameters. In MAML <ref type="bibr" target="#b4">[5]</ref>, the teacher network generates initial weights for the student network, which can help the student network converge quickly when facing new few-shot tasks. To further promote MAML, LEO <ref type="bibr" target="#b5">[6]</ref> proposed generating initial weights for student network from a lower-dimensional hidden space, which makes training more easily. The teacher network in LGM-Net <ref type="bibr" target="#b6">[7]</ref> directly generates all the network parameters for its student network to deal with the few-shot task successfully. The student network does not need to finetune itself with support set, that is, the teacher network provides a one-step guidance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Transduction Based Methods</head><p>The key of Transduction based methods is to integrate graphs into the algorithm. Through feature aggregation in a specific way, the features of the support set contain the information of the queries, which is equivalent to use dual information of support and query when updating the network. Through meta-training, transduction based methods can obtain excellent edge and vertex feature update modules. TPN <ref type="bibr" target="#b8">[9]</ref> constructs a graph in the feature space, in which vertices are defined by image features and the adjacency matrix is obtained via calculating the vertices similarity. Then it initializes and updates a node-labeling matrix, and finally classifies the queries by updating the node-labeling matrix. EGNN <ref type="bibr" target="#b9">[10]</ref> proposed a structure of graph similar to <ref type="bibr" target="#b8">[9]</ref>, but used edgelabeling framework instead of node-labeling framework in classification which helps to exploit both the intra-cluster similarity and the inter-cluster dissimilarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Unsupervised Methods</head><p>The base classes in unsupervised methods has no labels. Some existing methods use unsupervised learning or data enhancement methods to leverage these unlabeled base classes to artificially construct fake support set and query set for meta training. They are able to combine with the few-shot learning methods as mentioned above (such as MAML <ref type="bibr" target="#b4">[5]</ref> and Prototypical Net <ref type="bibr" target="#b1">[2]</ref>) to fulfil few-shot tasks. UFLST <ref type="bibr" target="#b13">[14]</ref> and CACTU <ref type="bibr" target="#b14">[15]</ref> use clustering to make pseudo-labels for unlabeled examples, then use the pseudo-labeled data as ordinary labeled data to construct fake few-shot tasks to complete meta-training. AAL <ref type="bibr" target="#b15">[16]</ref> and UMTRA <ref type="bibr" target="#b16">[17]</ref> took each instance as one class and randomly sample multiple examples to construct a fake support set, then generate a corresponding query set according to the support set by data augmentation techniques. ULDA <ref type="bibr" target="#b17">[18]</ref> developed a new simple data augmentation method to enhance the difference between the support set distribution and query set distribution when constructing the fake few-shot tasks for meta-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>The notations and problem formulation of self-supervised few-shot learning are introduced in III-A, and our paradigm is presented in III-B. Finally, the self-knowledge distillation and manifold augmentation are described in III-C and III-D respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Formulation</head><p>Given two datasets, namely D base and D task with disjoint classes. D base consists of a large number of unlabeled examples from the base classes. D task has a small number of labeled examples called the support set D s , along with some unlabeled ones called the query set D q , all from the new classes. They stand for the total data in a few-shot learning task. The number of classes in the novel dataset D task , the number of support samples and the number of query inputs for each of these classes are denoted N , k and q respectively. So there are totally N ? (k + q) examples in a few-shot learning task. Our aim is to predict the classes of the query set of D task . Different from the previous works like <ref type="bibr" target="#b7">[8]</ref>, our D base does not have any labels. So we train the classification network with only a few labeled examples namely D s in a real sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proposed Paradigm of Solution</head><p>We first train a backbone deep neural network able to extract useful and compact features from inputs, which will be used as a generic feature extractor. In this so called meta-training phase, we train the network with D base = x 1 , x 2 , ......, x n where x ? R w?h?3 via CMC in <ref type="bibr" target="#b18">[19]</ref>, a kind of contrastive self-supervised learning method, which promises a transferable feature extractor. Thus we obtain the extractor f ? : R w?h?3 ? R 2?e (consisting of two networks namely the f ?1 : R w?h ? R e and f ?2 : R w?h?2 ? R e , which will be described in detail later).</p><p>We then use f ? to obtain the features of the total data in D task (both D s and D q ) namely f ? (D task ) = {f ? (x)|x ? D task }. Then we step into the second phase namely the task-training phase. First we build a nearest neighbor graph using the cosine similarity according to <ref type="bibr" target="#b19">[20]</ref> :</p><formula xml:id="formula_0">cos (f ? (x 1 ) , f ? (x 2 )) = f ? (x 1 ) T f ? (x 2 ) f ? x 1 2 f ? x 2 2<label>(1)</label></formula><p>The base graph denoted G task (V, E) uses f ? (D task ) to construct vertices. In details, its vertices matrix V ? R [N ?(k+q)]?2e is the stacked representations of support set and query set i.e. each vertex represents an image's feature. We make the values of graph edges represent the similarity between vertices-that is, similar vertices have larger adjacency values. To get the adjacency matrix E ? R [N ?(k+q)]?[N ?(k+q)] , we first define a similarity matrix S with the same dimension computed as follows:</p><formula xml:id="formula_1">S i,j = cos(V i,: , V j,: ) i = j 0 i = j<label>(2)</label></formula><p>where V i,: denotes the i -th row in V . Then we just save the m largest values on each row and on the corresponding column in S to obtain a more sparse matrix helpful to reduce the interference. Finally, we normalize the resulting matrix to get the adjacency matrix:</p><formula xml:id="formula_2">E = D ? 1 2 SD ? 1 2<label>(3)</label></formula><p>where D is the degree diagonal matrix computed by D i,i = j S i,j . We can consider E as the Laplacian matrix in GCN <ref type="bibr" target="#b20">[21]</ref> used to aggregate information among vertices.</p><p>Then we aggregate features for each vertex via the graph structure to get V new and train a fully connected network Cls ? : R 2e ?R N using vertices defined by D s . During the training we take two sub-stages to achieve a better performance with quite few support examples. In the two sub-stages, we use manifold augmentation from which we can obtain augmented data mixed V new i,:</p><p>and self-knowledge distillation to alleviate overfitting and to achieve a better performance respectively.</p><p>Our paradigm is illustrated in <ref type="figure" target="#fig_0">Fig.2</ref>. In general CSSL-FSL has two phases: (1) Meta-training phase: training a generic feature extractor via contrastive self-supervised learning. (2) Task-training phase: adapting a classification network using  Input: D base = x 1 , x 2 , ......, x n without labels; Output: A meta-trained feature extractor f ? consists in f ?1 and f ?2 1 initialize f ? ; 2 if training is not completed then <ref type="bibr" target="#b2">3</ref> Choose a minibatch D batch from D base randomly; <ref type="bibr" target="#b3">4</ref> Feed D batch into f ? to obtain f ? (D batch ); <ref type="bibr" target="#b4">5</ref> Compute contrastive loss L SSL using f ? (D batch ) according to CMC <ref type="bibr" target="#b18">[19]</ref>; <ref type="bibr" target="#b5">6</ref> Update ? with L SSL ; 7 end 8 Return f ? ; the support set data after the feature aggregation through graph. Once the latter is finished, the performance of this model is evaluated on the vertices constructed from D q . The process of the meta-training phase is provided in Algorithm 1 and the process of task-training phase is formalized in Algorithm 2.</p><p>The details of the two phases are provided in the following, rst the meta-training phase then the task-training phase.</p><p>Meta-training phase: We follow the methodology called CMC, an effective contrastive self-supervised learning method, proposed in <ref type="bibr" target="#b18">[19]</ref>. More specifically, we consider an input image in Lab color space, spliting it into L view (luminance) called V iew L and the ab view (chrominance) called V iew ab . We aim to obtain a network able to extract compact and Algorithm 2: The process of task-training phase with two sub-stages.</p><p>Input: A N -way k -shot task with the dataset D task = {D s , D q }; The meta-trained feature extractor f ? consists in f ?1 and f ?2 Output: Parameters for the classifier Cls ? 1 Obtain features of all inputs including labeled and unlabeled ones, f ? (D task ); 2 Build the graph G task (V, E) based on f ? (D task ); 3 Aggregate vertices features V of the graph to get V new ; 4 randomly initialize ?;</p><p>The first sub-stage; <ref type="bibr" target="#b4">5</ref> Use manifold augmentation to extend labeled data in semantic level, and get the augmented feature set V new aug = {mixed V new ,V new }; 6 Train Cls ? using V new aug and cross entropy loss to obtain Cls ?0 ;</p><p>The second sub-stage; <ref type="bibr" target="#b6">7</ref> Use the predictions on D s from Cls ?0 and labels of D s to compute distillation loss; 8 Update ? from scratch and finally obtain ? 1 ; 9 Return Cls ?1 ; distinct features from the inputs. Through contrastive learning we learn a feature embedding, which can map views of similar images to nearby points while map views of different images to far apart points. The feature embedding have two parts, the L view part termed f ?1 and the ab view part f ?2 . So we have ? = (? 1 , ? 2 ). The total feature of the input is the concatenation of the outputs from these two parts, namely</p><formula xml:id="formula_3">f ? (x) = concat[f ?1 (x), f ?2 (x)].</formula><p>As in <ref type="bibr" target="#b18">[19]</ref> we use the contrast loss as a loss for the selfsupervised learning to train the feature embedding:</p><formula xml:id="formula_4">L SSL = L V iew L ,V iew ab contrast + L V iew ab ,V iew L contrast (4) in which L V iew L ,V iew ab contrast</formula><p>is the contrast loss computed by treating view V iew L as anchor and enumerates over V iew ab while L V iew ab ,V iew L contrast anchors at V iew ab . Task-training phase: We x the meta-trained parameters ? = {? 1 , ? 2 } in the backbone and train a task-dependent classier Cls ? on the transferred representations of the few-shot tasks dataset namely D task . Before training a linear classier with D s having few labeled examples, a method similar to simplied graph convolution <ref type="bibr" target="#b11">[12]</ref>, namely the graph aggregation is used. We construct the graph G task (V, E) for the current few-shot task through the steps introduced before. The vertices in G task contain the total representations of D task and the adjacency matrix stands for the coefficient of attention used in features aggregation.</p><p>We then propagate feature ( <ref type="bibr" target="#b11">[12]</ref>) to obtain new features for each vertex:</p><formula xml:id="formula_5">V new = (?I + E) ? V<label>(5)</label></formula><p>where I is the identity matrix and ? is a hyperparameter which plays an important role in getting better representation, denoting the number of times to aggregate feature. At the same time, ? is also a key value to balance between the neighbors representations and the self-ones. So we make it a trainable network parameter instead of a fixed value as in SGC. After aggregation, we use the labeled part of the vertices to train the task-dependent classifier Cls ? , a simple fully connected network. We may choose to extend the support set by manifold augmentation and train with the cross entropy loss, and we could use the self-distillation to further improve the performance, which are introduced in details in III-C and III-D respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Manifold Augmentation</head><p>Since the number of labeled samples in a few-shot task is too small, in the first sub-stage of the classifier training we use data augmentation to expand the training set. Manifold Mixup <ref type="bibr" target="#b21">[22]</ref> is a kind of effective data augmentation method, which leverages semantic interpolations as additional training signal along with the corresponding linearly combined labels. It was proved that the combination of hidden representations of training examples works better than the original image mixup.</p><p>In our method, we use Manifold Mixup based on the new vertices matrix as follows:</p><formula xml:id="formula_6">mixed V new i,: =?V new i,: + (1 ? ?) V new j,: 1 ? i ? N ? k (6) where V new i,:</formula><p>is the i -th row in the new vertices matrix as the base feature, V new j,:</p><p>plays as noise (j is randomly selected from [1, N ? k]), and mixed V new i,:</p><p>denotes the augmented embedding based on V new i,: . We make ? close to one to ensure that our base embedding wont get much change because we still use the original label of the base feature for the combined feature. We also tried to use the linear combined labels but it leads to worse performance. Then we leverage both the original and the augmented vertices to update both the parameters ? in Cls and ? used in graph aggregation with cross entropy loss. We regard ? as a part of ? for convenience in the following. D. Self-distillation <ref type="figure">Fig. 3</ref>. In the second training sub-stage, Cls ? 1 is learned with knowledge distilled from Cls ? 0 which has been trained in the first sub-stage.</p><p>Knowledge distillation <ref type="bibr" target="#b22">[23]</ref> is usually used to get a compact network called student network, which leverages a complex but well-performed teacher network to get soft-targets as auxiliary label during training. Distillation can be seen as a method of knowledge transfer. In the second stage of classifier training, the self-distillation is used, in other words, the student network and teacher network have the same architecture. We term the classifier produced by first training sub-stage Cls ?0 with parameters ? 0 , which works as the teacher network in the second training sub-stage. And Cls ?1 is the student network obtained from self-distillation with parameters ? 1 , which is shown in <ref type="figure">Fig.3</ref>. In this sub-stage we only use the original labeled vertices (no mixed vertices) as training data.</p><p>We use convex combination of the cross-entropy loss between the predictions and the one-hot labels and the Kullback Leibler divergence (KL) between predictions and soft targets predicted by Cls ?0 as loss function:</p><formula xml:id="formula_7">L distill = 1 N ? k N ?k i=1 ?L ce cls V new i,: ; ? , gt i + (1 ? ?) KL cls V new i,: ; ? , cls V new i,: ; ? 0 )<label>(7)</label></formula><p>where gt i is the groundtruth of the ith vertex. Cls ?1 is the final classifier to predict the class of query set in current fewshot task and we use unlabeled vertices get from aggregated vertices matrix V new as classifiers inputs to obtain predictions on the query set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We conduct experiments on the widely used few-shot image classification benchmark: miniImageNet <ref type="bibr" target="#b0">[1]</ref>, which is a derivative of ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Models and implementation details</head><p>Architecture. In the meta-training phase we use ResNet50-v1 <ref type="bibr" target="#b23">[24]</ref> as the structure of feature extractors f ?1 and f ?2 . This backbone has 50 convolutional layers grouped into 16 blocks.</p><p>-v1 means the width of each of the convolutional layers is half of the original ResNet50. We set the input size as 224 ? 224 and flatten the outputs after the average-pooling layer as inputs to the graph aggregation, so that e = 1024 in V ? R [N ?(k+q)]?2e .</p><p>In consideration of the extreme few labeled examples we take only one fully connected layer and a following softmax layer as the structure of the classifier Cls to avoid overfitting.</p><p>Optimization and hyper-parameters setup. For the metatraining phase, we train the backbone in a total of 240 epochs from scratch using the SGD optimizer <ref type="bibr" target="#b24">[25]</ref> and the contrast loss. For the task-training phase, we expand labeled vertices 120 times by manifold augmentation and in the first and the second training sub-stage we train the classifier in 11 epochs, 1000 epochs respectively, using the Adam optimizer <ref type="bibr" target="#b25">[26]</ref> and the loss function shown in the previous section. In the second training sub-stage we set ? as 0.95.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE I</head><p>Performance of CSSL-FSL in comparison to the previous works on miniImageNet on 5-way 1-shot and 5-way 5-shot tasks. Average accuracies are reported with 95% condence intervals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5-way Accuracy miniImageNet</head><p>1-shot 5-shot CACTUs-MAML <ref type="bibr" target="#b14">[15]</ref> 39.90?0.74% 53.97?0.70% CACTUs-ProtoNets <ref type="bibr" target="#b14">[15]</ref> 39.18?0.71% 53.36?0.70% UFLST <ref type="bibr" target="#b13">[14]</ref> 33.77?0.70% 45.03?0.73% UMTRA <ref type="bibr" target="#b16">[17]</ref> 39.93??% 50.73??% unsupervised AAL-ProtoNets <ref type="bibr" target="#b15">[16]</ref> 37.67?0.39% 40.29?0.68% AAL-MAML++ <ref type="bibr" target="#b15">[16]</ref> 34.57?0.74% 49.18?0.47% ULDA-ProtoNets <ref type="bibr" target="#b17">[18]</ref> 40.63?0.61% 55.41?0.57% ULDA-MetaOptNet <ref type="bibr" target="#b17">[18]</ref> 40 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results on miniImageNet</head><p>The miniImageNet dataset consists of 100 classes randomly sampled from the ImageNet and each class contains 600 images of size 84 ? 84. It is usually divided into three parts <ref type="bibr" target="#b7">[8]</ref>: training set with 64 base classes, validation set with 16 classes, and testing set with 20 novel classes. In the metatraining phase we use 64 base classes without labels as a small training dataset and 168 classes randomly chosen from ImageNet by ourselves as a bigger one, also having no labels. In the task-training phase we sample novel classes to design few-shot tasks as inputs. We ensure that the novel classes have never been seen in the meta-training phase.</p><p>We evaluate our method on 600 randomly sampled tasks and report their mean accuracy in TABLE I. We compare our method in both 5-way 1-shot and 5-way 5-shot setting with some classical supervised few-shot learning methods and novel unsupervised methods proposed recently. It can be found that our method is much better than previous unsupervised fewshot learning methods( <ref type="bibr" target="#b13">[14]</ref> etc.), improving them by more than 10%. Even compared with supervised methods( <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b1">[2]</ref>), our method still has improvement by 1-8% when pretrain with a larger dataset both on 5-way 1-shot and 5-way 5-shot tasks.</p><p>We notice that using larger training set in meta-training phase leads to an obvious improvement by 5-6%. This is in accordance with the property of contrastive self-supervised learning. When the backbone has seen more images, it can extract features better. So we believe that our method can achieve a better performance by further extending the metatraining dataset. And in the following we show the results of experiments with larger meta-training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results on multi-type tasks</head><p>To prove that our method can easily generalize to different types of few-shot tasks after only pretrainig once, we show the results compared with EGNN <ref type="bibr" target="#b9">[10]</ref>, a method needing metatraining based on fixed-type fake tasks, in TABLE II. For the meta-training in EGNN we use 5-way 5-shot fake tasks as training set, and then we evaluate it on multi-type tasks. For our method we directly use the meta-trained backbone to solve few-shot tasks with different k. This setting ensures that both methods have just one meta-training process.</p><p>When the number of support examples in a few-shot task increases, the model should perform better because the more labeled data usually leads to better generalization. So it is not reasonable that when k increases to 20, EGNN gets a worse performance. However the accuracy of our method keeps rising with the growth of k. Our method outperforms EGNN by 3% when k = 20 and improves further by 7% when k = 30. The results show that, compared to the previous works based on meta-training, our method is not limited by the type of tasks, and it only needs one meta-training to obtain a model with outstanding performance of generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE II</head><p>Performance of CSSL-FSL in comparison to EGNN on miniImageNet on different types of tasks. For each type of task, the best-performing method is in bold. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5-way</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation experiments</head><p>In this section, we conduct ablation experiments to analyze how self-distillation and manifold augmentation affects the few-shot image classification performance. TABLE III shows the results of the ablation studies on miniImageNet in 5-way 1-shot and 5-way 5-shot setting. We compare three kinds of ablation models in the following: (1) w/o distill: Dont use self-distillation, meaning that classifier is just trained with the first sub-stage. (2) w/o aug: This is the model without manifold augmentation in the fist sub-stage but it still have self-distillation in the second sub-stage. (3) w/o both: This ablation model just has the first training sub-stage without manifold augmentation. Overall, the original model performs best. Self-distillation improves accuracy by 0.4-0.9%. Manifold augmentation can provide 0.8-2% extra gain. And without both of distillation and augmentation, the result in 1-shot case is greatly affected, decreasing by 3%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>A novel paradigm of unsupervised few-shot learning is proposed in this paper, which consists of two phases: the contrastive self-supervised learning to obtain a transferable feature extractor, and the graph-aggregation followed with classifier training.</p><p>Experiments show a state-of-the-art performance on a standard vision dataset miniImageNet. It proves that, without a large number of labeled data an outstanding backbone can still be obtained to extract transferable features. And with just one model meta-training, different types of few-shot tasks can be achieved even better. This paradigm can be used in different areas other than the image classification. In the following study we will explore efficient ways in defining the graph and aggregating the features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>The overall architecture of the proposed paradigm. The left shows the meta-training phase using CMC resulting in a global feature extractor. The right is the task-training phase comprised of graph-aggregation, classifier training with the support set and predition on the query set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>The process of meta-training phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>74% 76.30% 77.40% 75.49% 72.83% CSSL-FSL Image168 54.17% 68.91% 74.82% 78.47% 80.83%</figDesc><table><row><cell>Accuracy</cell><cell>1shot</cell><cell>5shot</cell><cell>10shot</cell><cell>20shot</cell><cell>30shot</cell></row><row><cell>EGNN</cell><cell>44.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III</head><label>III</label><figDesc>Results of ablation studies on miniImageNet. The meta-training dataset consists of 168 classes from ImageNet for all the four models.</figDesc><table><row><cell>5-way Accuracy</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>CSSL-FSL -w/o distill</cell><cell>53.28?1.01%</cell><cell>68.25?0.91%</cell></row><row><cell>CSSL-FSL-w/o aug</cell><cell>51.92?1.04%</cell><cell>67.94?0.85%</cell></row><row><cell>CSSL-FSL-w/o both</cell><cell>50.99?1.05%</cell><cell>67.93?0.85%</cell></row><row><cell>CSSL-FSL Image168</cell><cell cols="2">54.17?1.31% 68.91?0.90%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="721" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05960</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Lgm-net: Learning to generate matching networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-G</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06331</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10002</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Edge-labeling graph neural network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Rethinking few-shot image classification: a good embedding is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.11539</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07153</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dense classification and implanting for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lifchitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bursuc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9258" to="9267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised few-shot learning via self-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12178</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unsupervised learning via metalearning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02334</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Assume, augment and learn: Unsupervised few-shot meta-learning via random labels and data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09884</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised meta-learning for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khodadadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Boloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unsupervised few-shot learning via distribution shift-based augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05805</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Exploiting unsupervised inputs for accurate few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gripon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pateux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09849</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05236</idno>
		<title level="m">Manifold mixup: Better representations by interpolating hidden states</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4133" to="4141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
