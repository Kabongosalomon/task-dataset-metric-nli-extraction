<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ClearGrasp: 3D Shape Estimation of Transparent Objects for Manipulation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreeyak</forename><forename type="middle">S</forename><surname>Sajjan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Moore</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Nagaraja</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnny</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Synthesis</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ClearGrasp: 3D Shape Estimation of Transparent Objects for Manipulation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transparent objects are a common part of everyday life, yet they possess unique visual properties that make them incredibly difficult for standard 3D sensors to produce accurate depth estimates for. In many cases, they often appear as noisy or distorted approximations of the surfaces that lie behind them. To address these challenges, we present ClearGrasp -a deep learning approach for estimating accurate 3D geometry of transparent objects from a single RGB-D image for robotic manipulation. Given a single RGB-D image of transparent objects, ClearGrasp uses deep convolutional networks to infer surface normals, masks of transparent surfaces, and occlusion boundaries. It then uses these outputs to refine the initial depth estimates for all transparent surfaces in the scene. To train and test ClearGrasp, we construct a large-scale synthetic dataset of over 50,000 RGB-D images, as well as a real-world test benchmark with 286 RGB-D images of transparent objects and their ground truth geometries. The experiments demonstrate that ClearGrasp is substantially better than monocular depth estimation baselines and is capable of generalizing to realworld images and novel objects. We also demonstrate that ClearGrasp can be applied out-of-the-box to improve grasping algorithms' performance on transparent objects. Code, data, and benchmarks will be released. Supplementary materials: https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Transparent objects are a common part of everyday life, from reading glasses to plastic bottles -yet they possess unique visual properties that make them incredibly difficult for machines to perceive and manipulate. In particular, transparent materials (which are both refractive and specular) do not adhere to the geometric light path assumptions made in classic stereo vision algorithms. This makes it challenging for standard 3D sensors to produce accurate depth estimates for transparent objects, which often appear as noisy or distorted approximations of the surfaces that lie behind them. Hence, while considerable research has been devoted to robotic manipulation of objects using 3D data (e.g. RGB-D images, point clouds) <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b33">34]</ref>, many of these algorithms cannot be immediately applied to transparent objectswhich remain critical for applications like dish washing or sorting/cleaning plastic containers.</p><p>In this work, we present ClearGrasp, an algorithm that leverages deep learning with synthetic training data to infer accurate 3D geometry of transparent objects for robotic manipulation. The design of ClearGrasp is driven by the following three key ideas:</p><p>? Commodity RGB-D cameras often provide good depth estimates for typical non-transparent surfaces. Therefore, rather than directly estimating all geometry from scratch, we conjecture that correcting initial depth estimates from RGB-D cameras is more practical: enabling us to use the depth from the non-transparent surfaces to inform the depth of transparent surfaces. For this to work reliably, we propose to predict pixel-wise masks of transparent surfaces (to detect and remove unreliable depth), as well as occlusion and contact edges between transparent surfaces and the background (to extend reliable depth). ? The refractive and specular patterns appearing on transparent objects provide stronger visual cues for their curvature (e.g. surface normals) than their absolute depth. This motivates using deep networks to infer surface normal information from RGB data, which we find to be substantially more reliable than directly inferring depth values. ? While real-world ground truth 3D training data for transparent objects is difficult to obtain, we show that it is possible use high-quality rendered synthetic images with domain randomization as training data to obtain reasonable </p><formula xml:id="formula_0">f(x) = 0 Fig. 2.</formula><p>Overview. Given an RGB-D image of a scene with transparent objects, ClearGrasp uses three networks to infer 1) surface normals, 2) masks of transparent surfaces, where depth is unreliable, and 3) occlusion and contact edges between the transparent surfaces and the rest of the scene. These outputs are then combined and used as input to a global optimization, which returns an adjusted depth map that corrects and completes the input depth. results on real-world data. Interestingly, we also find that by mixing synthetic training data with real-world out-ofdomain data (e.g. images without transparent objects), our model is able to generalize better to both real-world images and novel transparent objects unseen during training.</p><p>Our primary contributions are twofold. First, we propose an algorithm for estimating accurate 3D geometry of transparent objects from RGB-D images. Second, we construct a large-scale synthetic dataset of over 50,000 RGB-D images as well as a real-world test benchmark with 286 RGB-D images of transparent objects and their ground truth geometries. Our experiments demonstrate that ClearGrasp is capable of generalizing not only to transparent objects in the real-world, but also to novel objects unseen in training. ClearGrasp is substantially better than monocular depth estimation baselines, and our ablative studies show the importance of critical design decisions. We also demonstrate that ClearGrasp can be applied out-of-the-box with state-of-the-art manipulation algorithms to achieve 86% and 72% picking success rates with suction and parallel-jaw grasping respectively on a realworld robot platform. Code, data, pre-trained models, and benchmarks will be released.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Estimating geometry from color images. Surface normal estimation is a popular problem tackled by deep convolutional networks <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b57">57]</ref>. While predicted surface normals are useful for tasks like shading <ref type="bibr" target="#b22">[23]</ref>, 2D-3D alignment <ref type="bibr" target="#b2">[3]</ref>, and face morphing <ref type="bibr" target="#b27">[28]</ref>, it alone is insufficient to describe an object's complete 3D geometry, making it difficult to be directly used by manipulation algorithms that require 3D data (e.g. depth images, point clouds) <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b33">34]</ref>. More recent works study how to obtain 3D data from color images by directly inferring depth images <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b15">16]</ref>, or filling in missing depth values in RGB-D images captured by commodity 3D cameras <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b59">59]</ref>. However, none of these works explicitly handle transparent objects, for which ground truth 3D data is very difficult to obtain -data from commodity 3D stereo cameras often have inaccurate or missing depth estimates for transparent surfaces.</p><p>Recognizing transparent objects. Transparent objects have plagued computer vision since the inception of the field. Due to their refractive and reflective nature, their appearance can vary drastically according to background and illumination conditions. Classic methods for detecting transparent objects mostly relied on idiosyncrasies such as specular reflections or local characteristics of edges due to refraction <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b34">35]</ref>. Later methods rely on deep learning models like SSD <ref type="bibr" target="#b25">[26]</ref> or RCNN <ref type="bibr" target="#b28">[29]</ref> to predict bounding boxes enclosing transparent objects. Seib et al. <ref type="bibr" target="#b42">[43]</ref> proposed a method to exploit sensor failures in depth images for transparent object localization using convolutional networks. Wang et al. <ref type="bibr" target="#b50">[50]</ref> proposed localizing glass objects using a Markov Random Field to predict glass boundary and region jointly from multiple modalities from an RGB-D camera. Based on the localization, they recover depth readings by a piece-wise planar model. However, our method not only detects transparent objects, but also recovers detailed non-planar geometries, which are critical for manipulation algorithms.</p><p>Estimating geometry of transparent objects. Works on estimating transparent object geometry are often studied in a constrained environment: For example, the work might assume a specific capturing procedure <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b0">1]</ref>, known background pattern <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b18">19]</ref>, sensor type <ref type="bibr" target="#b43">[44]</ref> or known object 3D model <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b26">27]</ref>. Lysenkov et al. <ref type="bibr" target="#b32">[33]</ref> propose a method for the recognition and pose estimation of rigid transparent objects using a Kinect sensor. Using a segmentation mask of the transparent objects, 3D models of objects created at the training stage are fitted to extracted edges. Our approach is able to generalize to objects not seen during training and does not require prior knowledge of the 3D model of the objects or camera position.</p><p>Learning from synthetic data. Synthetic data has proven to be useful in various tasks such as depth estimation <ref type="bibr" target="#b41">[42]</ref>, 3D semantic scene completion <ref type="bibr" target="#b44">[45]</ref>, hand pose estimation <ref type="bibr" target="#b11">[12]</ref>, robotic grasping <ref type="bibr" target="#b33">[34]</ref>, automatic shading of sketches <ref type="bibr" target="#b22">[23]</ref>, and person re-identification for tracking <ref type="bibr" target="#b3">[4]</ref>. However, very few synthetic datasets support planar reflectors <ref type="bibr" target="#b48">[48]</ref>, let alone transparent objects. In our trials, we find that very high quality rendering and 3D models are required to --Reported Depth synthesize representative imagery of transparent objects and their related artifacts e.g. specular highlights and caustics. Datasets that do contain transparent objects have been used to study refractive flow estimation Chen et al. <ref type="bibr" target="#b7">[8]</ref>, semantic segmentation <ref type="bibr" target="#b46">[47]</ref>, or relative depth <ref type="bibr" target="#b45">[46]</ref>. These datasets are generated in a simplified setting (e.g. rendered transparent objects in front of random images from COCO <ref type="bibr" target="#b30">[31]</ref>). On the contrary, our method targets reconstructing detailed absolute depth of transparent objects within realistic environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>Given a single RGB-D image of transparent objects, Clear-Grasp first uses the color image as input to deep convolutional networks to infer a set of information: surface normals, masks of transparent surfaces, and occlusion boundaries. ClearGrasp then uses this information and the initial depth image as input to a global optimization, which outputs a new depth image that refines the initial depth estimates from the sensor for all transparent surfaces in the scene (Sec. III-A). For training and testing, we construct a synthetic dataset and a real-world benchmark for transparent objects (Sec. III-B, III-C). In Sec. III-B, we demonstrate the application of ClearGrasp to a real-word robotic pick-and-place system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Estimating 3D Geometry of Transparent Object</head><p>We adopt the depth completion pipeline proposed by Zhang and Funkhouser <ref type="bibr" target="#b59">[59]</ref> with a few critical modifications to address the unique challenges presented by transparent objects. First, instead of only filling in the missing depth regions, we train an additional network to predict a pixelwise mask for transparent surfaces and use it to remove unreliable depth measurements from the depth camera, see <ref type="figure">Fig. 3</ref>. Second, instead of predicting only occlusion edges (discontinuities in depth), we propose to predict both occlusion and contact edges (boundaries of objects in contact with other surfaces) so that the network can distinguish different type of edges and predict more accurate depth discontinuity boundaries, which is critical for the global optimization step, see <ref type="figure">Fig. 7</ref>. <ref type="figure">Fig. 2</ref> shows an overview of our approach. The following paragraphs provide details on each module. Transparent object segmentation. Due to the reflective and refractive nature of transparent objects, they cause erroneous readings in commodity RGB-D sensors. <ref type="figure">Fig. 3 explains 2</ref> types of errors. Type I error refers to missing depth, commonly caused by specular highlights. Type II error occurs when the light is refracted through the transparent material, only to reflect back from the surface behind the object. This causes the sensor to report the depth of surfaces behind the object instead of the object itself. These inaccurate non-zero depth estimates are difficult to detect using standard depth completion, which would only propagate the inaccurate depth and result in corrupted reconstructions. To address this issue, we predict the pixel-wise masks of transparent objects using a Deeplabv3+ <ref type="bibr" target="#b8">[9]</ref> with a DRN-D-54 backbone <ref type="bibr" target="#b53">[53]</ref> to remove all depth pixels corresponding to transparent surfaces. Surface normal estimation. This module predicts pixelwise surface normals for the input RGB color image using Deeplabv3+ with DRN-D-54. The last convolutional layer is modified to have 3 output classes. To ensure that estimated normals are unit vectors, the output is L2 normalized. Boundary detection. This module labels each pixel of the input color image as one of three classes: (a) Non-Edge, (b) Occlusion Boundary (depth discontinuity) (c) Contact Edges (points of contact between 2 objects). Contact edges, while not directly used by the optimization step, is very important because it helps the network better distinguish between different types of edges observed in color images, and therefore results in more accurate predictions of depth discontinuity boundaries. This significantly decreases chances of the model predicting a boundary around an entire object, which would prevent the global optimization step from solving back its depth using predicted surface normals. We use the same Deeplabv3+ model with a DRN-D-54 backbone. Since the pixel ratio of boundaries to background is low, we use a weighted cross-entropy loss with boundary pixels weighing 5x more than background pixels. Global optimization for depth. Using the depth image (with all pixels corresponding to transparent surface removed) and predictions of surface normals and occlusion and contact edges, ClearGrasp reconstructs the 3D surfaces of transparent objects (missing depth region) via the global optimization algorithm proposed by Zhang and Funkhouser <ref type="bibr" target="#b59">[59]</ref>. The optimization algorithm fills in the removed depth using the predicted normals to guide the shape of the reconstruction, while observing the depth discontinuities indicated by the occlusion boundaries. It solves a system of equations with the goal of minimizing the weighted sum of squared errors of three terms:</p><formula xml:id="formula_1">E = ? D E D + ? S E S + ? N E N B,</formula><p>where E D measures the distance between the estimated depth and the observed raw depth, E S measures difference between the depths of neighboring pixels and E N measures the consistency between estimated depth and predicted surface normal. B down-weights the normal terms based on the predicted probability that a pixel is on an occlusion boundary. In our experiments: ? D = 1000, ? S = 0.001 and ? N = 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Synthetic Training data generation</head><p>We selected Synthesis AI's platform to generate our synthetic data, using Blender's physics engine <ref type="bibr" target="#b5">[6]</ref>, as well as the   physically-based, ray-tracing Blender Cycles <ref type="bibr" target="#b4">[5]</ref> rendering engine. We selected this because it is highly configurable, and is able to simulate important effects for transparent objects like refraction and reflection through multiple surfaces, as well as soft shadows.</p><p>The dataset consists of 9 CAD models modeled after real-world transparent plastic objects, in which we hold out 4 of the objects during training to test the algorithm generalization ability. Additionally, one gray tote box is used as an background object. We employed 33 HDRI lighting environments and 65 textures for the ground plane underneath the transparent objects. Camera intrinsics were set to that of the Intel RealSense D415 camera. To generate each scene, between 1 and 5 CAD model objects were created above the plane surface, with or without a gray tote box and the CAD model objects were dropped so they would come to rest according to physics. Then, a random selection of HDRI lighting environments and ground plane surface textures would be applied to each scene as well.</p><p>For each scene, the ground truth data includes: (1) monocular RGB render, (2) aligned depth in meters, (3) semantic segmentation of all transparent objects, (4) pose of the camera (5) pose of each CAD object, and (6) surface normals of the scene. <ref type="figure" target="#fig_2">Fig. 4</ref> shows example rendered images and their corresponding ground truth geometry. The final training dataset consists of over 13,000 images of 3 objects each and 5000 images each of another 2 objects. 100 images of each were kept aside as a validation set. For the test set, we rendered around 100 images of each of the 4 testing objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Real-World Benchmark</head><p>To test the ability of our model to generalize to real-world images, we create a dataset of real-world transparent objects. The setup consists of a photography background cloth or wooden laminate spread across a flat surface kept against a wall. Five unique wooden laminates and five different background cloths were used. The scene was lit with ambient lighting to avoid sharp caustics. The camera was mounted on a tripod at a distance of 40-100cm from the objects.</p><p>To capture the depth of transparent objects, we separated the objects into 2 equal sets and spray painted one set with a rough stone texture, which gives much better depth than a flat color. A GUI app was developed that could overlay 2 frames read from the camera, as shown in <ref type="figure" target="#fig_3">Fig.  5</ref>. First the transparent objects were placed in the scene along with various random opaque objects like cardboard boxes, decorative mantelpieces and fruits. After capturing and freezing that frame, each object was replaced with an identical spray-painted instance. Subsequent frames would be overlaid on the frozen frame so that the overlap between the spray painted objects and the transparent objects they were replacing could be observed. With high resolution images, sub-millimeter accuracy can be achieved in the positioning of the objects.</p><p>The validation dataset consists of 173 images of 5 known objects used in synthetic training data. The testing set consist of 113 images of 5 novel objects, including 3 new glass objects not present in the synthetic dataset. Each image contains 1-6 objects, with an average of 2 objects per image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Grasp planning</head><p>By integrating ClearGrasp into a robotic picking system, we can investigate its benefits for downstream manipulation tasks. We adapted a state-of-the-art grasping algorithm for our experiment <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b56">56]</ref>, which consists of a convolutional network that predicts the probability of picking success for a scripted grasping primitive across a dense pixel-wise sampling of end effector locations and orientations across the completed depth images from ClearGrasp. Specifically, it uses an 18-layer fully convolutional residual network <ref type="bibr" target="#b19">[20]</ref> with dilated convolutions <ref type="bibr" target="#b53">[53]</ref> and ReLU activations <ref type="bibr" target="#b36">[37]</ref>, interleaved with 2 layers of max pooling, 2 layers of spatial bilinear 2x upsampling. The network takes as input a 4 channel image -the surface normal map (3 channels) concatenated channel-wise with the completed depth image (1 channel) inferred from ClearGrasp -and outputs a probability map with the same size and resolution as that of the input image. The picking system assumes that the 3D camera is calibrated with respect to robot coordinates using the calibration procedure in <ref type="bibr" target="#b54">[54]</ref> -hence each pixel in the depth image maps to a 3D location. The robot executes a top-down parallel-jaw grasp or suction where the tip of the end effector is centered at the 3D location of the pixel with the highest predicted probability from the network.</p><p>For our experiments with parallel-jaw grasping, as in <ref type="bibr" target="#b55">[55]</ref> we account for different grasping angles by constructing top-down orthographic heightmaps from ClearGrasp depth images, rotating the input heightmaps by 16 orientations (multiples of 22.5 ? ), and feeding each heightmap through the network for a total of 16 forward passes. The pixel and the corresponding rotation with the highest predicted probability among all 16 maps determines the respective grasping angle. The network is trained end-to-end using the binary cross-entropy error from predictions of grasp success Novel Objects</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Known Objects</head><p>Input Output Groundtruth <ref type="figure">Fig. 6</ref>. Qualitative results on real-world benchmark with known objects (rows 1-2) and novel objects (rows 3-4). More results can be found in the supplementary material website.</p><p>against the binary ground truth success labels. We pass gradients only through the single pixel on which the grasping primitive was executed. Since each pixel-wise prediction shares convolutional features for all grasping locations and orientations, the network is sample-efficient and trains within a few hundred trial-and-errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EVALUATION</head><p>We evaluate ClearGrasp's ability to estimate transparent object geometry on both synthetic and real-world benchmarks, then apply it to a real-world robotic picking system.</p><p>Datasets used to train and test our algorithm:</p><p>? Syn-train: Synthetic training set with 5 objects as described in Sec. III-B. ? Syn-known: Synthetic validation set for training objects. ? Syn-novel: Synthetic test set of 4 novel objects.</p><p>? MP+SN: Out-of-domain real-world RGB-D datasets of indoor scenes that do not contain transparent objects' depth (Matterport3D <ref type="bibr" target="#b6">[7]</ref> and ScanNet <ref type="bibr" target="#b10">[11]</ref>). ? Real-known: Real-world test set for all 5 of the training objects. Sec. III-C describes the capturing procedure. ? Real-novel: Real world test set of 5 novel objects, including 3 not present in synthetic data.</p><p>Metrics: For surface normal estimations, we calculate the mean and median errors (in degrees) and the percentages of pixels with estimated normals less than thresholds of 11.25, 22.5, and 30 degrees. For depth estimation, we use metrics standard among previous works <ref type="bibr" target="#b13">[14]</ref>: the Root Mean Squared Error in meters (RMSE), the median error relative to the depth (Rel) and percentages of pixels with predicted depths falling within an interval ([? = |predicted ? true|/true], where ? is 1.05, 1.10 or 1.25). Depth is evaluated by resizing the images and ground truth to 144x256p resolution. For mask prediction, we use pixel-wise intersection over union for evaluation as well as true positive rate. Unless specified, metrics are calculated on the real-known dataset only over the pixels belonging to transparent objects.</p><p>Generalization: real-world images. <ref type="table" target="#tab_2">Table I also</ref> shows results on an experiment to test the cross-domain performance of our models. Despite never being trained on realworld transparent objects, we find our models are able to adapt well to the real-world domain achieving very similar RMSE and Rel scores on known objects across domains. However, the surface normal prediction accuracy decreases on real images. We observe large errors in surface normal estimations when transparent object occlude novel opaque objects. Surprisingly, the metrics of Real-novel objects are better than Syn-novel. We attribute this to the 3 new glass objects used in real-world images which show more evident refraction characteristics due to their thicker material as compared to the thin plastic material of all other objects. Generalization: novel object shapes. We inspect the ability of our algorithm to generalize to previously unseen object shapes. <ref type="table" target="#tab_2">Table I</ref> shows the results of depth estimation on novel objects, conducted on both synthetic data and realworld data. We see that it is able to generalize remarkably well in both cases, achieving better results than on the known objects. This is likely due to the smaller size of novel objects, which cause a relatively smaller error in depth reconstruction.</p><p>Comparison with Monocular Depth Estimation. We compare our approach with DenseDepth [2], a monocular depth estimation method that has state-of-the-art performance. DenseDepth uses a deep neural network to directly predict the depth value from the color image. We train DenseDepth with the same training data as our approach. The results in <ref type="table" target="#tab_2">Table I</ref> show that our model outperforms the monocular depth estimation methods by a large factor. Effect of mask prediction. We test the effectiveness of cleaning the input depth by removing all pixels belonging to transparent objects, as shown in <ref type="table" target="#tab_2">Table II</ref>. By not removing the initial noisy depth values, we notice a significant increase in the final depth estimation error. <ref type="table" target="#tab_2">Table I</ref> reports the accuracy of the mask prediction in both intersection over union and true positive rate. In our approach, having a high true positive rate (&gt; 95%) is critical for removing all the incorrect initial depth values. <ref type="table" target="#tab_2">Table II</ref> shows the effect of using a weighted loss function and the impact of adding the additional class of contact edges to our occlusion boundary estimation model. Both of these methods contribute significant improvement in depth completion results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of contact edges and edge weights</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inputs w/ Contact Edges w/o Contact Edges</head><p>Real Val -#01</p><p>Real Val -#84 77 <ref type="figure">Fig. 7</ref>. Effects of contact edges. By training our boundary estimation model with contact edges (middle column), ClearGrasp predicts better depth for transparent objects than without contact edges (right column).</p><p>Effect of training data. Our main training dataset consists of synthetic images of transparent objects. Since it is expensive to capture real-world data with accurate geometry ground truth for transparent objects, we propose to mix in typical real-world RGB-D indoor scene images in training to reduce the domain gap. <ref type="table" target="#tab_2">Table III</ref> shows the model performance under different training procedure: with/without pre-training on out-of-domain real-world data (80k images from the Scannet and Matterport datasets) and with/without in-domain synthetic data fine-tuning. <ref type="figure">Fig. 6</ref> additionally shows the qualitative results of surface normal estimation for all the above cases. We see that a model trained on out-of-domain real-world data is not able to pick up transparent objects. However, pre-training with such data improves results, especially for real-world test sets. Robot manipulation. We also incorporate ClearGrasp as part of a real-world robotic picking system to observe how it influences the overall grasping performance of transparent objects. In this experiment, a pile of 3 to 5 transparent objects are presented on a table within the robot's workspace, of which RGB-D images are captured using a calibrated RealSense camera. The goal of the robot is to pick up objects from the table using a state-of-the-art grasping algorithm described in Sec. III-D. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the setup. We test the algorithm using two end-effectors: suction and a parallel-jaw gripper. For each end-effector type, with and without Clear-Grasp, we train a grasping algorithm using 500 trial and error grasping attempts, then test it with 50 attempts. We compute the average grasping success rate = # successful picks # picking attempts <ref type="bibr" target="#b54">[54]</ref> as the evaluation metric. With both end-effectors, we observe that ClearGrasp significantly improves the grasping success rate of transparent objects: it improves the grasping success from 64% to 86% for suction, and 12% to 72% for paralleljaw grasping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>We present ClearGrasp, an algorithm that leverages deep learning with synthetic training data and multiple sensor modalities (color and depth) to infer accurate 3D geometry of transparent objects for manipulation. However, the proposed system is still far from perfect. Possible future directions may include: explicitly leveraging lighting information during the inference step to improve the algorithm's accuracy under different lighting conditions, improving the algorithm robustness in cluttered environments where predicting accurate occlusion and contact edges is more challenging and making the algorithm robust to sharp caustics and shadows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>The appendix consists of additional system details, analysis, and experimental results. <ref type="figure">Fig. 8</ref> and <ref type="figure">Fig. 9</ref> showcase the objects used within our synthetic and real-world datasets. In the real-world dataset, all images of known objects are taken with a RealSense D435 camera and 80% of the images of novel objects were taken with a RealSense D415 camera instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Details on Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Novel Objects</head><p>Known Objects <ref type="figure">Fig. 8</ref>. Known and novel objects in Synthetic dataset. We have 5 known objects for training and 4 novel objects for testing. The test objects are all challenging: 2 are of thick glass (a different material from our plastic known objects) and 2 are of complex shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Novel Objects</head><p>Known Objects <ref type="figure">Fig. 9</ref>. Known and novel objects in Real-World benchmark dataset. We have 5 known objects and 5 novel objects. All 5 known objects and 2 of the novel objects were used to model the synthetic objects. 2 other novel objects are made of glass instead of plastic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Limitations and Failure Cases</head><p>We detail some of the failure cases of our models. Four examples are shown in <ref type="figure" target="#fig_0">Fig. 10</ref> ? The biggest limitation of our approach is that it is not always possible to reconstruct depth from surface normals directly <ref type="bibr" target="#b59">[59]</ref> -if a region is completely enclosed by an occlusion boundary, its depth is left indeterminate from the rest of the scene. In Case I, we see a bottle that is partially occluded, with its contact edges not visible. In Case II, we see the mouth of the glasses cause the inner portions to be (correctly) completely enclosed by an occlusion boundary. In both cases, the depths of such regions become indeterministic and can be assigned random values. ? Cluttered scenes are challenging. In cases where multiple transparent objects are partially or completely occluding each other, it becomes challenging to correctly predict surface normals and occlusion boundaries, which leads to errors in the output depth. Case III highlights such a scenario. Another situation which our models find challenging is when the background seen behind a transparent object is not constant -such as when a bottle is at the edge of a table or when its partially occluding an opaque object. ? As seen in Case IV, bright directional lighting and its associated caustics cause our model to mistakenly identify shadows of transparent objects as transparent objects. Our models seems to pick up on cues like specular highlights to identify transparent objects and may be confusing the caustics on shadows with specular highlights -hence detecting the shadow as a transparent object. Since our synthetic dataset does not contain accurate caustics due to the limitations of the Cycles rendering engine, our model is particularly susceptible to this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Training Details</head><p>We make use of Deeplabv3+ with a DRN-D-54 backbone <ref type="bibr" target="#b58">[58]</ref> in Pytorch for all 3 of our neural networks -surface normal estimation, occlusion boundary prediction and segmentation of transparent surfaces. For all 3 networks, we start with a model pre-trained for semantic segmentation on the COCO dataset and use the same hyperparameters: SGD Optimizer with constant learning late of 1e-6, momentum 0.9 and weight decay 5e-4. We used a GCP server with 8x Nvidia V100 GPUs enabling a batch size up to 128 at an input image size of 256x256p.</p><p>For surface normals, we initially pre-train our model on the Matterport3D (MP) and Scannet (SN) datasets by selecting a random subset of approximately 40k images from each, for a total dataset of 80k images. When training on transparent objects, we include a new random subset of 2k images from MP and 2k from SN each epoch. Our synthetic dataset contains only a flat plane and up to 5 transparent objects, lacking any other surfaces like walls and random opaque objects. Injecting MP+SN images every epoch allows the model to retain knowledge of the previous domain and predict more accurate normals for surfaces like walls. To train the model more quickly on the different task of surface normal estimation, we adopted a staggered training approach: First, we trained a small subset of 100 images at a reduced resolution of 128x128p. Second, we took an early checkpoint before the model starts to stabilize and train on a larger subset of our data. This step was repeated on subsequently larger subsets. Third, we repeat the procedure with the larger image size of 256x256p taking the checkpoint from the previous step.</p><p>To make the models more robust, the following data augmentations were utilized from the imgaug <ref type="bibr" target="#b24">[25]</ref>  -Channel-Wise Coarse Dropout up to 1/4th the size of the image. This makes the model more robust to varying backgrounds seen behind a transparent object. -Bright White Patches: We use a Simplex Noise blended with a white image to generate random white patches which are overlaid with transparency on our images. Since our synthetic images do not contain significant caustics, this augmentation attempts to make the model more robust to bright patches of light due to caustics or directional lights. Using this data augmentation strategy, we noticed a significant improvement in scenes with a patterned background cloth, bright caustics or directional lights and cases where the background behind a transparent object varied (like when it partially occludes an opaque object).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiment on Network Architectures</head><p>During our trials, we noticed that surface normal estimation was better on bottles that were kept further away from the camera. This led us to hypothesize that a larger receptive field might be helpful for transparent objects. <ref type="table" target="#tab_2">Table IV</ref> shows the results of experimenting with different models and input image sizes. We try Deeplabv3+ with 2 different backbones: Resnet-101 and DRN-54 (Dilated Residual Network). We also experiment with different input image sizes. The results indicate that a smaller input image size, which effectively increases receptive field width, performs better. Further, replacing Resnet with DRN, which increases the receptive field of the network <ref type="bibr" target="#b53">[53]</ref>, improves results even more -hence validating our hypothesis.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional Qualitative Results</head><p>Finally, we present some qualitative results on our ablation study and comparison with baselines (Ref <ref type="table" target="#tab_2">to Table II</ref>). <ref type="figure" target="#fig_0">Fig.  11</ref> shows the performance of our approach a) without masks, b) without contact edges and c) without weighted loss terms for the contact edges. <ref type="figure" target="#fig_0">Fig. 12</ref> shows the qualitative results of our method in comparison with DeepCompletion and DenseDepth. For more qualitative results please visit our website. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(a) Suction based grasping (b) Parallel jaw grasping (c) Geometry estimation for transparent objectsInputOutput ClearGrasp leverages deep learning with synthetic training data to infer accurate 3D geometry of transparent objects from a single RGB-D image. The estimated geometry can be directly used for downstream robotic manipulation tasks (e.g. suction and parallel-jaw grasping).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Synthetic data. Top row is the rendered image and its groundtruth (surface normal, boundary, depth and mask). Bottom two rows are rendering of different objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Real-world benchmark. From left to right: the data capturing process, screenshot of GUI showing the process of replacing transparent bottle with opaque bottle, RGB-D images of transparent objects, RGB-D images of replaced spray-painted objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 .Fig. 11 .Fig. 12 .</head><label>101112</label><figDesc>Failure Cases. Most of the errors in output depth (highlighted in red) are due to the errors in occlusion boundary prediction (highlighted in black) -either erroneous outputs or surfaces with no contact edges due to occlusion. Qualitative results -Ablation Study Qualitative results -Comparison with baselines</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table Cause</head><label>Cause</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>of Type I error</cell><cell></cell></row><row><cell>Type I error:</cell><cell>missing depth</cell><cell cols="2">Transparent Object Cause of Type II error</cell></row><row><cell>Type II error:</cell><cell>background depth</cell><cell>Table</cell><cell>--Actual Depth --Reported Depth RGB-D Camera</cell></row><row><cell cols="4">Fig. 3. Errors in depth for transparent objects: Type I errors, missing</cell></row><row><cell cols="4">depth, is often caused by specular reflections on the surface. Type II errors,</cell></row><row><cell cols="4">inaccurate depth estimates (returns background depth instead of the object</cell></row><row><cell cols="3">depth), is caused by transparency of the surface material.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I</head><label>I</label><figDesc>GENERALIZATION. CLEARGRASP GENERALIZES TO BOTH REAL IMAGES AND NOVEL TRANSPARENT OBJECTS UNSEEN IN TRAINING. MAE? ? 1.05 ? ? 1.10 ? ? 1.25 ? mean? med.? 11.25?? 22.5?? 30?? IoU TP RMSE? REL? MAE? ? 1.05 ? ? 1.10 ? ? 1.25 ?</figDesc><table><row><cell></cell><cell>Testset</cell><cell></cell><cell></cell><cell cols="2">Depth Estimation</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Surface Normal Estimation</cell><cell>Mask</cell></row><row><cell cols="4">Type RMSE? REL? Synthetic Known Object 0.044 0.047</cell><cell>0.033</cell><cell>71.23</cell><cell>92.60</cell><cell>98.24</cell><cell>15.64</cell><cell>10.62</cell><cell>53.71</cell><cell>78.28</cell><cell>85.83 0.93 95.90</cell></row><row><cell cols="2">Synthetic Novel</cell><cell>0.040</cell><cell>0.071</cell><cell>0.035</cell><cell>42.95</cell><cell>80.04</cell><cell>98.10</cell><cell>25.32</cell><cell>20.53</cell><cell>24.04</cell><cell>55.88</cell><cell>69.73 0.94 97.58</cell></row><row><cell>Real</cell><cell>Known</cell><cell>0.039</cell><cell>0.053</cell><cell>0.029</cell><cell>70.23</cell><cell>86.98</cell><cell>97.25</cell><cell>21.93</cell><cell>18.72</cell><cell>32.82</cell><cell>64.39</cell><cell>76.05 0.63 96.30</cell></row><row><cell>Real</cell><cell>Novel</cell><cell>0.028</cell><cell>0.040</cell><cell>0.022</cell><cell>79.18</cell><cell>92.46</cell><cell>98.19</cell><cell>22.29</cell><cell>18.09</cell><cell>31.63</cell><cell>63.44</cell><cell>76.06 0.58 96.95</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE II</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">BASELINE COMPARISONS AND ABLATION STUDY.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">DenseDepth [2]</cell><cell cols="5">0.270 0.428 0.259 18.67 34.34 58.29</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">DeepCompletion [59] 0.054 0.081 0.045 44.53 69.71 95.77</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-Mask</cell><cell></cell><cell cols="5">0.054 0.080 0.044 44.46 69.73 96.06</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">-Contact Edge</cell><cell cols="5">0.061 0.096 0.054 36.64 65.11 92.38</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">-Edge Weights</cell><cell cols="5">0.049 0.075 0.042 51.77 73.70 95.59</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Full</cell><cell></cell><cell cols="5">0.038 0.048 0.027 72.94 87.88 97.17</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III TRAINING</head><label>III</label><figDesc>DATA. NORMAL ESTIMATION PERFORMANCE UNDER DIFFERENT TRAINING PROCEDURES: WITH/WITHOUT OUT-OF-DOMAIN REALWORLD DATA (MP+SN) AND IN-DOMAIN SYNTHETIC DATA (SYN).</figDesc><table><row><cell>Pretrain</cell><cell cols="5">Train Mean? Median? 11.25 ? 22.5 ?</cell><cell>30 ?</cell></row><row><cell>MP+SN</cell><cell>-</cell><cell>43.92</cell><cell>45.31</cell><cell>9.51</cell><cell>22.69</cell><cell>32.03</cell></row><row><cell>-</cell><cell>Syn</cell><cell>21.59</cell><cell>24.74</cell><cell>24.74</cell><cell>55.97</cell><cell>70.40</cell></row><row><cell>MP+SN</cell><cell>Syn</cell><cell>21.93</cell><cell>18.72</cell><cell>32.82</cell><cell>64.39</cell><cell>76.05</cell></row><row><cell>RGB</cell><cell></cell><cell>MP+SN</cell><cell cols="2">Synthetic Only</cell><cell></cell><cell>Both</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>BackboneInput Size Mean? Median? 11.25 ? 22.5 ? 30 ?</figDesc><table><row><cell>Resnet101</cell><cell>512</cell><cell>34.9</cell><cell>32.8</cell><cell>16.0</cell><cell cols="2">42.6 56.8</cell></row><row><cell>Resnet101</cell><cell>256</cell><cell>25.3</cell><cell>22.2</cell><cell>25.7</cell><cell cols="2">56.9 70.0</cell></row><row><cell>DRN-54 [ours]</cell><cell>256</cell><cell>22.5</cell><cell>19.4</cell><cell>28.6</cell><cell>61.5</cell><cell>75.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV NETWORK</head><label>IV</label><figDesc></figDesc><table /><note>ARCHITECTURES FOR SURFACE NORMAL.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We would like to thank Ryan Hickman for managerial support, Ivan Krasin and Stefan Welker for fruitful technical discussions, Cameron (@camfoxmusic) for sharing 3D models of his potion bottles and Sharat Sajjan for helping on webpage design.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Seeing the unseen: Simple reconstruction of transparent objects from point cloud data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Albrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Marsland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">High quality monocular depth estimation via transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibraheem</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
		<idno type="arXiv">abs/1812.11941:arXiv:1812.11941</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Marr revisited: 2d-3d alignment via surface normal prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aayush</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5965" to="5974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Barbara Caputo, Aleksander Rognhaugen, and Theoharis Theoharis. Looking Beyond Appearances: Synthetic training data for deep cnns in re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><forename type="middle">Barros</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<ptr target="https://docs.blender.org/manual/en/2" />
	</analytic>
	<monogr>
		<title level="j">Blender. Blender Cycles</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Blender Physics Engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blender</surname></persName>
		</author>
		<ptr target="https://docs.blender.org/manual/en/latest/physics/index.html" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Matterport3d: Learning from rgb-d data in indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tom-net: Learning transparent object matting from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwan-Yee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Single-image depth perception in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<idno>abs/1604.03901</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02224</idno>
		<title level="m">Hand3d: Hand pose estimation using 3d neural network</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multiscale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno>abs/1406.2283</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An additive latent feature model for transparent object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="558" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayhan</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Guided depth enhancement via a fast marching method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="695" to="703" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transparent object detection and location based on RGB-D camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Guo-Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Jun-Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Ai-Jun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Physics: Conference Series</title>
		<imprint>
			<publisher>IOP Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1183</biblScope>
			<biblScope unit="page">12011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A fixed viewpoint approach for dense reconstruction of transparent objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwan-Yee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4001" to="4008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Depth map inpainting under a second-order smoothness prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Heikkil?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scandinavian Conference on Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="555" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Revisiting single image depth estimation: toward higher resolution maps with accurate object boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mete</forename><surname>Ozay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1043" to="1051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep normal estimation for automatic shading of hand-drawn characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matis</forename><surname>Hudon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mairead</forename><surname>Grogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV) Workshops</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
	<note>Rafael Pages, and Aljosa Smolic</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fusing depth and silhouette for scanning transparent object with RGB-D sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Optics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">B</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Wada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Crall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Graving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joy</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gbor</forename><surname>Vecsei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kraft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirka</forename><surname>Borovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Vallentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semen</forename><surname>Zhydenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Pfeiffer</surname></persName>
		</author>
		<ptr target="https://github.com/aleju/imgaug" />
		<imprint>
			<date type="published" when="2019" />
			<publisher>Matias Laporte</publisher>
			<biblScope unit="page" from="25" to="2019" />
			<pubPlace>Ben Cook, Ismael Fernndez, Weng Chi-Hung</pubPlace>
		</imprint>
	</monogr>
	<note>et al. imgaug</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Transparent object detection using convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">May</forename><surname>Phyo Khaing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukunoki</forename><surname>Masayuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Big Data Analysis and Deep Learning Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="86" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Transparent object detection and reconstruction on a mobile platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Klank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Carton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5971" to="5978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Face normals in-the-wild using fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Transparent object detection using regions with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jen</forename><surname>Po</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiou-Shann</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fuh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPPR Conference on Computer Vision, Graphics, and Image Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deeper depth prediction with fully convolutional residual networks. CoRR, abs/1606.00373</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1606.00373" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pose estimation of rigid transparent objects in transparent clutter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Lysenkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Rabaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<biblScope unit="page" from="162" to="169" />
			<date type="published" when="2013" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recognition and pose estimation of rigid transparent objects with a kinect sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Lysenkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Eruhimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics</title>
		<imprint>
			<biblScope unit="page">273</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacky</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherdil</forename><surname>Niyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Laskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Aparicio Ojea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09312</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Finding glass</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mchenry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A geodesic active contour framework for finding glass</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Mchenry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1038" to="1044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A novel stereoscopic cue for figure-ground segregation of semi-transparent objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1100" to="1107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Seeing glassware: from edge detection to pose estimation and shape recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Lecce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">3d reconstruction of transparent objects with positionnormal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minglun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Hong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Sharpnet: Fast and accurate recovery of occluding contours in monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ramamonjisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08598</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Soccer on your tabletop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Rematas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4738" to="4747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Friend or foe: exploiting sensor failures for transparent object localization and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Seib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Barthen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Marohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Conference on Robotics and Machine Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">10253</biblScope>
			<biblScope unit="page">102530</biblScope>
		</imprint>
	</monogr>
	<note>International Society for Optics and Photonics</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Depth reconstruction of translucent objects from a single timeof-flight camera using deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seongjong</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjung</forename><surname>Shim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10917</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 30th IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>30th IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Single-shot analysis of refractive shape using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Stets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeppe</forename><surname>Revall Frisvad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="995" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Jeppe Revall Frisvad, and Anders Bjorholm Dahl. Material-based segmentation of objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan Dyssel</forename><surname>Stets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scandinavian Conference on Image Analysis</title>
		<imprint>
			<biblScope unit="page">152</biblScope>
		</imprint>
		<respStmt>
			<orgName>Rasmus Ahrenkiel Lyngby</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-20205-713</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingni</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><forename type="middle">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shobhit</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Budge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaqing</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">June</forename><surname>Yon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimberly</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesus</forename><surname>Briales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Gillingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elias</forename><surname>Mueggler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Pesqueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hauke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renzo</forename><surname>Strasdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>De Nardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Goesele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Newcombe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05797</idno>
		<title level="m">The Replica dataset: A digital replica of indoor spaces</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Using geometry to detect grasp poses in 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Ten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pas</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics Research</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="307" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Glass object localization by joint inference of boundary and depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)</title>
		<meeting>the 21st International Conference on Pattern Recognition (ICPR2012)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3783" to="3786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Designing deep networks for surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="539" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">FastDepth: Fast Monocular Depth Estimation on Embedded Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Wofk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangchang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Ju</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sertac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning synergies between pushing and grasping with self-supervised deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Welker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnny</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4238" to="4245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Robotic pick-and-place of novel objects in clutter with multiaffordance grasping and cross-domain image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Ting</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliott</forename><surname>Donlon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Francois R Hogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daolin</forename><surname>Bauza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orion</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melody</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eudald</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Romo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnny</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11239</idno>
		<title level="m">Tossingbot: Learning to throw arbitrary objects with residual physics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep surface normal estimation with hierarchical rgb-d fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunmu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongtian</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Pytorch re-implementation of deeplabv3+</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://github.com/jfzhang95/pytorch-deeplab-xception.Online" />
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="25" to="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep depth completion of a single rgb-d image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Physically-based rendering for indoor scene understanding using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
