<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AggPose: Deep Aggregation Vision Transformer for Infant Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Cao</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoye</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Shenzhen Baoan Women&apos;s and Childiren&apos;s Hospital</orgName>
								<orgName type="institution" key="instit2">Jinan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liya</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Shenzhen Baoan Women&apos;s and Childiren&apos;s Hospital</orgName>
								<orgName type="institution" key="instit2">Jinan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Huang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Children&apos;s Hospital</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Feng</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Children&apos;s Hospital</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zening</forename><surname>Chen</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwu</forename><surname>Zeng</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Children&apos;s Hospital</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Cao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Children&apos;s Hospital</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenzhen</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Automatic Rehabilitation Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AggPose: Deep Aggregation Vision Transformer for Infant Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Movement and pose assessment of newborns lets experienced pediatricians predict neurodevelopmental disorders, allowing early intervention for related diseases. However, most of the newest AI approaches for human pose estimation methods focus on adults, lacking publicly benchmark for infant pose estimation. In this paper, we fill this gap by proposing infant pose dataset and Deep Aggregation Vision Transformer for human pose estimation, which introduces a fast trained full transformer framework without using convolution operations to extract features in the early stages. It generalizes Transformer + MLP to high-resolution deep layer aggregation within feature maps, thus enabling information fusion between different vision levels. We pre-train AggPose on COCO pose dataset and apply it on our newly released large-scale infant pose estimation dataset. The results show that AggPose could effectively learn the multi-scale features among different resolutions and significantly improve the performance of infant pose estimation. We show that AggPose outperforms hybrid model HRFormer and TokenPose in the infant pose estimation dataset. Moreover, our AggPose outperforms HRFormer by 0.8 AP on COCO val pose estimation on average. Our code is available at github.com/SZAR-LAB/AggPose.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Each year, approximately 5 million newborns around the world are suffering from neurodevelopmental disorder. Due to the lack of early diagnosis and intervention, many infants are severely disabled and abandoned by their parents, especially in countries with limited numbers of pediatricians with extensive experience in neurodevelopmental disorders. This has become a conundrum that plagues many families around the world. * project lead ? contributed equally to the first-author ? corresponding author Recent developments in deep learning based approaches open possibilities for developing computer-aid movement assessment tools in early intervention for neurodevelopmental disorder. One of the most predictive tools for early cerebral palsy diagnosis is general movements assessment (GMA), as it needs to discriminate fidgety from non-fidgety movements in many small-amplitude movements <ref type="bibr">[Silva et al., 2021]</ref>, where computers are more sensitive to detect such movements. Researchers used human pose estimation methods like OpenPose  to capture infant pose and then generate infant motion sequence to detect cerebral palsy. Compared with manual GMA detection, computer-based approaches are much faster with low cost. However, this task is challenging in real applications considering complex scenarios for infant pose and there is a lack of large-scale public infant pose datasets around the world. Besides, the 17 adult keypoints defined by the COCO dataset do not support infant movement detection well due to the lack of clinical significance and actionability.</p><p>Another problem is the performance of the pose estimation methods. Although CNN-based methods have pushed human pose estimation to a new level thanks to the intense representation learning and semantics understanding ability, it is still not performing well to understand global constraint relationships between body parts . Researchers combined Vision Transformer with CNN into hybrid models to address this issue, let the ViT expand the receptive field, and enhance the model's ability to capture constraint relationships between body parts. Among recent advancements, the local-window self-attention structure from Swin Transformer , and Mix Feed Forward Network (Mix-FNN) from SegFormer  showed great potential in the direction of multi-scale feature representation learning <ref type="bibr" target="#b2">[Gu et al., 2021]</ref>.</p><p>However, some issues still make it challenging to apply Transformer for human pose estimation: (1) The first stages of the hybrid models highly rely on the pretrained HR-Net convolutional layers, which can not utilize large-scale unlabeled data with newest self-supervised masked autoencoder <ref type="bibr" target="#b3">[He et al., 2021]</ref>; (2) Hard to converge during the training process; (3) Models are challenging to transfer from one domain to another domain.</p><p>In this paper, we propose AggPose, a generalization of multi-scale transformer architecture to the deep aggregation network. Different from HRFormer, AggPose does not use convolutional layers for initial feature extractor and fusion modules. Instead, it uses layer-by-layer Mix Transformers and a cross-resolution MLP fusion module. The Transformer receives input from the former layer, applies self-attention operation and Mix-FNN, and sends the message to the next layer. The MLP fusion module integrates richer spatial information from different resolution levels and sends the result to the next stage. We conduct experiments on COCO human pose estimation dataset and then fine-tune the model on our proposed large-scale labeled infant pose estimation dataset. AggPose achieves competitive performance on both benchmarks. For example, AggPose-L gains 0.8 AP and 0.6 AP over HRFormer and TokenPose on COCO val set. AggPose-L's robustness and fast convergence make it easy to transfer from the COCO dataset to our infant pose dataset and achieve the highest 95.0 AP.</p><p>The contributions are summarized as follows: </p><formula xml:id="formula_0">?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Vision Transformers for Pose Estimation</head><p>For many years, deep convolutional neural networks have been applied to human pose estimation. Among all CNNbased pose estimation algorithms, the schemes that maintain high-resolution representations throughout the network achieved great success. The most representative models are HRNet , HigherHRNet <ref type="bibr" target="#b1">[Cheng et al., 2020]</ref>, UDP <ref type="bibr" target="#b1">[Huang et al., 2020]</ref>, <ref type="bibr">DARK [Zhang et al., 2020]</ref>. However, it is still tricky for CNNs to capture constraint relationships between human keypoints, as CNN's receptive field restrict its ability to understand global spatial relationships. Recent several works have introduced Transformer for human pose estimation <ref type="bibr">[Yuan et al., 2021;</ref>. TokenPose  introduced Transformer with representing key-points as token embeddings for human Pose estimation. HRFormer <ref type="bibr">[Yuan et al., 2021]</ref> integrated HRNet with Swin Transformer , which makes full use of multi-resolution parallel information over different non-overlapping image windows. However, both HRFormer and TokenPose did not discard convolution operations to obtain initial features, as the first stage of HRFormer and TokenPose were fine-tuned on HRNet's CNN backbone. In this work, we propose Aggregation Vision Transformers (AViT), which provides a different way to solve the low-resolution problem of ViT and replace convolution operations with overlapping patch embedding to extract features in the early stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>Our goal is to propose a new infant pose dataset and build a new benchmark that can fast extract infant pose via vision transformers. <ref type="figure" target="#fig_1">Figure 2</ref> shows the pipeline of the model. Our infant pose estimation research have passed the ethics checks of Shenzhen Baoan Women's and Childiren's Hospital.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Infant Pose Detection Dataset</head><p>In this paper, we present a large-scale challenging dataset for newborn pose extraction and detection. It can be applied to predict infant movement sequence and design automatic clinical tools like automatic GMA. Despite the importance and difficulty of infant pose detection, existing datasets are either too small or too simple, and a large public annotated benchmark is needed to compare different methods. Besides, none of these datasets proposed suitable keypoints annotation for infant images, as they adopt the COCO's 17 keypoints format, while it loses many significant refined pose and movement features for the infant.  To collect data, we adopt GMA devices to record infant movement videos from 2013 to now. More than 216 hours of videos were collected, and 15 million frames were extracted. Both the size and the scalability of our dataset are much better than the MINI-RGBD dataset <ref type="bibr" target="#b4">[Hesse et al., 2018]</ref>. We randomly sampled 20,748 frames from the videos and let professional clinicians annotate infant keypoints. Then, we divided the dataset into 11,756 for the training set and validation set, 8,992 for the test set. The 21 keypoints format for infant pose is proposed by experienced clinicians who have researched neurodevelopmental disorders over 30 years. <ref type="figure" target="#fig_0">Figure 1</ref> shows some examples, considering clinical application requirements and protection of patient's privacy, our dataset reduces keypoints on infants' heads and comprises more refined body keypoints like fingers, toes, and navel. For public version, we will reformat the dataset to solve ethical issues: all infants' heads will be covered with mosaics in the final published keypoint dataset to preserve the patients' privacy. Commercial usage of infant pose dataset is prohibited.</p><p>In this paper, we focus on the human/infant supine position pose detection, which is the most straightforward application for the new presented dataset. However, this dataset can also be used in other clinical fields, as it contains over 200 hours of infant movement sequence and has a high relationship with the automated prediction of cerebral palsy and other neurodevelopmental disorders. We hope applying our dataset and Ag-gPose to early diagnosis and intervene disorders, promoting well-being for all at all ages, especially the children. In the future, we will also release more than 200 hours of new infant pose sequences generated from AggPose, and associated GMA labels. The retrospective study was approved by our institutional review board.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deep Aggregation Vision Transformers</head><p>Overlapped Patch Embedding Early convolutions were considered practical tools to extract low-level features for hybrid transformer architectures. It is due to that transformers in the early stage treat the input as 1D vectors and exclusively focus on modeling the global context, which lose detailed localization information. HRNet and its pre-trained CNN parameters are the cornerstones of almost all the latest models for human pose estimation.  ding can obtain better low-level features, enhancing the highresolution Transformer's feature representation, and reduce computation complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aggregation Vision Transformers (AViTs) Architecture</head><p>We follow the transformer module design from Mix Transformer  and start from high-resolution feature maps generated by the overlapped patch embedding with patch size = 7, stride = 4, and padding size = 3 as the first stage. Then, we add high-to-low resolution streams one by one via overlapped patch embedding. We use multiple multihead self-attention blocks for each resolution stream to update feature representation. To construct different depth of models, we propose small (AggPose-S), and large (AggPose-L) model, respectively. <ref type="table" target="#tab_4">Table 2</ref> shows the number of transformer layers for each stage in AggPose-L. Compared with Swin Transformer and HRFormer, we do not use local-window self-attention to augment local information understanding considering the usage of overlapped patches. Instead, we use the sequence reduction process refer to  and , which significantly reduces the amount of calculation inside the transformer and accelerates the convergence process during model training. For each Transformer block, the self-attention is estimated as:</p><formula xml:id="formula_1">K = Linear(?C, C)(K.Reshape( N ? , ?C)) (1) Attention(Q, K, V ) = Sof tmax( QK T ? d head )V<label>(2)</label></formula><p>,where K is the token representation with initial shape N ? C. ? is the reduction ratio that decrease the dimension of K from N ? C to N/? ? C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLP Cross-Layer Aggregation</head><p>Both ViT and Swin Transformer uses positional embedding to introduce the location information across layers. However, the resolution of positional embedding is fixed. For the local-window Transformer, there is lacking information exchange across the windows. Thus, both SegFormer and HRFormer introduced 3 ? 3 depth-wise convolution into the feed-forward network (FFN) to expand the receptive field and reduce the harmful effect caused by positional embedding. The FFN with depth-wise convolution (HRFormer) and Mix-FFN (SegFormer) used a very similar calculation: y = M LP (Activation(DW Conv(M LP (x)))) + x (3) where DW Conv is a 3 ? 3 depth-wise convolution operation.</p><p>In AggPose, we expand the usage of Mix-FFN into the deep aggregation approach across different resolution layers. For deep aggregation in CNN such as CAggNet  and HRNet, the aggregation begins at the shallowest, high resolution layer and then iteratively merges deeper, low resolution layer. In this way, shallow features are refined as they are propagated through different stages of aggregation. Related research showed that deep aggregation structure propagates the aggregation of all resolutions instead of the preceding block alone to better preserve features. It is widely used for semantic segmentation tasks and to achieve competitive performance. In our work, the proposed cross-layer aggregation module consists of two main steps for each resolution level. First, multi-level features from different resolutions go through a mixed feed-forward network with 3 ? 3 depth-wise convolution to unify the channel dimension and upsample or downsample (overlapped patch embedding) the feature map to the same shape. Then, we concatenate the feature vector from adjacent levels together and adopt an additional FFN layer to fuse the cross-layer information. Compared to convolutional multi-scale fusion modules in HRFormer and HRNet, MLP fusion modules accelerate convergence while improving model performance.</p><formula xml:id="formula_2">x i,j = OverlappedP E i,j (F F N (x i )) i &lt; j x i i = j U psample i,j (F F N (x i )) i &gt; j<label>(4)</label></formula><p>where x i,j is the input of aggregation MLP layer. x i denotes the feature map from adjacent resolution. The crosslayer aggregation module is defined as</p><formula xml:id="formula_3">x j = M ixF F N (Concat(x j?1 , x j , x j+1 )) + x j (5)</formula><p>where M ixF F N represents the Mix feed forward block in formula (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analysis</head><p>There are two main benefits of AggPose and our large-scale infant pose dataset over other CNN or hybrid CNN Transformer methods like HRFormer and TokenPose and other small dataset for infant pose estimation, which are concluded as follows.</p><p>(1) Potential of using self-supervised learning. Recently, Vision Transformers pre-trained with self-supervised learning have attracted much attention. MAE <ref type="bibr" target="#b3">[He et al., 2021]</ref> construct an inpainting masked autoencoder task to learn representation from unlabeled data and fine-tuning the model on any supervised tasks. Their results prove that full transformers can learn reasonable semantic from large-scale unlabeled dataset. As <ref type="table" target="#tab_2">Table 1</ref> shows, we have plenty of unlabeled infant movement frames from 5,187 videos. All these data would be helpful for pre-training transformer-based autoencoder via self-supervised learning.</p><p>(2) Faster convergence. In HRFormer, the feature passing is achieved via cross-layer convolution operation, which, is difficult to convergence. In our AggPose framework, messages are propagated by MLP across different layers. It can be viewed as a kind of modification to the deep layer aggregation model. As our experiments will show, such message pass scheme achieves better results than hybrid CNN-Transformer based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Variants</head><p>Considering that the training process of most Transformerbased pose estimation models is complicated, we provide an effective training policy in this paper. First, we load the Mix Transformer  pre-trained on ImageNet, training Mix Transformer on the COCO keypoints training set. After the Mix Transformer converges, we load the parameter of Mix Transformer into each layer of AggPose. Then, we fixed the parameters of AggPose at different resolution levels layer by layer and fine-tuned the model on COCO and infant pose dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Input size Backbone GFLOPs AP AP 50 AP 75 AP M AP L AR  The configuration details for the size of overlapped patch embedding and the number of transformer layers are presented in <ref type="table" target="#tab_4">Table 2</ref>. Note, <ref type="table" target="#tab_4">Table 2</ref> only provide the configuration for AggPose-L, which uses MiT-B5 as the backbone. For AggPose-S, we used MiT-B2 as backbone, the number of transformer layers is <ref type="bibr">[[3,3,3,3]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparing with SOTA Methods</head><p>Dataset. We study the performance of AggPose on the COCO human pose estimation dataset <ref type="bibr" target="#b5">[Lin et al., 2014]</ref>, which contains more than 250K person instances labeled with 17 keypoints, and the new infant pose estimation dataset, which contains 20k infant instances labeled with 21 keypoints. MPII dataset is not used in our experiment due to its size (25K) is much smaller than COCO and has different keypoints format.</p><p>Training setting. Following most of the default training and evaluation settings from HRNet and HRFormer, we trained the models using AdamW optimizer and an initial value of 0.001 as the learning rate. For the training batch size, we chose 32 due to limited GPU memory. The experiment takes 4 ? 48G-RTX8000 GPUs. We follow the data augmentation in  mainly.</p><p>Evaluation metric. For COCO dataset, we adopt the default standard average precision (AP) as our evaluation metric. AP is calculated based on Object Keypoint Similarity (OKS):</p><formula xml:id="formula_4">OKS = i exp(?d 2 i 2s 2 k 2 i )?(v i &gt; 0) i ?(v i &gt; 0)<label>(6)</label></formula><p>whered i is the L2 distance between the i-th keypoint and the groundtruth. v i denotes the visibility of the keypoint. k i is a keypoint-specific constant, which is different for different keypoint. We adopt the same evaluation metric to COCO for the infant pose dataset. As the new proposed infant pose has 21 keypoints, we set k i of each keypoint to the same value.</p><p>Keypoints detection on COCO pose estimation. <ref type="table" target="#tab_6">Table 3</ref> shows the comparisons on COCO val set. We compare AggPose with several state-of-art methods, including HRFormer <ref type="bibr">[Yuan et al., 2021]</ref>, TokenPose , <ref type="bibr">TransPose [Yang et al., 2021]</ref>, HRNet . For input size of 256?192, AggPose-L achieves      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Experiments</head><p>In previous sections, we compare AggPose with several stateof-art human pose estimation methods. To verify the techniques used in our method, we make detailed ablation studies in this subsection.</p><p>Influence of full Transformer backbone. Considering all of the other new proposed hybrid methods are using HR-Net's CNN encoder as backbone, we compare our method (without using convolution layers in the first stage) with the CNN scheme of HRNet in <ref type="table" target="#tab_6">Table 3</ref>. The Backbone column shows the difference of the first stage inside the model. Both AggPose-S and AggPose-L are using SegFormer, a Transformer-based method as the first stage layer. Although other authors claim Transformers in the early stage will lead to lack detailed localization information, we observe that the full Transformer-based early stage of AggPose can still achieve better performance.</p><p>Influence of Deep Aggregation Framework. We report the COCO pose estimation results based on two well-known full transformer models, Swin Transformer and SegFormer in <ref type="table" target="#tab_10">Table 5</ref>. Both the Swin-B and SegFormer-B5 are pre-trained on ImageNet21K and fine-tuned on COCO with 300 epochs. In fact, AggPose-L can be considered as a deep layer aggregation structure of SegFormer-B5 with MLP skip-connection. According to the results in <ref type="table" target="#tab_10">Table 5</ref>, our proposed multiresolution aggregation framework (AggPose) achieves better performance than both Swin Transformer and SegFormer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Visualization Analysis</head><p>We provide qualitative results on both COCO val set and infant pose test set, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>, <ref type="figure" target="#fig_3">Figure 4</ref> and <ref type="figure" target="#fig_4">Figure 5</ref>. <ref type="figure" target="#fig_2">Figure 3</ref> shows a group of predictions and dependency areas for infant pose heatmap. Although infant pose data formats use more keypoints than COCO. AggPose still learns good representations in capturing constraint relationships between human keypoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>By leveraging a new dataset with pose labels and clinical labels, we built a Transformer-based infant pose estimation framework which can accurately detect infant supine position pose from movement frames in video. The key insight of the AggPose model is the deep aggregation Transformer with cross-layer MLP connection. The pose sequence generated by our model has been used in neurodevelopmental disorder prediction for newborns and early evaluation for related diseases. Besides, our method can be packaged to mobile devices in the future and solve inequality in medical resources and privacy protection for patients. Although our results are promising, we acknowledge that there is still a long path to apply our model completely end-to-end with currently available hardware. In addition to testing the utility of AggPose in real-time infant pose extraction and evaluation, a clear next step would be predicting the cerebral palsy via pose sequence understanding models in the future-before it is even visible to a trained pediatrician eye. For countries with limited pediatricians, this will greatly reduce the risk of severe disability in children.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Examples for our proposed InfantPose dataset. (b) 21 infant body key-points</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The proposed AggPose architecture. Each module consists of multiple successive Mix Transformer blocks. Features across different resolutions are connected by MLP layer (blue square in the figure).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of the pose estimation heatmap results based on AggPose-L on infant pose test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of the pose estimation results based on AggPose-L on COCO val.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of the pose estimation results based on AggPose-L on infant pose test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison between other infant pose dataset. Inspired by [Silva et al., 2021; Huang et al., 2021], we publish our new open-source infant pose dataset and new infant keypoints format.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Inspired by SegFormer [Xie et al., 2021], we adopt full Transformer with Overlapped Patch Embedding to replace HRNet's CNN feature extractor and down-sampling stem of each stage. Compared with the early convolutions in HR-Net, HRFormer, and TokenPose, Overlapped Patch Embed-</figDesc><table><row><cell cols="5">Features level Stage 1 Stage 2 Stage 3 Stage 4</cell></row><row><cell>1/4</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>3</cell></row><row><cell>1/8</cell><cell></cell><cell>6</cell><cell>3</cell><cell>3</cell></row><row><cell>1/16</cell><cell></cell><cell></cell><cell>40</cell><cell>3</cell></row><row><cell>1/32</cell><cell></cell><cell></cell><cell></cell><cell>3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>The number of Transformer layers for each stage.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Comparisons on the COCO validation set, provided with the same detected human boxes from HRNet.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="4">: Comparisons on the infant pose test set, provided with the</cell></row><row><cell cols="4">same object detection boxes (OpenPose do not need object detec-</cell></row><row><cell cols="4">tion, as it is a bottom-up method. We select OpenPose for com-</cell></row><row><cell cols="4">parison is because most of newest proposed infant pose estimation</cell></row><row><cell cols="3">frameworks are choose OpenPose as backbone.)</cell></row><row><cell>Method</cell><cell cols="2">image size GFLOPs</cell><cell>AP</cell></row><row><cell>Swin-B</cell><cell>256?192</cell><cell cols="2">17.6 74.3</cell></row><row><cell>SegFormer-B5</cell><cell>256?192</cell><cell cols="2">12.3 74.2</cell></row><row><cell>AggPose-L</cell><cell>256?192</cell><cell cols="2">15.0 76.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5</head><label>5</label><figDesc>Keypoints detection on infant pose estimation.Table 4reports the comparisons on our infant pose test set. We compare AggPose to the most representative bottom-up method OpenPose, as it is used by almost all newest proposed infant pose estimation frameworks[Silva et al., 2021]. We also compare AggPose to several recent CNN and hybrid Transformer models such as HRNet, TokenPose, and HRFormer. AggPose gains the highest 95.0 AP with an input size of 256?192. During the training, we also find that both Agg-Pose and HRNet perform better and converge faster than hybrid model such as TokenPose, and HRFormer. Though all of these models are pre-trained on COCO dataset, full CNN or full Transformer based methods are more robust after we fine-tune them on other domain like infant pose data.</figDesc><table><row><cell>: Comparisons to Non-aggregation framework (Swin Trans-</cell></row><row><cell>former, SegFormer) on COCO pose estimation val</cell></row><row><cell>76.4 AP, which is best among all methods. We believe</cell></row><row><cell>that AggPose-L can achieve better results after applying the</cell></row><row><cell>newest distribution-aware coordinate representation [Zhang</cell></row><row><cell>et al., 2020] or UDP [Huang et al., 2020]. AggPose-L</cell></row><row><cell>achieves 75.7 AP on the COCO test-dev set with 256 ? 192</cell></row><row><cell>input size.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by Sanming Project of Medicine in Shenzhen, China (SZSM202011005).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Assessing child postural variability: Development, feasibility, and reliability of a video coding system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>References [abbruzzese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<editor>Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh</editor>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="172" to="186" />
		</imprint>
	</monogr>
	<note>Openpose: realtime multi-person 2d pose estimation using part affinity fields. IEEE transactions on pattern analysis and machine intelligence</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5386" to="5395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multi-scale high-resolution vision transformer for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.01236</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Feng Guo, and Guan Huang. The devil is in the details: Delving into unbiased data processing for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hesse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV Workshops</title>
		<meeting>ECCV Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5700" to="5709" />
		</imprint>
	</monogr>
	<note>Proceedings of CVPR</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Invariant representation learning for infant pose estimation with small data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)</title>
		<editor>Tsung-Yi Lin, Michael Maire, Serge Belongie</editor>
		<meeting><address><addrLine>James Hays, Pietro Perona, Deva Ramanan</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The future of general movement assessment: The role of computer vision and machine learninga scoping review</title>
		<idno type="arXiv">arXiv:2103.14030</idno>
	</analytic>
	<monogr>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<editor>Silva et al., 2021] Nelson Silva, Dajie Zhang</editor>
		<meeting><address><addrLine>Alexander Gail, Carla Barreiros</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">103854</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Establishing pose based features using histograms for the detection of abnormal infant movements. Research in developmental disabilities</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<idno>arXiv:2110.09408</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<editor>Yuan, Rao Fu, Lang Huang, Weihong Lin, Chao Zhang, Xilin Chen, and Jingdong Wang</editor>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7093" to="7102" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of CVPR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
