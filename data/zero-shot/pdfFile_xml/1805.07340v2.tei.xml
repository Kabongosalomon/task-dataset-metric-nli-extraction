<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Sentence Modeling using Suffix Bidirectional LSTM</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Brahma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI</orgName>
								<address>
									<settlement>Almaden</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Sentence Modeling using Suffix Bidirectional LSTM</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recurrent neural networks have become ubiquitous in computing representations of sequential data, especially textual data in natural language processing. In particular, Bidirectional LSTMs are at the heart of several neural models achieving state-of-the-art performance in a wide variety of tasks in NLP. However, BiLSTMs are known to suffer from sequential biasthe contextual representation of a token is heavily influenced by tokens close to it in a sentence. We propose a general and effective improvement to the BiLSTM model which encodes each suffix and prefix of a sequence of tokens in both forward and reverse directions. We call our model Suffix Bidirectional LSTM or SuBiLSTM. This introduces an alternate bias that favors long range dependencies. We apply SuBiLSTMs to several tasks that require sentence modeling. We demonstrate that using SuBiLSTM instead of a BiLSTM in existing models leads to improvements in performance in learning general sentence representations, text classification, textual entailment and paraphrase detection. Using SuBiLSTM we achieve new state-of-the-art results for fine-grained sentiment classification and question classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Suffix Bidirectional LSTM</head><p>Let s be a sequence with n tokens. We use s[i : j] to denote the sequence of embeddings of the tokens from s[i] to s[j], where j maybe less than i. Let ? L p represent a LSTM that arXiv:1805.07340v2 [cs.LG]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Recurrent Neural Networks (RNN) <ref type="bibr" target="#b10">(Elman 1990</ref>) have emerged as a powerful tool for modeling sequential data. Vanilla RNNs have largely given way to more sophisticated recurrent architectures like Long Short-Term Memory <ref type="bibr" target="#b20">(Hochreiter and Schmidhuber 1997)</ref> and the simpler Gated Recurrent Unit <ref type="bibr" target="#b5">(Cho et al. 2014</ref>), owing to their superior gradient propagation properties. The importance of LSTMs in natural language processing, where a sentence as a sequence of tokens represents a fundamental unit, has risen exponentially over the past few years. A LSTM processing a sentence in the forward direction produces distributed representations of its prefixes. A Bidirectional LSTM (BiLSTM in short) <ref type="bibr" target="#b41">(Schuster and Paliwal 1997)</ref> <ref type="bibr" target="#b14">(Graves and Schmidhuber 2005)</ref> additionally processes the sentence in the reverse direction (starting from the last token) producing representations of the suffixes (in the reverse direction). For every token t in the sentence, a BiLSTM thus produces a contextual representation of t based on its prefix and suffix in the sentence.</p><p>Despite their sophisticated design, it is well known that LSTMs suffer from sequential bias <ref type="bibr">(Pascanu, Mikolov, and</ref> Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. <ref type="bibr" target="#b38">Bengio 2013)</ref>. The hidden state of a LSTM is heavily influenced by the last few tokens it has processed. This implies that the contextual representation of t is highly influenced by the tokens close to it in the sequential order, with tokens farther away being less influential. Computing contextual representations that capture long range dependencies is a challenging research problem, with numerous applications.</p><p>In this paper, we propose a simple, general and effective technique to compute contextual representations that capture long range dependencies. For each token t, we encode both its prefix and suffix in both the forward and reverse direction. Notably, the encoding of the suffix in the forward direction is biased towards tokens sequentially farther away to the right of t. Similarly, the encoding of the prefix in the reverse direction is biased towards tokens sequentially farther away to the left of t. Further, we combine the prefix and suffix representations by a simple max-pooling operation to produce a richer contextual representation of t in both the forward and reverse direction. We call our model Suffix BiLSTM or SuBiLSTM in short. A SuBiLSTM has the same representation length as a BiLSTM with the same hidden dimension.</p><p>We consider two versions of SuBiLSTMs -a tied version where the suffixes and prefixes in each direction are encoded using the same LSTM and an untied version where two different LSTMs are used. Note that, as in a BiLSTM, we always use different LSTMs for the forward and reverse direction. In general a SuBiLSTM can be used as a drop in replacement in any model that uses the intermediate states of a BiLSTM, without changing any other parts of the model. However, the main motivation for introducing SuBiLSTMs is to apply it to problems that require whole sentence modeling e.g. text classification, where the richer contextual information can be helpful. We demonstrate the effectiveness of SuBiLSTM on several sentence modeling tasks in NLP -general sentence representation, text classification, textual entailment and paraphrase detection. In each of these tasks, we show gains by simply replacing BiLSTMs in strong base models, achieving a new state-of-the-art in fine grained sentiment classification and question classification. </p><formula xml:id="formula_0">1 i n h p,i (L p ) ? h s,i (L s ) ? h p,i (L p ) ? h s,i (L s ) ? ? ? ? ? , ) Max( , ) Max( SuBiLSTM</formula><formula xml:id="formula_1">[i], we have ? h p,i = ? L p (s[1 : i]) (1) Let ? L s represent a LSTM that encodes suffixes of s in the forward direction. ? h s,i = ? L s (s[i : n])<label>(2)</label></formula><p>Note that the ? h p,i can be computed in a single pass over s, while computing ? h s,i needs a total of n passes over progressively smaller suffixes of s. Now consider ? L p and ? L s that encodes the prefixes and suffixes of s in the reverse direction. </p><formula xml:id="formula_2">? h p,i = ? L p (s[i : 1]) (3) ? h s,i = ? L s (s[n : i])<label>(4)</label></formula><formula xml:id="formula_3">H SuBiLSTM i = max ? h p,i , ? h s,i ; max ? h p,i , ? h s,i<label>(5)</label></formula><p>Here ; is the concatenation operator. This defines the SuBiL-STM model. We also define another representation where the two LSTMs encoding the sequence in the same direction are the same or their weights are tied. This defines the SuBiLSTM-Tied model, which concretely is</p><formula xml:id="formula_4">H SuBiLSTM-Tied i = max ? h p,i , ? h s,i ; max ? h p,i , ? h s,i (6) where ? L p ? ? L s , ? L p ? ? L s</formula><p>In contrast to SuBiLSTM, a standard BiLSTM uses the following contextual representation of s <ref type="bibr">[i]</ref>.</p><formula xml:id="formula_5">H BiLSTM i = ? h p,i ; ? h s,i<label>(7)</label></formula><p>For a fixed hidden dimension, SuBiLSTM and SuBiLSTM-Tied have the same representation length as a BiLSTM. Importantly, SuBiLSTM-Tied uses the same number of parameters as a BiLSTM, while SuBiLSTM uses twice as many. can be interpreted as the max-pooling of the bidirectional representations of the prefix and suffix of s[i] into a compact representation. This may be contrasted with a BiLSTM where the prefix is encoded by a LSTM in the forward direction and the suffix is encoded by another LSTM in the reverse direction. SuBiLSTM thus tries to capture more information by encoding the prefix and suffix in a bidirectional manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpretations of SuBiLSTM</head><p>In general, the prefix and suffix encodings can be combined in other ways e.g. concatenation, mean or through a learned gating function. However, we use max-pooling because it is a simple parameterless operation and it performs better in our experiments. Since both SuBiLSTM and SuBiLSTM-Tied produces representations of each token s[i] in the same way as a BiLSTM, they can be used as drop in replacements for a BiLSTM in any model that uses these representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time complexity of a SuBiLSTM</head><p>To compute the contextual representations of a minibatch of sentences using a SuBiLSTM, we calculate all the ? h p,i in one pass using ? L p . We then create several minibatches (determined by the maximum length of a sentence in the minibatch n max ) of successively smaller suffixes starting at i, for each i ? [1 : n max ] and use ? L s to compute the encodings ? h s,i . The same procedure is repeated for the minibatch of sentences with tokens reversed to compute ? h s,i and ? h p,i . As an optimization, several of the minibatches of the shorter suffixes can be combined to form larger minibatches. The worst case time complexity of computing all the representations is quadratic in n max , as compared to the linear time complexity using a BiLSTM. As we show in later sections, the increased time complexity is offset by the consistent gains in performance on several sentence modeling tasks. The encodings of the different can be computed in parallel, which can speed up computation greatly on modern hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation, Datasets, Training and Testing</head><p>We evaluate the representational power of SuBiLSTM using several sentence modeling tasks and datasets from NLP. We do not concern ourselves with designing new models for SuBiLSTM. Rather, for each task, we take a strongly performing base model that uses the token representations of a BiLSTM and replace it with SuBiLSTM. The training procedures are kept exactly the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Sentence Representation</head><p>First, we investigate whether a SuBiLSTM can be trained to produce good general sentence representations that transfer well to several NLP tasks. As the base model, we use the recently proposed InferSent <ref type="bibr" target="#b7">(Conneau et al. 2017)</ref>. It was shown to give strong results on a set of 10 NLP tasks encapsulated in the SentEval benchmark <ref type="bibr" target="#b6">(Conneau and Kiela 2018)</ref>. The representation of a sentence is a max-pooling of the token representations produced by a SuBiLSTM.</p><formula xml:id="formula_6">H SuBiLSTM (s) = max i?[1:n] H SuBiLSTM i (8)</formula><p>where H SuBiLSTM i is defined in (5). The representation for H SuBiLSTM-Tied i is defined similarly. We train the model on the textual entailment task, where a pair of sentences (premise and hypothesis) needs to be classified into one of three classes -entailment, contradiction and neutral. Let u be the encoding of the premise according to (8) and let v be the encoding of the hypothesis. Using a Siamese architecture, the combined vector of [u; v; |u ? v|; u ? v] is used as the representation of the pair which is then passed through two fully connected layers and a final classification layer.</p><p>Training. We use the combination of the Stanford Natural Language Inference (SNLI) <ref type="bibr">(Bowman et al. 2015)</ref> and the MultiNLI (Williams, Nangia, and Bowman 2018) datasets to train. We set the hidden dimension of the LSTMs in SuBiL-STM to 2048, which produces a 4096 dimensional encoding for each sentence. The two fully connected layers are of 512 dimensions each. The tokens in the sentence are embedded using GloVe embeddings <ref type="bibr" target="#b39">(Pennington, Socher, and Manning 2014)</ref> which are not updated during training. We follow the same training procedure used for training the InferSent model in <ref type="bibr" target="#b7">(Conneau et al. 2017)</ref>.</p><p>Testing. We test the sentence representations learned by SuBiLSTM on the SentEval benchmark. This benchmark consists of 6 text classification tasks (MR, CR, SUBJ, MPQA, SST, TREC) with accuracy as the performance measure. There is one task on paraphrase detection (MRPC) with accuracy and F1 and one on entailment classification (SICK-E) with accuracy as the performance measure, respectively. There are two tasks on textual semantic relatedness (SICK-R and STSB) for which the performance measure is the Pearson correlation between the predicted scores and ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Classification</head><p>We pick two representative tasks for text classification -sentiment classification and question classification. As the base model, we use the Biattentive-  <ref type="bibr" target="#b44">(Socher et al. 2013)</ref>, both in its binary (SST-2) and fine-grained (SST-5) forms. For question classification, we use the TREC <ref type="bibr" target="#b48">(Voorhees 2001)</ref> dataset, both in its 6 class (TREC-6) and 50 class (TREC-50) forms. The hidden dimension of the LSTMs is set to 300. Distinct from <ref type="bibr" target="#b30">(McCann et al. 2017)</ref>, we use dropout after the embedding layer and before the classification layer. The two maxout layers are fixed at reduction factors of 4 and 2. We also apply weight decay to the parameters during optimization, which is done using Adam <ref type="bibr" target="#b24">(Kingma and Ba 2015)</ref> with a learning rate of 1e-3. We experiment with two versions of the initial embedding -one using GloVe only and the other using both GloVe and CoVe, both of which are fixed during training. Validation and testing are done using the sets associated with the SST and TREC datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Textual Entailment</head><p>As mentioned above, the textual entailment problem is the task of classifying a pair of sentences into three classesentailment, contradiction and neutral. It is an important and canonical text matching problem in NLP. To test SuBiLSTM for this task, we pick ESIM <ref type="bibr" target="#b4">(Chen et al. 2017</ref>) as the base model. ESIM has been shown to achieve state-of-the-art results on the SNLI dataset and has been the basis of further improvements. Like BCN above, ESIM uses two BiLSTM layers to encode sentences, with an inter-sentence attention   <ref type="bibr" target="#b19">(Hill, Cho, and Korhonen 2016)</ref>, SkipThought is described in <ref type="bibr" target="#b25">(Kiros et al. 2015)</ref>, DisSent in <ref type="bibr" target="#b37">(Nie, Bennett, and Goodman 2018)</ref>, CNN-LSTM in <ref type="bibr" target="#b11">(Gan et al. 2017)</ref>, Byte mLSTM in <ref type="bibr" target="#b40">(Radford, J?zefowicz, and Sutskever 2017)</ref>, QuickThoughts in <ref type="bibr" target="#b27">(Logeswaran and Lee 2018)</ref> and MultiTask in <ref type="bibr" target="#b45">(Subramanian et al. 2018</ref>). Our base model is InferSent <ref type="bibr" target="#b7">(Conneau et al. 2017)</ref>. Bold indicates the best performance among the SuBiLSTM models and the base model. For MRPC we use F1 percentage, for SICK-R and STSB we use 100?Pearson correlation and for the rest accuracy percentages. The Avg. ? is the average of the 10 values. mechanism in between. In our experiments, we only replace the first BiLSTM with a SuBiLSTM.</p><p>Training and Testing. We use 300 dimensional GloVe embeddings to initialize the word embeddings (which are also updated during training) and use 300 dimensional LSTMs. We follow the same training procedure as <ref type="bibr" target="#b4">(Chen et al. 2017)</ref>. Validation and testing are on the corresponding sets in the SNLI dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paraphrase Detection</head><p>In this task, a pair of sentences need to be classified according to whether they are paraphrases of each other. To demonstrate the effectiveness of SuBiLSTM in a model that does not use any attention mechanism on the token representations, we use the same Siamese architecture used for training general sentence representations described above, except with one fully connected layer at the end followed by ReLU activation.</p><p>Training and Testing. We use 300 dimensional GloVe embeddings to initialize the word embeddings (which are also updated during training) and use 600 as the hidden dimension of all LSTMs and also the dimension of the fully connected layer. We apply dropout after the word embedding layer and after the ReLU activation. Training is done using the Adam optimizer with a learning rate of 1e-3. We use the QUORA dataset <ref type="bibr" target="#b22">(Iyer et al. 2017</ref>) to train and test our models. A summary of the various datasets used in our evaluation is given in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>For each of the tasks, we compare SuBiLSTM and SuBiLSTM-Tied with a single-layer BiLSTM and a 2-layer BiLSTM encoder with the same hidden dimension. While a SuBiLSTM-Tied encoder has the same number of parameters as single-layer BiLSTM, a SuBiLSTM has twice as many. In contrast, a 2-layer BiLSTM has more parameters than either of the SuBiLSTM variants if the hidden dimension is at least as large as the input dimension, which is the case in all out models. By comparing with a 2-layer BiLSTM baseline, we account for the larger number of parameters used in SuBiLSTM and also check whether the long range contextual information captured by SuBiLSTM can easily be replicated by adding more layers to the BiLSTM.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>In this section, for the sake of brevity, the terms SuBiLSTM and SuBiLSTM-Tied will sometimes refer to the base models where the BiLSTM has been replaced by our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Sentence Representation</head><p>The performance of SuBiLSTM and SuBiLSTM-Tied on the 10 transfer tasks in SentEval is shown in <ref type="table" target="#tab_5">Table 2</ref>. In all the tasks, SuBiLSTM and SuBiLSTM-Tied matches or exceeds the performance of the base model InferSent that uses a BiLSTM. For SuBiLSTM, among the classification tasks, the gains for SUBJ (0.8%), MPQA (0.5%) and TREC (1.6%) over InferSent are particularly notable. There is also a substantial gain of 1.2% in the semantic textual similarity task (STSB). The performance of SuBiLSTM-Tied also follows a similar trend, gaining 0.6% for SUBJ, 0.5% for SST , 2.2% for TREC and and 1.3% for STSB. The better performance on STSB is noteworthy as the sentence representations derived from a SuBiLSTM can take advantage of the long range dependencies it encodes. The 2-layer BiLSTM based model performs comparably to the single layer BiLSTM, despite using a much larger number of parameters.</p><p>In <ref type="figure" target="#fig_1">Fig. 2</ref> we plot the absolute gains made by SuBiLSTM and SuBiLSTM-Tied over BiLSTM for all the 10 tasks. It is interesting to note that both models perform comparably on an average, although SuBiLSTM has twice as many param-eters as SuBiLSTM-Tied. The performance of our models is still some way off from MultiTask <ref type="bibr" target="#b45">(Subramanian et al. 2018)</ref>; but they use a training dataset which is two orders of magnitude larger with a complex set of learning objectives. QuickThoughts (Logeswaran and Lee 2018) also uses a much larger unsupervised dataset. It is possible that SuBiLSTM coupled with training objectives and datasets used in these two works will provide substantial gains over the existing results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Classification</head><p>The performance of SuBiLSTM and SuBiLSTM-Tied on the four text classification datasets is shown in <ref type="table" target="#tab_7">Table 3</ref>. In three of these tasks (SST-2, SST-5 and TREC-50), SuBiLSTM-Tied using GloVe and CoVe embeddings performs the best. It performs notably better than the single layer BiLSTM based base model BCN on SST-2 and SST-5, achieving a new state-of-the-art accuracy of 56.2% on fine-grained sentiment classification (SST-5). On TREC-6, the best result is obtained for SuBiLSTM-Tied using GloVe embeddings only, a new state-of-the-art accuracy of 96.2%.There is no substantial improvement on the TREC-50 dataset.</p><p>For text classification, we observe that SuBiLSTM-Tied performs better than SuBiLSTM and CoVe embeddings give a boost in most cases. The performance of the base model BCN with a 2-layer BiLSTM is slightly better than with the Model Test ESIM with BiLSTM <ref type="bibr" target="#b4">(Chen et al. 2017)</ref> 88.0 DIIN <ref type="bibr" target="#b13">(Gong, Luo, and Zhang 2018)</ref> 88.0 BCN+Char+CoVe (McCann et al. 2017) 88.1 DR-BiLSTM <ref type="bibr" target="#b12">(Ghaeini et al. 2018)</ref> 88.5 CAFE <ref type="bibr" target="#b46">(Tay, Tuan, and Hui 2018)</ref> 88  This implies that the richer contextual information captured by a SuBiLSTM cannot easily be replicated by adding more layers to the BiLSTM. Note that BCN uses a self-attention mechanism on top of the token representations and it is able to exploit the richer representations provided by SuBiLSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Textual Entailment</head><p>The performance of SuBiLSTM and SuBiLSTM-Tied on the SNLI dataset is shown in <ref type="table" target="#tab_9">Table 4</ref>. Our implementation of ESIM, when using a BiLSTM, achieves 87.8% accuracy. Using a SuBiLSTM, the accuracy jumps to 88.3% and to 88.2% for the Tied version. On using the 2-layer BiLSTM, accuracy improves only marginally by 0.1%. This is aligned with the results shown for text classification above. Here again, the attention mechanism on top of the token representations benefit from the long range contextual information captured by SuBiLSTM. Note that ESIM uses an inter-sentence attention mechanism and is able to exploit the better token representations provided by SuBiLSTM across sentences. We also report the performance of an ensemble of 5 models. Both the SuBiLSTM versions achieve an accuracy of 89.1%, while the BiLSTM based ones perform worse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paraphrase Detection</head><p>The accuracies obtained on the QUORA dataset are shown in <ref type="table" target="#tab_11">Table 5</ref>. Note that unlike the BCN and ESIM models, we use a simple Siamese architecture without any attention mechanism. In fact, the representation of a sentence in this case is simply the max-pooling of all the intermediate representations of the SuBiLSTM. Even in this case, we observe gains over both single layer and 2-layer BiLSTMs, although slightly lesser than the attention based models. The best model (SuBiLSTM) achieves 88.2%, at par with a more complex attention based model BiMPM <ref type="bibr" target="#b49">(Wang, Hamza, and Florian 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Test BIMPM <ref type="bibr" target="#b49">(Wang, Hamza, and Florian 2017)</ref> 88.2 pt-DECATTchar <ref type="bibr" target="#b47">(Tomar et al. 2017)</ref> 88.4 DIIN <ref type="bibr" target="#b13">(Gong, Luo, and Zhang 2018)</ref> 89.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of SuBiLSTM and SuBiLSTM-Tied</head><p>The results shown above clearly show the efficacy of using SuBiLSTMs in existing models geared towards four different sentence modeling tasks. The relative performance of SuBiL-STM and SuBiLSTM-Tied are fairly close to each other, as shown by the relative gains in <ref type="figure" target="#fig_2">Fig. 3</ref>. SuBiLSTM-Tied works better on small datasets (SST and TREC), probably owing to the regularizing effect of using the same LSTM to encode both suffixes and prefixes. For the larger datasets (SNLI and QUORA), SuBILSTM slightly edges out the tied version owing to its larger capacity. The training complexity for both the models is similar and hence, with half the parameters, SuBILSTM-Tied should be the more favored model for sentence modeling tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Recurrent Neural Networks <ref type="bibr" target="#b10">(Elman 1990</ref>) have emerged as one of the most powerful tools for computing distributed representations of sequential data. The problems of training vanilla RNNs <ref type="bibr" target="#b0">(Bengio, Simard, and Frasconi 1994)</ref> were addressed by more sophisticated models -most notably the Long Short Term Memory (LSTM) <ref type="bibr" target="#b20">(Hochreiter and Schmidhuber 1997)</ref> and the simpler GRU <ref type="bibr" target="#b5">(Cho et al. 2014</ref>  <ref type="bibr" target="#b16">(Graves, Wayne, and Danihelka 2014)</ref> and TopicRNNs <ref type="bibr" target="#b9">(Dieng et al. 2017)</ref>.</p><p>In this paper, we focus on LSTMs. As shown by the work of <ref type="bibr" target="#b23">(Jzefowicz, Zaremba, and Sutskever 2015)</ref> and <ref type="bibr" target="#b17">(Greff et al. 2016)</ref>, LSTMs represent a robust recurrent neural network architecture for modeling sequential data. In particular, LSTMs are a core component in several state-of-the-art neural models for NLP tasks like language modeling <ref type="bibr" target="#b31">(Melis, Dyer, and Blunsom 2018;</ref><ref type="bibr" target="#b32">Merity, Keskar, and Socher 2018)</ref>, textual entailment <ref type="bibr" target="#b4">(Chen et al. 2017)</ref>, question answering <ref type="bibr" target="#b42">(Seo et al. 2017)</ref>, semantic role labeling ) and named entity recognition <ref type="bibr" target="#b28">(Ma and Hovy 2016)</ref>.</p><p>A unidirectional RNN processes a sequence in a single direction, usually following the natural order specific to the sequence. Bidirectional RNNs, where two distinct recurrent networks process the input sequence in opposite directions was first proposed by <ref type="bibr" target="#b41">(Schuster and Paliwal 1997)</ref>. This allows the model to have a representation of the prefix and the suffix at each intermediate point in the sequence, thereby providing context in both directions. Following the work by <ref type="bibr" target="#b14">(Graves and Schmidhuber 2005)</ref>, Bidirectional LSTMs have become a mainstay for sequence representation tasks. The concept of having encodings of different contexts has since been generalized to Multidimensional LSTMs <ref type="bibr" target="#b15">(Graves and Schmidhuber 2008)</ref> and Grid LSTMs (Kalchbrenner, Danihelka, and Graves 2016).</p><p>In the recently proposed Twin-Networks <ref type="bibr" target="#b43">(Serdyuk et al. 2018)</ref>, the authors show that forcing the prefix encoding in a BiLSTM to be close to the suffix encoding in the reverse direction acts as a regularizer and helps capture more long term dependencies. We take a more direct approach -explicitly encoding the suffix in the forward direction and forcing an interaction with the prefix encoding through a max-pooling. Although we focus on LSTMs in this paper, our idea generalizes trivially to other RNN cells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We propose SuBiLSTM and SuBiLSTM-Tied, a simple, general and effective improvement to the BiLSTM model, where the prefix and suffix of each token in a sentence is encoded in both forward and reverse directions to capture long range dependencies. We demonstrate gains in performance by replacing BiLSTMs in existing models for several sentence modeling tasks. The main drawback of our method is the quadratic time complexity required to compute the representations in a SuBiLSTM. As future direction of work, we intend to explore variants of SuBiLSTM, where only suffixes of fixed or small random lengths are computed. We also plan to utilize the information (e.g. encodings of subsequences) exposed by SuBiLSTM in more novel ways.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Schematics of SuBiLSTM. The large solid purple arrow represents prefixes and large solid seagreen arrow represents suffixes. Their directions represent the encoding direction of the corresponding LSTMs. Best viewed in color. encodes prefixes of s in the forward direction. For the i-th token s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Gains by using SuBiLSTM in the SentEval tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Gains from using SuBiLSTM and SuBilSTM-Tied over single layer BiLSTM on all the datasets. The difference is between the best figures obtained for each model. For SentEval we use the average score and accuracy for the rest.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The BCN model uses two BiLSTMs to encode a sentence. The intermediate states of the first BiLSTM are used to compute a self-attention matrix. This is followed by further processing and a second BiLSTM before a final classification layer. Our hypothesis is that the richer contextual representations of SuBiLSTM should help such attention based sentence models. For our experiments, we replace only the first BiLSTM with a SuBiLSTM.Training and Testing. For sentiment classification, we use the Stanford Sentiment Treebank dataset</figDesc><table><row><cell>Classification-Network (BCN)</cell></row><row><cell>proposed by (McCann et al. 2017), which was shown to give</cell></row><row><cell>strong performance on several text classification datasets,</cell></row><row><cell>especially in association with CoVe embeddings (McCann</cell></row><row><cell>et al. 2017).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Performance of SuBiLSTM on the SentEval benchmark. The first 8 methods contain both unsupervised and supervised ones. FastSent is from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Comparison of text classification methods on the four datasets -SST-2, SST-5, TREC-6 and TREC-50. For each of them, we show accuracy numbers for BCN with SuBiLSTM and BCN with BiLSTM (base model), both with and without CoVe embeddings. The best performing ones among these is shown in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Accuracy of SuBiLSTM and BiLSTM on the SNLI test set with ESIM as the base model.</figDesc><table><row><cell>single layer BiLSTM in all cases except TREC-50. However,</cell></row><row><cell>despite using a larger number of parameters, it does not</cell></row><row><cell>perform better than both SuBiLSTM and SuBiLSTM-Tied.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Accuracy of SuBiLSTM and BiLSTM on the QUORA test set with a Siamese base model. All previous results use attention mechanisms.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning longterm dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dilated recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enhancing and combining sequential and tree LSTM for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Senteval: An evaluation toolkit for universal sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<idno>abs/1803.05449</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">From symbolic to sub-symbolic information in question classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P C G</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Coheur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wichert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Topicrnn: A recurrent neural network with long-range semantic dependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised learning of sentence representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DR-BiLSTM: Dependent reading bidirectional LSTM for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ghaeini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Z</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Natural language inference over interaction space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Offline handwriting recognition with multidimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>In NIPS</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">LSTM: A Search Space Odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep semantic role labeling: What works and whats next</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning distributed representations of sentences from unlabelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Encoding syntactic knowledge in neural networks for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">First quora dataset release: Question pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dandekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Csernai</surname></persName>
		</author>
		<ptr target="https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jzefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<editor>Bach, F. R., and Blei, D. M., eds., ICML. Kalchbrenner, N.</editor>
		<editor>Danihelka, I.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Grid long short-term memory</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A clockwork rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutn?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An efficient framework for learning sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">High accuracy rule-based question classification using question syntax and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Madabushi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learned in translation: Contextualized word vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the state of the art of evaluation in neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Regularizing and optimizing LSTM language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tree-based convolution: A new neural architecture for sentence modeling</title>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural semantic encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural tree indexers for text understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">DisSent: Sentence representation learning from explicit discourse relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>In ICML</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning to generate reviews and discovering sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>J?zefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Twin networks: Using the future as a regularizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning general purpose distributed sentence representations via large scale multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<editor>ICLR. Tan, C.</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Multiway attention networks for modeling sentence pairs</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A comparepropagate architecture with alignment factorization for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Neural paraphrase identification of questions with noisy pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Tomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>T?ckstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SWCN@EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The TREC question answering track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="361" to="378" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Bilateral multiperspective matching for natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Text classification improved by integrating bidirectional LSTM with two-dimensional max pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<editor>COL-ING</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Recurrent highway networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Zilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutn?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
