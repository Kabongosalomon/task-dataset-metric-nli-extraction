<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking Class-Balanced Methods for Long-Tailed Visual Recognition from a Domain Adaptation Perspective</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Abdullah</forename><surname>Jamal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced 3 Google</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
						</author>
						<title level="a" type="main">Rethinking Class-Balanced Methods for Long-Tailed Visual Recognition from a Domain Adaptation Perspective</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object frequency in the real world often follows a power law, leading to a mismatch between datasets with longtailed class distributions seen by a machine learning model and our expectation of the model to perform well on all classes. We analyze this mismatch from a domain adaptation point of view. First of all, we connect existing classbalanced methods for long-tailed classification to target shift, a well-studied scenario in domain adaptation. The connection reveals that these methods implicitly assume that the training data and test data share the same classconditioned distribution, which does not hold in general and especially for the tail classes. While a head class could contain abundant and diverse training examples that well represent the expected data at inference time, the tail classes are often short of representative training data. To this end, we propose to augment the classic class-balanced learning by explicitly estimating the differences between the class-conditioned distributions with a meta-learning approach. We validate our approach with six benchmark datasets and three loss functions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Big curated datasets, deep learning, and unprecedented computing power are often referred to as the three pillars of recent advances in visual recognition <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b35">37]</ref>. As we continue to build the big-dataset pillar, however, the power law emerges as an inevitable challenge. Object frequency in the real world often exhibits a long-tailed distribution where a small number of classes dominate, such as plants and animals <ref type="bibr" target="#b49">[51,</ref><ref type="bibr">1]</ref>, landmarks around the globe <ref type="bibr" target="#b39">[41]</ref>, and common and uncommon objects in contexts <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b21">23]</ref>.</p><p>In this paper, we propose to investigate long-tailed visual recognition from a domain adaptation point of view. The long-tail challenge is essentially a mismatch problem between datasets with long-tailed class distributions seen by a machine learning model and our expectation of the * Work done while M. Jamal was an intern at Google.  <ref type="figure">Figure 1</ref>. The training set of iNaturalist 2018 exhibits a long-tailed class distribution <ref type="bibr">[1]</ref>. We connect domain adaptation with the mismatch between the long-tailed training set and our expectation of the trained classifier to perform equally well in all classes. We also view the prevalent class-balanced methods in long-tailed classification as the target shift in domain adaptation, i.e., Ps(y) = Pt(y) and Ps(x|y) = Pt(x|y), where Ps and Pt are respectively the distributions of the source domain and the target domain, and x and y respectively stand for the input and output of a classifier. We contend that the second part of the target shift assumption does not hold for tail classes, e.g., Ps(x|King Eider) = Pt(x|King Eider), because the limited training images of King Eider cannot well represent the data at inference time. model to perform well on all classes (and not bias toward the head classes). Conventional visual recognition methods, for instance, training neural networks by a cross-entropy loss, overly fit the dominant classes and fail in the underrepresented tail classes as they implicitly assume that the test sets are drawn i.i.d. from the same underlying distribution as the long-tailed training set. Domain adaptation explicitly breaks the assumption <ref type="bibr" target="#b44">[46,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b19">21]</ref>. It discloses the inference-time data or distribution (target domain) to the machine learning models when they learn from the training data (source domain).</p><p>Denote by P s (x, y) and P t (x, y) the distributions of a source domain and a target domain, respectively, where x and y are respectively an instance and its class label. In long-tailed visual recognition, the marginal class distribution P s (y) of the source domain is long-tailed, and yet the class distribution P t (y) of the target domain is more balanced, e.g., a uniform distribution.</p><p>In generic domain adaptation, there could be multiple causes of mismatches between two domains. Covariate shift <ref type="bibr" target="#b44">[46]</ref> causes domain discrepancy on the marginal distribution of input, i.e., P s (x) = P t (x), but often maintains the same predictive function across the domains, i.e., P s (y|x) = P t (y|x). Under the target-shift cause <ref type="bibr" target="#b56">[58]</ref>, the domains differ only by the class distributions, i.e., P s (y) = P t (y) and P s (x|y) = P t (x|y), partially explaining the rationale of designing class-balanced weights to tackle the long-tail challenge <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b60">62,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr">3,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b12">14]</ref>.</p><p>These class-balanced methods enable the tail classes to play a bigger role than their sizes suggest in determining the model's decision boundaries. The class-wise weights are inversely related to the class sizes <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b36">38]</ref>. Alternatively, one can derive these weights from the cost of misclassifying an example of one class to another <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b60">62]</ref>. Cui et al. proposed an interesting weighting scheme by counting the "effective number" of examples per class <ref type="bibr" target="#b5">[7]</ref>. Finally, over/under-sampling head/tail classes <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr">3,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b12">14]</ref> effectively belongs to the same family as the class-balanced weights, although they lead to practically different training algorithms. Section 2 reviews other methods for coping with the long-tail challenge.</p><p>One the one hand, the plethora of works reviewed above indicate that the target shift, i.e., P s (y) = P t (y) and P s (x|y) = P t (x|y), is generally a reasonable assumption based on which one can design effective algorithms for learning unbiased models from a training set with a longtailed class distribution. On the other hand, however, our intuition challenges the second part of the target shift assumption; in other words, P s (x|y) = P t (x|y) may not hold. While a head class (e.g., Dog) of the training set could contain abundant and diverse examples that well represent the expected data at inference time, the tail classes (e.g., King Eider) are often short of representative training examples. As a result, training examples drawn from the conditional distribution P s (x|Dog) of the source domain can probably well approximate the conditional distribution P t (x|Dog) of the target domain, but the discrepancy between the conditional distributions P s (x|King Eider) and P t (x|King Eider) of the two domains is likely big because it is hard to collect training examples for King Eider (cf. <ref type="figure">Figure 1</ref>).</p><p>To this end, we propose to augment the class-balanced learning by relaxing the assumption that the source and target domains share the same conditional distributions P s (x|y) and P t (x|y). By explicitly accounting for the differences between them, we arrive at a two-component weight for each training example. The first part is inherited from the classic class-wise weighting, carrying on its effectiveness in various applications. The second part corresponds to the conditional distributions, and we estimate it by the meta-learning framework of learning to re-weight examples <ref type="bibr" target="#b41">[43]</ref>. We make two critical improvements over this framework. One is that we can initialize the weights close to the optima because we have substantial prior knowledge about the two-component weights as a result of our analysis of the long-tailed problem. The other is that we remove two constraints from the framework such that the search space is big enough to cover the optima with a bigger chance.</p><p>We conduct extensive experiments on several datasets, including both long-tailed CIFAR <ref type="bibr" target="#b29">[31]</ref>, ImageNet <ref type="bibr" target="#b8">[10]</ref>, and Places-2 <ref type="bibr" target="#b59">[61]</ref>, which are artificially made long-tailed <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b34">36]</ref>, and iNaturalist 2017 and 2018 <ref type="bibr" target="#b49">[51,</ref><ref type="bibr">1]</ref>, which are longtailed by nature. We test our approach with three different losses (cross-entropy, focal loss <ref type="bibr" target="#b32">[34]</ref>, and a labeldistribution-aware margin loss <ref type="bibr" target="#b2">[4]</ref>). Results validate that our two-component weighting is advantageous over the class-balanced methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Our work is closely related to the class-balanced methods briefly reviewed in Section 1. In this section, we discuss domain adaptation and the works of other types for tackling the long-tailed visual recognition.</p><p>Metric learning, hinge loss, and head-to-tail knowledge transfer. Hinge loss and metric learning are flexible tools for one to handle the long-tailed problem <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b55">57,</ref><ref type="bibr" target="#b57">59,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b52">54]</ref>. They mostly contain two major steps. One is to sample or group the data being aware of the long-tailed property, and the other is to construct large-margin losses. Our approach is loss-agnostic, and we show it can benefit different loss functions in the experiments. Another line of research is to transfer knowledge from the head classes to the tail. Yin et al. transfer intra-class variance from the head to tail <ref type="bibr" target="#b54">[56]</ref>, Liu et al. add a memory module to the neural networks to transfer semantic features <ref type="bibr" target="#b34">[36]</ref>, and Wang et al. employ a meta network to regress network weights between different classes <ref type="bibr" target="#b51">[53]</ref>.</p><p>Hard example mining and weighting. Hard example mining is prevalent and effective in object detection <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b32">34]</ref>. While it is not particularly designed for the longtailed recognition, it can indirectly shift the model's focus to the tail classes, from which the hard examples usually originate (cf. <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b14">16]</ref> and our experiments). Nonetheless, such methods could be sensitive to outliers or unnecessarily allow a minority of examples to dominate the training. The recently proposed instance weighting by meta-learning methods <ref type="bibr" target="#b41">[43,</ref><ref type="bibr" target="#b45">47]</ref> alleviate those issues. Following the general meta-learning principle <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b31">33]</ref>, they set aside a validation set to guide how to weigh the training examples by gradient descent. Similar schemes are used in learning from noisy data <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b50">52]</ref>.</p><p>Domain adaptation. In real-world applications, there often exist mismatches between the distributions of training data and test data for various reasons <ref type="bibr" target="#b47">[49,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b58">60]</ref>. Domain adaptation methods aim to mitigate the mismatches so that the learned models can generalize well to the inferencetime data <ref type="bibr" target="#b44">[46,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b18">20]</ref>. There are some approaches that handle the imbalance problem in domain adaptation. Zou et al. <ref type="bibr" target="#b61">[63]</ref> deal with the class imbalance by controlling the pseudo-label learning and generation using the confidence scores that are normalized class-wise. Yan et al. <ref type="bibr" target="#b53">[55]</ref> use a weighted maximum mean discrepancy to handle the class imbalance in unsupervised domain adaptation. We understand the long-tail challenge in visual recognition from the perspective of domain adaptation. While domain adaptation methods need to access a large amount of unlabeled (and sometimes also a small portion of labeled) target domain data, we do not access any inference-time data in our approach. Unlike existing weighting methods in domain adaptation <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b56">58]</ref>, we meta-learn the weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Class balancing as domain adaptation</head><p>In this section, we present a detailed analysis of the classbalanced methods <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b37">39]</ref> for long-tailed visual recognition from the domain adaptation point of view.</p><p>Suppose we have a training set (source domain)</p><formula xml:id="formula_0">{(x i , y i )} n i=1 drawn i.i.d</formula><p>. from a long-tailed distribution P s (x, y) -more precisely, the marginal distribution P s (y) of classes are heavy-tailed because, in visual recognition, it is often difficult to collect examples for rare classes. Nonetheless, we expect to learn a visual recognition model to make as few mistakes as possible on all classes:</p><formula xml:id="formula_1">error = E Pt(x,y) L(f (x; ?), y),<label>(1)</label></formula><p>where we desire a target domain P t (x, y) whose marginal class distribution P t (y) is more balanced (e.g., a uniform distribution) at the inference time, f (?; ?) is the recognition model parameterized by ?, and L(?, ?) is a 0-1 loss. We abuse the notation L(?, ?) a little and let it be a differentiable surrogate loss (i.e., cross-entropy) during training. Next, we apply the importance sampling trick to connect the expected error with the long-tailed source domain,</p><formula xml:id="formula_2">error = E Pt(x,y) L(f (x; ?), y) (2) = E Ps(x,y) L(f (x; ?), y)P t (x, y)/P s (x, y) (3) = E Ps(x,y) L(f (x; ?), y) P t (y)P t (x|y) P s (y)P s (x|y)<label>(4)</label></formula><formula xml:id="formula_3">:= E Ps(x,y) L(f (x; ?), y)w y (1 +? x,y ),<label>(5)</label></formula><p>where w y = P t (y)/P s (y) and? x,y = P t (x|y)/P s (x|y)?1.</p><p>Existing class-balanced methods focus on how to determine the class-wise weights {w y } and result in the follow-ing objective function for training,</p><formula xml:id="formula_4">min ? 1 n n i=1 w yi L(f (x i ; ?), y i ),<label>(6)</label></formula><p>which approximates the expected inference error (eq. (5)) by assuming? x,y = 0 or, in other words, by assuming P s (x|y) = P t (x|y) for any class y. This assumption is referred to as target shift <ref type="bibr" target="#b56">[58]</ref> in domain adaptation. We contend that the assumption of a shared conditional distribution, P s (x|y) = P t (x|y), does not hold in general, especially for the tail classes. One may easily compile a representative training set for Dog, but not for King Eider.</p><p>We propose to explicitly model the difference? x,y between the source and target conditional distributions and arrive at an improved algorithm upon the class-balanced methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Modeling the conditional differences</head><p>For simplicity, we introduce a conditional weight x,y := w y? x,y and re-write the expected inference error as</p><formula xml:id="formula_5">error = E Ps(x,y) L(f (x; ?), y)(w y + x,y ) (7) ? 1 n n i=1 (w yi + i )L(f (x i ; ?), y i ),<label>(8)</label></formula><p>where the last term is an unbiased estimation of the error.</p><p>Notably, we do not make the assumption that the conditional distributions of the source and target domains are the same, i.e., we allow P s (x|y) = P t (x|y) and i = 0. Hence, the weight for each training example consists of two parts. One component is the class-wise weight w yi , and the other is the conditional weight i . We need to estimate both components to derive a practical algorithm from eq. (8) because the underlying distributions of data are unknownalthough we believe the class distribution of the training set must be long-tailed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Estimating the class-wise weights {w y }</head><p>We let the class-wise weights resemble the empirically successful design in the literature. In particular, we estimate them by the recently proposed "effective numbers" <ref type="bibr" target="#b5">[7]</ref>. Supposing there are n y training examples for the y-th class,</p><formula xml:id="formula_6">we have w y ? (1 ? ?)/(1 ? ? ny ) where ? ? [0, 1)</formula><p>is a hyper-parameter with the recommended value ? = (n ? 1)/n, and n is the number of training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Meta-learning the conditional weights { i }</head><p>We estimate the conditional weights by customizing a meta-learning framework <ref type="bibr" target="#b41">[43]</ref>. We describe our approach below and then discuss two critical differences from the original framework in Section 4.3.</p><p>The main idea is to hold out a balanced development set D from the training set and use it to guide the search for the conditional weights that give rise to the best-performing recognition model f (?; ?) on the development set. Denote by T the remaining training data. We seek the conditional weights := { i } by solving the following problem,</p><formula xml:id="formula_7">min 1 |D| i?D L(f (x i ; ? * ( )), y i ) with (9) ? * ( ) ? arg min ? 1 |T | i?T (w yi + i )L(f (x i ; ?), y i ) (10)</formula><p>where we do not weigh the losses over the development set which is already balanced. Essentially, the problem above searches for the optimal conditional weights such that, after we learn a recognition model f (?; ?) by minimizing the error estimation (eqs <ref type="formula" target="#formula_1">(10)</ref> and <ref type="formula" target="#formula_5">(8)</ref>), the model performs the best on the development set (eq. <ref type="formula">(9)</ref>). It would be daunting to solve the problem above by brute-force search, e.g., iterating all the possible sets { } of conditional weights. Even if we can, it is computationally prohibitive to train for each set of weights a recognition model f (?; ? * ( )) and then find out the best model from all.</p><p>Instead, we modify the meta-learning framework <ref type="bibr" target="#b41">[43]</ref> and search for the conditional weights in a greedy manner. It interleaves the quest for the weights with the updates to the model parameters ?, given current time step t,</p><formula xml:id="formula_8">? t+1 ( t ) ? ? t ? ? ? i?T (w yi + t i )L(f (x i ; ? t ), y i ) ?? t+1 ? t ? ? ? i?D L(f (x i ;? t+1 ( t )), y i ) ? ? t+1 ? ? t ? ? ? i?T (w yi + t+1 i )L(f (x i ; ? t ), y i ) ?? .</formula><p>The first equation tries a one-step gradient descent for ? t using the losses weighted by the current conditional weights t (plus the class-wise weights). The updated model parameters? t+1 ( t ) are then scrutinized on the balanced development set D, which updates the conditional weights by one step. The updated weights t+1 are better than the old ones, meaning that the model parameters ? t+1 returned by the last equation should give rise to smaller recognition error on the development set than? t+1 do. Starting from ? t+1 and t+1 , we then move on to the next round of updates. We present our overall algorithm in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Overall algorithm and discussion</head><p>We are ready to present Algorithm 1 for long-tailed visual recognition. The discussions in the previous sections consider all the training examples in a batch setting. Algorithm 1 customizes it into a stochastic setting so that we can easily integrate it with deep learning frameworks.</p><p>There are two learning stages in the algorithm. In the first stage (lines 1-5), we train the neural recognition network f (?; ?) by using the conventional cross-entropy loss Algorithm 1 Meta-learning for long-tailed recognition Require: Training set T , balanced development set D Require: Class-wise weights {w y } estimated by using <ref type="bibr" target="#b5">[7]</ref> Require: Learning rates ? and ? , stopping steps t 1 and t 2 Require: Initial parameters ? of the recognition network 1: for t = 1, 2, ? ? ? , t 1 do 2:</p><p>Sample a mini-batch B from the training set T 3:</p><formula xml:id="formula_9">Compute loss L B = 1 |B| i?B L(f (x i ; ?), y i ) 4:</formula><p>Update ? ? ? ? ?? ? L B 5: end for 6: for t = t 1 + 1, ? ? ? , t 1 + t 2 do 7:</p><p>Sample a mini-batch B from the training set T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Set i ? 0, ?i ? B, and denote by :</p><formula xml:id="formula_10">= { i , i ? B} 9: Compute L B = 1 |B| i?B (w yi + i )L(f (x i ; ?), y i ) 10: Update?( ) ? ? ? ?? ? L B</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>Sample B d from the balanced development set D 12:</p><formula xml:id="formula_11">Compute L B d = 1 |B d | i?B d L(f (x i ;?( )), y i ) 13: Update ? ? ? ? L B d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>Compute new loss with the updated</p><formula xml:id="formula_12">L B = 1 |B| i?B (w yi + i )L(f (x i ; ?), y i ) 15:</formula><p>Update ? ? ? ? ?? ?LB 16: end for over the long-tailed training set. The second stage (lines 6-16) meta-learns the conditional weights by resorting to a balanced development set and meanwhile continues to update the recognition model. We highlight the part for updating the conditional weights in lines 11-13.</p><p>Discussion. It is worth noting some seemingly small and yet fundamental differences between our algorithm and the learning to re-weight (L2RW) method <ref type="bibr" target="#b41">[43]</ref>. Conceptually, while we share the same meta-learning framework as L2RW, both the class-wise weight, w y = P t (y)/P s (y), and the conditional weight, x,y = w y? x,y = P t (y)/P s (y) P t (x|y)/P s (x|y) ? 1 , have principled interpretations as oppose to a general per-example weight in L2RW. We will explore other machine learning frameworks (e.g., <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b46">48]</ref>) to learn the conditional weights in future work, but the interpretations of them remain the same.</p><p>Algorithmically, unlike L2RW, we employ twocomponent weights, estimate the class-wise components by a different method <ref type="bibr" target="#b5">[7]</ref>, do not clip negative weights { i } to 0, and do not normalize them such that they sum to 1 within a mini-batch. The clipping and normalization operations in L2RW unexpectedly reduce the search space of the weights, and the normalization is especially troublesome as it depends on the mini-batch size. Hence, if the optimal weights actually lie outside of the reduced search space, there is no chance to hit the optima by L2RW. In contrast, our algorithm searches for each conditional weight i in the full real space. One may wonder whether or not our total effective weight, w yi + i , could become negative. Careful investigation reveals that it never goes below 0 in our experiments, likely due to that the good initialization (as explained below) to the conditional weights makes it unnecessary to update the weights too wildly by line 13 in Algorithm 1.</p><p>Computationally, we provide proper initialization to both the conditional weights, by i ? 0 (line 8), and the model parameters ? of the recognition network, by pre-training the network with a vanilla cross-entropy loss (lines 1-5). As a result, our algorithm is more stable than L2RW (cf. Section 5.1). Note that 0 is a reasonable a priori value for the conditional weights thanks to the promising results obtained by existing class-balanced methods. Those methods assume that the discrepancy is as small as 0 between the conditional distributions of the source and target domains, meaning that P t (x|y)/P s (x|y) ? 1 is close to 0, so are the conditional weights { i }. Hence, our approach should perform at worst the same as the class-balanced method <ref type="bibr" target="#b5">[7]</ref> by initializing the conditional weights to 0 (and the class-wise weights by <ref type="bibr" target="#b5">[7]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Datasets. We evaluate and ablate our approach on six datasets of various scales, ranging from the manually created long-tailed CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b5">[7]</ref>, ImageNet-LT, and Places-LT <ref type="bibr" target="#b34">[36]</ref>, to the naturally long-tailed iNaturalist 2017 <ref type="bibr" target="#b49">[51]</ref> and 2018 [1]. Following <ref type="bibr" target="#b5">[7]</ref>, we define the imbalance factor (IF) of a dataset as the class size of the first head class divided by the size of the last tail class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Long-Tailed CIFAR (CIFAR-LT):</head><p>The original CIFAR-10 (CIFAR-100) dataset contains 50,000 training images and 10,000 test images of size 32x32 uniformly falling into 10 (100) classes <ref type="bibr" target="#b29">[31]</ref>. Cui et al. <ref type="bibr" target="#b5">[7]</ref> created long-tailed versions by randomly removing training examples. In particular, the number of examples dropped from the y-th class is n y ? y , where n y is the original number of training examples in the class and ? ? (0, 1). By varying ?, we arrive at six training sets, respectively, with the imbalance factors (IFs) of 200, 100, 50, 20, 10, and 1, where IF=1 corresponds to the original datasets. We do not change the test sets, which are balanced. We randomly select ten training images per class as our development set D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet-LT:</head><p>In spirit similar to the long-tailed CIFAR datasets, Liu et al. <ref type="bibr" target="#b34">[36]</ref> introduced a long-tailed version of ImageNet-2012 <ref type="bibr" target="#b8">[10]</ref> called ImageNet-LT. It is created by firstly sampling the class sizes from a Pareto distribution with the power value ? = 6, followed by sampling the corresponding number of images for each class. The resultant dataset has 115.8K training images in 1,000 classes, and its imbalance factor is 1280/5. The authors have also provided a validation set with 20 images per class, from which we sample ten images to construct our development set D. The original balanced ImageNet-2012 validation set is used as the test set (50 images per class). Places-LT: Liu et al. <ref type="bibr" target="#b34">[36]</ref> have also created a Places-LT dataset by sampling from Places-2 <ref type="bibr" target="#b59">[61]</ref> using the same strategy as above. It contains 62.5K training images from 365 classes with an imbalance factor 4980/5. This large imbalance factor indicates that it is more challenging than ImageNet-LT. Places-LT has 20 (100) validation (test) images per class. Our development set D contains ten images per class randomly selected from the validation set. iNaturalist (iNat) 2017 and 2018: The iNat 2017 <ref type="bibr" target="#b49">[51]</ref> and 2018 [1] are real-world fine-grained visual recognition datasets that naturally exhibit long-tailed class distributions. iNat 2017 (2018) consists of 579,184 (435,713) training images in 5,089 (8,142) classes, and its imbalance factor is 3919/9 (1000/2). We use the official validation sets to test our approach. We select five (two) images per class from the training set of iNat 2017 (2018) for the development set. <ref type="table" target="#tab_1">Table 1</ref> gives an overview of the six datasets used in the following experiments.</p><p>Evaluation Metrics. As the test sets are all balanced, we simply use the top-k error as the evaluation metric. We report results for k = 1, 3, 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Object recognition with CIFAR-LT</head><p>We run both comparison experiments and ablation studies with CIFAR-LT-10 and CIFAR-LT-100. We use ResNet-32 <ref type="bibr" target="#b23">[25]</ref> in the experiments.</p><p>Competing methods. We compare our approach to the following competing ones.</p><p>? Cross-entropy training. This is the baseline that trains ResNet-32 using the vanilla cross-entropy loss. ? Class-balanced loss <ref type="bibr" target="#b5">[7]</ref>. It weighs the conventional losses by class-wise weights, which are estimated based on effective numbers. We apply this classbalanced weighting to three different losses: crossentropy, the focal loss <ref type="bibr" target="#b32">[34]</ref>, and the recently proposed label-distribution-aware margin loss <ref type="bibr" target="#b2">[4]</ref>. ? Focal loss <ref type="bibr" target="#b32">[34]</ref>. The focal loss can be understood as a smooth version of hard example mining. It does not directly tackle the long-tailed recognition problem. However, it can penalize the examples of tail classes more than those of the head classes if the network is biased toward the head classes during training. ? Label-distribution-aware margin loss <ref type="bibr" target="#b2">[4]</ref>. It dynamically tunes the margins between classes according to their degrees of dominance in the training set. ? Class-balanced fine-tuning <ref type="bibr" target="#b6">[8]</ref>. The main idea is to first train the neural network with the whole imbalanced training set and then fine-tune it on a balanced subset of the training set. ? Learning to re-weight (L2RW) <ref type="bibr" target="#b41">[43]</ref>. It weighs training examples by meta-learning. Please see Section 4.3 for more discussions about L2RW and our approach. ? Meta-weight net <ref type="bibr" target="#b45">[47]</ref>. Similarly to L2RW, it also weighs examples by a meta-learning method except that it regresses the weights by a multilayer perceptron.</p><p>Implementation details. For the first two baselines, we use the code of <ref type="bibr" target="#b5">[7]</ref> to set the learning rates and other hyperparameters. We train the L2RW model using an initial learning rate of 1e-3. We decay the learning rate by 0.01 at the 160th and 180th epochs. For our approach, we use an initial learning rate of 0.1 and then also decay the learning rate at the 160th and 180th epochs by 0.01. The batch size is 100 for all experiments. We train all models on a single GPU using the stochastic gradient descent with momentum.</p><p>Results. <ref type="table" target="#tab_2">Table 2</ref> shows the classification errors of ResNet-32 on the long-tailed CIFAR-10 with different imbalance factors. We group the competing methods into three sessions according to which basic loss they use (crossentropy, focal <ref type="bibr" target="#b32">[34]</ref>, or LDAM <ref type="bibr" target="#b2">[4]</ref>). We test our approach with all three losses. We can see that our method outperforms the competing ones in each session by notable margins. Although the focal loss and the LDAM loss already have the capacity of mitigating the long-tailed issue, respectively, by penalizing hard examples and by distributionaware margins, our method can further boost their performances. In general, the advantages of our approach over existing ones become more significant as the imbalance factor increases. When the dataset is balanced (the last column), our approach does not hurt the performance of vanilla losses compared to L2RW. We can draw about the same observations as above for the long-tailed CIFAR-100 from <ref type="table" target="#tab_3">Table 3</ref>.</p><p>Where does our approach work? <ref type="figure" target="#fig_0">Figure 2</ref> presents three confusion matrices respectively by the models of the cross-entropy training, L2RW, and our method on CIFAR-LT-10. The imbalance factor is 200. Compared with the cross-entropy model, L2RW improves the accuracies on the tail classes and yet sacrifices the accuracies for the head classes. In contrast, ours maintains about the same performance as the cross-entropy model on the head classes and meanwhile significantly improves the accuracies for the last five tail classes.</p><p>What are the learned conditional weights? We are interested in examining the conditional weights { i } for each class throughout the training. For a visualization purpose, we average them within each class. <ref type="figure" target="#fig_1">Figure 3</ref> demonstrates how they change over the last 20 epochs for the 1st, 4th, 7th, and 10th classes of CIFAR-LT-10. The two panels correspond to the imbalance factors of 100 and 10, respectively. Interestingly, the learned conditional weights of the tail classes are more prominent than those of the head classes in most epochs. Moreover, the conditional weights of the two head classes (the 1st and 4th) are even below 0 at certain epochs. Such results verify our intuition that the scarce training examples of the tail classes deserve more attention in training to make the neural network perform in a balanced fashion at the test phase.</p><p>Ablation study: ours vs. L2RW. Our overall algorithm differs from L2RW mainly in four ways: 1) pre-training the network, 2) initializing the weights by a priori knowledge, 3) two-component weights, and estimating the class-wise components by a separate algorithm <ref type="bibr" target="#b5">[7]</ref>, and 4) no clipping or normalization of the weights. <ref type="table" target="#tab_4">Table 5</ref> examines these components by applying them one after another to L2RW. First, pre-training the neural network boosts the performance of the vanilla L2RW. Second, if we initialize the sample weights by our class-wise weights {w y }, the errors increase a little probably because the clipping and normalization steps in L2RW require more careful initialization to the sample weights. Third, if we replace the sample weights by our two-component weights, we can bring the performance of L2RW closer to ours. Finally, after we remove the clipping and normalization, we arrive at our algorithm, which gives rise to the best results among all variations.</p><p>Ablation study: the two-component weights. By <ref type="table" target="#tab_4">Table 5</ref>, we also highlight the importance of the twocomponent weights {w yi + i } motivated from our domain adaptation point of view to the long-tailed visual recognition. First of all, they benefit L2RW (comparing "L2RW, pre-training, w yi + i " with "L2RW, pre-training" in <ref type="table" target="#tab_4">Table 5</ref>). Besides, they are also vital for our approach. If we drop the class-wise weights, our results would be about the same as L2RW with pre-training. If we drop the conditional weights and meta-learn the class-wise weights (cf. "Ours   <ref type="table" target="#tab_4">Table 5</ref>), implying that the learned class-wise weights give rise to better models than the effective-number-based class-wise weights <ref type="bibr" target="#b5">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Object recognition with iNat 2017 and 2018</head><p>We use ResNet-50 <ref type="bibr" target="#b23">[25]</ref> as the backbone network for the iNat 2017 and 2018 datasets. The networks are pre-trained on ImageNet for iNat 2017 and on ImageNet plus iNat 2017 for iNat 2018. We experiment with the mini-batch size of 64 and the learning rate of 0.01. We train all the models using the stochastic gradient descent with momentum. For the meta-learning stage of our approach, we switch to a small learning rate, 0.001. <ref type="table">Table 4</ref> shows the results of our two-component weighting applied to the cross-entropy loss. We shrink the text size for iNat 2018 to signify that we advocate experiments with iNat 2017 instead because there are only three validation/test images per class in iNat 2018 (cf. <ref type="table" target="#tab_1">Table 1</ref>). Our ap-   proach boosts the cross-entropy training by about 2% more than the class-balanced weighting <ref type="bibr" target="#b5">[7]</ref> does. As we have reported similar effects for the focal loss and the LDAM loss on CIFAR-LT with extensive experiments, we do not run them on the large-scale iNat datasets to save computation costs. Nonetheless, we include the results reported in the literature of the focal loss, LDAM loss, and a classifier retraining method <ref type="bibr" target="#b28">[30]</ref>, which was published after we submitted the work to CVPR 2020.  <ref type="bibr" target="#b34">[36]</ref>, we employ ResNet-32 and ResNet-152 for the experiments on ImageNet-LT and Places-LT, respectively. For ImageNet-LT, we adopt an initial learning rate of 0.1 and decay it by 0.1 after every 35 epochs. For Places-LT, the initial learning rate is 0.01 and is decayed by 0.1 every 10 epochs. For our own approach, we switch from the cross-entropy training to the meta-learning stage when the first decay of the learning rate happens. The mini-batch size is 64, and the optimizer is stochastic gradient descent with momentum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experiments with ImageNet-LT and Places-LT</head><p>Results. <ref type="table" target="#tab_5">Table 6</ref> shows that the class-balanced training improves the vanilla cross-entropy results, and our twocomponent weighting further boosts the results. We expect the same observation with the focal and LDAM losses. Finally, we find another improvement by updating the classification layers only in the meta-learning stage. We arrive at 62.90% top-1 error (39.86/29.87% top-3/5 error) on Places-LT, which is on par with 64.1% by OLTR <ref type="bibr" target="#b34">[36]</ref> or 63.3% by cRT <ref type="bibr" target="#b28">[30]</ref>, while noting that our two-component weighting can be conveniently applied to both OLTR and cRT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we make two major contributions to the long-tailed visual recognition. One is the novel domain adaptation perspective for analyzing the mismatch problem in long-tailed classification. While the training set of real-world objects is often long-tailed with a few classes that dominate, we expect the learned classifier to perform equally well in all classes. By decomposing this mismatch into class-wise differences and the discrepancy between class-conditioned distributions, we uncover the implicit assumption behind existing class-balanced methods, that the training and test sets share the same class-conditioned distribution. Our second contribution is to relax this assumption to explicitly model the ratio between two classconditioned distributions. Experiments on six datasets verify the effectiveness of our approach.</p><p>Future work. We shall explore other techniques <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b46">48]</ref> for estimating the conditional weights. In addition to the weighting scheme, other domain adaptation methods <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b4">6]</ref>, such as learning domain-invariant features <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b48">50]</ref> and data sampling strategies <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b40">42]</ref>, may also benefit the longtailed visual recognition problem. Especially, the domaininvariant features align well with Kang et al.'s recent work on decoupling representations and classifications for longtailed classification <ref type="bibr" target="#b28">[30]</ref>.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Confusion matrices by the cross-entropy training, L2RW, and our method on CIFAR-LT-10 (the imbalance factor is 200).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Mean conditional weights { i} within each class vs. training epochs on CIFAR-LT-10 (left: IF = 100; right: IF = 10).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Following</head><label></label><figDesc>Liu et al.'s experiment setup</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Common Slider King Eider Training Test</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Overview of the six datasets used in our experiments. (IF stands for the imbalance factor) Dataset # Classes IF # Train. img. Tail class size Head class size # Val. img. # Test img.</figDesc><table><row><cell>CIFAR-LT-10</cell><cell cols="3">10 1.0-200.0 50,000-11,203</cell><cell>500-25</cell><cell>5,000</cell><cell>-</cell><cell>10,000</cell></row><row><cell>CIFAR-LT-100</cell><cell cols="2">100 1.0-200.0</cell><cell>50,000-9,502</cell><cell>500-2</cell><cell>500</cell><cell>-</cell><cell>10,000</cell></row><row><cell>iNat 2017</cell><cell>5,089</cell><cell>435.4</cell><cell>579,184</cell><cell>9</cell><cell>3,919</cell><cell>95,986</cell><cell>-</cell></row><row><cell>iNat 2018</cell><cell>8,142</cell><cell>500.0</cell><cell>437,513</cell><cell>2</cell><cell>1,000</cell><cell>24,426</cell><cell>-</cell></row><row><cell>ImageNet-LT</cell><cell>1,000</cell><cell>256.0</cell><cell>115,846</cell><cell>5</cell><cell>1,280</cell><cell>20,000</cell><cell>50,000</cell></row><row><cell>Places-LT</cell><cell>365</cell><cell>996.0</cell><cell>62,500</cell><cell>5</cell><cell>4,980</cell><cell>7,300</cell><cell>36,500</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Test top-1 errors (%) of ResNet-32 on CIFAR-LT-10 under different imbalance settings. * indicates results reported in [47]. 29.64 25.19 17.77 13.61 7.53/7.11* Class-balanced cross-entropy loss [7] 31.11 27.63 21.95 15.64 13.23 7.53/7.11*</figDesc><table><row><cell>Imbalance factor</cell><cell>200</cell><cell>100</cell><cell>50</cell><cell>20</cell><cell>10</cell><cell>1</cell></row><row><cell cols="2">Cross-entropy training 34.32 Class-balanced fine-tuning [8] 33.76</cell><cell>28.66</cell><cell>22.56</cell><cell>16.78</cell><cell>16.83</cell><cell>7.08</cell></row><row><cell>Class-balanced fine-tuning [8]*</cell><cell>33.92</cell><cell>28.67</cell><cell>22.58</cell><cell>13.73</cell><cell>13.58</cell><cell>6.77</cell></row><row><cell>L2RW [43]</cell><cell>33.75</cell><cell>27.77</cell><cell>23.55</cell><cell>18.65</cell><cell>17.88</cell><cell>11.60</cell></row><row><cell>L2RW [43]*</cell><cell>33.49</cell><cell>25.84</cell><cell>21.07</cell><cell>16.90</cell><cell>14.81</cell><cell>10.75</cell></row><row><cell>Meta-weight net [47]</cell><cell cols="3">32.8 26.43 20.9</cell><cell cols="3">15.55 12.45 7.19</cell></row><row><cell>Ours with cross-entropy loss</cell><cell cols="6">29.34 23.59 19.49 13.54 11.15 7.21</cell></row><row><cell>Focal loss [34]</cell><cell cols="6">34.71 29.62 23.29 17.24 13.34 6.97</cell></row><row><cell>Class-balanced focal Loss [7]</cell><cell cols="6">31.85 25.43 20.78 16.22 12.52 6.97</cell></row><row><cell>Ours with focal Loss</cell><cell cols="2">25.57 21.1</cell><cell cols="2">17.12 13.9</cell><cell cols="2">11.63 7.19</cell></row><row><cell>LDAM loss [4] (results reported in paper)</cell><cell>-</cell><cell cols="2">26.65 -</cell><cell>-</cell><cell cols="2">13.04 11.37</cell></row><row><cell>LDAM-DRW [4] (results reported in paper)</cell><cell>-</cell><cell cols="2">22.97 -</cell><cell>-</cell><cell cols="2">11.84 -</cell></row><row><cell>Ours with LDAM loss</cell><cell cols="2">22.77 20.0</cell><cell cols="3">17.77 15.63 12.6</cell><cell>10.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell cols="8">Test top-1 errors (%) of ResNet-32 on CIFAR-LT-100 under different imbalance settings. * indicates results reported in [47].</cell></row><row><cell></cell><cell cols="2">Imbalance factor</cell><cell></cell><cell></cell><cell>200</cell><cell>100</cell><cell>50</cell><cell>20</cell><cell>10</cell><cell>1</cell></row><row><cell></cell><cell cols="2">Cross-entropy training</cell><cell></cell><cell></cell><cell cols="3">65.16 61.68 56.15 48.86 44.29 29.50</cell></row><row><cell></cell><cell cols="4">Class-balanced cross-entropy loss [7]</cell><cell cols="3">64.30 61.44 55.45 48.47 42.88 29.50</cell></row><row><cell></cell><cell cols="3">Class-balanced fine-tuning [8]</cell><cell></cell><cell>61.34</cell><cell>58.5</cell><cell>53.78</cell><cell>47.70</cell><cell>42.43</cell><cell>29.37</cell></row><row><cell></cell><cell cols="3">Class-balanced fine-tuning [8]*</cell><cell></cell><cell>61.78</cell><cell>58.17</cell><cell>53.60</cell><cell>47.89</cell><cell>42.56</cell><cell>29.28</cell></row><row><cell></cell><cell>L2RW [43]</cell><cell></cell><cell></cell><cell></cell><cell>67.00</cell><cell>61.10</cell><cell>56.83</cell><cell>49.25</cell><cell>47.88</cell><cell>36.42</cell></row><row><cell></cell><cell>L2RW [43]*</cell><cell></cell><cell></cell><cell></cell><cell>66.62</cell><cell>59.77</cell><cell>55.56</cell><cell>48.36</cell><cell>46.27</cell><cell>35.89</cell></row><row><cell></cell><cell cols="2">Meta-weight net [47]</cell><cell></cell><cell></cell><cell cols="3">63.38 58.39 54.34 46.96 41.09 29.9</cell></row><row><cell></cell><cell cols="3">Ours with cross-entropy loss</cell><cell></cell><cell cols="3">60.69 56.65 51.47 44.38 40.42 28.14</cell></row><row><cell></cell><cell cols="2">Focal Loss [34]</cell><cell></cell><cell></cell><cell cols="3">64.38 61.59 55.68 48.05 44.22 28.85</cell></row><row><cell></cell><cell cols="3">Class-balanced focal Loss [7]</cell><cell></cell><cell cols="3">63.77 60.40 54.79 47.41 42.01 28.85</cell></row><row><cell></cell><cell cols="2">Ours with focal loss</cell><cell></cell><cell></cell><cell cols="2">60.66 55.3</cell><cell>49.92 44.27 40.41 29.15</cell></row><row><cell></cell><cell cols="4">LDAM Loss [4] (results reported in paper)</cell><cell>-</cell><cell cols="2">60.40 -</cell><cell>-</cell><cell>43.09 -</cell></row><row><cell></cell><cell cols="4">LDAM-DRW [4] (results reported in paper)</cell><cell>-</cell><cell cols="2">57.96 -</cell><cell>-</cell><cell>41.29 -</cell></row><row><cell></cell><cell cols="2">Ours with LDAM loss</cell><cell></cell><cell></cell><cell cols="3">60.47 55.92 50.84 47.62 42.0</cell><cell>-</cell></row><row><cell cols="5">Table 4. Classification errors on iNat 2017 and 2018. (*results</cell><cell></cell><cell></cell></row><row><cell cols="5">reported in paper. CE=cross-entropy, CB=class-balanced)</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell cols="2">iNat 2017</cell><cell cols="2">iNat 2018</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Top-1</cell><cell>Top-3/5</cell><cell>Top-1</cell><cell>Top-3/5</cell><cell></cell><cell></cell></row><row><cell>CE</cell><cell cols="4">43.49 26.60/21.00 36.20 19.40/15.85</cell><cell></cell><cell></cell></row><row><cell>CB CE [7]</cell><cell cols="4">42.59 25.92/20.60 34.69 19.22/15.83</cell><cell></cell><cell></cell></row><row><cell>Ours, CE</cell><cell cols="4">40.62 23.70/18.40 32.45 18.02/13.83</cell><cell></cell><cell></cell></row><row><cell cols="2">CB focal [7]* 41.92</cell><cell>-/20.92</cell><cell>38.88</cell><cell>-/18.97</cell><cell></cell><cell></cell></row><row><cell>LDAM [4]*</cell><cell>-</cell><cell>-</cell><cell>35.42</cell><cell>-/16.48</cell><cell></cell><cell></cell></row><row><cell>LDAM-drw*</cell><cell>-</cell><cell>-</cell><cell>32.00</cell><cell>-/14.82</cell><cell></cell><cell></cell></row><row><cell>cRT [30]*</cell><cell>-</cell><cell>-</cell><cell>34.8</cell><cell>-</cell><cell></cell><cell></cell></row><row><cell>cRT+epochs*</cell><cell>-</cell><cell>-</cell><cell>32.4</cell><cell>-</cell><cell></cell><cell></cell></row><row><cell cols="5">updating w y "), the errors become larger than our original</cell><cell></cell><cell></cell></row><row><cell cols="5">algorithm. Nonetheless, the results are better than the class-</cell><cell></cell><cell></cell></row><row><cell cols="3">balanced training (cf. last row in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation study of our approach by using the crossentropy loss on CIFAR-LT-<ref type="bibr" target="#b8">10</ref>. The results are test top-1 errors%. -training, init. by w y 26.26 22.50 17.44 L2RW, pre-training, w yi + i 24.54 20.47 14.38 Ours 23.59 19.49 13.54 Ours updating w y 25.42 20.13 15.62 Class-balanced [7] 27.63 21.95 15.64</figDesc><table><row><cell>Imbalance factor</cell><cell>100</cell><cell>50</cell><cell>20</cell></row><row><cell>L2RW [43]</cell><cell cols="3">27.77 23.55 18.65</cell></row><row><cell>L2RW, pre-training</cell><cell cols="3">25.96 22.04 15.67</cell></row><row><cell>L2RW, pre</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Classification errors on ImageNet-LT and Places-LT. 61.35/52.12 73.00 52.05/41.44 CB CE [7] 73.41 59.22/50.49 71.14 51.58/41.96 Ours, CE 70.10 53.29/45.18 69.20 47.95/38.00</figDesc><table><row><cell cols="5">(*reported in paper. CE=cross-entropy, CB=class-balanced)</cell></row><row><cell>Dataset</cell><cell cols="2">ImageNet-LT</cell><cell cols="2">Places-LT</cell></row><row><cell>Method</cell><cell>Top-1</cell><cell>Top-3/5</cell><cell>Top-1</cell><cell>Top-3/5</cell></row><row><cell>CE</cell><cell>74.74</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Multiple runs of our approach by using the cross-entropy loss on CIFAR-LT-10. The results are top-1 errors% on the test sets. Ours with cross-entropy loss 29.32 ? 0.23 23.71 ? 0.22 19.45 ? 0.28</figDesc><table><row><cell>Imbalance factor</cell><cell>200</cell><cell>100</cell><cell>50</cell></row><row><cell>Cross-entropy training</cell><cell>34.32</cell><cell>29.64</cell><cell>25.19</cell></row><row><cell>Class-balanced cross-entropy loss [7]</cell><cell>31.11</cell><cell>27.63</cell><cell>21.95</cell></row><row><cell>Class-balanced fine-tuning [8]</cell><cell>33.76</cell><cell>28.66</cell><cell>22.56</cell></row><row><cell>Class-balanced fine-tuning [8]*</cell><cell>33.92</cell><cell>28.67</cell><cell>22.58</cell></row><row><cell>L2RW [43]</cell><cell>33.75</cell><cell>27.77</cell><cell>23.55</cell></row><row><cell>L2RW [43]*</cell><cell>33.49</cell><cell>25.84</cell><cell>21.07</cell></row><row><cell>Meta-weight net [47]</cell><cell>32.8</cell><cell>26.43</cell><cell>20.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Test top-1 errors (%) of different methods on ImageNet-LT. * indicates the re-run results.</figDesc><table><row><cell>Methods</cell><cell>NN</cell><cell>Initialization Sampling</cell><cell>Loss</cell><cell>Stage-1 Trainable Variables</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">While we include the comparison on iNaturalist 2018 due to that most existing related works report results on this dataset, we reiterate that we advocate the use of iNaturalist 2017, instead of 2018, in this and future work due to the extremely small validation set of iNaturalist 2018.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. The authors thank the support of NSF awards 1149783, 1741431, 1836881, and 1835539.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>? Multiple runs of experiments on CIFAR-LT-10 under different imbalance factors (IFs) (Section A).</p><p>? Detailed comparison of various methods on large-scale long-tailed datasets (Section B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multiple runs on CIFAR-LT-10</head><p>In this experiment, we further validate our approach by running each setting 5 times with different random seeds. <ref type="table">Table 7</ref> shows the mean top-1 errors (%) and the standard deviations under the imbalance factors of 200, 100, and 50. We can see that the mean error rates are consistent with the results provided in <ref type="table">Table 2</ref> of the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Detailed comparison of various methods on large-scale long-tailed datasets</head><p>In this section, we present the top-1 errors (%) of various methods for ImageNet-LT, Places-LT, and iNaturalist 2018 1 . As the experiment setups of the existing works vary by network initialization, the sampling strategy of minibatches, losses, trainable layers of a network, etc., it is hard to have a fair comparison by the end results. Hence, besides their top-1 errors, we also report the experiment setups for each method. <ref type="table">Tables 8, 9</ref>, and 10 show the results of the different methods on ImageNet-LT, Places-LT, and iNaturalist 2018, respectively. Our approach outperforms the classbalanced weighting scheme for both the cross-entropy loss and the focal loss, as we observed in the main paper. Moreover, our results are on par with the best reported ones except on ImageNet-LT. Finally, we stress that almost all existing methods employ a class-balanced weighting or sampling strategy no matter what their main techniques are to tackle the long-tailed problem. Hence, given our consistent improvements over the class-balanced weighting, we expect the methods which have benefited from the class-balancing can gain further from our two-component weighting. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Discriminative learning under covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Br?ckner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Scheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2009-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1106.1813</idno>
		<title level="m">SMOTE: synthetic minority oversampling technique</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with labeldistribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sample selection bias correction theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on algorithmic learning theory</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Domain adaptation in computer vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.05555</idno>
		<title level="m">Class-balanced loss based on effective number of samples</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Large scale fine-grained categorization and domain-specific transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.06193</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Jaap Kamps, and Bernhard Schlkopf. Fidelity-weighted learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Mehrjou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Class rectification hard mining for imbalanced deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03162</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">C4.5, class imbalance, and cost sensitivity: Why under-sampling beats oversampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Holte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML&apos;03 Workshop on Learning from Imbalanced Datasets</title>
		<meeting>the ICML&apos;03 Workshop on Learning from Imbalanced Datasets</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The foundations of cost-sensitive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="139" />
			<date type="published" when="1997-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning attributes equals multi-source domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.7495</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Connecting the dots with landmarks: Discriminatively learning domaininvariant features for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Overcoming dataset bias: An unsupervised domain adaptation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Large Scale Visual Recognition and Retrieval</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Domain adaptation for visual recognition. Foundations and Trends R in Computer Graphics and Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghuraman</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="285" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lvis: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Max-margin class imbalanced learning with gaussian affinity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waqas</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07711</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Correcting sample selection bias by unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Task agnostic meta-learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Abdullah</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamal</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09217</idno>
		<title level="m">Jiashi Feng, and Yannis Kalantidis. Decoupling representation and classifier for long-tailed recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Metasgd: Learning to learn quickly for few shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollr. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00932</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ensemble of exemplar-svms for object detection and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.4546</idno>
		<title level="m">Distributed representations of words and phrases and their compositionality</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Large-scale image retrieval with attentive deep local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Domain adaptation meets active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avishek</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daum?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Venkatasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 Workshop on Active Learning for Natural Language Processing</title>
		<meeting>the NAACL HLT 2010 Workshop on Active Learning for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09050</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidetoshi</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical planning and inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Meta-weight-net: Learning an explicit mapping for sample weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Covariate shift adaptation by importance weighted cross validation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Krauledat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>M??ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The iNaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Robust probabilistic modeling with bayesian data reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence K</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Mind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Feature transfer learning for face recognition with under-represented data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A scalable exemplar-based subspace clustering algorithm for class-imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Domain adaptation under target and conditional shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhikun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Range loss for deep face recognition with longtailed training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">A curriculum domain adaptation approach to the semantic segmentation of urban scenes. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Foroosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">On multi-class costsensitive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu-Ying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="232" to="257" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Bvk Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Stage-2 Trainable Variables Results Vanilla Model ResNet-10 No-pretrain Class-Balanced CE</title>
		<idno>All - 80.0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Vanilla Model [34] ResNet-10 No-pretrain Class-Balanced Focal All -69</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Vanilla Model ResNet-10 No-pretrain Class-Balanced Lifted All -69</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Vanilla Model [59] ResNet-10 No-pretrain Class-Balanced Range</title>
		<idno>All - 69.3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">No-pretrain Class-Balanced CE All All 65.2 NCM [30] ResNet-10 No-pretrain Class-Balanced CE All Classifier layer 64.5 cRT [30] ResNet-10 No-pretrain Class-Balanced CE All Classifier layer 58.2 ? -normalized</title>
		<idno>ResNet-10 No-pretrain Class-Balanced CE All Classifier layer 59.4</idno>
		<imprint>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title/>
		<idno>OLTR* [36] ResNet-10</idno>
	</analytic>
	<monogr>
		<title level="j">Class-Balanced CE All All</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">No-pretrain Class-Balanced CE All All 64.4 Ours ResNet-10 No-pretrain None CE All Classifier layer 63.5 Ours ResNet-10 No-pretrain None Focal All Classifier layer</title>
		<idno>OLTR [36] ResNet-10</idno>
		<imprint>
			<biblScope unit="volume">63</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Vanilla Model ResNet-50 No-pretrain None CE</title>
		<idno>All - 59.0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">ResNet-50 No-pretrain Class-Balanced CE All Classifier layer 53.3 Ours ResNet-50 No-pretrain None CE All Classifier</title>
		<idno>layer 52.0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Test top-1 errors (%) of different methods on Places-LT. * indicates the re-run results</title>
	</analytic>
	<monogr>
		<title level="m">Methods NN Initialization Sampling Loss Stage-1</title>
		<imprint/>
	</monogr>
	<note>Table 9</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Vanilla Model ResNet-152 ImageNet Class-Balanced Lifted FC layers -64</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Vanilla Model [59] ResNet-152 ImageNet Class-Balanced Range FC layers</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
