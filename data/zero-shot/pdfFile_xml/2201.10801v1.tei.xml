<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangting</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanxin</forename><surname>Tang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Luo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Attention mechanism has been widely believed as the key to success of vision transformers (ViTs), since it provides a flexible and powerful way to model spatial relationships. However, is the attention mechanism truly an indispensable part of ViT? Can it be replaced by some other alternatives? To demystify the role of attention mechanism, we simplify it into an extremely simple case: ZERO FLOP and ZERO parameter. Concretely, we revisit the shift operation. It does not contain any parameter or arithmetic calculation. The only operation is to exchange a small portion of the channels between neighboring features. Based on this simple operation, we construct a new backbone network, namely ShiftViT, where the attention layers in ViT are substituted by shift operations. Surprisingly, ShiftViT works quite well in several mainstream tasks, e.g., classification, detection, and segmentation. The performance is on par with or even better than the strong baseline Swin Transformer. These results suggest that the attention mechanism might not be the vital factor that makes ViT successful. It can be even replaced by a zeroparameter operation. We should pay more attentions to the remaining parts of ViT in the future work. Code is available at github.com/microsoft/SPACH.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Designing backbone networks plays a fundamental role in computer vision. Since the revolutionary progress of AlexNet <ref type="bibr" target="#b10">(Krizhevsky, Sutskever, and Hinton 2012)</ref>, convolution neural networks (CNNs) have dominated this area for nearly 10 years. However, the recently developed Vision Transformers (ViTs) have shown potential to challenge this throne. The advantage of ViT was first demonstrated in image classification task <ref type="bibr" target="#b4">(Dosovitskiy et al. 2020)</ref>, where the ViT backbone outperforms its CNN counterparts by a remarkable margin. Thanks to the promising results, the flourish of ViT variants rapidly broadcasts to many other computer vision tasks, such as object detection, semantic segmentation, and action recognition.</p><p>Despite the impressive performances of recent ViT variants, it is still not yet clear what makes ViT good for vi- <ref type="figure">Figure 1</ref>: An illustration of our shift building block. We propose to replace the attention layer with a simple shift operation in vision transformers. It spatially shifts a small portion of the channels along four directions, and the rest of the channels remain unchanged. sual recognition tasks. Some conventional wisdom leans to credit the success to the attention mechanism, since it provides a flexible and powerful way to model spatial relationships. Concretely, the attention mechanism leverages a selfattention matrix to aggregate features from arbitrary locations. Compared with the convolution operation in CNN, it has two significant strengths. First, this mechanism opens a possibility to simultaneously capture both short-and longranged dependencies, and get rid of the local restriction of the convolution. Second, the interaction between two spatial locations dynamically depends on their own features, rather than a fixed convolutional kernel. Due to such good properties, some pieces of work believe it is the attention mechanism that facilitates the powerful expressive ability of ViTs.</p><p>However, are these two advantages truly the key to success? The answer is probably NOT. Some existing work proves that, even without these properties, the ViT variants can still work well. For the first one, the fully-global dependencies may not be inevitable. More and more ViTs introduce a local attention mechanism to restrict their attention scope within a small local region, e.g., Swin Transformer <ref type="bibr" target="#b15">(Liu et al. 2021b)</ref> and Local ViT . The experiments show that the performance does not drop due to the local restriction. Besides, another line of research investigates the necessity of the dynamic aggregation. MLP-Mixer <ref type="bibr" target="#b16">(Tolstikhin et al. 2021)</ref> proposes to substitute the attention layer with a linear projection layer, where the linear weights are not dynamically generated. In this case, it can still reach a leading performance on the ImageNet dataset. Now that both global and dynamic properties might not be crucial for the ViT framework, what is the essential reason for the success of ViT? To figure it out, we further simplify the attention layer into an extremely simple case: NO global scope, NO dynamics, and even NO parameter and NO arithmetic calculation. We desire to know whether ViT can retain the good performance under this extreme case.</p><p>Conceptually, this zero-parameter alternative must rely on the handcrafted rule to model spatial relationships. In this work, we revisit the shift operation, which we believe is one of the simplest spatial modeling module. As depicted in <ref type="figure">Figure</ref> 1, the standard ViT building block consists of two parts: the attention layer and the feed-forward network (FFN). We replace the former attention layer with a shift operation, while keeping the latter FFN part untouched. Given an input feature, the proposed building block will first shift a small portion of the channels along four spatial directions, namely left, right, top, and down. As such, the information of neighboring features is explicitly mingled by the shifted channels. Then, the subsequent FFN performs channel-wise mixing to further fuse the information from neighbors.</p><p>Based on this shift building block, we construct a ViT-like backbone network, namely ShiftViT. Surprisingly, this backbone can also work well for the mainstream visual recognition tasks. The performance is on par with or even better than the strong Swin Transformer baseline. Concretely, within the same computational budgets as Swin-T model, our ShiftViT achieves a top-1 classification accuracy of 81.7% (against Swin-T's 81.3%) on ImageNet dataset. For the dense prediction task, it attains a mean average precision (mAP) score of 45.7% (against Swin-T's 43.7%) on COCO detection dataset, and a mean IoU (mIoU) score of 46.3% (against Swin-T's 44.5%) on ADE20k segmentation dataset.</p><p>Since the shift operation is already the simplest spatial modelling module, the excellent performance must come from the remaining components, e.g., the linear layers and the activation function in FFN. These components are less studied in existing work, because they look trivial. However, to further demystify the reasons why ViT works, we argue that we should pay more attentions to these components, instead of just focusing on the attention mechanism. We hope our work can shed a new light on the ViT research. As a summary, the contributions of this work are two folds:</p><p>? We present a ViT-like backbone, where the vanilla attention layer is replaced by an extremely simple shift operation. The proposed model can achieve an even better performance than Swin Transformer. ? We analyze the reasons behind the success of ViTs. It hints that the attention mechanism might not be the vital factor that makes ViT work. We should take the remaining components seriously in the future study of ViTs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work Attention and Vision Transformers</head><p>Transformer architecture <ref type="bibr" target="#b18">(Vaswani et al. 2017</ref>) is first introduced in the area of natural language processing (NLP). It solely adopts attention mechanism to build the connections between different language tokens. Thanks to the great performance, Transformers have rapidly dominated the NLP area and become the de facto standard. Inspired by the successful application in NLP, attention mechanism has also received increasing interests from the computer vision community. The early explorations can be roughly divided into two categories. On the one hand, some literature considers attention as a plug-and-play module, which can be seamlessly integrated into the existing CNN architectures. The representative work includes non-local network <ref type="bibr" target="#b20">(Wang et al. 2018)</ref>, relation network <ref type="bibr" target="#b6">(Hu et al. 2018)</ref>, and CCNet <ref type="bibr" target="#b8">(Huang et al. 2019</ref>). On the other hand, some pieces of work aim to substitute all convolution operations with the attention mechanism, such as local relation network <ref type="bibr" target="#b7">(Hu et al. 2019</ref>) and self-attention network <ref type="bibr" target="#b23">(Zhao, Jia, and Koltun 2020)</ref>.</p><p>Although these two kinds of work have shown promising results, they are still built on the CNN architecture. ViT <ref type="bibr" target="#b4">(Dosovitskiy et al. 2020</ref>) is the pioneering work that leverages a pure transformer architecture for visual recognition tasks. Thanks to its impressive performance, the community recently bursts out a rising wave of research on vision transformers. Along this line of research, the main focus is to improve the attention mechanism, so that it can satisfy the intrinsic properties of visual signals. For example, MSViT ) builds hierarchical attention layers to obtain multi-scale features. Swin Transformers <ref type="bibr" target="#b15">(Liu et al. 2021b)</ref> introduces a locality constrain into its attention mechanism. The related efforts also include pyramid attention <ref type="bibr" target="#b19">(Wang et al. 2021)</ref>, local-global attention ), cross attention <ref type="bibr" target="#b0">(Chen, Fan, and Panda 2021)</ref>, to name a few.</p><p>Unlike the particular interests in attention mechanism, the remaining components of ViT are less studies. DeiT <ref type="bibr" target="#b17">(Touvron et al. 2020)</ref> has setup a standard training pipeline for vision transformers. Most follow-up work inherits its setting, and only make some modifications on the attention mechanism. Our work also follows this paradigm. However, the goal of this work is not to complex the design of attention. On the contrary, we aim to show that the attention mechanism might not be the critical part of making ViTs work. It can be even replaced by an extremely simple shift operation. We hope these results can inspire researchers to rethink the role of attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLP Variants</head><p>Our work is related to the recent multi-layer-perceptron (MLP) variants. Specifically, MLP variants propose to extract image features through a pure MLP-like architecture. They also jump out of the attention-based framework in  Our ShiftViT can be also categorized into the pure MLP architecture, where the shift operation is viewed as a special token-mixing layer. Compared with the existing MLP work, our shift operation is even much simpler, since it contains no parameter and no FLOP. Moreover, the vanilla MLP variants fail to handle variable input size because of the fixed linear weights. Our shift operation overcomes this obstacle and therefore make the backbone feasible for more vision tasks like object detection and semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shift Operation</head><p>Shift operation is not new in computer vision. As early as in 2017, it was proposed to be an efficient alternative to the spatial convolution operation <ref type="bibr" target="#b21">(Wu et al. 2018)</ref>. Concretely, it uses a sandwich-like architecture, two 1?1 convolutions and a shift operation, to approximate a K ? K convolution. In the follow-up work, the shift operation is further extended into different variants, such as active shift <ref type="bibr" target="#b9">(Jeon and Kim 2018)</ref>, sparse shift <ref type="bibr" target="#b2">(Chen et al. 2019</ref>) and partial shift <ref type="bibr" target="#b13">(Lin, Gan, and Han 2019)</ref>.</p><p>In this work, we adopt the partial shift operation <ref type="bibr" target="#b13">(Lin, Gan, and Han 2019)</ref>. It is notable that the goal of this work is not to present a novel operation. Instead of that, we integrate the existing shift operation with the popular ViT to verify the effectiveness of attention mechanism. The similar vision are shared with the concurrent work ShiftMLP ) and AS-MLP <ref type="bibr" target="#b12">(Lian et al. 2021</ref>), but the design details are quite different. Their building blocks are more complex, which involve some auxiliary layers like pre-transformation and post-transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shift Operation Meets Vision Transformer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture Overview</head><p>For a fair comparison, we follow the architecture of Swin Transformer <ref type="bibr" target="#b15">(Liu et al. 2021b</ref>). The architecture overview is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref> (a). Specifically, given an input image of shape H ? W ? 3, it first splits the images into nonoverlapping patches. The patch size is 4 ? 4 pixels. Therefore, the output of patch partition is is H 4 ? W 4 tokens, where each token has a channel size of 48.</p><p>The modules followed by can be divided into 4 stages. Each stage contains two parts: embedding generation and stacked shift blocks. For the embedding generation of the first stage, a linear projection layer is used to map each token into an embedding of channel size C. For the rest stages, we merge neighbouring patches through the convolution with a kernel size of 2 ? 2. After patch merging, the spatial size of the output is half down-sampled, while channel size is twice the input, i.e., from C to 2C.</p><p>The stacked shift block is built by some repeated basic units. The detail design of each shift block is shown in <ref type="bibr">Figure 2 (b)</ref>. It composes of a shift operation, a layer normalization and a MLP network. This design is almost the same as the standard transformer block. The only difference is that we use a shift operation rather than a attention layer. For each stage, the number of shift blocks can be various, which is denoted as N 1 , N 2 , N 3 , N 4 respectively. In out implementation, we carefully choose the value of N i so that the overall model share a similar number of parameters with the baseline Swin Transformer model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shift Block</head><p>The detail architecture of our shift block is depicted in <ref type="figure" target="#fig_0">Figure 2</ref> (b). Specifically, this block consists of three sequentially-stacked components: shift operation, layer normalization and MLP network.</p><p>Shift operation has been well studied in CNNs. It can have many design choices, such as active shift <ref type="bibr" target="#b9">(Jeon and Kim 2018)</ref> and sparse shift <ref type="bibr" target="#b2">(Chen et al. 2019)</ref>. In this work, we follow the partial shift operation in TSM <ref type="bibr" target="#b13">(Lin, Gan, and Han 2019)</ref>. The illustration is presented in <ref type="figure">Figure 1 (b)</ref>. Given an input tensor, a small portion of channels will be shifted along 4 spatial directions, namely left, right, top, and down, while the remaining channels keep unchanged. After shifting, the out-of-scope pixels are simply dropped and the vacant pixels are zero padded. In this work, the shift step is set to 1 pixel.</p><p>Formally, we assume that the input feature z is of shape H ? W ? C, where C is the number of channels, H and W are spatial height and width, respectively. The output featur? z has the same shape as input. It can be written as:</p><formula xml:id="formula_0">z[0 : H, 1 : W, 0 : ?C] ? z[0 : H, 0 : W ? 1, 0 : ?C] z[0 : H, 0 : W ? 1, ?C : 2?C] ? z[0 : H, 1 : W, ?C : 2?C] z[0 : H ? 1, 0 : W, 2?C : 3?C] ? z[1 : H, 0 : W, 2?C : 3?C] z[1 : H, 0 : W, 3?C : 4?C] ? z[0 : H ? 1, 0 : W, 3?C : 4?C] z[0 : H, 0 : W, 4?C : C] ? z[0 : H, 0 : W, 4?C : C]</formula><p>where ? is a ratio factor to control how many percentages of channels will be shifted. In most experiments, the value of ? is set to 1 12 . It is notable that shift operation does not hold any parameter or arithmetic calculation. The only implementation is memory copying. Therefore, shift operation is highly efficient and it is very easy to implement. The pseudo code is presented in Algorithms 1. Compared with the self-attention mechanism, shift operation is clean, neat, and more friendly to deep learning inference library like TensorRT.</p><p>The rest of the shift block is the same as the standard building block of ViT. The MLP network has two linear layers. The first one increases the channel of the input feature to a higher dimension, e.g., from C to ? C. Then the second linear layer projects the high-dimensional feature into the original channel size of C. Between these two layers, we adopt GELU as the non-linear activation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture Variants</head><p>For a fair comparison with the baseline Swin Transformer, we also build multiple models with various number of parameters and computational complexity. Specifically, we introduce Shift-T(iny), Shift-S(mall), Shift-B(ase) variants 1 , which is corresponded to Swin-T, Swin-S and Swin-B, respectively. Shift-T is the smallest one, which shares a similar size with Swin-T and ResNet-50. Another two variants, Shift-S and Shift-B, are roughly 2? and 4? more complex than ShiftViT-T. The detail configurations of basic embedding channels C and number of blocks {N i } are presented as following:</p><p>? Beside the model size, we also have a closer look at the model depth. In our proposed model, nearly all parameters are concentrated in the MLP part. Therefore, we can control the expand ratio of MLP ? to obtain a deeper network depth. If not specified, the expand ratio ? is set to 2. We have an ablation analysis to show that the deeper model achieve a better performance.</p><p>1 For simplification, we ignore the suffix of "ViT" and use Shift-T to denote ShiftViT-T in this work.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Implementation Details</head><p>We conduct experiments on three mainstream visual recognition benchmarks: image classification on ImageNet-1k dataset <ref type="bibr" target="#b3">(Deng et al. 2009</ref>), object detection on COCO dataset <ref type="bibr" target="#b14">(Lin et al. 2014</ref>) and semantic segmentation on ADE20k dataset <ref type="bibr" target="#b24">(Zhou et al. 2019</ref>).</p><p>For image classification task, we exactly follow the protocol as in Swin Transformer <ref type="bibr" target="#b15">(Liu et al. 2021b</ref>). An average pooling layer and a linear classification layer are appended after the backbone network. All the parameters are randomly initialized and trained for 300 epochs with an AdamW optimizer. The learning rate starts from 0.001 and gradually decay to 0 with a cosine schedule. We include all data augmentations and regularization tricks as in Swin Transformer <ref type="bibr" target="#b15">(Liu et al. 2021b</ref>). The batch size is set to 1024.</p><p>For object detection task, there exists many off-the-shelf detection frameworks, such as Faster R-CNN, Mask R-CNN and RetinaNet. For a fair comparison with other methods, we follow the common practice of using Mask R-CNN and Cascade Mask R-CNN. In such detection frameworks, the backbone is our proposed Shift network, while the rest of components like FPN and detection head remain the same. We initialize the backbone with pretrained weights of the ImageNet-1k classifier. The training duration lasts for 12 epochs (denoted as 1? schedule) or 36 epochs (denoted as 3? schedule). The optimizer is AdamW, with an initial learning rate 0.0001. The batch size is 16. During training period, we utilize the multi-scale training trick, i.e., the shorter side of the input image is resized into a range from 480 pixels to 800 pixels. We report the mean average precision (mAP) metrics on the validation set of COCO dataset.</p><p>For semantic segmentation task, we evaluate our method on ADE20K dataset, which contains 20K images for training and 2K images for validation. In these experiments, the base segmentation framework is UperNet. The model is trained on the training set of ADE20K and the evaluation metric is the mean IoU (mIoU) score on the validation set. Similar to the setting of object detection, our Shift backbones are also pretrained on ImageNet-1k. The rest of settings are same as Swin-Transformer. The training batch size is 16 and we train the model for 160k iterations. For the comparison with the state-of-the-arts, we adopt the multi-scale testing strategy.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with Baseline</head><p>The goal of this work is to demystify the role of attention mechanism and explore whether it can be replaced by an extremely simple shift operation. Concretely, our proposed backbones are based on the architecture of Swin Transformer, which is one of the most representative ViT variants. We therefore consider Swin Transformer as the baseline model, and compare our ShiftViT to it. For an apple-to-apple comparison, we first build a lightweight version of ShiftViT. It is nearly the same as the Swin Transformer counterpart, except that the attention layers are substituted by the shift operations. We denote this backbone with a suffix /light, because replacing attention with shift will lead to a reduction in parameters and FLOPs. The experimental results are presented in <ref type="table" target="#tab_1">Table 1</ref>. We exhaustively compare all variants in three different sizes. The results show that the shift operation is weaker than the attention mechanism, because it does not contain any learnable parameter or arithmetic calculation. For example, the Shift-T/light model has only 20M parameters and 3.0 FLOPs, which are nearly 33% less than the Swin-T model. Therefore, there is no wonder that its performance is marginally worse than the baseline. Despite the relative gap to the baseline, it is worth noting that the absolute accuracy of the lightweight ShiftViT is not bad. Compared with the typical ResNet-50 backbone, Shift-T/light is more powerful and more efficient.</p><p>To remedy the complexity gap between shift operation and attention mechanism, we can adopt more building blocks in ShiftViT to make sure it has a similar number of parameters with the Swin baseline. In such fair comparisons, our models achieve even better results than Swin-Transformer. For the small-size models, our Shift-T backbone attains an mAP score of 45.4% on COCO and an mIoU score of 46.3% on ADE20k, which outperform the Swin-T backbone by a remarkable margin. For the large-size mod-els, ShiftViT seems to be saturated. But the performance is still on par with the Swin baseline.</p><p>Although the shift operation is weaker than the attention mechanism in spatial modelling, its simple architecture allows the network to grow deeper. As such, the weakness of the shift operation is greatly alleviated. Within the same computational budget, the overall performance of ShiftViT is comparable to the attention-based Swin Transformer. These experiments prove that the attention mechanism might not be necessary for ViTs. Even an extremely simple operation can achieve the similar results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with State-of-the-Art</head><p>To further demonstrate the effectiveness, we compare ShiftViT backbones with existing state-of-the-art methods. For image classification task on ImageNet-1k, our proposed models are compared to three different types of models, namely CNN, ViT and MLP. The results are detailed in <ref type="table" target="#tab_3">Table 2</ref>. Overall, our method can achieve a comparable performance with the state-of-the-arts. For ViT-based and MLPbased methods, the best performances are around 83.5% top-1 accuracy, while our model achieves an accuracy of 83.3%. For CNN-based methods, our model is slightly worse than EfficientNet series, but the comparison is not fully fair because EfficientNet takes a larger input size.</p><p>Another interesting thing is the comparison with two concurrent work S 2 -MLP ) and AS-MLP <ref type="bibr" target="#b12">(Lian et al. 2021</ref>). These two pieces of work share the similar idea on shift operation , but they introduce some auxiliary modules into the building block, e.g., the pre-and post-projection layers. In <ref type="table" target="#tab_3">Table 2</ref>, our performances are slightly better than these two work. It justifies our design choice that building backbone solely with a simple shift operation is good enough.</p><p>Beside the classification task, the similar performance tread can be also observed in the object detection task and semantic segmentation task. It is notable that some ViTbased and MLP-based methods cannot be easily extended to such dense prediction tasks, because the high-resolution inputs yield unaffordable computational burdens. Our method does not suffer from this obstacle thanks to the high efficiency of shift operation. As shown in <ref type="table" target="#tab_4">Table 3</ref> and <ref type="table" target="#tab_5">Table 4</ref>, the advantages of our ShiftViT backbones are clear. Shift-T attains an mAP score of 47.1 on object detection and an mIoU score of 47.8 on semantic segmentation, which outperform other methods by a considerable margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Analysis</head><p>In this section, we aim to explore what factors contribute to the good performance of ShiftViT. We first analyze the impact of two hyper-parameters in ShiftViT. Then, we dive into the training scheme of ViT series.</p><p>Expand ratio of MLP The previous experiments have justified our design principle, i.e., a great model depth can remedy the weakness of each building block. Generally, there exists a trade-off between the model depth and the complexity of building blocks. With a fixed computational   To further investigate this trade-off, we present some ShiftViT models with different depths. For ShiftViT, most parameters exist in the MLP part. We can change the expand ratio of MLP ? to control the model depth. As shown in <ref type="table" target="#tab_6">Table 5</ref>, we choose Shift-T as our baseline model. We explore the expand ratio ? within a range from 1 to 4. It is worth noting that the parameters and FLOPs for different entries are almost the same. From <ref type="table" target="#tab_6">Table 5</ref>, we can observe a trend that a deeper model results in a better performance. When the depth of ShiftViT increases to 225, it outperforms the 57-layer counterpart by 0.5%, 1.2% and 2.9% absolute gains on classification, detection and segmentation, respectively. This trend supports our conjecture that a powerfuland-heavy module, like attention, may not be the optimal choice for backbone. We hope it can help the future work to rethink such trade-off when designing backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Percentage of shifted channels</head><p>The shift operation has only one hyper-parameter, namely the percentage of shifted channels. By default, it is set to 33%. In this section, we explore some other settings. Specifically, we set the percentage of shifted channels to 20%, 25%, 33% and 50%, respectively. The results are presented in <ref type="figure">Figure 3</ref>. It shows that the final performance is not very sensitive to this hyperparameter. Shifting 25% of channels only results in 0.3% absolute loss compared to the best setting. Within the reasonable range (from 25% to 50%), all the settings achieve a better accuracy than the Swin-T baseline.</p><p>Shifted pixels In the shift operation, a small portion of channels are shifted by one pixel along four directions. To have a comprehensive exploration, we also try different shifted pixels. When the shifted pixel is zero, i.e., no shifting happens, the top-1 accuracy on the ImageNet dataset is only 72.9%, which is significantly lower than our baseline (81.7%). This is not surprising because no shifting means there is no interaction between different spatial location. Besides, if we shift two pixels in the shift operation, the model achieves 80.2% top-1 accuracy on ImageNet, which is also slightly worse than the default setting.</p><p>ViT-style training scheme Shift operation has been well studied in CNNs. However, the previous work does not show 3HUFHQWDJHRIVKLIWHGFKDQQHOV 7RSDFFXUDF\RQ,PDJH1HWN 6ZLQ7EDVHOLQH <ref type="figure">Figure 3</ref>: Ablation analysis on the percentage of shifted channels. We plot the top-1 classification accuracy on ImageNet-1k. The red line indicates Swin-T baseline. the impressive performance as ours. Shift-ResNet-50 <ref type="bibr" target="#b21">(Wu et al. 2018</ref>) only achieve an accuracy of 75.6% on ImageNet, which is far behind our 81.7% accuracy. This gap raise a natural concern about what makes good for our ShiftViT.</p><p>We suspect the reason might lie in the ViT-style training scheme. Specifically, most existing ViT variants follow the setting as in DeiT <ref type="bibr" target="#b17">(Touvron et al. 2020)</ref>, which is quite different from the standard pipeline of training CNNs. For example, ViT-style scheme adopts AdamW optimizer and the training duration lasts for 300 epochs on ImageNet. As a comparison, CNN-style scheme prefers SGD optimizer and the training schedule is usually 90 epochs only. Since our model inherit the ViT-style training scheme, it is interesting to see how such differences affect the performance.</p><p>Due to the resource limitation, we cannot fully align all settings between ViT-style and CNN-style. Therefore, we pick four important factors that we believe can bring some insights, i.e. optimizer, activation function, normalization layer and training schedule. From <ref type="table" target="#tab_7">Table 6</ref>, we can observe that such factors can significantly influence the accuracy, especially the training schedule. These results shows that the good performance of ShiftViT is partly brought by the ViTstyle training scheme. Similarly, the success of ViT may be also related to its special training scheme. We should take it seriously in the future study of ViTs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this work, we move a small step toward demystifying the essential reason why ViT works. The experiments show that the attention mechanism might not be the vital factor for the success of ViT. We can even use an extremely simple shift operation to replace the attention layer. The proposed backbone, namely ShiftViT, can work as well as the Swin Transformer baseline. Since the shift operation is already the simplest spatial modelling module, we argue that the good performance must come from the remaining components of ViT, e.g., the FFN and the training scheme. In future work, we plan to have more analysis on such factors and investigate more ViT variants.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>(a) The overall architecture of our ShiftViT. We follow Swin Transformer<ref type="bibr" target="#b15">(Liu et al. 2021b</ref>) to build hierarchical representations. (b) The detail design of a shift block. We only use a simple shift operation to model spatial relationships.ViT. For example, instead of using the self-attention matrix, MLP-Mixer<ref type="bibr" target="#b16">(Tolstikhin et al. 2021</ref>) introduces a tokenmixing MLP to directly connect all spatial locations. It eliminates the dynamic property of ViT, but without losing accuracy. The follow-up work investigates more MLP designs, like the spatial gating unit<ref type="bibr" target="#b15">(Liu et al. 2021a</ref>) or cyclic connection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Shift-T: C = 96, {N i } = {6, 8, 18, 6}, ? = 1/12 ? Shift-S: C = 96, {N i } = {10, 18, 36, 10}, ? = 1/12 ? Shift-B: C = 128, {N i } = {10, 18, 36, 10}, ? = 1/16</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>g:1 * g, :, :-1] = x[:, 0 * g:1 * g, :, 1:] 9 out[:, 1 * g:2 * g, :, 1:] = x[:, 1 * g:2 * g, :, :-1] 10 out[:, 2 * g:3 * g, :-1, :] = x[:, 2 * g:3 * g, 1:, :] 11 out[:, 3 * g:4 * g, 1:, :] = x[:, 3 * g:4 * g, :-1, :]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Pytorch-like pseudo code of shift 1 def shift(feat, gamma=1/12): 2 # feat is a tensor with a shape of 3 # [Batch, Channel, Height, Width] 4 B, C, H, W = feat.shape</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison with the baseline Swin Transformer on three mainstream tasks: image classification, object detection and semantic segmentation. The suffix /light denotes the lightweight version of our ShiftViT, where we only replace attention layers with the shift operation and keep remaining parts unchanged. The throughput speed is evaluated on a single NVidia GTX1080-Ti GPU.The gree and gray colors indicate the gain and loss, respectively.</figDesc><table><row><cell>Model</cell><cell>Param (M)</cell><cell cols="2">ImageNet FLOPs Speed (G) (FPS) Acc.(%) Top-1</cell><cell cols="2">Mask R-CNN 1? AP b AP m</cell><cell>COCO</cell><cell cols="2">Mask R-CNN 3? AP b AP m</cell><cell>ADE20k UpperNet mIoU</cell></row><row><cell>ResNet-50</cell><cell>26</cell><cell>4.1</cell><cell>676 76.1</cell><cell>38.0</cell><cell>34.4</cell><cell cols="2">41.0</cell><cell>37.1</cell><cell>-</cell></row><row><cell>Swin-T</cell><cell>29</cell><cell>4.5</cell><cell>356 81.3</cell><cell>43.7</cell><cell>39.5</cell><cell cols="2">46.0</cell><cell>41.6</cell><cell>44.5</cell></row><row><cell>Shift-T/light</cell><cell>20</cell><cell>3.0</cell><cell>790 79.4</cell><cell>41.3</cell><cell>38.0</cell><cell cols="2">43.2</cell><cell>39.2</cell><cell>42.6</cell></row><row><cell>Shift-T</cell><cell>29</cell><cell>4.5</cell><cell cols="7">396 81.7 (+0.4) 45.4 (+1.7) 40.9 (+1.4) 47.1 (+1.1) 42.3 (+0.7) 46.3 (+1.8)</cell></row><row><cell>Swin-S</cell><cell>50</cell><cell>8.7</cell><cell>217 83.0</cell><cell>46.4</cell><cell>41.7</cell><cell cols="2">48.5</cell><cell>43.3</cell><cell>47.6</cell></row><row><cell>Shift-S/light</cell><cell>34</cell><cell>5.7</cell><cell>457 81.6</cell><cell>44.8</cell><cell>40.4</cell><cell cols="2">46.0</cell><cell>41.1</cell><cell>45.4</cell></row><row><cell>Shift-S</cell><cell>50</cell><cell>8.8</cell><cell cols="7">215 82.8 (-0.2) 47.2 (+0.8) 42.2 (+0.5) 48.6 (+0.1) 43.4 (+0.1) 47.8 (+0.2)</cell></row><row><cell>Swin-B</cell><cell>88</cell><cell>15.4</cell><cell>158 83.5</cell><cell>46.9</cell><cell>42.1</cell><cell cols="2">48.7</cell><cell>43.4</cell><cell>48.1</cell></row><row><cell>Shift-B/light</cell><cell>60</cell><cell>10.2</cell><cell>312 82.3</cell><cell>45.7</cell><cell>41.0</cell><cell cols="2">46.0</cell><cell>41.2</cell><cell>45.8</cell></row><row><cell>Shift-B</cell><cell>89</cell><cell>15.6</cell><cell cols="7">154 83.3 (-0.2) 47.7 (+0.8) 42.7 (+0.6) 48.0 (-0.7) 42.8 (-0.6) 47.9 (-0.2)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison with state-of-the-art methods on the ImageNet-1k classification task.</figDesc><table><row><cell>Model</cell><cell>Input resolution</cell><cell cols="3"># Params FLOPs Top-1 (M) (B) Acc. (%)</cell></row><row><cell></cell><cell cols="2">CNN-based</cell><cell></cell><cell></cell></row><row><cell>RegNetY-4G</cell><cell>224 2</cell><cell>21</cell><cell>4.0</cell><cell>80.0</cell></row><row><cell>RegNetY-8G</cell><cell>224 2</cell><cell>39</cell><cell>8.0</cell><cell>81.7</cell></row><row><cell>RegNetY-16G</cell><cell>224 2</cell><cell>84</cell><cell>16.0</cell><cell>82.9</cell></row><row><cell>EfficientNet-B4</cell><cell>380 2</cell><cell>19</cell><cell>4.2</cell><cell>82.9</cell></row><row><cell>EfficientNet-B5</cell><cell>456 2</cell><cell>30</cell><cell>9.9</cell><cell>83.6</cell></row><row><cell>EfficientNet-B6</cell><cell>528 2</cell><cell>43</cell><cell>19.0</cell><cell>84.0</cell></row><row><cell cols="3">ViT-based and MLP-based</cell><cell></cell><cell></cell></row><row><cell>DeiT-S</cell><cell>224 2</cell><cell>22</cell><cell>4.6</cell><cell>79.8</cell></row><row><cell>DeiT-B</cell><cell>224 2</cell><cell>86</cell><cell>17.5</cell><cell>81.8</cell></row><row><cell>PVT-S</cell><cell>224 2</cell><cell>25</cell><cell>3.8</cell><cell>79.8</cell></row><row><cell>PVT-L</cell><cell>224 2</cell><cell>61</cell><cell>9.8</cell><cell>81.7</cell></row><row><cell>Swin-T</cell><cell>224 2</cell><cell>29</cell><cell>4.5</cell><cell>81.3</cell></row><row><cell>Swin-S</cell><cell>224 2</cell><cell>50</cell><cell>8.7</cell><cell>83.0</cell></row><row><cell>Swin-B</cell><cell>224 2</cell><cell>88</cell><cell>15.4</cell><cell>83.5</cell></row><row><cell>MLP-Mixer-B/16</cell><cell>224 2</cell><cell>79</cell><cell>-</cell><cell>76.4</cell></row><row><cell>gMLP-S</cell><cell>224 2</cell><cell>20</cell><cell>4.5</cell><cell>79.4</cell></row><row><cell>gMLP-B</cell><cell>224 2</cell><cell>73</cell><cell>15.8</cell><cell>81.6</cell></row><row><cell>S 2 -MLP-D</cell><cell>224 2</cell><cell>71</cell><cell>14.0</cell><cell>80.0</cell></row><row><cell>S 2 -MLP-W</cell><cell>224 2</cell><cell>51</cell><cell>10.5</cell><cell>80.7</cell></row><row><cell>AS-MLP-T</cell><cell>224 2</cell><cell>28</cell><cell>4.4</cell><cell>81.3</cell></row><row><cell>AS-MLP-S</cell><cell>224 2</cell><cell>50</cell><cell>8.5</cell><cell>83.1</cell></row><row><cell>AS-MLP-B</cell><cell>224 2</cell><cell>88</cell><cell>15.2</cell><cell>83.3</cell></row><row><cell></cell><cell cols="2">Ours</cell><cell></cell><cell></cell></row><row><cell>Shift-T</cell><cell>224 2</cell><cell>28</cell><cell>4.4</cell><cell>81.7</cell></row><row><cell>Sfhit-S</cell><cell>224 2</cell><cell>50</cell><cell>8.5</cell><cell>82.8</cell></row><row><cell>Sfhit-B</cell><cell>224 2</cell><cell>88</cell><cell>15.2</cell><cell>83.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison with state-of-the-art methods on the COCO object detection task. Following the common practice, we couple the backbones with two detection frameworks, namely Mask R-CNN and Cascade Mask R-CNN.</figDesc><table><row><cell>Backbone</cell><cell cols="3">Params (M) FLOPs (G) AP b AP m</cell></row><row><cell></cell><cell cols="2">Mask R-CNN 3?</cell><cell></cell></row><row><cell>Res-50</cell><cell>44</cell><cell>260</cell><cell>41.0 37.1</cell></row><row><cell>PVT-S</cell><cell>44</cell><cell>245</cell><cell>43.0 39.9</cell></row><row><cell>AS-MLP-T</cell><cell>48</cell><cell>260</cell><cell>46.0 41.5</cell></row><row><cell>Swin-T</cell><cell>48</cell><cell>264</cell><cell>46.0 41.6</cell></row><row><cell>Shift-T</cell><cell>48</cell><cell>265</cell><cell>47.1 42.3</cell></row><row><cell>Res-101</cell><cell>63</cell><cell>336</cell><cell>42.8 38.5</cell></row><row><cell>PVT-M</cell><cell>64</cell><cell>302</cell><cell>44.2 40.5</cell></row><row><cell>AS-MLP-S</cell><cell>69</cell><cell>346</cell><cell>47.8 42.9</cell></row><row><cell>Swin-S</cell><cell>69</cell><cell>354</cell><cell>48.5 43.3</cell></row><row><cell>Shift-S</cell><cell>70</cell><cell>350</cell><cell>48.6 43.4</cell></row><row><cell></cell><cell cols="2">Cascade Mask R-CNN 3?</cell><cell></cell></row><row><cell>Res-50</cell><cell>82</cell><cell>739</cell><cell>46.3 40.1</cell></row><row><cell>AS-MLP-T</cell><cell>86</cell><cell>745</cell><cell>50.1 43.5</cell></row><row><cell>Swin-T</cell><cell>86</cell><cell>739</cell><cell>50.4 43.7</cell></row><row><cell>Shift-T</cell><cell>86</cell><cell>743</cell><cell>50.3 43.4</cell></row><row><cell>ResX-101</cell><cell>101</cell><cell>819</cell><cell>48.1 41.6</cell></row><row><cell>AS-MLP-S</cell><cell>107</cell><cell>824</cell><cell>51.1 44.2</cell></row><row><cell>Swin-S</cell><cell>107</cell><cell>838</cell><cell>51.8 44.7</cell></row><row><cell>Shift-S</cell><cell>107</cell><cell>827</cell><cell>50.9 44.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="5">: Comparison with state-of-the-art methods on the</cell></row><row><cell cols="5">ADE20k semantic segmentation task. We report the mIoU</cell></row><row><cell cols="2">metrics on the validation set.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">Params FLOPs (M) (G)</cell><cell>val mIoU</cell></row><row><cell>DANet</cell><cell>ResNet-101</cell><cell>69</cell><cell>1119</cell><cell>45.2</cell></row><row><cell>DNL</cell><cell>ResNet-101</cell><cell>69</cell><cell>1249</cell><cell>46.0</cell></row><row><cell cols="2">DeepLabV3 ResNet-101</cell><cell>63</cell><cell>1021</cell><cell>44.1</cell></row><row><cell>OCRNet</cell><cell>ResNet-101</cell><cell>89</cell><cell>1381</cell><cell>44.9</cell></row><row><cell cols="2">DeepLabV3 ResNeSt-101</cell><cell>66</cell><cell>1051</cell><cell>46.9</cell></row><row><cell cols="2">DeepLabV3 ResNeSt-200</cell><cell>88</cell><cell>1381</cell><cell>48.4</cell></row><row><cell>OCRNet</cell><cell>HRNet-w64</cell><cell>71</cell><cell>664</cell><cell>45.7</cell></row><row><cell>UperNet</cell><cell>ResNet-101</cell><cell>89</cell><cell>1029</cell><cell>44.9</cell></row><row><cell>UperNet</cell><cell>Swin-T</cell><cell>60</cell><cell>945</cell><cell>45.8</cell></row><row><cell>UperNet</cell><cell>AS-MLP-T</cell><cell>60</cell><cell>937</cell><cell>46.5</cell></row><row><cell>UperNet</cell><cell>Shift-T</cell><cell>60</cell><cell>942</cell><cell>47.8</cell></row><row><cell>UperNet</cell><cell>Swin-S</cell><cell>81</cell><cell>1038</cell><cell>49.5</cell></row><row><cell>UperNet</cell><cell>AS-MLP-S</cell><cell>81</cell><cell>1024</cell><cell>49.2</cell></row><row><cell>UperNet</cell><cell>Shift-S</cell><cell>81</cell><cell>1029</cell><cell>49.6</cell></row><row><cell>UperNet</cell><cell>Swin-B</cell><cell>121</cell><cell>1188</cell><cell>49.7</cell></row><row><cell>UperNet</cell><cell>AS-MLP-B</cell><cell>121</cell><cell>1166</cell><cell>49.5</cell></row><row><cell>UperNet</cell><cell>Shift-B</cell><cell>121</cell><cell>1174</cell><cell>49.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation analysis on the expand ratio of MLP. The first row shows the Swin-T baseline. The row with blue background denotes the default setting in our experiments. All entries share the same number of parameters and FLOPs.</figDesc><table><row><cell cols="2">Expand Depth Ratio</cell><cell cols="2">ImgNet Acc. (%) AP b AP m COCO</cell><cell>ADE20k mIoU</cell></row><row><cell>Swin</cell><cell>48</cell><cell>81.3</cell><cell>43.7 39.5</cell><cell>44.5</cell></row><row><cell>4</cell><cell>57</cell><cell>81.3</cell><cell>44.0 39.8</cell><cell>44.4</cell></row><row><cell>3</cell><cell>75</cell><cell>81.5</cell><cell>44.4 40.2</cell><cell>45.5</cell></row><row><cell>2</cell><cell>114</cell><cell>81.7</cell><cell>45.4 40.9</cell><cell>46.3</cell></row><row><cell>1</cell><cell>225</cell><cell>81.8</cell><cell>45.2 40.6</cell><cell>47.3</cell></row><row><cell cols="5">budget, a lightweight building block can enjoy a deeper net-</cell></row><row><cell cols="2">work architecture.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Ablation analysis on the typical configurations of CNNs and ViTs. We gradually transfer the training configuration from the CNN's setting to the ViT's setting, and investigate how these factors influence the model performances.</figDesc><table><row><cell>SGD</cell><cell cols="3">ReLU BN 90ep</cell><cell>ImageNet</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>Top-1 Acc.</cell></row><row><cell cols="4">AdamW GELU LN 300ep</cell><cell>(%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>76.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>77.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>78.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>78.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>81.7</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Crossvit: Crossattention multi-scale vision transformer for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14899</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.10224</idno>
		<title level="m">Cyclemlp: A mlp-like architecture for dense prediction</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">All you need is a few shifts: Designing efficient convolutional neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7241" to="7250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11227</idno>
		<title level="m">Multiscale vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3464" to="3473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Constructing Fast Network through Deconstruction of Convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="5951" to="5961" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Imagenet classification with deep convolutional neural networks. NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Localto-Global Self-Attention in Vision Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.04735</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">AS-MLP: An Axial Shifted MLP Architecture for Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08391</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08050</idno>
		<title level="m">Pay Attention to MLPs</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Swin transformer: Hierarchical vision transformer using shifted windows. arXiv</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<title level="m">Mlp-mixer: An all-mlp architecture for vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers and distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Nonlocal neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Shift: A zero flop, zero parameter alternative to spatial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Golmant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gholaminejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9127" to="9135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07477</idno>
		<title level="m">S 2 -MLP: Spatial-Shift MLP Architecture for Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploring selfattention for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10076" to="10085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic understanding of scenes through the ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="321" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
