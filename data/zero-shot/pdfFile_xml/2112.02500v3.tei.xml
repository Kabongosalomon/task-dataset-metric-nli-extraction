<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Global-Local Context Network for Person Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zheng</surname></persName>
							<email>zhengpeng0108@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Qin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
						</author>
						<title level="a" type="main">Global-Local Context Network for Person Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Person search aims to jointly localize and identify a query person from natural, uncropped images, which has been actively studied over the past few years. In this paper, we delve into the rich context information globally and locally surrounding the target person, which we refer to as scene and group context, respectively. Unlike previous works that treat the two types of context individually, we exploit them in a unified global-local context network (GLC-Net) with the intuitive aim of feature enhancement. Specifically, re-ID embeddings and context features are simultaneously learned in a multi-stage fashion, ultimately leading to enhanced, discriminative features for person search. We conduct the experiments on two person search benchmarks (i.e., CUHK-SYSU and PRW) as well as extend our approach to a more challenging setting (i.e., character search on MovieNet). Extensive experimental results demonstrate the consistent improvement of the proposed GLCNet over the state-of-the-art methods on all three datasets. Our source codes, pre-trained models, and the new setting w.r.t. character search will be publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person search aims at identifying a query person from natural, uncropped images across different views <ref type="bibr" target="#b44">[42,</ref><ref type="bibr" target="#b55">53]</ref>. To tackle this task, we need not only address the challenges in person re-identification (re-ID) (e.g., pose variations, occlusions, and varied camera views), but also unify pedestrian detection and person re-ID into a joint framework. By simultaneously solving the two sub-tasks, person search exhibits its unique advantages in terms of efficiency, accuracy, and practicability. In the past few years, rapid progresses in person search have been made with the renaissance of deep learning techniques. Existing person search approaches can be generally divided into two categories, i.e., two-step approaches <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b40">38,</ref><ref type="bibr" target="#b45">43]</ref> and onestep ones <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b44">42,</ref><ref type="bibr" target="#b47">45,</ref><ref type="bibr" target="#b50">48]</ref>. In two-step approaches, person search is decomposed into two sub-tasks, i.e., pedestrian detection <ref type="bibr" target="#b25">[24,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b52">50]</ref> and person re-ID <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b41">39]</ref>. Detection and re-ID models typically work in a sequential manner, i.e., the detection model is used to crop the person regions, which are subsequently fed to the re-ID model to extract discriminative embeddings for identification. However, sequentially dealing with the two tasks using two separate networks is both time-and resource-consuming. In contrast, one-step approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">25,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b44">42,</ref><ref type="bibr" target="#b47">45,</ref><ref type="bibr" target="#b50">48]</ref> integrate detection and re-ID into a unified end-to-end network. Instead of explicitly employing the cropped person regions from the detection results, one-step models apply an RoI-Align layer to aggregate the features within the detected bounding boxes.</p><p>Nevertheless, both two-step and one-step methods concentrate on the person-centric regions, without perceiving the surrounding environment. In other words, they fail to fully exploit the holistic and rich context, while mining such context is proved to be very effective in other related tasks, such as object detection <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b27">26]</ref> and group re-ID <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b60">57]</ref>. Motivated by the above facts, several recent attempts <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b29">28,</ref><ref type="bibr" target="#b30">29]</ref> have been made to learn the underlining context information beyond the detected person regions. They typically exploit two types of context information, i.e., scene and group. For instance, scene context is used in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b29">28]</ref> to help identify the target via measuring the correspondence between the detected box of the query and the scene in the gallery. Group context is often mined in a pair-wise fashion, e.g., siamese networks are developed in <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b30">29]</ref> to find the potential co-travelers; <ref type="bibr" target="#b26">[25,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b49">47]</ref> attempt to achieve the globally optimized matching based on the input pairs. However, most existing methods resort to either scene or group context, while both types of context make their own contributions to the final performance, as a person is not only related to the global scene, but also associated with the local neighbors. Therefore, an open question is naturally posed -Is it possible to leverage both context information to further improve the person search performance? In this paper, we attempt to answer the question by exploiting both scene and group context in a unified framework. To this end, we develop a simple yet effective framework for hybrid context learning, i.e., global-local context network (GLCNet), which effectively and efficiently exploits the global scene and local group context. On the one hand, scene context can facilitate learning global discriminative features, which can enhance the re-ID features in the current scene. On the other hand, we employ group context to establish the relationships between the target person and his/her neighboring co-travelers.</p><p>To be specific, we inject global and local context into an existing one-step person search model <ref type="bibr" target="#b26">[25]</ref>, where two levels of context information work as the additional features to enhance the re-ID embeddings of the target person. The feature enhancement is performed in terms of three hierarchical stages as follows. 1) In the first stage, we design a context encoder to learn enhanced context features by playing with the context features themselves.</p><p>2) The second stage employs the norm-aware embedding (NAE) layer <ref type="bibr" target="#b4">[5]</ref> to improve the diversity for re-ID and context features. 3) Finally, in the third stage, we aggregate all the features and feed them to a self-attention block for the ultimate channelwise feature interactions. In other words, the learned features are regarded as the 'memory' for the target -the scenes where he/she often appears and the surrounding concurrent people will be recorded and updated. In this way, re-ID embeddings can perceive rich context features, leading to improved person search performance.</p><p>The proposed GLCNet exhibits its unique advantages over the existing context-based person search approaches from three perspectives. First, we investigate into both scene and group context, implicitly leveraging their complementary characteristics. Second, our network architecture is more efficient than those siamese frameworks that require the time-consuming pair generation. In the meantime, our network is more flexible and concise, bringing less computational overhead to the base model. Third, our context features are simultaneously updated and interacted with the re-ID features in a multi-stage fashion.</p><p>In addition to person search, we evaluate the proposed method in a more challenging setting, i.e., character search (joint character detection and identification) in movies <ref type="bibr" target="#b20">[19]</ref>, where context also plays a critical role. We study character search based on a newly-proposed large-scale movie dataset, i.e., MovieNet <ref type="bibr" target="#b20">[19]</ref>. This dataset is more diverse and realistic than existing person search datasets which mostly focus on surveillance scenarios. In character search, the same character may wear different clothes in distinct scenes, bringing significant difficulty to existing person search approaches. In particular, since <ref type="bibr" target="#b20">[19]</ref> only introduces the separate settings of character detection and identification, we re-organize the data to evaluate this new setting, which we refer to as MovieNet-CS. We follow CUHK-SYSU <ref type="bibr" target="#b44">[42]</ref> to provide multiple difficulty levels for character search by varying the number of images in the training/test set on MovieNet-CS. From the subsequent experimental results, we find that our approach notably outperforms the state-of-the-art methods by large margins.</p><p>In summary, our main contributions include:</p><p>? We propose a novel global-local context network (GLCNet) for person search, by enhancing the re-ID features with global scene context and local group context, resulting in more discriminative and semantically meaningful representations.</p><p>? We equip an existing framework with the additional ability of perceiving the global-local context information by updating and interacting both re-ID and context features in terms of multiple stages, while bringing little computational overhead to its concise architecture.</p><p>? Extensive experiments on two widely-adopted person search benchmarks clearly show the superiority of our method over the state-of-the-art approaches. The additional evaluation in character search, where our method outperforms other competitors by large margins, further confirms the effectiveness of the proposed context learning scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Person Search. Although tremendous achievements have been made in person re-ID <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b54">52]</ref>, there still remains a gap to apply re-ID to practical applications, due to that gallery images are cropped person images instead of natural images consisting of persons and their surroundings. To address this problem, person search is introduced to jointly localize and identify query persons. Existing person search frameworks can be generally divided into two categories: twostep and one-step models. In two-step frameworks, persons are detected and then fed into a re-ID model for identification, i.e., detection and re-ID models are independent of each other and organized in a sequential way. Inspired by the development in object detection <ref type="bibr" target="#b39">[37]</ref>, the first one-step person search approach was proposed in <ref type="bibr" target="#b44">[42]</ref>. Since then, many more successful attempts <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">22,</ref><ref type="bibr" target="#b26">25,</ref><ref type="bibr" target="#b47">45,</ref><ref type="bibr" target="#b49">47,</ref><ref type="bibr" target="#b53">51]</ref> have been made to make the joint framework more effective and efficient. For instance, to improve the efficiency, Yan et al. <ref type="bibr" target="#b47">[45]</ref> proposed the first anchor-free person search framework, which equipped an existing detector with three levels of alignment. In <ref type="bibr" target="#b26">[25]</ref>, detection and re-ID were considered as a progressive process and tackled with two sub-networks to obtain more effective results. Generally, the re-ID branch in either one-step or two-step frameworks only employs the information within the detected bounding box to learn re-ID features. However, person search images naturally contain much richer information, including labeled persons, unlabeled persons, and complete background, which could provide complementary information for learning more robust re-ID features, as shown in previous attempts <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b46">44]</ref>. Therefore, it is highly desirable that a person search framework can fully exploit the context information to enhance </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context Encoder</head><p>Global Scene Context Sampling <ref type="figure">Figure 1</ref>. Overall framework of the proposed global-local context network (GLCNet). <ref type="bibr" target="#b26">[25]</ref> is adopted as our base network. We enhance the original re-ID features by simultaneously exploiting global scene context and local group context. The complementary three levels of features are then fed to the attentive embedding and aggregation (AEA) head for learning more discriminative and robust representations, leading to the final improved person search performance.</p><p>the overall performance.</p><p>Context Learning in Person Search. Over the past few years, context learning has played an important role in the sub-tasks of person search, i.e., object detection <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b35">34]</ref> and person re-ID <ref type="bibr" target="#b22">[21,</ref><ref type="bibr" target="#b48">46]</ref>. In object detection, global context information is often exploited as there exist certain relationships between the target object and the global scene, e.g., a beach often involves a boat, and an umbrella often appears in a rainy day. To be short, more information in the holistic scene could be absorbed to better assist localizing target objects. For instance, Chen et al. <ref type="bibr" target="#b7">[7]</ref>proposed a correlation estimation procedure to select useful context information from the surrounding regions, which improved the quality of their detection results. Lim et al. <ref type="bibr" target="#b27">[26]</ref> extracted the features of higher layers to obtain the context information from the surrounding pixels. In terms of re-ID methods, context information is often represented as the concept of group, which helps model the relationships between the target person and his/her neighbors. In <ref type="bibr" target="#b6">[6]</ref>, unlabelled target instances were leveraged as the contextual guidance for image generation. Besides, group re-ID methods <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b48">46,</ref><ref type="bibr" target="#b60">57]</ref> explore more on how to incorporate the additional visual context from the neighboring group members to enhance the re-ID performance. As for person search, there exist several recent methods <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b26">25,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b49">47]</ref> that explore context information. In <ref type="bibr" target="#b26">[25,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b49">47]</ref>, group context was used by graph matching. In <ref type="bibr" target="#b46">[44]</ref> and <ref type="bibr" target="#b14">[14]</ref>, context was employed in an implicit way by using memory units in the weakly-supervised setting. However, all the above methods exploit eithor global scene context or local group context, while this work simultaneously captures both of them and fully unleash their complementarity. More importantly, the proposed framework focuses on feature-level enhancement without bells and whistles, introducing negligible computational overhead to the efficient base network.</p><p>From Person Search to Character Search. In 2017, Xiao et al. <ref type="bibr" target="#b44">[42]</ref> introduced the first person search dataset, i.e., CUHK-SYSU, collected from street snaps and movie/TV frames. PRW <ref type="bibr" target="#b55">[53]</ref> is a collection of images from surveillance videos. These two existing datasets have been used to evaluate methods for searching persons in different places. In the era of social media and big data, there is an increasing need for searching persons on the Internet, especially movie stars and political celebrities. However, a big gap between searching a person on the street and retrieving a person from the Internet is that the clothing of the target person usually changes across different scenes. To be more specific, a movie cast tends to wear different clothes in different movies or even a single movie. In such cases, the clothing is not a reliable pattern for distinguishing different persons any more. Furthermore, suspects may intentionally change their clothes from place to place, making the basic assumption of existing person re-ID/search methods unreliable. Given the above facts and observations, it is important to extend the scope of person search to a more general one, i.e., character search. Recently, Huang et al. <ref type="bibr" target="#b20">[19]</ref> proposed a holistic dataset for movie understanding, where rich annotations for character detection and character identification have been provided. Owing to this great work, an apposite dataset and a baseline method for character identification have been given, which provides a great chance to extend character identification to character search and makes this work much more in line with practical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Global-Local Context Network</head><p>In this section, we introduce the proposed global-local context network in detail. Specifically, we first provide an overview of our framework. Subsequently, we elaborate how the two kinds of context information (i.e., global scene context and local group context) are exploited, and how the multi-level features are embedded and aggregated into a unified discriminative one. Finally, the training and inference procedures of our GLCNet are given in detail by presenting all the loss functions. <ref type="figure">Fig. 1</ref> illustrates the overall framework of the proposed GLCNet. We adopt the recently proposed SeqNet <ref type="bibr" target="#b26">[25]</ref> as our base network due to its concise and effective architecture, i.e., the sequential pipeline in the middle of <ref type="figure">Fig. 1</ref>. The base network combines the sub-networks of detection and re-ID in a sequential and end-to-end manner. Built upon the above base network, our goal is to fully exploit the rich context existing in the input image. Unlike previous works <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b42">40,</ref><ref type="bibr" target="#b49">47]</ref> that develop sophisticated schemes to capture the context information, we simply aim for featurelevel enhancement to avoid bringing much computational overhead. Specifically, we equip the original output features with different levels of context features, capturing both global scene context and local group context. More specifically, global scene context (GSC) features are encoded with a context encoder, based on the feature map of the entire input image. Local group context (LGC) features are obtained from the RoI-Aligned person-level features through a context encoder of the same architecture as above. Finally, an attentive embedding and aggregation head is developed to further embed and aggregate the above different levels of features to output the final discriminative features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Framework Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Global Scene Context</head><p>Scene context has been proven to play an important role <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b35">34]</ref> in person search related tasks (e.g., object detection and segmentation). For instance, Liu et al. <ref type="bibr" target="#b33">[32]</ref> adopted a graph model to establish the relationship between scene/instance context and the target object to infer the object state. This motivates us to exploit the global scene context to facilitate learning more discriminative re-ID features.</p><p>To confirm the rationality of exploiting global scene context for person search, we need to find out whether there is a connection between a certain person with the corresponding scene. To this end, we take a deep look into the two existing person search datasets. On CUHK-SYSU, many images are key frames collected from movie shots, where there exist significant differences of scenes between different shots and high similarities between key frames in a single shot. In such cases, we expect there is a strong connection between the person identity and the corresponding scenes. While on PRW, there are few differences between the images since all of them are collected from eight surveillance cameras. As a result, the connections between the identity and the scenes should not be that tight, so that capturing the global scene context is not expected to be so effective on this dataset, which is also verified in the following experiments.</p><p>Based on the above observations, we can conjecture that global scene context does help to distinguish different identities and the contributions depend on the characteristics of the specific datasets. To capture such context for enhancing re-ID features, we directly adopt the output features of the ResNet50 backbone, which is a global response to the entire scene. Moreover, we propose the context encoder to further enhance the global scene context features. To avoid that the improvement comes from the high complexity of the network, we design the context encoder as simple as possible, which is made up of several 1?1 convolutional layers and an adaptive max pooling layer. We employ 1?1 convolutional layers as they can squeeze and embed the context information without bells and whistles. After encoding, the global context features, together with other levels of features, are fed to the attentive embedding and aggregation (AEA) head to obtain the final enhanced representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Local Group Context</head><p>As shown in person re-ID <ref type="bibr" target="#b42">[40,</ref><ref type="bibr" target="#b48">46,</ref><ref type="bibr" target="#b56">54]</ref>, group context information has the potential to bring extra useful information and reduce ambiguity for achieving more accurate re-ID. For group context mining, a trivial solution is to use a graph neural network (GNN), which views all the identities as a vertex set and computes the edges among these identities. However, directly feeding the persons into the GNN lacks enough interpretability and increases the computational complexity a lot. Based on the above considerations, we learn local group context (LGC) features from the neighboring persons in a similar way as learning the GSC features.</p><p>More concretely, our LGC learning branch consists of a sampling layer and a context encoder. The sampling layer is used for selecting the ROI-Aligned features corresponding to the person regions. After sampling, the person-level features are fed into the context encoder for feature enhancement. Finally, the group context features, together with the features from the other levels, are fed to the AEA head to obtain the final enhanced re-ID embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Attentive Embedding and Aggregation</head><p>Given the contextual information from two perspectives, i.e., global scene and local group, we need to aggregate these two context features and the original re-ID features as a unified discriminative embedding. To make full use of the above kinds of features, we design an attentive embedding and aggregation (AEA) head to transform, aggregate, and interact all the features from different levels. In practice, we design the AEA head in an implicit way and an explicit way, respectively, by considering either embedding or aggregation as the first task.</p><p>The architecture of the implicit AEA head is shown in <ref type="figure">Fig. 2</ref>, where we adopt the spirit of 'aggregation first, embedding second'. More concretely, we first concatenate re-ID features and two context features, and then employ a channel attention block <ref type="bibr" target="#b17">[17]</ref> to assign different weights to each type of feature. Finally, we feed the unified feature vector to the NAE layer for learning more robust features, supervised by the OIM <ref type="bibr" target="#b44">[42]</ref> loss.</p><p>The second design of the AEA head is orthogonal to the first one but tends to be more explicit. As shown in <ref type="figure">Fig. 3</ref>, all the features are first fed to an NAE layer for embedding, and then aggregated to form a unified feature. As a result, the identity features of a target person consist of three parts: its original re-ID features, global context features that perceive the scene, and local context features representing its surrounding persons. Subsequently, the attention mechanism is applied to the unified feature to focus on the most informative channels. Finally, we learn the final representation of the target person through optimizing the OIM <ref type="bibr" target="#b44">[42]</ref> loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training &amp; Inference</head><p>In this subsection, we elaborate the training and inference procedures of the proposed GLCNet by introducing all the involved loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Training</head><p>There are totally three heads in our framework, i.e., the first and the second sequential heads, and the AEA head, which are supervised by the five losses as follows:</p><p>1) Regression Losses: L reg1 , L reg2 . The regression losses of the sequential heads adopt the Smooth-L 1 -Loss <ref type="bibr" target="#b12">[12]</ref> as follows:</p><formula xml:id="formula_0">L reg = 1 N p Np i=1 L smooth?L1 (t i , t * i ),<label>(1)</label></formula><p>where N p denotes the number of positive samples, t i denotes the coordinates of the i-th positive sample, and t * i denotes the coordinates of the corresponding ground-truth box.</p><p>2) Classification Losses: L cls1 , L cls2 . In the first sequential head, the log loss over two classes is used to measure the classification ability.</p><formula xml:id="formula_1">L cls1 = 1 N N i=1 L CE (p i , p * i ),<label>(2)</label></formula><p>where L CE is the cross-entropy loss function, N denotes the number of samples, p i is the predicted classification probability of the i-th sample, and p * i denotes the corresponding ground-truth label. In the second sequential head, the original re-ID features along with the context features are used for classification.</p><formula xml:id="formula_2">f AEA = NAE(SE[f reid , f GSC , f LGC ]),<label>(3)</label></formula><formula xml:id="formula_3">L cls2 = 1 N N i=1 p * i L CE (f AEA , p * i ),<label>(4)</label></formula><p>where f reid , f GSC , and f LGC denote the extracted 256-d original re-ID features, 128-d global scene context features, and 128-d local group context features, respectively. Here, we take the implicit AEA head as an example to compute the final re-ID feature. SE means the SE-attention operation.</p><p>3) Re-ID Loss: L reid . We directly adopt the OIM loss <ref type="bibr" target="#b44">[42]</ref> as the re-ID loss.</p><p>L reid = OIM(f AEA ).</p><p>(</p><p>The overall loss function is given as:</p><formula xml:id="formula_5">L = ? 1 L reg1 +? 2 L cls1 +? 3 L reg2 +? 4 L cls2 +? 5 L reid .<label>(6)</label></formula><p>In our experiments, we simply follow <ref type="bibr" target="#b26">[25]</ref> to set ? 1 to 10 and the others to 1 across all the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Inference</head><p>In the inference stage, we follow <ref type="bibr" target="#b26">[25]</ref> to apply the nonmaximum suppression (NMS) before the re-ID sub-network and learn the re-ID features based on the output candidate boxes of the first sequential head, which shows higher efficiency and robustness on detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct the experiments on two widely-used person search benchmarks, i.e., CUHK-SYSU <ref type="bibr" target="#b44">[42]</ref> and PRW <ref type="bibr" target="#b55">[53]</ref>, as well as on the MovieNet dataset <ref type="bibr" target="#b20">[19]</ref> for the task of character search, where conventional person search approaches may fail in this challenging setting due to scene variations, appearance changes, etc.  <ref type="bibr" target="#b20">[19]</ref> is a holistic dataset for movie understanding. It contains 1,100 movies with rich annotations, including more than 1.1M characters with millions of bounding boxes and 3,087 cast identities. MovieNet can be used as the benchmark for many tasks, including character identification. Considering that simultaneously detecting the characters and identifying them is much closer to realistic application scenarios, we extend character identification to character search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Settings</head><p>Data Splits. We carefully reorganize the original MovieNet dataset as the new MovieNet-CS to fulfill the more challenging task of character search. Specifically, the dataset is split into the training and test sets based on the 3,087 identities. 1,000 identities of them are included in the test set, while the rest 2,087 form the training set. However, considering the extremely large scale of the whole 100K training images, we define 3 different settings for the training sets. Specifically, we adopt at most 10, 30, and 70 instances per identity, contributing to the training sets of 20K, 54K, and 100K training images, respectively. We follow <ref type="bibr" target="#b44">[42]</ref> to introduce different gallery sizes to bring various levels of difficulties for comprehensive evaluation. More  <ref type="figure">Figure 4</ref>. Comparison among the samples from CUHK-SYSU <ref type="bibr" target="#b44">[42]</ref>, PRW <ref type="bibr" target="#b55">[53]</ref>, and MovieNet <ref type="bibr" target="#b20">[19]</ref>. MovieNet covers various person scales, clothing, occlusions, and scenes, making it more challenging and closer to practical application scenarios. details about each subset of the training set can be found in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>As shown in <ref type="table" target="#tab_2">Table 1</ref> and <ref type="figure">Fig. 4</ref>, compared to conventional person search benchmarks, MovieNet-CS exhibits its unique challenges, such as extremely large data scale, large number of camera views, large variations in person appearance, and abundant instances of the same identity, while avoiding any ethical concerns as the images are collected from movies that are privacy insensitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Evaluation Metrics</head><p>We adopt the identical metrics (i.e., mean average precision (mAP) and top-1 accuracy) for evaluating the performance of different methods in terms of both person search and character search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We implement our model based on the PyTorch <ref type="bibr" target="#b38">[36]</ref> library and all the experiments are conducted on a single NVIDIA Tesla V100 GPU. During training, the batch size is set to 5 and each image is resized to 900?1500 on CUHK-SYSU <ref type="bibr" target="#b44">[42]</ref> and PRW <ref type="bibr" target="#b55">[53]</ref>. We follow <ref type="bibr" target="#b44">[42]</ref> to set the circular queue size of OIM to 5,000/500/2,000. The only data augmentation used here is random horizontal flip. We follow <ref type="bibr" target="#b26">[25]</ref> to set some hyper-parameters as follows. Specifically, our model is optimized by Stochastic Gradient Descent (SGD) for 18 epochs on CUHK-SYSU, PRW, and MovieNet-CS with the initial learning rate of 0.003 which is warmed up during the first epoch and decreased by 10 at the 16-th epoch. The momentum and weight decay of SGD are set to 0.9 and 5?10 ?4 individually. The IoU threshold values of the two sequential heads are both set to 0.5. During test, the thresholds of NMS are set to 0.4 and 0.5 w.r.t. the first and second heads, respectively. The parameters of the CBGM module used in SeqNet also follow the same settings as in <ref type="bibr" target="#b26">[25]</ref>, i.e., k1/k2 = 10/3 and k1/k2 = 30/4 on CUHK-SYSU and PRW, respectively.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-Art Methods</head><p>We compare our model with the state-of-the-art methods, including both one-step <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">25,</ref><ref type="bibr" target="#b44">42,</ref><ref type="bibr" target="#b47">45,</ref><ref type="bibr" target="#b49">47]</ref> and two-step methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b40">38]</ref>, in terms of both tasks of person search and character search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Person Search on CUHK-SYSU</head><p>As shown in <ref type="table">Table 4</ref>, our GLCNet outperforms all one-step and two-step person search models without any tricks, such as using additional auxiliary model or data (e.g., knowledge distillation or domain adaptation). Compared with the state-of-the-art two-step model TCTS <ref type="bibr" target="#b40">[38]</ref>, our GLCNet-Exp+CBG outperforms it by 2.2% and 1.3% w.r.t. mAP and top-1 accuracy, respectively. More importantly, TCTS adopts the sophisticated tricks, e.g., random erasing <ref type="bibr" target="#b59">[56]</ref>, label smoothing, and triplet loss <ref type="bibr" target="#b16">[16]</ref>, while our approach only employs random horizontal flip. We also illustrate the results with different gallery sizes in <ref type="figure" target="#fig_2">Fig. 5. From the figure</ref>, we can observe that our GLCNet outperforms all the existing models by notable margins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Person Search on PRW</head><p>As stated in Sec. 2, PRW contains less context so that the context information might not be so effective as with CUHK-SYSU. Nevertheless, as shown in <ref type="table">Table 4</ref>, our GLCNet-Exp+CBGM still achieves the best top-1 accuracy among all the one-step and two-step models. Although the mAP of our model on PRW only ranks the third among all the methods, knowledge distillation or external models are adopted in their approaches, which are quite time-and resource-consuming. For brevity, as the best performance is achieved by the explicit AEA head, we report the results of GLCNet-Exp (i.e., 'GLCNet' is identical to 'GLCNet-Exp') in the following experiments, unless otherwise stated.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Character Search on MovieNet-CS</head><p>We first report the performance of different methods on MovieNet-CS under the setting of 'Training Set:N=10'-'Gallery Size:2K' in <ref type="table" target="#tab_4">Table 3</ref>, where we can observe the significant improvement of GLCNet over other competitors, especially in mAP (e.g., 34% relatively higher than the second best SeqNet). In addition, since we have 3 levels of training sets and 3 gallery sets on MovieNet-CS, we conduct detailed comparisons between the base network (Se-qNet) and GLCNet under all the 9 settings. As shown in Tables 5 and 6, our GLCNet consistently outperforms SeqNet in terms of all the settings on MovieNet-CS. The above results on such a highly diverse dataset verifies the effectiveness of exploiting both global and local context in our GLC-Net, while other person search approaches cannot perform well in the challenging setting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Analytical Results</head><p>In this section, we conduct the analytical experiments on CUHK-SYSU and PRW for an in-depth analysis of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Effects of Context</head><p>We first conduct experiments to verify the effectiveness of global scene context and local group context. As stated above, we adopt SeqNet as our baseline and context information is exploited for feature enhancement w.r.t. the original re-ID features. As can be seen from <ref type="table">Table 7</ref>, global scene context brings more benefits than local group context, showing that the entire scene highly correlates with the target identity. Interestingly, only adding local group context to the baseline cannot achieve notable improvement, which is probably because more people may bring some ambiguity to the target. However, when integrating both context features into the baseline, significant performance gains are observed, indicating the necessity of exploiting both context information, which are complementary to each other. In addition, we conduct experiments by neglecting the original re-ID features and only using the scene/group context features for the re-ID task. More concretely, only the scene/group context features in green/yellow in <ref type="figure">Fig. 1</ref> are fed to the AEA head for learning re-ID features. In this case, we cannot achieve satisfactory performance, which reveals that both context can only supplement the original re-ID features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Effects of Detection</head><p>As more accurate detection results generally lead to higher person search performance, we evaluate the results of some representative methods, including NAE <ref type="bibr" target="#b4">[5]</ref>, SeqNet <ref type="bibr" target="#b26">[25]</ref>, and our GLCNet, when using the ground-truth bounding boxes. From <ref type="table">Table 8</ref>, we can observe that the performance of all the methods increases when using the ground-truth boxes. Interestingly, the margin between our GLCNet and other competitors also increases compared to that when using the detected bounding boxes. This indicates that the  proposed method benefits more from the enhanced re-ID features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Runtime</head><p>Here, we compare the speed of different person search models. For fair comparisons, we follow previous approaches and resize images to 1500 ? 900. We perform inference on a single Tesla V100 GPU with batch size of 3. <ref type="table">Table  9</ref> depicts the results, where we can see our GLCNet takes 97 milliseconds (ms) to process an image, which is still acceptable and even a bit faster than NAE+. Moreover, despite adding several modules onto SeqNet, the inference speed is only decreased by 11 ms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Results</head><p>To fully understand the effectiveness of the proposed framework, we show some qualitative results on CUHK-SYSU from three perspectives, as shown in <ref type="figure" target="#fig_3">Fig. 6</ref>. Firstly, we show some difficult cases to visualize the top-1 matches Methods GPU (TFLOPs) Time (ms) QEEPS <ref type="bibr" target="#b36">[35]</ref> P6000 (12.6) 300 NAE <ref type="bibr" target="#b4">[5]</ref> V100 (14.1) 83 NAE+ <ref type="bibr" target="#b4">[5]</ref> V100 (14.1) 98 SeqNet <ref type="bibr" target="#b26">[25]</ref> V100 (14.1) 86 AlignPS <ref type="bibr" target="#b47">[45]</ref> V100 (14.1) 61 AlignPS+ <ref type="bibr" target="#b47">[45]</ref> V100 (14.1) 67 GLCNet V100 (14.1) 97 <ref type="table">Table 9</ref>. Runtime comparisons of different one-step models.</p><p>directly. As shown in (a)-(b) columns, our GLCNet clearly outperforms other methods, especially for images with complicated scenes, such as varied illumination and severe occlusion. Secondly, we also visualize the quality of detected boxes in the (c)-(d) columns, where we can see our GLCNet obtains more accurate and tighter boxes, eliminating the interference from surrounding persons in crowded scenes for better re-ID. Last but not least, as we can see from the (e)-(f) columns, our GLCNet can localize more persons with a higher recall rate, which has more potential to find the target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Cross-Dataset Evaluation</head><p>To quantitatively demonstrate the domain gap between MovieNet-CS and person search datasets, we conduct cross-dataset experiments based on a direct transfer scheme, i.e., training our GLCNet on one dataset and then directly testing it on the other datasets. As shown in <ref type="table" target="#tab_2">Table 10</ref>, the model trained on MovieNet-CS can achieve 41.5% and 4.5% in mAP on CUHK-SYSU and PRW, respectively, while the models trained on CUHK-SYSU and PRW can  <ref type="table" target="#tab_2">Table 10</ref>. Cross-dataset experimental results by our GLCNet on different datasets. only achieve 0.7% and 4.6% in mAP on MovieNet-CS, respectively. The above results reveal the large domain gap between the character search dataset and the person search datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose the context-aware person search framework (i.e., GLCNet) to exploit both global scene context and local group context in a unified manner. The proposed GLCNet injects the context information by feature enhancement at multiple stages, leading to the final discriminative representations of the target person. Our approach outperforms the existing methods on two widelyused person search datasets, i.e., CUHK-SYSU and PRW. Additional experiments on the large-scale MovieNet dataset demonstrate that our method has a great potential to be applied to the more challenging task of character search.</p><p>Although our model achieves promising performance on existing person search datasets and MovieNet, it still has some limitations. Firstly, the effectiveness of our contextaware method relies on the diversity of context. On some datasets with a low degree of diversity (e.g., PRW), the improvement by context is very limited. Besides, the speed of our model is not high enough. In the future, more efforts will be put into making our context in a more adaptive way to simplify the architecture. Furthermore, character search is a more challenging task that deserves more attention. Different from person search, facial information may play an important role in improving the character search performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Structure of the attentive embedding and aggregation (AEA) head in the implicit style. Structure of the attentive embedding and aggregation (AEA) head in the explicit style.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>1 http://movienet.site/ CUHK-SYSU PRW MovieNet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Performance comparison in terms of different gallery sizes on CUHK-SYSU. The dashed and solid lines represent twostep and one-step methods, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Visualization of (a)-(b) difficult cases; (c)-(d) quality of detected boxes; (e)-(f) recall of detection on CUHK-SYSU. Yellow/green/red boxes denote queries/top-1 matches/other detected persons, respectively. Zoom in for better viewing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>,900 query persons and 6,978 images. Unless otherwise stated, the results are reported by using the default gallery size of 100. PRW [53] contains 11,816 images from the surveillance videos captured by six static cameras in a university campus. After manual annotation, there exist 932 labeled persons with 43,110 bounding boxes. Following the standard data split, the training set contains 5,704 images with 482 different identities, while the test set includes 2,057 query persons and 6,112 images. For each query, we adopt the whole test set as the gallery.</figDesc><table><row><cell>4.1.1 Person Search</cell></row><row><cell>CUHK-SYSU [42] consists of 18,184 images, with 96,143 annotated bounding boxes and 8,432 identities. Two types of data sources are collected, exhibiting high degrees of di-versities and variations, making it quite challenging. We follow [42] to adopt the standard training/test split, with a training set of 5,532 identities and 11,206 images, and a test set of 24.1.2 Character Search</cell></row><row><cell>MovieNet 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Comparison between MovieNet-CS and person search datasets. *LSPS is not yet publicly available.</figDesc><table><row><cell>Datasets</cell><cell cols="2">#Images #Boxes</cell><cell cols="2">Gallery Size</cell><cell>#Identities</cell><cell>#Avg. Instances per Identity</cell><cell>#Views</cell><cell>Scenes</cell></row><row><cell>CUHK-SYSU [42]</cell><cell>18,184</cell><cell>23,430</cell><cell cols="2">50?4,000</cell><cell>8,432</cell><cell>3</cell><cell>N/A</cell><cell>surveillance, movies/TV</cell></row><row><cell>PRW [53]</cell><cell>11,816</cell><cell>34,304</cell><cell>6,112</cell><cell></cell><cell>932</cell><cell>36</cell><cell>6 cameras</cell><cell>surveillance</cell></row><row><cell>LSPS* [55]</cell><cell>51,836</cell><cell>60,433</cell><cell cols="2">33,673</cell><cell>4,067</cell><cell>15</cell><cell>17 cameras</cell><cell>surveillance</cell></row><row><cell>MovieNet-CS</cell><cell cols="4">160,816 274,274 2,000?10,000</cell><cell>3,087</cell><cell>10?50</cell><cell>132,115 shots</cell><cell>movies</cell></row><row><cell>MovieNet-CS</cell><cell></cell><cell>N=10</cell><cell>N=30</cell><cell>N=70</cell><cell></cell><cell></cell><cell></cell></row><row><cell>#Identities</cell><cell></cell><cell>2,087</cell><cell>2,087</cell><cell>2,087</cell><cell></cell><cell></cell><cell></cell></row><row><cell>#Images</cell><cell></cell><cell cols="4">20,158 54,047 104,081</cell><cell></cell><cell></cell></row><row><cell>#Boxes</cell><cell></cell><cell cols="4">32,927 89,079 174,116</cell><cell></cell><cell></cell></row><row><cell cols="2">#Avg. Instances/Identity</cell><cell>10</cell><cell>26</cell><cell>50</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Different settings of the training set on MovieNet-CS.</figDesc><table><row><cell cols="5">'N=10/30/70' means the maximum number of instances per iden-</cell></row><row><cell cols="2">tity is 10/30/70, respectively.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Method OIM NAE NAE+ SeqNet GLCNet</cell></row><row><cell>mAP</cell><cell>27.5 30.5</cell><cell>31.2</cell><cell>34.2</cell><cell>45.9</cell></row><row><cell>top-1</cell><cell>70.1 73.8</cell><cell>75.4</cell><cell>80.9</cell><cell>85.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note>Performance of different methods on MovieNet-CS under the setting of 'Training Set:N=10'-'Gallery Size:2K'.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Table 8. Comparative results using the detected (det.) and groundtruth (gt.) bounding boxes. The numbers in the brackets correspond to our implementation.</figDesc><table><row><cell>CUHK-SYSU Methods Boxes mAP top-1</cell><cell>mAP</cell><cell>PRW</cell><cell>top-1</cell></row><row><cell cols="4">NAE [5] det. NAE [5] gt. SeqNet [25] det. 93.8(93.9)94.6(94.3) 46.7(46.5)83.4(82.7) 91.5 92.4 43.3 80.9 94.1 94.6 --SeqNet [25] gt. 94.6(94.4)95.3(94.8) -(47.8) -(84.2) 95.7 96.3 46.9 85.1 GLCNet det. GLCNet gt. 96.2 96.6 48.3 87.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>MovieNet-CS CUHK-SYSU PRWTest \Train mAP top-1 mAP top-1 mAP top-1</figDesc><table><row><cell cols="2">MovieNet-CS -</cell><cell>-</cell><cell>0.7</cell><cell>5.6</cell><cell>4.6 28.5</cell></row><row><cell cols="3">CUHK-SYSU 41.5 42.1</cell><cell>-</cell><cell>-</cell><cell>54.3 56.4</cell></row><row><cell>PRW</cell><cell>4.5</cell><cell cols="3">31.0 25.3 77.0</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rcaa: Relational context-aware agents for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Dong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hierarchical online instance matching for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10518" to="10525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Person search via a mask-guided two-stream cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Person search by separated modeling and A maskguided two-stream CNN model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="4669" to="4682" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Norm-aware embedding for efficient person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="12612" to="12621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Instanceguided context rendering for cross-domain person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="232" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Context refinement for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An empirical study of context in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Santosh Kumar Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1271" to="1278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bi-directional interaction network for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenkai</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Instance guided proposal network for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenkai</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2582" to="2591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Contextual Multi-Scale Feature Learning for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runze</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rengang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="655" to="663" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-toend trainable trident person search network using adaptive gradient propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeong-Ju</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuhyeun</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Young</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="925" to="933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Weakly supervised person search with region siamese networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuchu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Changxin Gao, and Nong Sang. Re-id driven localization refinement for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuchu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunshan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9813" to="9822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno>abs/1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification. ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unifying identification and context learning for person recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqiu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2217" to="2225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Movienet: A holistic dataset for movie understanding. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqiu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Illumination-invariant person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia, MM &apos;19</title>
		<meeting>the 27th ACM International Conference on Multimedia, MM &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="365" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Probabilistic matching pair selection for surf-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohamed Ibn Khedher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moun?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernadette</forename><surname>El-Yacoubi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dorizzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BIOSIG -Proceedings of the International Conference of Biometrics Special Interest Group (BIOSIG)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Prototype-guided saliency feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghun</forename><surname>Joung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ig-Jae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Person search by multi-scale matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Box guided convolution for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangzhi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1615" to="1624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sequential end-to-end network for efficient person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengjia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duoqian</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Small object detection using context and attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong-Seon</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Astrid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyungjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Ik</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Group re-identification via unsupervised transfer of sparse features encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Lisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Martinel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gian</forename><forename type="middle">Luca</forename><surname>Foresti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural person search machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayashree</forename><surname>Karlekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meibin</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dual context-aware refinement network for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengjun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dual Context-Aware Refinement Network for Person Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computing Machinery</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3450" to="3459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">High-level semantic feature detection: A new perspective for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5182" to="5191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Structure inference net: Object detection using scene-level context and instance-level relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bag of tricks and a strong baseline for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Hao Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1487" to="1495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Loddon</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Query-guided end-to-end person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharti</forename><surname>Munjal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Galasso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="811" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno>abs/1912.01703</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Tcts: A task-consistent two-stage framework for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingpeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Multimedia, MM &apos;18</title>
		<meeting>the 26th ACM International Conference on Multimedia, MM &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Group re-identification: Leveraging and integrating multigrain information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Hao Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM international conference on Multimedia</title>
		<meeting>the 26th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">IAN: the individual aggregation network for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tammam</forename><surname>Tillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaizhu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="332" to="340" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bochao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Person search in a scene by jointly modeling people commonness and person uniqueness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingpeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Multimedia, MM &apos;14</title>
		<meeting>the 22nd ACM International Conference on Multimedia, MM &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="937" to="940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Exploring visual context for weakly supervised person search. ArXiv, abs/2106.10506</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Anchor-free person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning multi-attention context graph for group-based re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning context graph for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2153" to="2162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Bottom-Up Foreground-Aware Feature Fusion for Person Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dangwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3404" to="3412" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Joint person objectness and repulsion for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hantao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="685" to="696" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Occluded pedestrian detection through guided attention in cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6995" to="7003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Diverse knowledge distillation for end-to-end person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Wang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Relation-aware global attention for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3183" to="3192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Person reidentification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoyan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3346" to="3355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Associating groups of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Robust partial matching for person search in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingji</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6826" to="6834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/1708.04896, 2020. 7</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Group re-identification with group context graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
