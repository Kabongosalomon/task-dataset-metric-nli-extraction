<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Perceptual Contrast Stretching on Target Feature for Speech Enhancement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Chao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CSIE</orgName>
								<orgName type="institution">NCKU</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CITI</orgName>
								<orgName type="institution" key="instit2">Academia Sinica</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CITI</orgName>
								<orgName type="institution" key="instit2">Academia Sinica</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szu-Wei</forename><surname>Fu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xugang</forename><surname>Lu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">NICT</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tsao</surname></persName>
							<email>yu.tsao@citi.sinica.edu.tw</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CITI</orgName>
								<orgName type="institution" key="instit2">Academia Sinica</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Perceptual Contrast Stretching on Target Feature for Speech Enhancement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: Speech enhancement</term>
					<term>contrast stretching</term>
					<term>per- ceptual importance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Speech enhancement (SE) performance has improved considerably owing to the use of deep learning models as a base function. Herein, we propose a perceptual contrast stretching (PCS) approach to further improve SE performance. The PCS is derived based on the critical band importance function and is applied to modify the targets of the SE model. Specifically, the contrast of target features is stretched based on perceptual importance, thereby improving the overall SE performance. Compared with post-processing-based implementations, incorporating PCS into the training phase preserves performance and reduces online computation. Notably, PCS can be combined with different SE model architectures and training criteria. Furthermore, PCS does not affect the causality or convergence of SE model training. Experimental results on the VoiceBank-DEMAND dataset show that the proposed method can achieve state-of-the-art performance on both causal (PESQ score = 3.07) and noncausal (PESQ score = 3.35) SE tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Speech enhancement (SE) is performed to remove noise components from noisy speech to improve speech quality and intelligibility. SE has been used at an important front-end of many speech-related studies, such as automatic speech recognition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, speaker recognition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, and assistive listening devices <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. Traditionally, SE algorithms are typically designed based on the assumptions of speech and noise signals. Notable approaches include those presented in <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b7">[8]</ref>. These approaches are effective in some stationary noise scenarios, wherein the signals conform to the assumptions introduced. However, in most real-world noisy scenarios, timevarying noise exhibits nonstationary properties, resulting in the suboptimal performance of these conventional SE methods.</p><p>In recent years, deep learning (DL) has been widely used in various research fields, including SE. Using DL models as a base mapping function has notably improved SE <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. Although these DL-based methods achieve satisfactory performance under the testing conditions associated with the training data, their performance degrades when they are operated under unexpected conditions, attributed to two factors: first, the network architecture may not adequately consider the sequential nature of the speech signals; second, a regression function optimized by L1/L2 distance-based objective functions may average out important signal patterns, which can result in low precision of enhanced speech.</p><p>Numerous approaches have been proposed to further improve DL-based SE systems, including those that aims to determine suitable acoustic features to improve SE, such as waveforms <ref type="bibr" target="#b12">[13]</ref> and complex spectral features <ref type="bibr" target="#b13">[14]</ref>. Additionally, advanced networks have been proposed to model sequential signals more accurately, such as recurrent neural networks <ref type="bibr" target="#b14">[15]</ref>, fully convolutional networks <ref type="bibr" target="#b12">[13]</ref>, long-short term memory <ref type="bibr" target="#b15">[16]</ref>, transformers <ref type="bibr" target="#b16">[17]</ref>, and generative adversarial networks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. In another approach, advanced objective functions are derived to provide more accurate training to achieve the desired speech quality or intelligibility. Notable examples include differentiable speech metrics <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b12">13]</ref>, and deep feature losses <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, where losses are computed in representative or discriminative feature spaces. Aside from the above-mentioned approaches, this study investigates another direction to improve the SE performance: modifying the target features of the SE model and post-processing (PP).</p><p>PP has been derived to further modify enhanced speech to match the statistical properties of clean speech <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. Experimental results show that such an approach can further sharpen the structure of enhanced speech and suppress residual noise. Moreover, PP is compatible with any system for further improving SE. Accordingly, we propose perceptual contrast stretching (PCS)-a novel method to enhance the contrasts of the target features of an SE model. PCS can be implemented as PP for SE or incorporated into the SE training phase, where the implementation of the latter can avoid an increase computation cost during inference.</p><p>Gamma-correction approaches have proven to be effective for image enhancement <ref type="bibr" target="#b23">[24]</ref>. Based on these approaches, the characteristics of the human auditory system are employed in PCS. In this study, we first examined the effectiveness of PCS by incorporating it into the SE training phase. Next, we implemented it as a PP, and notable improvements were achieved. Specifically, PCS stretches the contrasts of the target features in the training data based on a set of auditory weights. The weights are designed based on the critical band importance <ref type="bibr" target="#b24">[25]</ref>, which is perceptually correlated with the human auditory system. The proposed PCS offers three major advantages: First, it is compatible with different SE systems (conventional or DL based). Second, it does not require additional parameters in the SE model. Third, it does not affect the causal property of the causal SE models. We compared PCS with several different contrast-stretching strategies in our experiments. The evaluation scores show that PCS outperformed the other feature enhancement approaches. State-of-the-art (SOTA) results (PESQ score = 3.35) were achieved when the best SE model was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Studies</head><p>In this section, we introduce two primary categories of related studies. Our proposed PCS training strategy for SE was designed based on these previous studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Gamma correction</head><p>The proposed PCS was partially inspired by image processing. First, we introduce gamma correction <ref type="bibr" target="#b25">[26]</ref>, widely adopted as a contrast-enhancement approach in computer vision research. the eye from graphical information is affected by the brightest region of the image. Specifically, the relationship between perceived and physical brightness can be derived as a nonlinear transfer function. The gamma correction was designed based on this function and can be derived as a power-law operation, expressed as follows:</p><formula xml:id="formula_0">Vout = AV ? in (1)</formula><p>where Vin is the input signal value, ? the modulation parameter, A the scaling function (typically a constant), and Vout the output signal value. For example, the standard red-green-blue (sRGB), widely used in monitors and printers, uses a gamma value of 2.2 (? = 2.2) in its transfer function to provide better perception. Another color space, i.e., the Digital Cinema Initiatives -Protocol 3 (DCI-P3), uses a gamma value of 2.6 (? = 2.6). In these operations, the input signals are normalized to a range between 0 and 1 to ensure that the boundary values and the minimal and maximal values of the input signal remain invariant after the operation. In recent SE research, Zhang et al. <ref type="bibr" target="#b26">[27]</ref> applied a weighting mechanism to the training target, similar to gamma correction using a dynamic scaling function (i.e., based on the ratio of input and target features).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Critical band importance</head><p>In human auditory systems, the importance of the signal components varies based on the frequency region. More specifically, humans can perceive differences in frequency bands ranging from 400 to 4400 Hz better than in other frequency bands. Consequently, a set of critical band importance weights was measured and defined. Some conventional SE approaches adopted critical bands to perform spectral subtraction <ref type="bibr" target="#b27">[28]</ref>, whereas some DL-based SE approaches adopted critical band importance to improve their model <ref type="bibr" target="#b28">[29]</ref>. These approaches demonstrate improvements in perceptual evaluation scores compared with the baseline approaches. In this study, we demonstrate that SE can be further improved to achieve SOTA performance by combining gamma correction with critical band importance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PCS on target feature for speech enhancement</head><p>In this section, the derivation of the proposed PCS on the target feature for SE based on two related studies is presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Auditory nonlinearity</head><p>Similar to the human vision system, the human auditory system exhibits a nonlinear relationship with speech signals. The sound pressure level (SPL) is measured in decibels (dB) as follows:</p><formula xml:id="formula_1">?(dB) = 10log10(I/I0)<label>(2)</label></formula><p>This equation standardizes the relationship between physical loudness and perceived loudness. The notations ?, I, and I0 denote the dB level, measured signal power, and reference signal power, respectively. The SPL of the human auditory and visual systems exhibits a similar property because the human eye perceives brightness based on reference to the brightest region of an image, as mentioned in 2.1. Thus, we designed a transfer function suitable for the human auditory systems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Modified gamma correction</head><p>We begin by implementing gamma correction on timefrequency features (spectrogram) of speech signals to enhance the contrast. Based on Eq. 1, the range of input features must be suitable for our task. In Eq. 1, the range of Vin is regulated between [0, 1]. However, the range of speech features cannot be regulated within this range. Hence, a new regulation is required, wherein the input values range between [0, V ] (here, V represents the maximum value of the input features). The processed signal was normalized when it was recovered in the waveform domain. Subsequently, the gamma correction equation for the time-frequency feature of speech signals was modified as follows:</p><formula xml:id="formula_2">Y t,f = A(M t,f ) ?<label>(3)</label></formula><p>where the value of input feature M t,f ranges from [0, M ]. The notations Y t,f , A, ?, and M t,f denote the modified feature, scaling function, gamma value, and input feature, respectively. Furthermore, the training features of our SE models were moved to the log(1 + p) domain (Log1p features) <ref type="bibr" target="#b29">[30]</ref>. We can thus derive Eq. 3 as follows:</p><formula xml:id="formula_3">Log1p C.S. Input STFT STFT Magnitude M t,f Log1p Magnitude X t,f SE Model Phase Model Prediction SE( Log1p( X t,f )) Objective Function Enhanced Feature Log1p(Y t,f ) iSTFT Target Enhanced Inverse of Log1p</formula><formula xml:id="formula_4">log1p(Y t,f ) = log(1 + Y t,f ) = ? * log(1 + M t,f )<label>(4)</label></formula><p>where the scaling function A is and (e) contain more contrastive patterns. This also suggests that feature enhancement can be implemented using different ? values to serve specific purposes.</p><formula xml:id="formula_5">(1 + 1/M t,f ) ? ? (1/M t,f ) ? in this case,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">PCS</head><p>Contrast-stretching was applied to perceptually enhance the target features on the training data. The waveform was first processed via short-time Fourier transform (STFT), wherein the phase component was excluded from the input stream for inverse STFT (iSTFT) (see <ref type="figure" target="#fig_2">Fig. 3</ref>). Subsequently, we applied contrast stretching to the target stream feature for the enhanced feature Y t,f . The Log1p feature was then obtained using the feature (here, Y t,f and X t,f denote the target and input streams, respectively). Consequently, the loss L can be computed as:</p><formula xml:id="formula_6">L = D(SE(Log1p(X t,f )), Log1p(Y t,f ))<label>(5)</label></formula><p>where D(?) denotes the objective functions, and SE(?) denotes the transformation by the SE. To determine the best performance afforded using a fixed ?, we tested several different hyperparameters (? = 0.5-2.0, with a step size of 0.1) in our validation set and evaluated their effectiveness. Experimental results on the VoiceBank-DEMAND dataset show that the best perceptual scores were achieved using ? = 1.4. Hence, we adopted 1.4 as the fundamental ? value. We reviewed Cochlea's knowledge in Sec. 2.2, its applications, and its importance in perceptual scores when applied to SE tasks. To further improve our contrast stretching for better perceptual performance, we designed feature enhancement based on the critical band importance. As most SE approaches adopt time-frequency features, we designed a feature enhancement that weights the features based on their frequencies. The band importance function (BIF) <ref type="bibr" target="#b30">[31]</ref> is listed in <ref type="table" target="#tab_0">Table 1</ref>. We designed the PCS based on the BIF and rescaled it into the range  BIFMax and BIFmin denote the maximum and minimum values of BIF , respectively. Meanwhile, P CSmin was set to 1, and ? = 1.4 was adopted. As shown in <ref type="figure" target="#fig_0">Fig. 1(f)</ref>, the proposed PCS 1 is effective. We can sharpen the formant peaks by applying PCS, where <ref type="figure" target="#fig_0">Fig. 1(f)</ref> is more contrastive compared with those presented in Figs. 1(a) and 1(b), but does not indicate severe distortions as in Figs. 1(d) and 1(e).</p><formula xml:id="formula_7">?P CS [k] = (? ? P CSmin) (BIFMax ? BIFmin) * BIF [k] + P CSmin</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>We evaluated our method using well-known network architectures, including the transformer <ref type="bibr" target="#b29">[30]</ref>, CRNN <ref type="bibr" target="#b31">[32]</ref>, Metric-GAN+ <ref type="bibr" target="#b32">[33]</ref> (from SpeechBrain <ref type="bibr" target="#b33">[34]</ref>), and DPT-FSNet <ref type="bibr" target="#b34">[35]</ref>. The transformer contains four convolutional encoder layers, eight self-attention heads, and a fully connected decoder layer. The CRNN comprises CNN layers, with one bidirectional long short-term memory (BLSTM) layer and two fully connected layers. The MetricGAN+ comprises a BLSTM-based generator with two bidirectional LSTM layers and a CNN-based discriminator. The dual-path transformer-based full-band and subband fusion network (DPT-FSNet) is a dual-path architecture that uses an improved transformer. To compare the proposed PCS with other methods, we used a publicly available dataset, VoiceBank-DEMAND, to evaluate the SE. The Voice Bank-DEMAND dataset is widely used as a benchmark for monaural SE approaches. The training set with 11572 utterances comprises 28 speakers corrupted with four signal-to-noise ratio (SNR) levels (15, 10, 5, and 0 dB). The test sets set with 824 utterances comprises two speakers corrupted at four SNR levels <ref type="bibr">(17.5, 12.5, 7</ref>.5, and 2.5 dB). Details regarding this dataset are available in <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison of different feature enhancement methods</head><p>As contrast-stretching for image processing can be considered a feature enhancement method for SE, we compared our proposed PCS with a fixed gamma of 1.4 with three typically used contrast enhancement methods-min-max normalization, histogram equalization (HE) <ref type="bibr" target="#b36">[37]</ref>, and adaptive equalization (AE) <ref type="bibr" target="#b37">[38]</ref>. These methods were adopted in the spectrum domain as contrast-stretching methods. HE was implemented along the time axis of each frequency band. A causal transformer SE model was used to evaluate the effectiveness of these methods. Feature enhancement was applied to the spectral domain and then transferred to the log1p features space in these experiments. <ref type="table" target="#tab_1">Table 2</ref> shows that the causal transformer (T) can be improved notably via PCS method, with a 0.31 PESQ score improvement. Moreover, except for HE, which failed to converge in training, the enhancement results are beneficial when feature enhancement approaches are used. To evaluate the effectiveness of the causal transformer with PCS, we tested performance of several causal models (e.g., causal DEMUCS <ref type="bibr" target="#b39">[40]</ref> and Conv-TasNet (scores from <ref type="bibr" target="#b38">[39]</ref>)). To the best of our knowledge, the proposed method outperformed other causal SE methods on this dataset in terms of the PESQ, CSIG, and COVL scores. <ref type="table" target="#tab_2">Table 3</ref> shows that the performance of different SE models, including the causal transformer (T(c)), non-causal transformer (T(nc)), CRNN, MetricGAN+ (MGAN+), and DPT-FSNet 2 (We used a frame size of 64 and a single batch size in all DPT-FSNet reproduction experiments and denote it as DPT*), can be improved by applying PCS. To the best of our knowledge, The DPT* + PCS achieved SOTA performance with a PESQ score of 3.35 and outstanding scores for other evaluation metrics. In general, we can infer from the improvements presented in <ref type="table" target="#tab_2">Table  3</ref> that PCS is a general and effective contrast stretching training strategy for DL-based SE approaches.  <ref type="table" target="#tab_3">Table 4</ref>. Note that for some CS methods (e.g., HE), information from the entire spectrogram is required, causing the SE system noncausal. The operation of PCS is similar to gamma correction; therefore the causality of the SE model will not be changed when PCS is applied as PP. From <ref type="table" target="#tab_3">Table  4</ref>, using PCS as a PP module can also improve the SE performance effectively. When comparing the results of Tables. 3 and 4, using PCS as a PP and applying PCS to the target feature enhancement yield comparable improvements. Especially, implementing PCS as a PP module allows it to be used alone (Noisy+PP-PCS) and combined with conventional SE methods (e.g., Wiener+PP-PCS); the detailed results and codes can be found at https://github.com/RoyChao19477/PCS. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Effectiveness using different SE models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We proposed a PCS for target features to further boost SE performance. PCS exerts a perceptual emphasis on target features to overcome the average-out problem (not precise) caused by distance-based objective functions. Three major contributions of the proposed PCS are noted: First, PCS is compatible with different SE models (both conventional and DL based). Second, no additional parameters are required in the SE model. Third, it does not affect the causality of causal SE models. We conclude that the proposed PCS can further improve the performance of previous SOTA SE models with an efficient operation of the target features. To the best of our knowledge, the causal transformer + PCS (causal) and DPT* + PCS (noncausal) approaches achieved the best PESQ score and competitive scores in other metrics on the Voice Bank-DEMAND dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>=Figure 1 :</head><label>1</label><figDesc>Based on human vision systems, the brightness perception of arXiv:2203.17152v4 [cs.SD] 15 Jul 2022 Normalized time-frequency feature (spectral magnitude) is stretched by different gamma values. The original clean feature (? = 1) is shown in (c); stretching based on different ? values is shown in (a) to (e); (f) shows the proposed PCS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Scaling curves of our modified gamma correction using different gamma values, where x and y represent input and output, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>High-level system block diagram of proposed perceptual contrast stretching training strategy for SE, where the brown block C.S. represent contrast-stretching step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>which is dynamic and based on M t,f . As shown in Fig 2, the modified gamma correction attenuates the features (decreases the contrast) when ? &lt; 1, and increases the value of the features (enhances the contrast) when ? &gt; 1. This effect is similarly observed on the speech spectrogram shown in Fig. 1, where (a) and (b) contain more blurry patterns, whereas (d)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 6 )</head><label>6</label><figDesc>where k denotes the index of the frequency bands, ?P CS [k] and BIF [k] are the ? values of PCS and BIF at band k, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Critical band importance and proposed PCS.</figDesc><table><row><cell>Frequency bands (Hz)</cell><cell>BIF</cell><cell>? P CS</cell></row><row><cell>0?100</cell><cell cols="2">0.000 1.0000</cell></row><row><cell>100?200</cell><cell cols="2">0.010 1.0702</cell></row><row><cell>200?300</cell><cell cols="2">0.026 1.1825</cell></row><row><cell>300?400</cell><cell cols="2">0.041 1.2877</cell></row><row><cell>400?4400</cell><cell>0.057</cell><cell>1.4</cell></row><row><cell>4400?5300</cell><cell cols="2">0.046 1.3228</cell></row><row><cell>5300?6400</cell><cell cols="2">0.034 1.2386</cell></row><row><cell>6400?7700</cell><cell cols="2">0.023 1.1614</cell></row><row><cell>7700?9500</cell><cell cols="2">0.011 1.0772</cell></row></table><note>[1.0, 1.4]. The rescaling function is formulated as follows:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>SE models with different feature enhancement methods on VoiceBank-DEMAND.</figDesc><table><row><cell></cell><cell cols="4">PESQ STOI CSIG COVL</cell></row><row><cell>Noisy</cell><cell>1.97</cell><cell>0.92</cell><cell>3.34</cell><cell>2.63</cell></row><row><cell>Wiener [18]</cell><cell>2.22</cell><cell>-</cell><cell>3.23</cell><cell>2.67</cell></row><row><cell>Conv-TasNet [39]</cell><cell>2.53</cell><cell>-</cell><cell>3.95</cell><cell>3.23</cell></row><row><cell>Demucs [40]</cell><cell>2.93</cell><cell>0.95</cell><cell>4.22</cell><cell>3.52</cell></row><row><cell>T [30]</cell><cell>2.76</cell><cell>0.94</cell><cell>4.10</cell><cell>3.44</cell></row><row><cell>T + min-max</cell><cell>2.80</cell><cell>0.93</cell><cell>4.09</cell><cell>3.45</cell></row><row><cell>T + HE</cell><cell>2.20</cell><cell>0.93</cell><cell>3.04</cell><cell>2.60</cell></row><row><cell>T + AE</cell><cell>2.82</cell><cell>0.94</cell><cell>4.12</cell><cell>3.48</cell></row><row><cell>T + ?=1.4</cell><cell>2.90</cell><cell>0.94</cell><cell>4.18</cell><cell>3.55</cell></row><row><cell>T + ?=PCS</cell><cell>3.07</cell><cell>0.94</cell><cell>4.26</cell><cell>3.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Different models with PCS on VoiceBank-DEMAND.</figDesc><table><row><cell></cell><cell cols="5">PESQ STOI CSIG COVL Cau.</cell></row><row><cell>Noisy</cell><cell>1.97</cell><cell>0.92</cell><cell>3.34</cell><cell>2.63</cell><cell>-</cell></row><row><cell>SEGAN [18]</cell><cell>2.16</cell><cell>-</cell><cell>3.48</cell><cell>2.80</cell><cell>No</cell></row><row><cell>T (c) [30]</cell><cell>2.76</cell><cell>0.94</cell><cell>4.10</cell><cell>3.44</cell><cell>Yes</cell></row><row><cell>T (c) + PCS</cell><cell>3.07</cell><cell>0.94</cell><cell>4.26</cell><cell>3.67</cell><cell>Yes</cell></row><row><cell>T (nc) [30]</cell><cell>2.84</cell><cell>0.94</cell><cell>4.20</cell><cell>3.51</cell><cell>No</cell></row><row><cell>T (nc) + PCS</cell><cell>3.15</cell><cell>0.94</cell><cell>4.34</cell><cell>3.75</cell><cell>No</cell></row><row><cell>CRNN [32]</cell><cell>2.83</cell><cell>0.94</cell><cell>4.18</cell><cell>3.51</cell><cell>No</cell></row><row><cell>CRNN + PCS</cell><cell>3.11</cell><cell>0.94</cell><cell>4.31</cell><cell>3.72</cell><cell>No</cell></row><row><cell>MGAN+ [33]</cell><cell>3.15</cell><cell>0.93</cell><cell>4.14</cell><cell>3.64</cell><cell>No</cell></row><row><cell>MGAN+ + PCS</cell><cell>3.21</cell><cell>0.93</cell><cell>4.15</cell><cell>3.67</cell><cell>No</cell></row><row><cell>DPT [35]</cell><cell>3.33</cell><cell>0.96</cell><cell>4.58</cell><cell>4.00</cell><cell>No</cell></row><row><cell>DPT*</cell><cell>3.11</cell><cell>0.95</cell><cell>4.30</cell><cell>3.72</cell><cell>No</cell></row><row><cell>DPT* + PCS</cell><cell>3.35</cell><cell>0.95</cell><cell>4.43</cell><cell>3.92</cell><cell>No</cell></row><row><cell cols="2">4.4. Effectiveness of PP</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">PCS can be implemented as a PP module, aiming to further</cell></row><row><cell cols="6">improve enhanced speech from an SE model. The results are</cell></row><row><cell>presented in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>PP-PCS with different SE models on the VoiceBank-Demand dataset. "Noisy" denotes original speech without SE.</figDesc><table><row><cell></cell><cell cols="4">PESQ STOI CSIG COVL</cell></row><row><cell>Noisy</cell><cell>1.97</cell><cell>0.92</cell><cell>3.34</cell><cell>2.63</cell></row><row><cell>Noisy + PP-PCS</cell><cell>2.47</cell><cell>0.92</cell><cell>3.63</cell><cell>3.03</cell></row><row><cell>Wiener</cell><cell>2.22</cell><cell>0.91</cell><cell>3.21</cell><cell>2.65</cell></row><row><cell>Wiener+ PP-PCS</cell><cell>2.63</cell><cell>0.91</cell><cell>3.39</cell><cell>2.95</cell></row><row><cell>MGAN+ + PP-PCS</cell><cell>3.20</cell><cell>0.92</cell><cell>4.08</cell><cell>3.63</cell></row><row><cell>DPT* + PP-PCS</cell><cell>3.30</cell><cell>0.95</cell><cell>4.35</cell><cell>3.84</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The actual frequency band regions we used to implement PCS are slightly different fromTable 1owing to the STFT limitations. Detailed settings can be fount at https://github.com/RoyChao19477/ PCS/PCS.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The contrast-stretched target features are transferred back to the waveform domain as training targets.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Speech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. LVA/ICA</title>
		<meeting>LVA/ICA</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A speech enhancement algorithm by iterating single-and multi-microphone processing and its application to robust ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial networks for speech enhancement and noise-robust speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Michelsanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust speaker recognition based on single-channel and multi-channel speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Taherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1293" to="1302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning reinvents the hearing aid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE spectrum</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="32" to="37" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Harnessing the power of artificial intelligence to transform hearing healthcare and research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Lesica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Manjaly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-G</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="840" to="849" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Suppression of acoustic noise in speech using spectral subtraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on acoustics, speech, and signal processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="120" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Speech enhancement based on a priori signal to noise estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Scalart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Filho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Speech enhancement based on deep denoising autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A regression approach to speech enhancement based on deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="19" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Experiments on deep learning for speech denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning spectral mapping for speech dereverberation and denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Merks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="982" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Endto-end waveform utterance enhancement for direct evaluation metrics optimization by fully convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kawai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1570" to="1584" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Complex ratio masking for monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">speech, and language processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="483" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint optimization of masks and deep recurrent neural networks for monaural source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2136" to="2147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Speech enhancement and recognition using multi-task learning of long short-term memory recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">T-gsa: Transformer with gaussian-weighted self-attention for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Segan: Speech enhancement generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serr?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Metricgan: Generative adversarial networks based black-box metric scores optimization for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Speech denoising with deep feature losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving perceptual quality by phone-fortified perceptual loss using Wasserstein Distance for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-A</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTER-SPEECH</title>
		<meeting>INTER-SPEECH</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A perceptually-motivated approach for lowcomplexity, real-time enhancement of fullband speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Valin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Phansalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Helwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnaswamy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.04259</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Truth-to-estimate ratio mask: A post-processing method for speech enhancement direct at low signal-to-noise ratios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An adaptive gamma correction for image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abdullah-Al-Wadud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Al-Quaderi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shoyaib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sii-speech intelligibility index standard: ANSI s3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pavlovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">the Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1906" to="1906" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Poynton</surname></persName>
		</author>
		<title level="m">Digital video and HD: Algorithms and Interfaces</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Low-delay speech enhancement using perceptually motivated target and loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Speech enhancement using critical band spectral subtraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICSLP</title>
		<meeting>ICSLP</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bone-conducted speech enhancement using deep denoising autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Fuh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="106" to="112" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Boosting objective scores of a speech enhancement model by metricgan postprocessing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-A</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zezario</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. APSIPA</title>
		<meeting>APSIPA</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">S3. 5-1997, methods for the calculation of the speech intelligibility index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ansi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">American National Standards Institute</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="90" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Convolutionalrecurrent neural networks for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zarar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tashev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Metricgan+: An improved version of metricgan for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-A</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Plantinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.03538</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Speechbrain: A general-purpose speech toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Plantinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rouhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Subakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dawalatabad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04624</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Dpt-fsnet: Dual-path transformer based full-band and sub-band fusion network for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13002</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Noisy speech database for training speech enhancement algorithms and TTS models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Valentini-Botinhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adaptive histogram equalization and its variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Pizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Amburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cromartie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geselowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Greer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ter Haar Romeny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zuiderveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision, graphics, and image processing</title>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="355" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adaptive equalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">U</forename><surname>Qureshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1349" to="1387" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Exploring the best loss function for DNN-based low-latency speech enhancement with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11611</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Real time speech enhancement in the waveform domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D?fossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
