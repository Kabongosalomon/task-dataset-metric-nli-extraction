<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Balance Specificity and Invariance for In and Out of Domain Generalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithvijit</forename><surname>Chattopadhyay</surname></persName>
							<email>prithvijit3@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
							<email>yogesh@cs.umd.com</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Balance Specificity and Invariance for In and Out of Domain Generalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Distribution Shift, Domain Generalization</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce Domain-specific Masks for Generalization, a model for improving both in-domain and out-of-domain generalization performance. For domain generalization, the goal is to learn from a set of source domains to produce a single model that will best generalize to an unseen target domain. As such, many prior approaches focus on learning representations which persist across all source domains with the assumption that these domain agnostic representations will generalize well. However, often individual domains contain characteristics which are unique and when leveraged can significantly aid in-domain recognition performance. To produce a model which best generalizes to both seen and unseen domains, we propose learning domain specific masks. The masks are encouraged to learn a balance of domain-invariant and domain-specific features, thus enabling a model which can benefit from the predictive power of specialized features while retaining the universal applicability of domain-invariant features. We demonstrate competitive performance compared to naive baselines and state-of-the-art methods on both PACS and DomainNet. ?</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The success of deep learning has propelled computer vision systems from purely academic endeavours to key components of real-world products. This deployment into unconstrained domains has forced researchers to focus attention beyond a closed-world supervised learning paradigm, where learned models are only evaluated on held-out in-domain test data, and instead produce models capable of generalizing to diverse test time data distributions. This problem has been formally studied and progress measured in the domain generalization literature <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b14">15]</ref>. Most prior work in domain generalization focuses on learning a model which generalizes to unseen domains by either directly optimizing for domain invariance <ref type="bibr" target="#b35">[36]</ref> or designing regularizers that induce such a bias <ref type="bibr" target="#b2">[3]</ref>, the idea being that features which are present across multiple <ref type="figure">Fig. 1</ref>: Balancing specificity and invariance. At training time, we optimize for a combination of domain-specific (shown in blue, yellow, red) and domain invariant (shown in black) learned representations. Partially invariant representations are indicated as color combinations (i.e. blue + yellow = green). At testtime, these learned representations that capture a balance of domain-specificity and invariance allow the classifier to make a better prediction for given testinstance by leveraging domain-specific features from the most similar source domains.</p><p>training distributions are more likely to persist in the novel distributions. However, in practice, as the number of training time data sources increases it becomes ever more likely that at least some of the data encountered at test time will be very similar to one or more source domains. In such a situation, ignoring features specific to only a domain or two may artificially limit the efficacy of the final model. However, leveraging a balance between "invariance" -features that are shared across domains -and "specificity" -features which are specific to individual domains -might actually aid the model in making a better prediction.</p><p>It is important to note that the similarity of data encountered at test-time to a source domain can be understood clearly only in the context of the other available source domains. Consider the example in <ref type="figure">Fig. 1</ref>, where a classifier trained on clipart, sketch and painting encounters an instance from a novel domain quickdraw at test-time. Due to the severe domain-shift involved, leveraging the relative similarity of the test-instance to samples from sketch might result in a better prediction compared to a setting where the model relies solely on invariant characteristics across domains. However, manually crafting such a balance or creating an explicit separation between domain-specificity and invariance <ref type="bibr" target="#b19">[20]</ref> is not scalable as the number and diversity of the source distributions available during training increases.</p><p>In this paper, we propose DMG: Domain-specific Masks for Generalization, an algorithm for automatically learning to balance between domain-invariant and domain-specific features producing a single model capable of simultaneously achieving strong performance across multiple distinct domains. At a high-level, we cast this problem of balanced feature selection as one of learning distribution-specific binary masks over features of a shared deep convolutional network (CNN). Specifically, for a given layer in the CNN, we associate domainspecific mask parameters for each neuron which decide whether to turn that neuron on or off during a forward pass. We learn these masks end-to-end via backpropagation along with the network parameters. To promote discriminative features and strong end-task performance, we simultaneously minimize the standard classification error and, to encourage domain-specificity in the selected features, we penalize for overlap amongst masks from different source domains. Importantly, our approach uses straightforward optimization across all pooled source data without any need for multi-stage training or meta-learning. At testtime we average the predictions obtained by applying all the individual source domain masks thus making a prediction that is informed by both characteristics which are shared across the source domains and are specific to individual domains. Based on our experiments, we find that not only does our modeling choice result in at par or improved performance compared to other complex alternatives that explicitly model domain-shift during training, but also allows us to explicitly characterize activations specific to individual source domains. Compared to prior work, we find that our approach is much more scalable and is faster to train as training time is essentially equivalent to the same as training a vanilla aggregate baseline which pools data from multiple source domains and trains a single deep network.</p><p>Additionally, we note that efforts towards domain generalization in the computer vision literature have focused primarily on measuring novel domain performance at test time. Since it is likely that in a realistic scenario the model might also encounter data from the source distributions at test-time, it is equally important to retain strong performance on the source distributions in addition to improved generalization to novel domains. Thus, given that measuring continued holistic progress in domain generalization requires benchmarking proposed solutions in terms of both in and out-of-domain generalization performance, we also report in-domain generalization performance on the large DomainNet <ref type="bibr" target="#b37">[38]</ref> benchmark proposed for domain adaptation. Concretely, we make the following contributions.</p><p>-We introduce an approach, DMG: Domain-specific Masks for Generalization, that learns models capable of balancing specificity and invariance over multiple distinct domains. We demonstrate that despite our relatively simple approach, DMG achieves competitive out-of-domain performance on the commonly used PACS <ref type="bibr" target="#b25">[26]</ref> benchmark and on the challenging DomainNet <ref type="bibr" target="#b37">[38]</ref> dataset. In addition, we demonstrate that our model can be used as a drop-in replacement for an aggregate model when evaluated on in-domain test samples, or can be trivially converted into a high performing domain-specific model given a known test time domain label.</p><p>-We verify that our model does indeed lead to the emergence of domain specificity and show that our test time performance is stable across a variety of allowed domain overlap settings. Though not the focus of this paper, this domain specificity may be a helpful tool towards model interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Domain Adaptation. Significant progress has been made in the problem of unsupervised domain adaptation where given access to a labeled source and an unlabeled target dataset, the task is to improve performance on the target domain. One popular line of approaches include learning a domain invariant representation by minimizing the distributional shift between source and target feature distributions using an adversarial loss <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b46">47]</ref>, or MMD-based loss <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>. While these approaches perform alignment in the feature space, pixel-level alignment is performed using cross-domain generative models such as GANs in <ref type="bibr" target="#b5">[6]</ref>. A combination of feature-level and pixel-level aligment is explored in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b42">43]</ref>. In addition, several regularization strategies have also been proven to be effective for domain adaptation such as dropout regularization <ref type="bibr" target="#b40">[41]</ref>, classifier discrepancy <ref type="bibr" target="#b41">[42]</ref>, self-ensembling <ref type="bibr" target="#b12">[13]</ref>, etc. Most existing domain adaptation methods consider the setting where the source and the target datasets contain one domain each. In multi-source domain adaptation, the source dataset consists of a mixture of multiple domains where domain alignment is performed using an adversarial interplay involving a k-way domain discriminator in <ref type="bibr" target="#b49">[50]</ref>, and multi-domain moment matching in <ref type="bibr" target="#b37">[38]</ref>. Domain Generalization. Similar to the multi-source domain adaptation problem, domain generalization considers multiple domains in the input data distribution. However, no access to the target distribution (including the unlabeled target) is assumed during training. This makes domain generalization a much harder problem than multi-source adaptation. One common approach to the problem involves decomposing a model into domain-specific and domaininvariant components, and using the domain-invariant component to make predictions at test time <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21]</ref>. Recently, the use of meta-learning for domain generalization has gained much attention. <ref type="bibr" target="#b27">[28]</ref> extends the MAML framework of <ref type="bibr" target="#b11">[12]</ref> for domain generalization by learning parameters that adapt quickly to target domains. In <ref type="bibr" target="#b2">[3]</ref>, a regularization function is estimated using meta-learning, which when used with multi-domain training results in a robust minima with improved domain generalization. Use of data augmentation techniques for domain generalization is explored in <ref type="bibr" target="#b48">[49]</ref>. Recently, a novel variant of empirical risk minimization framework, called Invariant Risk Minimization (IRM) has been proposed in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref> to make machine learning models invariant to spurious correlations in data when training across multiple sources. Disentangled Representations. The goal of learning disentangled representations is to be able to disentangle learned features into multiple factors of variations, each factor representing a semantically meaningful concept. The problem has primarily been studied in the unsupervised setting. Typical approaches involve training a generative model such as a GAN or VAE while imposing constraints in the latent space using KL-divergence <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b7">8]</ref> or mutual information <ref type="bibr" target="#b8">[9]</ref>.</p><p>In the context of domain adaptation, disentangling features into domain-specific and domain-independent factors have been proposed in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b38">39]</ref>. The domainindependent factors are then used to obtain predictions in the target domain. Our approach performs a similar implicit disentanglement, where domain-specific and domain-invariant factors are mined using a masking operation. Dropout, Pruning, Sparsification and Attention. Our approach to learn domain-specific masks is similar to the techniques adopted in the network pruning and sparsification literature. Relevant to our work are approaches that directly learn a pruning strategy during training <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48]</ref>. <ref type="bibr" target="#b43">[44]</ref> involves learning masks over parameters under a sparsity constraint to discover small subnetworks. In addition to model compression, pruning strategies have also been used in multi-task and continual learning. In <ref type="bibr" target="#b44">[45]</ref>, catastrophic forgetting is prevented while learning tasks (and subsequently attending over them) in a sequential manner. In <ref type="bibr" target="#b32">[33]</ref>, a binary mask corresponding to individual tasks are learnt for a fixed backbone network. The resulting task-specific network is obtained by applying the learnt masks on the backbone network. In <ref type="bibr" target="#b33">[34]</ref>, weights of a network are iteratively pruned to free up packets of neurons. The free neurons are in-turn updated to learn new tasks without forgetting. A similar approach is proposed in <ref type="bibr" target="#b4">[5]</ref> for multi-domain learning where domain-specific networks are constructed by masking convolution filters under a budget on new parameters being introduced for each domain. Similarly, several approaches building on top of Dropout <ref type="bibr" target="#b45">[46]</ref> have also been proposed for domain adaptation. In <ref type="bibr" target="#b40">[41]</ref>, a pair of sub-networks are sampled from dropout that give maximal classifier discrepancy. Feature network is trained to minimize this discrepancy, thus making it insensitive to perturbations in classifier weights. An efficient implementation of this idea using adversarial dropout is proposed in <ref type="bibr" target="#b24">[25]</ref>. In [51], saliency supervision is used to develop explainable models for domain generalization. While DMG is akin to attention being used as learned masks for subset selection <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b32">33]</ref>, our focus is on implicitly learning to disentangle domain-specific and invariant feature components for multi-source domain generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Our motivation to ensure a balance between specificity and invariance is to aid prediction in situations where an instance at test-time might benefit from some of the domain-specific components captured by the domain-specific masks. In what follows, we first describe the problem setup, ground associated notations and then describe our proposed approach, DMG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Setup</head><p>Domain generalization involves training a model on data, denoted as X , sampled from p source distributions that generalizes well to q unknown target distributions which lack training data. Without loss of generality we focus on the classification case, where the goal is to learn a model which maps inputs to the Post feature extraction, an elementwise product of the obtained binary masks is performed with the neurons of the task network layer (L) to obtain the effective activations being passed on to the next layer (L + 1). The mask and network parameters are learned end-to-end based on the standard cross-entropy coupled with the sIoU loss penalizing mask overlap among the source domains.</p><formula xml:id="formula_0">desired output label, M : X ? Y. Let {D i } p+q i=1 denote the p + q distributions with same support X ?Y. Let D i = {(x (i) j , y (i) j )} |Di| i=1</formula><p>refer to the dataset sampled from the i th distribution, i.e., D i ? D i . We operate in the setting where all the distributions share the same label space and distributional variations exist only in the input data (space X ). We are interested in learning a parametric model M ? : X ? Y, that we can decompose into a feature extractor (F ? ) and a task-</p><formula xml:id="formula_1">network (T ? ) i.e., M ? (x) = (T ? ? F ? )(x),</formula><p>where ?, ?, ? denote the parameters of the complete, feature and the task networks respectively. For the remaining subsections, we refer to the set of source domains as D S and index individual source domains by d. We learn domain specific masks only on the neurons present in the task network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Activation or Feature Selection via Domain-Specific Masks</head><p>Our goal is to learn representations which capture a balance of domain specific components (useful for predictive performance on a specific domain) and domain invariant components (useful in general for the discriminative task). Capturing information contained in multiple source distributions in such a manner allows us to make better predictions by automatically relying more on characteristics of a specific source domain in situations where an instance observed at test-time is relatively similar to one of the sources. We cast this problem of disentangling domain-specific and domain-invariant feature components as that of learning binary masks on the neurons of the task network specific to individual source domains. More specifically, for each of the p source distributions, we initialize masks m d over neurons (or activations) of the task-network T ? . Our masks can be viewed as layer-wise gates which decide which neurons to turn on or off during a forward pass through the network.</p><p>Given k neurons at some layer L of T ? , we introduce parametersm d ? R k for each of the source distributions d ? D S . During training, for instances x d i from domain d, we first form mask probabilities m d via a sigmoid operation as m d = ?(m d ). Then, the binary masks m d i are sampled from a bernoulli distribution given by the mask probabilities. i.e., m d i ? m d , with m d i ? {0, 1} k . Upon sampling masks for individual neurons, the effective activations which are passed on to the next layer L + 1 are? L = a L m d i , i.e., an elementwise product of the obtained activations and the sampled binary masks (see <ref type="figure">Fig. 2</ref>, right). During training, we sample such binary masks corresponding to the source domain of the input instance, thereby making feedforward predictions by only using domain-specific masks. Under this setup, the prediction made by the entire network M ? for an instance</p><formula xml:id="formula_2">x d i ? d can be expressed as? i = M ? (x d i ; m d i ) where m d</formula><p>i denotes the sampled mask (for domain d) being applied to all neurons in the task-network T ? . Note that, akin to dropout <ref type="bibr" target="#b45">[46]</ref>, these domain-specific masks identify domain-specific sub-networks -for an instance x d i , the sampled binary mask m d i identifies a specific "thinner" subnetwork. We learn the mask-parameters in addition to the parameters of the network during training. However, note that the mask-parametersm d cannot be updated directly using back-propagation as the sampled binary mask is discrete. We approximate gradients through sampled discrete masks using the straight-through estimator <ref type="bibr" target="#b3">[4]</ref>, i.e., we use a discretized m d i during a forward pass but use the continuous version m d during the backward pass by approximating ? m d i L ? ? m d L. Even though the hard sampling step is non-differentiable, gradients with respect to m d i serve as a noisy estimator of ? m d L. Incentivizing Domain-Specificity. To ensure the masks capture neurons that are specific to individual source domains, we need to encourage specificity in the masks while maximizing predictive performance on the source set of distributions. To incentivize domain-specificity, we introduce an additional soft-overlap loss that ensures masks associated with each of the source distributions overlap minimally. To quantify overlap we compute the Jaccard Similarity Coefficient <ref type="bibr" target="#b18">[19]</ref> (also known as IoU score) among pairs of source domain masks. However, as IoU is non-differentiable it is not possible to directly optimize for the same using gradient descent. Therefore, inspired by prior work <ref type="bibr" target="#b39">[40]</ref>, we minimize the following soft-overlap loss for every pair of source domain masks {m di , m dj } at a layer L as,</p><formula xml:id="formula_3">sIoU(m di , m dj ) = m di ? m dj k (m di + m dj ? m di m dj )<label>(1)</label></formula><p>where m di ? m dj approximates the intersection for the pair of source domain masks as the inner product of the mask distributions, denotes the elementwise product and k denotes the number of neurons in layer L. During training sIoU ensures predictions for instances from different source domains are made using different sub-networks (as identified by the domain-specific binary masks). To summarize, for a set of source domains D S the overall objective we optimize during training ensures -(1) good predictive performance on the discriminative task at hand and (2) minimal overlap among source-domain masks,</p><formula xml:id="formula_4">L(?, ?,m d1 , ..,m d |D S | ) = d?D S x d i ?d L class (?, ?, m d i ) +? O L?T ? (di,dj )?D S sIoU(m di , m dj )<label>(2)</label></formula><p>where m d i ? m d i for every instance x d i of the source domain d and L class (?) denotes the standard cross entropy loss. <ref type="figure">Fig. 2</ref> summarizes our training pipeline in context of a standard aggregation method where a CNN is trained jointly on data pooled from all the source domains. Prediction at Test-time. To obtain a prediction at test-time, we follow a soft-scaling scheme similar to Dropout <ref type="bibr" target="#b45">[46]</ref>. Recall that sampling from domainspecific soft-masks essentially amounts to sampling a "thinned" sub-network from the orginal task-network. However, since it is intractable to obtain predictions from all such possible (exponential) domain-specific sub-networks, we follow a simple averaging scheme that ensures that the expected output under the distribution induced by the masks is the same as the actual output at test-time. Specifically, we scale every neuron by the associated domain-specific soft-mask m d instead of turning neurons on or off based on a discrete mask m ? m d and average the predictions obtained by applying m d for all the source domains to the task network. ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings.</head><p>Datasets and Metrics. We conduct domain generalization (DG) experiments on the following datasets:</p><p>PACS [26] -PACS is a recently proposed benchmark for domain generalization which consists of only 9991 images of 7 classes, distributed across 4 domainsphoto, art-painting, cartoon and sketch. Following standard practice, we conduct 4 sets of experiments -treating one domain as the unseen target and the rest as the source set of domains. The authors of <ref type="bibr" target="#b25">[26]</ref> provide specified train and val splits for each domain to ensure fair comparison and treat the entirety of train + val as the test-split of the target domain. We use the same ? We experimented with learning a domain-classifier on source domains to use the predicted probabilities as weights for test-time averaging. We observed insignificant difference in out-of-domain performance but significantly worse in-domain performance, though we believe this may be dataset-specific.  <ref type="bibr" target="#b37">[38]</ref> recently released annotated train and test splits for all the 6 domains. We divide the train split from <ref type="bibr" target="#b37">[38]</ref> randomly in a 90-10% proportion to obtain train and val splits for our experiments. Similar to PACS, we conduct 6 sets of leave-one-out experiments. We report out-of-domain performance as the accuracy on the test split of the unseen domain. For in-domain performance, we report accuracy averaged over all the source domain test splits. Models. We experiment with ImageNet <ref type="bibr" target="#b9">[10]</ref> pretrained AlexNet <ref type="bibr" target="#b23">[24]</ref>, ResNet-18 <ref type="bibr" target="#b16">[17]</ref> and ResNet-50 <ref type="bibr" target="#b16">[17]</ref> backbone architectures. For AlexNet, we apply domainspecific masks on the input activations of the last three fully-connected layers -our task network T ? -and turn dropout <ref type="bibr" target="#b45">[46]</ref> off while learning the domainspecific masks. For ResNet-18 and 50, we apply domain specific masks on the input activations of the last residual block and the first fully connected layer. ? ? Specifically, for ResNet, the domain-specific masks are trained to drop or keep specific channels in the input activations as opposed to every spatial feature in every channel in order to reduce complexity in terms of the number of mask parameters to be learnt. Baselines and Points of Comparison. We compare DMG with two simple baselines (treating dropout <ref type="bibr" target="#b45">[46]</ref> as usual if present in the backbone CNN) -(1) Aggregate -the CNN backbone trained jointly on data accumulated from all the source domains and (2) Multi-Headed -the CNN backbone with different classifier heads corresponding to each of the source domains (at test-time we average predictions from all the classifier heads). Note, this baseline has more parameters than our model due to the repeated classification heads. In addition to the above baselines, we also compare with the recently proposed domain generalization approaches (cited in <ref type="table" target="#tab_1">Tables 1,2 and 3)</ref>. Please refer to the appendix for implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>We report results on both PACS (out-of-domain) and DomainNet (in-domain and out-of-domain). For DomainNet, we use C, I, P, Q, R, S to denote the domainsclipart, infograph, painting, quickdraw, real and sketch respectively. On PACS, we use A, C, P and S to denote the domainsart-painting, cartoon, photo and sketch respectively. We summarize the observed trends below:  <ref type="bibr" target="#b2">[3]</ref> using AlexNet as the backbone architecture in terms of overall performance -with an improvement of 2.7% over MetaReg <ref type="bibr" target="#b2">[3]</ref> and 2.6% over the Aggregate baseline. Interestingly, this corresponds to an almost 2.89% improvement on the I,P,Q,R,S?C and a 2.63% improvement on the C,I,P,Q,S?R shifts (see <ref type="table" target="#tab_1">Table 1</ref>, AlexNet set of rows). Using ResNet-18 as the backbone architecture, we observe that DMG is competitive with MetaReg <ref type="bibr" target="#b2">[3]</ref> (improvement margin of 0.43%) accompanied by improvements on the I,P,Q,R,S?C and C,I,P,R,S?Q shifts. We observe similar trends using ResNet-50, where DMG is competitive with the best performing Aggregate-SGD ? baseline.</p><p>PACS -To compare DMG with prior work in the Domain Generalization literature, we also report results on the more commonly used PACS <ref type="bibr" target="#b25">[26]</ref> benchmark in <ref type="table" target="#tab_2">Table 2</ref>. We find that in terms of overall performance, DMG with AlexNet as the backbone architecture outperforms baselines and prior approaches including  Due to its increased size, both in terms of number of images and number of categories, DomainNet proves to be a more challenging benchmark than PACS. Likely due to this difficulty, we find that performance on some of the hardest shifts (with Quickdraw and Infograph as the target domain) is significantly low (&lt;25% for Quickdraw). Furthermore, DMG and prior domain generalization approaches perform comparably to naive baselines (ex. Aggregate) on these shifts, indicating that there is significant room for improvement. In-Domain Generalization. For each of the domain-shifts in <ref type="table">Table.</ref> 1, we further report in-domain generalization performance on DomainNet in <ref type="table">Table.</ref> 3. For in-domain evaluation, we present both our standard approach as well as a version which assumes knowledge of the domain corresponding to each test instance. For the latter, we report the performance of DMG using only the mask corresponding to the known domain (KD) label and refer to this as DMG-KD. Notably, for this case where a test instance is drawn from one of the source domains, DMG-KD provides significant performance improvement over the baselines (see <ref type="table" target="#tab_3">Table 3</ref>). Compared to DMG, we observe that DMG-KD results in a consistent improvement of ?1-2%. This alludes to the fact that the learnt domain-specific masks are indeed specialized for individual source domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>Domain Specialization. We demonstrate that as an outcome of DMG, using masks corresponding to the source domain at hand leads to siginificantly improved in-domain performance compared to a mismatched domain-mask pair, indicating the emergence of domain-specialized masks. In <ref type="table">Table.</ref> 4, we report results on the I,P,Q,R,S?C (easy) and C,I,P,R,S?Q (hard) shifts using AlexNet as the backbone CNN. We report both in and out-of-domain performance using each of the source domain masks and compare it with the setting when predictions from all the source domain masks are averaged. The cells highlighted in gray represent in-domain accuracies when masks are paired with the corresponding source domain. Clearly, using the mask corresponding to the source domain instance at test-time (also see DMG-KD in <ref type="table">Table.</ref> 3) leads to significantly improved performance compared to the mis-matched pairs -with differences with the second best source domain mask ranging from ?2-4% for I,P,Q,R,S?C and ?3-6% for C,I,P,R,S?Q. This indicates that not only do the source domain masks overlap minimally, but they are also "specialized" for each of the source domains in terms of predictive performance. We further observe that averaging predictions obtained from all the source domain masks leads to performance that is relatively closer to the DMG-KD setting compared to a mismatched mask-domain pair (but still falls behind by ?2-3%). We note that certain source domain masks do lead to out-of-domain accuracies which are close (within 1%) to the combined settingm Quickdraw for the I,P,Q,R,S?C shift and m Clipart , m Infograph , m Sketch for the C,I,P,R,S?Q shift. This highlights the motivation at the heart of our approach -how leveraging characteristics specific to individual source domains in addition to the invariant ones are useful for generalization. Sensitivity to ? O . A key component of our approach is the soft-IoU loss which encourages domain specificity by minimizing overlapping features across domains. During optimization, we require setting of a loss balancing hyperparameter, ? O . Here, we explore the sensitivity of our model to ? O by sweeping from 0 to 1 in logarithmic increments. <ref type="figure">Fig. 4</ref> shows the final in and out-of-domain accuracies ( <ref type="figure">Fig. 4 (b)</ref> and (a)) and overlap ( <ref type="figure">Fig. 4 (c)</ref>) measured as the IoU <ref type="bibr" target="#b18">[19]</ref> among pairs of discrete source domain masks obtained by thresholding the softmask values per-domain at 0.5, i.e., m = 1 m d &gt;0.5 for domain d. We observe that both in and out-of-domain generalization performance is robust to the choice of ? O , with only minor variations and a slight drop in in-domain performance at extreme values of ? O (0.1 and 1). In <ref type="figure">Fig. 4 (c)</ref>, we observe that initially average pairwise IoU measures stay stable till ? O = 10 ?3 but drop at high values of ? O = 0.1 and 1 (as low as &lt; 60% for some shifts)-indicating an increase in the "domain specificity" of the masks involved. Note that low IoU at high-values of ? O is accompanied only by a minor drop in in-domain performance and almost no-drop in out-of-domain performance! It is crucial to note here that although there is an expected trade-off between specificity and generalization performance this trade-off does not result in large fluctuations for DMG. Please refer to the appendix for more analysis of DMG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>To summarize, we propose DMG: Domain-specific Masks for Generalization, a method for multi-source domain learning which balances domain-specific and domain-invariant feature representations to produce a single strong model capable of effective domain generalization. We learn this balance by introducing domain-specific masks over neurons and optimizing such masks so as to minimize cross-domain feature overlap. Thus, our model, DMG, benefits from the predictive power of features specific to individual domains while retaining the generalization capapbilities of components shared across the source domains. DMG achieves competitive out-of-domain performance on the commonly used PACS dataset and competitive in and out-of-domain performance on the challenging DomainNet dataset. Although beyond the scope of this paper, encouraging a blend of domain specificity and invariance may be useful not only in the context of generalization performance but also in terms of model interpretability. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Appendix</head><p>In this appendix, we further discuss the specificity of the obtained domainspecific masks (Section. 7.1). Following this, we discuss how sparsity as an incentive compares with sIoU in terms of learning a balance between specificity and invariance and in terms of performance (Section. 7.2). In Section. 7.3, we discuss alternative techniques for directly ensembling masks instead of the output predictions in response to each mask. In Section. 7.4, we provide more extensive comparisons to prior work on the PACS <ref type="bibr" target="#b25">[26]</ref> dataset. Finally, in Section. 7.5, we describe in detail the implementation and other details associated with our experiments. We use C, I, P, Q, R, S to denote the domainsclipart, infograph, painting, quickdraw, real and sketch respectively on the DomainNet <ref type="bibr" target="#b37">[38]</ref> dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Domain Specificity</head><p>As discussed in Section. 3.2 (main paper), we incentivize domain specificity by optimizing the soft-IoU (sIoU) objective (see Eqn. 2 in main paper). To understand the extend of domain-specificity achieved at convergence, we measure the Jaccard Similarity Coefficient <ref type="bibr" target="#b18">[19]</ref> (also known as IoU) among pairs of discrete source domain masks, which we obtain by thresholding the soft-mask values per-domain at 0.5, i.e., m = 1 m d &gt;0.5 for domain d. <ref type="figure">Fig. 4</ref> shows the IoU among pairs of source domain masks in addition to the overall average on DomainNet for the I,P,Q,R,S?C and C,I,P,R,S?Q shifts with AlexNet as the backbone architecture (? O = 0.1 during training). Note that the above metric provides information about the fraction of overlapping neurons which are shared among pairs of source domains but only considers them among the ones which are activated (turned on) based on the discrete masks m. Therefore, in addition to the IoU statistics (as represented by the bars), we also report the fraction of activated neurons on average. We note that domain specificity does emerge by learning masks in the manner described in Sec. 3.2 of the main paper, as evident by the IoU measures across pairs being lower than -(1) ?96% for the maximal pairwise IoU and (2) ?92% for overall IoU measures across both the shifts. <ref type="figure">Fig. 5</ref> shows how the layerwise overall IoU measure evolves as ? O increases. While at lower values of ? O , the amount of specificity is relatively low and similar across layers, at higher values of ? O we see an increase of varying degrees across layers -the relative ordering among layers in terms of IoU being fc6&gt;fc7&gt;fc8, indicating the importance of having more shared neurons in the earlier layers.</p><p>Finally, note that since the pairwise IoU measures indicate the fraction of neurons which are shared among the neurons which are turned on, upon convergence we can essentially categorize the neurons present in the task network into three categories -(1) equally useless -neurons turned off across all the source domain masks, (2) equally useful or shared -neurons turned on across all the source domain masks and (3) domain-specific -neurons turned on only for specific source domains. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Choice of Incentive: sIoU vs Sparsity</head><p>As described in Section. 3.2 (main paper), to ensure feature selection, we impose a soft-IoU loss in addition to standard cross-entropy training to penalize overlap among pairs of source domain masks. However, in practice, one could also impose a sparsity constraint on the domain-sepcific masks being learned ensure minimality in the number of features or neurons selected during learning. However, just incorporating a sparsity constraint does not explicitly incentivize domainspecificity -masks corresponding to all the source domains could just end up picking the same set of neurons, which is equivalent to learning a bottleneck layer during training. We investigate the consequences of incorporating a sparsity regularizer in <ref type="figure">Figure.</ref>  as our backbone architecture. Specifically, instead of the sIoU loss, we penalize the L1-norm of the soft-mask values, i.e., m d for all the source domains -d?D S ||m d || 1 ? ? . We run a sweep over different values of the coefficient (? S ) of this sparsity incentive from 0 to 1 in logarithmic increments. <ref type="figure" target="#fig_3">Fig. 6 (a) and (b)</ref> show how out-of-domain and in-domain generalization performances and <ref type="figure" target="#fig_3">Fig. 6</ref> (b) shows how the pairwise IoU measure among the source domain masks -indicating domain-specificity, vary with ? S . Unlike ? O (see Sec. 5, main paper), we find that generalization performance is quite sensitive to the choice of ? S , with both out-of-domain and in-domain accuracies degrading significantly at relatively high values of ? S . We find performance comparable to our approach only at values of ? = 10 ?5 . For the pairwise IoU measures, we observe that while specificity increases to some extent till ? S = 10 ?3 , but decreases sharply with further increase in ? S . At high-values of ? S , we observe that the source domain masks are extremely sparse and have high overlap indicating the fact that the masks essentially encourage learning just a bottleneck layer. This further demonstrates the efficacy of the sIoU loss in maintaining a reasonable balance between encouraging specificty while retaining predictive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Ensembling Choices at Test-time</head><p>In Section. 3.2 (main paper), we describe how we follow a soft-scaling scheme akin to dropout <ref type="bibr" target="#b45">[46]</ref> at test-time. Specifically, we obtain predictions corresponding to neurons in the task network soft-scaled by individual source domain masks and average them (call this Pred-Ens  dataset, we find that Mask-Ens leads to very minor (&lt; 1%) drop in both in and out-of-domain performance compared to Pred-Ens at test-time. The columns identify the held out sixth domain for each of the multi-source shifts. ? We were unable to optimize the MetaReg <ref type="bibr" target="#b2">[3]</ref> objective with Adam <ref type="bibr" target="#b22">[23]</ref> as the optimizer and therefore, we also include comparisons with Aggregate and MetaReg trained with SGD.</p><p>the setting where we average the soft masks (m d for source domain d) and draw a single prediction by scaling neurons with the averaged soft-mask -Mask-Ens.</p><p>In <ref type="table">Table.</ref> 5, we compare DMG (Pred-Ens) and DMG (Mask-Ens) in terms of both in and out-of-domain performances on all the multi-source shifts on Do-mainNet using AlexNet as the backbone architecture. We observe that Mask-Ens performs comparatively with Pred-Ens, with the margin of difference being within ?1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">More Results</head><p>In <ref type="table">Table.</ref> 6, we present more extensive comparisons of DMG with prior work on the PACS <ref type="bibr" target="#b26">[27]</ref> using AlexNet, ResNet-18 and ResNet-50 as the backbone CNN architectures. We now describe briefly the prior approaches we compare to.</p><p>DICA <ref type="bibr" target="#b35">[36]</ref> is a kernel-based optimization algorithm that aims a learn a transformation that renders representations invariant across domains by minimizing  <ref type="table">Table 6</ref>: Out of Domain Generalization Results on PACS. We compare performance (accuracy in %) against prior work in the standard domain generalization setting of training on three domains as source and evaluating on the held-out fourth domain (identified by the column headers). We include the aggregate baseline both as reported in <ref type="bibr" target="#b28">[29]</ref> as well as our own implementation (indicated as Aggregate * ) the dissimilarity across the source domains. D-MTAE <ref type="bibr" target="#b14">[15]</ref> is an autoencoder based approach which aims to learn invariant representations by cross-domain reconstruction. DSN <ref type="bibr" target="#b6">[7]</ref> aims to extract representations that can be partitioned into domain-specific and domain-invariant components. TF-CNN <ref type="bibr" target="#b26">[27]</ref> learns a low-rank parameterized CNN for end-to-end domain-generalization training. Fusion <ref type="bibr" target="#b34">[35]</ref> fuses predictions from all classifiers trained on all the source domains at test-time. DANN <ref type="bibr" target="#b13">[14]</ref> leverages the source domain features extractor from Domain Adversarial Neural Networks to generalize to target domains. MetaReg <ref type="bibr" target="#b2">[3]</ref> learns regularizers by modeling domain-shifts within the source set of distributions. MLDG <ref type="bibr" target="#b27">[28]</ref> learns network parameters using meta-learning. Epi-FCR <ref type="bibr" target="#b28">[29]</ref> is a recently proposed episodic scheme to learn network parameters robust to domain-shift. MASF <ref type="bibr" target="#b10">[11]</ref> is a recent approach which introduces complementary losses to explicitly regularize the semantic structure of the feature space via a model-agnostic episodic learning procedure. Cross-Grad <ref type="bibr" target="#b48">[49]</ref> uses Bayesian Networks to perturb the input manifold for domain generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Experimental Details</head><p>We summarize several experimental details in this section. For all our experiments, we use Adam <ref type="bibr" target="#b22">[23]</ref> as the optimizer with a batch size of 64. For PACS, we use an initial learning rate of 10 ?4 for both the network and mask parameters decayed exponentially with a rate of 0.99 every epoch and set weight decay to 10 ?5 . For DomainNet, we use an initial learning rate of 10 ?4 for both the network and mask parameters decayed per-epoch using an inverse learning rate schedule ? ? and set weight decay to 0. We conduct a sweep over values of ? O -coefficient of the sIoU loss -in the range {0, 10 ?5 , 10 ?4 , 10 ?3 , 10 ?2 , 10 ?1 , 1}. Our backbone CNN architectures are initialized with ImageNet <ref type="bibr" target="#b23">[24]</ref> pretrained checkpoints. We initialize the final linear layer weights (to be learned from scratch) from a zero centered normal distribution (N (0, 0.001)) and a uniform distribution (standard in PyTorch) for DomainNet and PACS respectively. For all our experiments, we initialize the mask parameters from the uniform distribution, i.e.,m d ? U(0, 1). We select the best checkpoints across 50 epochs of training based on overall in-domain validation accuracy. We implement everything in the Pytorch <ref type="bibr" target="#b36">[37]</ref> framework ? ? . Our code is available at https://github.com/prithv1/DMG ? ? lrt = lr 0 (1+?(t?1)) p where ? = 10 ?4 , p = 0.75, t identifies the epoch and lr0 is the initial learning rate.</p><p>? ? The authors of <ref type="bibr" target="#b37">[38]</ref> indicated in communication that they used Caffe to implement the multi-source baselines. We re-implement the multi-source baselines in PyTorch <ref type="bibr" target="#b36">[37]</ref> to ensure consistency across all our reported results. The subsequent differences in multi-source baseline accuracies can be attributed to the differences in how AlexNet is implemented in PyTorch and Caffe.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>For</head><label></label><figDesc>more comparisons to prior work, please refer to the appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Sensitivity to ? O . DMG is relatively insensitive to the setting of the hyper-parameter ? O as measured by out-of-domain accuracy (a), in-domain accuracy (b), and average IoU score measured among pairs of source domain masks (c). The legends in (c) indicate the target domain in the corresponding multisource shift. AlexNet is the backbone CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :Fig. 5 :</head><label>45</label><figDesc>P, Q P, R P, S Q,R Q,S R,S Overall fc6 (95.93% neurons activated) fc7 (95.75% neurons activated) fc8 (91.80% neurons activated) (a) I,P,Q,R,S ? C (O.O.D. Acc 50.06%) .00% neurons activated) fc7 (95.98% neurons activated) fc8 (92.75% neurons activated) (b) C,I,P,R,S ? Q (O.O.D. Acc 13.07%) Emergence of domain-specificity in AlexNet with ? O = 0.1. We show the IoU overlap among pairs of discrete source domain masks for the two shifts (a) I,P,Q,R,S?C and (b) C,I,P,R,S ?Q on DomainNet [38] with out-of-domain accuracies 48.70% and 12.7% respectively. We find that domainspecificity does indeed emerge, as indicated by the IoU measures. Layerwise IoU sensitivity to ? O . The average IoU score among pairs of source domain masks decreases as ? O increases, indicating the degree to which domain-specificity emerges in individual layers (fc6, fc7, fc8).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>6 on all the multi-source shifts of the DomainNet dataset using AlexNet Out-of-Domain Acc (%) Sensitivity to ? S . We replace the sIoU with a differentiable sparsity term (coefficient ? S ) -L1-norm of the soft-source domain masks, i.e., Di?D S ||m i || 1 -and study the sensitivity to ? S as measured by out-of-domain accuracy (a), in-domain accuracy (b) and average IoU score measured among pairs of source domain masks. The legends in (b) indicate the target domain in the corresponding multi-source shift. We find that predictive performance and specificity (Avg. IoU) is very sensitive to ? S .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Standard Aggregate Training DMG: Domain-specific Masks for GeneralizationStandard Aggregate Training DMG: Domain-specific Masks for GeneralizationStandard Aggregate Training DMG: Domain-specific Masks for GeneralizationStandard Aggregate Training DMG: Domain-specific Masks for GeneralizationStandard Aggregate Training DMG: Domain-specific Masks for GeneralizationStandard Aggregate Training DMG: Domain-specific Masks for GeneralizationStandard Aggregate Training DMG: Domain-specific Masks for GeneralizationStandard Aggregate Training DMG: Domain-specific Masks for GeneralizationStandard Aggregate Training DMG: Domain-specific Masks for Generalization</head><label></label><figDesc></figDesc><table><row><cell cols="2">All source domains pooled together All source domains pooled together All source domains pooled together All source domains pooled together All source domains pooled together All source domains pooled together All source domains pooled together All source domains pooled together</cell><cell>All source domains pooled together with domain-specific masks All source domains pooled together with domain-specific masks All source domains pooled together with domain-specific masks All source domains pooled together with domain-specific masks All source domains pooled together with domain-specific masks All source domains pooled together with domain-specific masks All source domains pooled together with domain-specific masks All source domains pooled together with domain-specific masks</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Activation Masking Activation Masking Activation Masking Activation Masking</cell><cell></cell></row><row><cell>Real Real Real Real</cell><cell>Feature Extractor Task Network Sketch Feature Extractor Real Task Network Gradients update Gradients update the network Infograph the network Sketch Infograph Feature Extractor Task Network Gradients update the network Real Sketch Infograph Feature Extractor Task Network Gradients update the network Sketch Infograph Feature Extractor Task Network Gradients update the network Real Sketch Infograph Feature Extractor Task Network Gradients update the network Sketch Infograph Feature Extractor Task Network Gradients update the network Real Sketch Infograph Network Task Feature Extractor the network Gradients update Sketch Infograph</cell><cell>Real Feature Extractor Task Network Image Real Sketch Feature Extractor Mask Sketch Mask Predictor m Domain Infograph Gradients update the network and masks ID Layer L + 1 Layer L Masked activations are forwarded from L to L+1 Layer L + 1 Layer L Masked activations are forwarded from L to L+1 Infograph Gradients update the network and masks Predictor m Domain ID Task Network Image Network Gradients update Feature Extractor Task the network All source domains pooled together Real Sketch Infograph All source domains pooled together with domain-specific masks Real Sketch Infograph Feature Extractor Gradients update the network and masks Mask Predictor m Domain ID Task Network Image Activation Masking Neurons Layer L + 1 Layer L Masked activations are forwarded from L to L+1 Sketch Binary mask sampled from soft-distribution Neurons Layer L + 1 Layer L Masked activations are forwarded from L to L+1 Sketch Binary mask sampled from soft-distribution Feature Extractor Task Network Gradients update the network All source domains pooled together Real Sketch Infograph All source domains pooled together with domain-specific masks Real Sketch Infograph Feature Extractor Gradients update the network and masks Mask Predictor m Domain ID Task Network Standard Aggregate Training DMG: Domain-specific Masks for Generalization Image Real Sketch Infograph Feature Extractor Gradients update the network and masks Mask Predictor m Domain ID Task Network Image Layer L + 1 Layer L Masked activations are forwarded from L to L+1 Layer L + 1 Layer L Masked activations are forwarded from L to L+1 Real Sketch Infograph Feature Extractor Gradients update the network and masks Mask Predictor m Domain ID Task Network Image Real Sketch Infograph Feature Extractor Gradients update the network and masks Mask Predictor m Domain ID Task Network Image Layer L + 1 Layer L Masked activations are forwarded from L to L+1 Layer L + 1 Layer L Masked activations are forwarded from L to L+1 Real Sketch Infograph Feature Extractor Gradients update the network and masks Mask Predictor m Domain ID Task Network Image Real Sketch Infograph Feature Extractor Gradients update the network and masks Mask Predictor m Domain ID Network Network Task Image Layer L + 1 activations are L to L+1 Task L to L+1 forwarded from activations are forwarded from Masked Masked Layer L Layer L + 1 Layer L masks Feature Gradients update the network and m Predictor Extractor Mask Domain ID Image Real Sketch Infograph</cell><cell>Feature Extractor Task Network Gradients update the network Sketch Infograph Extractor Real Task the network Network Gradients update Feature Sketch Infograph All source domains pooled together Real All source domains pooled together Standard Aggregate Training Standard Aggregate Training</cell><cell>Neurons Binary mask Sketch sampled from soft-distribution Neurons Sketch Binary mask sampled from soft-distribution Neurons Sketch Binary mask sampled from soft-distribution Neurons Sketch Binary mask sampled from soft-distribution Neurons Sketch Binary mask sampled from soft-distribution Neurons Sketch Binary mask sampled from soft-distribution Neurons Sketch Binary mask sampled from soft-distribution Sketch Binary mask sampled from soft-distribution Real Sketch Infograph Feature Extractor Gradients update the network and masks Mask Predictor m Domain ID Network Network Task Image Layer L + 1 Layer L Masked activations are L to L+1 Task L to L+1 forwarded from Sketch Binary mask sampled from soft-distribution Layer L Masked activations are forwarded from Sketch Binary mask sampled from soft-distribution masks Feature Gradients update the network and m Predictor Extractor Mask ID Layer L + 1 Domain Neurons Neurons Real Sketch Infograph Neurons All source domains pooled together with domain-specific masks Activation Masking All source domains pooled together with domain-specific masks DMG: Domain-specific Masks for Generalization</cell></row></table><note>DMG: Domain-specific Masks for Generalization Image Fig. 2: Illustration of our approach (DMG): We introduce domain-specific activation masks for learning a balance between domain-specific and domain- agnostic features. [Left] Our training pipeline involves incorporating domain- specific masks in the vanilla aggregate training process. [Middle] For an image belonging to sketch, we sample a binary mask from the corresponding mask parameters, which is then applied to the neurons of the task-network. [Right]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Out of Domain Accuracy (%) on DomainNet (? O = 0.1) We were unable to optimize the MetaReg<ref type="bibr" target="#b2">[3]</ref> objective with Adam<ref type="bibr" target="#b22">[23]</ref> as the optimizer and therefore, we also include comparisons with Aggregate and MetaReg trained with SGD. dataset for domain adaptation which consists of ?0.6 million images of 345 classes distributed across 6 domainsreal, clipart, sketch, painting, quickdraw and infograph. DomainNet surpasses all prior datasets for domain adaptation significantly in terms of size and diversity. The authors of</figDesc><table><row><cell></cell><cell>Method</cell><cell>C</cell><cell>I</cell><cell>P</cell><cell>Q</cell><cell>R</cell><cell>S</cell><cell>Overall</cell></row><row><cell></cell><cell>Aggregate</cell><cell>47.17</cell><cell>10.15</cell><cell>31.82</cell><cell>11.75</cell><cell>44.35</cell><cell>26.33</cell><cell>28.60</cell></row><row><cell>AlexNet</cell><cell>Aggregate-SGD ? Multi-Headed MetaReg [3] ?</cell><cell>42.30 45.96 42.86</cell><cell>12.42 10.56 12.68</cell><cell>31.45 31.07 32.47</cell><cell>9.52 12.05 9.37</cell><cell>42.76 43.56 43.43</cell><cell>29.34 25.93 29.87</cell><cell>27.97 28.19 28.45</cell></row><row><cell></cell><cell>DMG (Ours)</cell><cell>50.06</cell><cell>12.23</cell><cell cols="4">34.44 13.07 46.98 30.13</cell><cell>31.15</cell></row><row><cell></cell><cell>Aggregate</cell><cell>57.15</cell><cell>17.69</cell><cell>43.21</cell><cell>13.87</cell><cell>54.91</cell><cell>39.41</cell><cell>37.71</cell></row><row><cell>ResNet-18</cell><cell>Aggregate-SGD ? Multi-Headed MetaReg [3] ?</cell><cell>56.56 55.46 53.68</cell><cell>18.44 17.51 21.06</cell><cell>45.30 40.85 45.29</cell><cell>12.47 11.19 10.63</cell><cell cols="2">57.90 52.92 58.47 42.31 38.83 38.65</cell><cell>38.25 36.10 38.57</cell></row><row><cell></cell><cell>DMG (Ours)</cell><cell>60.07</cell><cell>18.76</cell><cell>44.53</cell><cell>14.16</cell><cell>54.72</cell><cell>41.73</cell><cell>39.00</cell></row><row><cell></cell><cell>Aggregate</cell><cell>62.18</cell><cell>19.94</cell><cell>45.47</cell><cell>13.81</cell><cell>57.45</cell><cell>44.36</cell><cell>40.54</cell></row><row><cell>ResNet-50</cell><cell>Aggregate-SGD ? Multi-Headed MetaReg [3] ?</cell><cell>64.04 61.74 59.77</cell><cell>23.63 21.25 25.58</cell><cell>51.04 46.80 50.19</cell><cell>13.11 13.89 11.52</cell><cell cols="2">64.45 58.47 64.56 50.09 47.75 45.43</cell><cell>44.00 41.27 43.62</cell></row><row><cell></cell><cell>DMG (Ours)</cell><cell>65.24</cell><cell>22.15</cell><cell>50.03</cell><cell>15.68</cell><cell>59.63</cell><cell>49.02</cell><cell>43.63</cell></row></table><note>?splits for our experiments. As such, the proposed splits do not include an in- domain test-split, thereby limiting us from computing in-domain performance in addition to measuring out-of-domain generalization. DomainNet [38] -DomainNet is a recently proposed large-scale</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Out of Domain Accuracy (%) on PACS (? O = 0.1) * We include the aggregate baseline both as reported in<ref type="bibr" target="#b28">[29]</ref> as well as our own implementation (indicated as Aggregate * ).</figDesc><table><row><cell></cell><cell>Method</cell><cell>A</cell><cell>C</cell><cell>P</cell><cell>S</cell><cell>Overall</cell></row><row><cell></cell><cell>Aggregate [29]</cell><cell>63.40</cell><cell>66.10</cell><cell>88.50</cell><cell>56.60</cell><cell>68.70</cell></row><row><cell></cell><cell>Aggregate*</cell><cell>56.20</cell><cell>70.69</cell><cell>86.29</cell><cell>60.32</cell><cell>68.38</cell></row><row><cell></cell><cell>Multi-Headed</cell><cell>61.67</cell><cell>67.88</cell><cell>82.93</cell><cell>59.38</cell><cell>67.97</cell></row><row><cell></cell><cell>DSN [7]</cell><cell>61.10</cell><cell>66.50</cell><cell>83.30</cell><cell>58.60</cell><cell>67.40</cell></row><row><cell>AlexNet</cell><cell>Fusion [35] MLDG [28] MetaReg [3]</cell><cell>64.10 66.20 63.50</cell><cell>66.80 66.90 69.50</cell><cell>90.20 88.00 87.40</cell><cell>60.10 59.00 59.10</cell><cell>70.30 70.00 69.90</cell></row><row><cell></cell><cell>CrossGrad [49]</cell><cell>61.00</cell><cell>67.20</cell><cell>87.60</cell><cell>55.90</cell><cell>67.90</cell></row><row><cell></cell><cell>Epi-FCR [29]</cell><cell>64.70</cell><cell>72.30</cell><cell>86.10</cell><cell>65.00</cell><cell>72.00</cell></row><row><cell></cell><cell>MASF [11]</cell><cell cols="3">70.35 72.46 90.68</cell><cell>67.33</cell><cell>75.21</cell></row><row><cell></cell><cell>DMG (Ours)</cell><cell>64.65</cell><cell>69.88</cell><cell>87.31</cell><cell>71.42</cell><cell>73.32</cell></row><row><cell></cell><cell>Aggregate [29]</cell><cell>77.60</cell><cell>73.90</cell><cell>94.40</cell><cell>74.30</cell><cell>79.10</cell></row><row><cell></cell><cell>Aggregate*</cell><cell>72.61</cell><cell>78.46</cell><cell>93.17</cell><cell>65.20</cell><cell>77.36</cell></row><row><cell></cell><cell>Multi-Headed</cell><cell>78.76</cell><cell>72.10</cell><cell>94.31</cell><cell>71.77</cell><cell>79.24</cell></row><row><cell>ResNet-18</cell><cell>MLDG [28] MetaReg [3] CrossGrad [49]</cell><cell>79.50 79.50 78.70</cell><cell>77.30 75.40 73.30</cell><cell>94.30 94.30 94.00</cell><cell>71.50 72.20 65.10</cell><cell>80.70 80.40 77.80</cell></row><row><cell></cell><cell>Epi-FCR [29]</cell><cell>82.10</cell><cell>77.00</cell><cell>93.90</cell><cell>73.00</cell><cell>81.50</cell></row><row><cell></cell><cell>MASF [11]</cell><cell>80.29</cell><cell>77.17</cell><cell>94.99</cell><cell>71.68</cell><cell>81.03</cell></row><row><cell></cell><cell>DMG (Ours)</cell><cell>76.90</cell><cell>80.38</cell><cell>93.35</cell><cell>75.21</cell><cell>81.46</cell></row><row><cell>ResNet-50</cell><cell>Aggregate* Multi-Headed MASF [11] DMG (Ours)</cell><cell>75.49 75.15 82.89 82.57</cell><cell>80.67 76.37 80.49 78.11</cell><cell>93.05 95.27 95.01 94.49</cell><cell>64.29 75.26 72.29 78.32</cell><cell>78.38 80.51 82.67 83.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>In Domain Accuracy (%) on DomainNet (? O = 0.1). For the case where inputs have known domain (KD) label, we can use the corresponding learning mask (DMG-KD) to achieve the strongest performance without requiring additional models or parameters. Column headers identify the target domains in the corresponding multi-source shifts. We were unable to optimize the MetaReg<ref type="bibr" target="#b2">[3]</ref> objective with Adam<ref type="bibr" target="#b22">[23]</ref> as the optimizer and therefore, we also include comparisons with Aggregate and MetaReg trained with SGD.Out-of-Domain Generalization.Tables 1 and 2summarize out of domain generalization results on the DomainNet and PACS datasets, respectively.DomainNet -On DomainNet, we observe that DMG beats the naive aggregate baseline, the multi-headed baseline and MetaReg</figDesc><table><row><cell></cell><cell>Method</cell><cell>C</cell><cell>I</cell><cell>P</cell><cell>Q</cell><cell>R</cell><cell>S</cell><cell>Overall</cell></row><row><cell></cell><cell>Aggregate</cell><cell>48.56</cell><cell>57.24</cell><cell>51.38</cell><cell>49.60</cell><cell>47.48</cell><cell>50.72</cell><cell>50.83</cell></row><row><cell></cell><cell>Aggregate-SGD ?</cell><cell>48.14</cell><cell>54.93</cell><cell>50.55</cell><cell>48.33</cell><cell>47.57</cell><cell>49.98</cell><cell>49.92</cell></row><row><cell>AlexNet</cell><cell>Multi-Headed MetaReg [3] ? DMG (Ours)</cell><cell>48.16 48.87 49.63</cell><cell>56.73 56.06 58.47</cell><cell>51.31 51.23 52.88</cell><cell>49.75 49.60 51.33</cell><cell>47.65 48.66 49.07</cell><cell>50.82 50.12 52.42</cell><cell>50.74 50.76 52.30</cell></row><row><cell></cell><cell>DMG-KD (Ours)</cell><cell cols="6">51.91 61.01 54.93 53.84 51.08 54.47</cell><cell>54.54</cell></row><row><cell></cell><cell>Aggregate</cell><cell>56.58</cell><cell>65.27</cell><cell>59.29</cell><cell>59.15</cell><cell>55.47</cell><cell>58.84</cell><cell>59.10</cell></row><row><cell>ResNet-18</cell><cell>Aggregate-SGD ? Multi-Headed MetaReg [3] ? DMG (Ours)</cell><cell>55.32 47.79 56.25 57.39</cell><cell>63.63 56.80 63.07 65.73</cell><cell>57.40 50.85 57.74 58.87</cell><cell>57.98 54.86 58.73 59.66</cell><cell>53.99 46.92 55.40 55.95</cell><cell>57.37 49.50 58.04 58.63</cell><cell>57.62 51.12 58.21 59.37</cell></row><row><cell></cell><cell>DMG-KD (Ours)</cell><cell cols="6">58.61 66.98 59.86 60.98 57.24 59.84</cell><cell>60.59</cell></row><row><cell></cell><cell>Aggregate</cell><cell>61.68</cell><cell>69.73</cell><cell>63.90</cell><cell>63.88</cell><cell>60.29</cell><cell>63.62</cell><cell>63.85</cell></row><row><cell>ResNet-50</cell><cell>Aggregate-SGD ? Multi-Headed MetaReg [3] ? DMG (Ours)</cell><cell>61.64 53.77 61.86 61.78</cell><cell>69.36 62.09 68.80 69.49</cell><cell>63.65 56.54 63.23 63.93</cell><cell>64.08 60.32 64.75 64.09</cell><cell>60.52 51.38 60.59 59.92</cell><cell>63.82 55.10 63.21 63.50</cell><cell>63.85 56.53 63.74 63.79</cell></row><row><cell></cell><cell>DMG-KD (Ours)</cell><cell cols="6">63.16 70.79 65.03 65.67 61.30 64.86</cell><cell>65.14</cell></row></table><note>?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Domain-Specialized Masks (? O = 0.1). We show how optimizing for sIoU leads to masks which are specialized for the individual source domains in terms of predictive performance. We consider two multi-source shifts I,P,Q,R,S?C [top-half] and C,I,P,R,S?Q [bottom-half] on DomainNet<ref type="bibr" target="#b37">[38]</ref> with the AlexNet as the backbone architecture and find that using corresponding source domain masks leads to significantly improved in-domain performance. * * -which learns regularizers by modeling domain-shifts within the source set of distributions, MLDG<ref type="bibr" target="#b27">[28]</ref> -which learns robust network parameters using meta-learning and Epi-FCR [29] -a recently proposed episodic scheme to learn network parameters robust to domain-shift, and performs competitively with MASF<ref type="bibr" target="#b10">[11]</ref> -which introduces complementary losses to explicitly regularize the semantic structure of the feature space via a model-agnostic episodic learning procedure. Notice that this improvement also comes with a 4.09% improvement over MASF<ref type="bibr" target="#b10">[11]</ref> on the A,C,P?S shift. Using ResNet-18 and ResNet-50 as the backbone architectures, we observe that DMG leads to comparable and improved overall performance, with margins of 0.04% and 0.7% for ResNet-18 and ResNet-50, respectively. For ResNet-18, this is accompanied with a 0.91% and 1.92% improvement on the A,C,P?S and A,P,S?C shifts. Similarily for ResNet-50, we observe a 3.06% improvement on the A,C,P?S shift.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Source</cell><cell></cell><cell></cell><cell>Target</cell></row><row><cell></cell><cell>Chosen Mask</cell><cell>I</cell><cell>P</cell><cell>Q</cell><cell>R</cell><cell>S</cell><cell>C</cell></row><row><cell></cell><cell>m Infograph</cell><cell>23.84</cell><cell>45.56</cell><cell>59.13</cell><cell>62.43</cell><cell>46.70</cell><cell>46.91</cell></row><row><cell>AlexNet</cell><cell>m Painting m Quickdraw m Real m Sketch</cell><cell>19.88 21.72 18.42 19.45</cell><cell>52.41 48.47 43.48 45.41</cell><cell>59.00 62.52 58.80 57.64</cell><cell>60.36 65.32 68.62 61.78</cell><cell>45.75 48.69 44.81 52.16</cell><cell>46.87 50.33 47.69 48.36</cell></row><row><cell></cell><cell>Combined</cell><cell>22.28</cell><cell>49.55</cell><cell>60.45</cell><cell>66.14</cell><cell>49.72</cell><cell>50.06</cell></row><row><cell></cell><cell>Chosen Mask</cell><cell>C</cell><cell>I</cell><cell>P</cell><cell>R</cell><cell>S</cell><cell>Q</cell></row><row><cell></cell><cell>m Clipart</cell><cell>66.70</cell><cell>21.36</cell><cell>46.60</cell><cell>64.35</cell><cell>49.70</cell><cell>13.37</cell></row><row><cell>AlexNet</cell><cell>m Infograph m Painting m Real m Sketch</cell><cell>60.71 59.21 59.62 60.97</cell><cell>24.95 20.59 19.41 20.29</cell><cell>47.06 53.21 43.82 45.69</cell><cell>63.78 60.67 69.82 62.40</cell><cell>49.36 48.14 47.22 54.51</cell><cell>12.58 12.01 11.31 13.08</cell></row><row><cell></cell><cell>Combined</cell><cell>64.13</cell><cell>23.21</cell><cell>50.05</cell><cell>67.03</cell><cell>52.24</cell><cell>13.07</cell></row><row><cell>MetaReg [3]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>51. Zunino, A., Bargal, S.A., Volpi, R., Sameki, M., Zhang, J., Sclaroff, S., Murino, V., Saenko, K.: Explainable deep classification models for domain generalization. arXiv preprint arXiv:2003.06498 (2020)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>). In this section, we further investigate if the choice of ensembling method at test-time matters. We compare Pred-Ens with</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="6">Clipart Infograph Painting Quickdraw Real Sketch Overall</cell></row><row><cell></cell><cell>Out-of-Domain</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Aggregate</cell><cell>47.17</cell><cell>10.15</cell><cell>31.82</cell><cell>11.75</cell><cell>44.35 26.33</cell><cell>28.60</cell></row><row><cell>AlexNet</cell><cell>Aggregate-SGD ? Multi-Headed MetaReg [3] ? DMG (Pred-Ens)</cell><cell>42.30 45.96 42.86 50.06</cell><cell>12.42 10.56 12.68 12.23</cell><cell>31.45 31.07 32.47 34.44</cell><cell>9.52 12.05 9.37 13.07</cell><cell cols="2">42.76 29.34 43.56 25.93 43.43 29.87 46.98 30.13 31.15 27.97 28.19 28.45</cell></row><row><cell></cell><cell>DMG (Mask-Ens)</cell><cell>50.10</cell><cell>12.17</cell><cell>34.38</cell><cell>13.14</cell><cell>46.79 30.01</cell><cell>31.10</cell></row><row><cell></cell><cell>In-Domain</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Aggregate</cell><cell>48.56</cell><cell>57.24</cell><cell>51.38</cell><cell>49.60</cell><cell>47.48 50.72</cell><cell>50.83</cell></row><row><cell>AlexNet</cell><cell>Aggregate-SGD ? Multi-Headed MetaReg [3] ? DMG (Pred-Ens)</cell><cell>48.14 48.16 48.87 49.63</cell><cell>54.93 56.73 56.06 58.47</cell><cell>50.55 51.31 51.23 52.88</cell><cell>48.33 49.75 49.60 51.33</cell><cell>47.57 49.98 47.65 50.82 48.66 50.12 49.07 52.42</cell><cell>49.92 50.74 50.76 52.30</cell></row><row><cell></cell><cell>DMG (Mask-Ens)</cell><cell>49.49</cell><cell>58.38</cell><cell>52.81</cell><cell>51.16</cell><cell>48.90 52.29</cell><cell>52.17</cell></row><row><cell></cell><cell>DMG-KnownDomain</cell><cell>51.91</cell><cell>61.01</cell><cell>54.93</cell><cell>53.84</cell><cell cols="2">51.08 54.47 54.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Ensembling Choices at Test-time. We study how different ensembling choices at test-time -(1) Mask-Ens: ensemble predictions from all the source domain masks and (2) Pred-Ens: combine masks and then make a prediction -compare in terms of in [bottom-half] an out-of-domain [top-half] performance. Using AlexNet as the backbone architecture on the DomainNet<ref type="bibr" target="#b37">[38]</ref> </figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? ? Since the soft-mask probabilities (m d ) are positive, ||m d ||1 is essentially the sum of mask probabilities per-neuron and is therefore differentiable and can be optimized using gradient descent.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank Viraj Prabhu, Daniel Bolya, Harsh Agrawal and Ramprasaath Selvaraju for fruitful discussions and feedback. This work was partially supported by DARPA award FA8750-19-1-0504.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhurandhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04692</idno>
		<title level="m">Invariant risk minimization games</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Invariant risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Metareg: Towards domain generalization using meta-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="998" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>L?onard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Budget-aware adapters for multi-domain learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lathuillere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oliveira-Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="382" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03599</idno>
		<title level="m">Understanding disentangling in ?-vae</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain generalization via model-agnostic learning of semantic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>De Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6447" to="6458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-ensembling for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkpoTaxA-" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Domain generalization for object recognition with multi-task autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2551" to="2559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Domain generalization for object recognition with multi-task autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm?ssan</title>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm?ssan<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1994" to="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Etude de la distribution florale dans une portion des alpes et du jura</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jaccard</surname></persName>
		</author>
		<idno type="DOI">10.5169/seals-266450</idno>
		<ptr target="https://doi.org/10.5169/seals-266450" />
	</analytic>
	<monogr>
		<title level="j">Sciences Naturelles</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="547" to="579" />
			<date type="published" when="1901-01" />
		</imprint>
	</monogr>
	<note>Bulletin de la Societe Vaudoise des</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Undoing the damage of dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="158" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Undoing the damage of dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="158" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Disentangling by factorising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning. Proceedings of Machine Learning Research</title>
		<editor>Dy, J., Krause, A.</editor>
		<meeting>the 35th International Conference on Machine Learning. Machine Learning Research<address><addrLine>Stockholmsmssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Drop to adapt: Learning discriminative features for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5542" to="5550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to generalize: Metalearning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Episodic training for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1446" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno>abs/1602.04433</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Precup, D., Teh, Y.W.</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Piggyback: Adapting a single network to multiple tasks by learning to mask weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="67" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Packnet: Adding multiple tasks to a single network by iterative pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7765" to="7773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Best sources forward: domain generalization through source-specific nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1353" to="1357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1406" to="1415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Domain agnostic learning with disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Optimizing intersection-over-union in deep neural networks for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International symposium on visual computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="234" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adversarial dropout regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02560</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Winning the lottery with continuous sparsification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04427</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting with hard attention to the task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Suris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karatzoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Calibrate and prune: Improving reliability of lottery tickets through prediction calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Thiagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thopalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sattigeri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03875</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Generalizing to unseen domains via adversarial data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5334" to="5344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep cocktail network: Multi-source unsupervised domain adaptation with category shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3964" to="3973" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
