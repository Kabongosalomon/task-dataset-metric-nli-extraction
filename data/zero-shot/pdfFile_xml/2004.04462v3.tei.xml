<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FKACONV: FEATURE-KERNEL ALIGNMENT FOR POINT CLOUD CONVOLUTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Boulch</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ecole des Ponts</orgName>
								<orgName type="institution" key="instit1">LIGM</orgName>
								<orgName type="institution" key="instit2">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<addrLine>Marne-la-Vall?e</addrLine>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valeo</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ecole des Ponts</orgName>
								<orgName type="institution" key="instit1">LIGM</orgName>
								<orgName type="institution" key="instit2">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<addrLine>Marne-la-Vall?e</addrLine>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Gilles</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ecole des Ponts</orgName>
								<orgName type="institution" key="instit1">LIGM</orgName>
								<orgName type="institution" key="instit2">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<addrLine>Marne-la-Vall?e</addrLine>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puy</forename><forename type="middle">Valeo</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ecole des Ponts</orgName>
								<orgName type="institution" key="instit1">LIGM</orgName>
								<orgName type="institution" key="instit2">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<addrLine>Marne-la-Vall?e</addrLine>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Renaud</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ecole des Ponts</orgName>
								<orgName type="institution" key="instit1">LIGM</orgName>
								<orgName type="institution" key="instit2">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<addrLine>Marne-la-Vall?e</addrLine>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marlet</forename><forename type="middle">Valeo</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ecole des Ponts</orgName>
								<orgName type="institution" key="instit1">LIGM</orgName>
								<orgName type="institution" key="instit2">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<addrLine>Marne-la-Vall?e</addrLine>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Ecole des Ponts</orgName>
								<orgName type="institution" key="instit1">LIGM</orgName>
								<orgName type="institution" key="instit2">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<addrLine>Marne-la-Vall?e</addrLine>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FKACONV: FEATURE-KERNEL ALIGNMENT FOR POINT CLOUD CONVOLUTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent state-of-the-art methods for point cloud processing are based on the notion of point convolution, for which several approaches have been proposed. In this paper, inspired by discrete convolution in image processing, we provide a formulation to relate and analyze a number of point convolution methods. We also propose our own convolution variant, that separates the estimation of geometry-less kernel weights and their alignment to the spatial support of features. Additionally, we define a point sampling strategy for convolution that is both effective and fast. Finally, using our convolution and sampling strategy, we show competitive results on classification and semantic segmentation benchmarks while being time and memory efficient. FKAConv code is available on the valeo.ai github: https://github.com/valeoai/FKAConv arXiv:2004.04462v3 [cs.CV] 24 Nov 2020</p><p>FKAConv: Feature-Kernel Alignment for Point Cloud Convolution of the features: using a geometry-less kernel domain, we stick to a discrete convolution scheme, which is efficient and has been successful on grid data; the spatial domain however keeps its continuous flavor, as point clouds are generally sampled on manifolds.</p><p>Our contributions are the following: (1) we provide a formulation to relate and analyze existing point convolution methods; (2) we propose a new convolution method (FKAConv) that explicitly separates the estimation of geometry-less kernel weights and their alignment to the spatial support of features; (3) we define a point sampling strategy for convolution that is both efficient and fast; (4) experiments on large-scale datasets for classification and semantic segmentation show we reach the state of the art, while being memory and time efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Projection in 2D. Some methods project the point cloud in a space suitable for using standard discrete CNNs. 2D CNNs have been use for 3D data converted as range images <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">29]</ref> or viewed from virtual viewpoints <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b44">44]</ref>. As neighboring points in the resulting image can be far away in 3D space, 2D CNNs often fail to capture well 3D relations. 2D CNNs can also be applied locally to point-specific neighborhoods by projecting data on the tangent plane [45]; the result is then highly dependant on the tangent plane estimation. Other approaches use a volumetric data representation, such as voxels <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b54">54]</ref>. These approaches however suffer from encoding mostly empty volumes, calling for sparsity handling, e.g., with octree-based 3D-CNNs <ref type="bibr" target="#b37">[37]</ref> or sparse convolution <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Graph convolution, geometric deep learning. Graph Neural Networks (GNNs) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b41">41]</ref> extend neural networks to irregular structures (not on a grid), using edges between nodes for message passing <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">24]</ref> or defining convolution in the spectral domain <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">18]</ref>. Point convolution using GNNs requires first explicitly building a graph from the point cloud <ref type="bibr" target="#b36">[36]</ref>. To scale to large point clouds, SPG [20] defines a graph over nodes corresponding to point segments. In contrast, our approach directly applies to the raw point cloud, with no predefined relation between points, somehow making point association as part of the method.</p><p>MLP processing. PointNet [33] directly processes point coordinates with a multi-layer perceptron (MLP), gathering context information with a permutation-invariant max-pooling. PointNet++ <ref type="bibr" target="#b35">[35]</ref> and  reduce the loss of local information due to subsampling with a cascade of MLPs at different scales.</p><p>Point convolution. A first line of work considers an explicit spatial location for the kernel, in the same space as the point cloud. Kernel elements can be located on a regular grid (voxels) <ref type="bibr" target="#b16">[16]</ref>, at the vertices of a polyhedron <ref type="bibr" target="#b47">[47]</ref> or randomly sampled and optimized at training <ref type="bibr" target="#b2">[3]</ref>. In KPConv [47], an adjustment of the kernel locations may also be predicted at test time to better fit the data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional Neural Networks (CNNs) have been a breakthrough in machine learning for image processing <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">19]</ref>. The discrete formulation of convolution allows a very efficient processing of grid-structured data such as images in 2D or videos in 3D. Yet a number of tasks require processing unstructured data such as point clouds, meshes or graphs, with application domains such as autonomous driving, robotics or urban modeling. However discrete convolution does not directly apply to point clouds as 3D points are not usually sampled on a grid.</p><p>The most straightforward workaround is to voxelize the 3D space to use discrete CNNs <ref type="bibr" target="#b31">[31]</ref>. However, as 3D points are usually sampled on a surface, most of the voxels are empty. For efficient large-scale processing, a sparse formulation is thus required <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b60">60]</ref>. Other deep learning approaches generalize convolution to less structured data, such as graphs or meshes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b41">41]</ref>, but applying them to point clouds requires addressing the issue of sensible graph construction first.</p><p>Deep-learning techniques that directly process raw data have been developed to overcome the problem of point cloud pre-processing <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b50">50]</ref>. Just as for structured data, such networks are usually designed as a stack of layers and are optimized using stochastic gradient descent and back-propagation. Key issues when designing these networks include speed and memory efficiency.</p><p>In this context, we propose a new convolution method for point cloud processing. It is a mixed discretecontinuous formulation that disentangles the geometry of the convolution kernel and the spatial support Another type of approaches models kernel locations implicitly. The kernel can be a family of polynomials like in SpiderCNN <ref type="bibr" target="#b55">[55]</ref>, or it can be estimated with an MLP, like in PCCN <ref type="bibr" target="#b50">[50]</ref>, RSConv <ref type="bibr" target="#b27">[27]</ref> or PointConv <ref type="bibr" target="#b53">[53]</ref>. The weights of the input features are then directly estimated based on the local geometry of points. In contrast, we learn the weights of a discrete kernel and, at inference time, we only estimate the spatial relation between the kernel and input points. PointConv <ref type="bibr" target="#b53">[53]</ref> reweights the input features based on local point densities. Our method reaches state-of-the-art performances without the need of such a mechanism.</p><p>Finally, PointCNN <ref type="bibr" target="#b23">[23]</ref> shares apparent similarities with our work as one of its main components is the estimation of a matrix, that actually differs from ours. Besides, geometric information in <ref type="bibr" target="#b23">[23]</ref> is lifted to the feature space and used as additional features. Our work shows it is sufficient to use the geometry only for features-kernel alignment, mimicking the discrete convolution on a regular grid.</p><p>Our approach lies in between these lines of work. On the one hand, our kernel weights are explicitly modeled as in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b47">47]</ref>, which gives a discrete flavor to our method; on the other hand, we estimate a transformation of input points to apply the convolution as in <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b53">53]</ref>, which operates in the continuous domain, avoiding kernel spatialization. The key is that, contrary to fully-continuous approaches that re-estimate at inference time how to weigh given sets of points to operate the convolution, we estimate separately a kernel while learning, and we predict the relation between the kernel and input points while testing. Besides, we perform the convolution with a direct matrix multiplication rather than getting indirectly results from a network output. This separation and the explicit matrix multiplication (outside the network) allows a better learning of kernel weights and spatial relations, without the burden and inaccuracy of estimating their composition, resulting in a time and memory efficient method.</p><p>Point sampling. Like PointNet <ref type="bibr" target="#b33">[33]</ref>, several methods maintain point clouds at full resolution during the whole processing <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b56">56]</ref>. These methods suffer from a high memory cost, which requires to either limit the input size <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b56">56]</ref>, split the input into parts <ref type="bibr" target="#b26">[26]</ref>, or use a coarse voxel grid <ref type="bibr" target="#b28">[28]</ref>. Other approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b35">35]</ref>, as ours, use an internal sub-sampling of the point cloud. The choice of sampled points forming a good support is a key step for this reduction. Furthest point sampling (FPS) <ref type="bibr" target="#b35">[35]</ref>, where points are chosen iteratively by selecting the furthest point from all the previously picked points at each iteration, yields very good performance but is slow and its performance depends on the initialization. In <ref type="bibr" target="#b56">[56]</ref>, point sampling is based on a learned attention, which induces a high memory cost. Our sampling strategy, based on the quantization of the 3D space, ensures a good sampling of the space, like FPS, and is fast and memory efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A general formulation of point cloud convolution</head><p>We base our convolution formulation on the discrete convolution used in image or voxel grid processing. The formulation is general enough to cover a wide range of state-of-the-art convolution methods for point clouds, and to relate them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.0.1">Discrete convolution.</head><p>Let F be the dimension of the input feature space, d the spatial dimension (e.g., 2 for images, 3 for voxel grids), K the convolution kernel, and f the input features. The classical discrete convolution, noted h, is:</p><formula xml:id="formula_0">h[n] = f ?{1,...,F } m?{?M/2,...,M/2} d K f [m] f f [n + m],<label>(1)</label></formula><p>where M d is the grid kernel size, f indexes the feature space, n is the spatial index, and</p><formula xml:id="formula_1">K f [m] and f f [n + m] are scalars. Defining vectors K f = (K f [m], m ? {?M/2, . . . , M/2} d }) and f f (n) = (f f [n + m], m ? {?M/2, . . . , M/2} d ),</formula><p>we can highlight the separation between the kernel space (K) and the feature space</p><formula xml:id="formula_2">(f ): h[n] = f ?{1,...,F } K f Kernel space f f (n) Feature space .<label>(2)</label></formula><p>The kernel K f and the features f f (n) are perfectly aligned: the grid index m associates a kernel element K f [m] with a single input element f f [n + m] ( <ref type="figure" target="#fig_0">Fig. 1(a)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.0.2">Point convolution.</head><p>To generalize this discrete convolution to point clouds, we first consider a hypothetical misalignment between the feature and kernel spaces, assuming the feature grid is rotated with respect to the kernel grid ( <ref type="figure" target="#fig_0">Fig. 1(b)</ref>),  thus obfuscating the correspondence between kernel elements and feature elements. Yet, provided that the rotation matrix A ? R M d ? R M d is known, the correspondences can be recovered by rotating the support points of features:</p><formula xml:id="formula_3">h[n] = f ?{1,...,F } K f A f f (n).<label>(3)</label></formula><p>This equation actually holds in a more general setting, with an arbitrary linear transformation between the feature space and the kernel space; A is then the alignment matrix that associates the feature values to the kernel elements.</p><p>The discrete convolution on a regular grid becomes a particular case of Equation <ref type="formula" target="#formula_3">(3)</ref>, with A = I M d , the identity matrix. In the case of a point cloud, f f (n) is the feature associated to the point at spatial location n, typically computed on a neighborhood N[n]. These features are generally not grid-aligned. But Equation <ref type="formula" target="#formula_3">(3)</ref> can still apply, provided we can estimate an alignment matrix A that distributes each input point onto the kernel elements ( <ref type="figure" target="#fig_0">Fig. 1(c)</ref>).</p><p>In this context, a fixed matrix A is suboptimal as it cannot cope well with both a regular grid (A = I M d ) and arbitrary point configurations in a point cloud. A thus has to be a function of the input points, which in practice have to be limited to neighbors N[n] at location n. The convolution becomes:</p><formula xml:id="formula_4">h[n] = f ?{1,...,F } K f A(N[n]) f f (n).<label>(4)</label></formula><p>It is a mixed discrete-continuous formulation: K f and f f (n) have a discrete support and continuous values, while A(N[n]) provides a continuous mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.0.3">Analysis of exiting methods.</head><p>This formulation happens to be generic enough to describe a range of existing methods for point convolution <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b50">50]</ref>.</p><p>Using spatial kernel points. The most common approach to discrete convolution on a point cloud assigns a spatial point to each kernel element. The distribution of features on kernel elements is then based on the distance between kernel points and points in N[n], corresponding to an association matrix A invariant by rotation. A simple method would be to assign the features to the nearest kernel point, but it is unstable as a small perturbation in the point position may result in a different kernel point attribution. A workaround is to distribute the input points to several close kernel points. In SplatNet <ref type="bibr" target="#b43">[43]</ref>, an interpolation distribute points onto the kernel space. However, this handcrafted assignment is arbitrary and heavily relies on the geometry of kernel points. KPConv <ref type="bibr" target="#b47">[47]</ref> chooses to distribute the input points over all the neighboring kernel points, with a weight inversely proportional to their distance to kernel points. Moreover, KPConv allows deformable kernels, for which local shifts of kernel points are estimated, offering more adaptation to input points. Yet, this handcrafted distribution is still arbitrary and still relies on the geometry of kernel points. ConvPoint <ref type="bibr" target="#b2">[3]</ref> randomly samples the kernel points, and their position is learned along with an assignment function A, with an MLP is applied to the kernel points represented in the coordinate system centered on the input points. All these methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b47">47]</ref> raise the issue of defining and optimizing the position of kernel points.</p><p>Feature combination and geometry lifting. In PointCNN <ref type="bibr" target="#b23">[23]</ref>, geometric information is extracted with an MLP ? , parameterized by ?, and concatenated with the input features to create mixed spectral-geometric features. The summands in Equation <ref type="formula" target="#formula_4">(4)</ref> </p><formula xml:id="formula_5">become K A(N[n])[f f (n), MLP ? (N[n])].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint estimation of K A(N[n]</head><p>). In fully implicit approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b50">50]</ref>, MLPs are used to directly estimate the weights W(n) to apply to input features f f (n), i.e., not separating W(n) into a product K ? A(N[n]), and thus mixing estimations in the spatial and feature domains.</p><p>Kernel separation. As acknowledged by the authors of PCCN themselves <ref type="bibr" target="#b50">[50]</ref>, a direct estimation of W(n) = K A(N[n]) is too computationally expensive to be used in practice. Instead, they resort to an implementation which falls into our formulation: for N out output channels, they consider N out parallel convolution layers with a size-1 kernel, corresponding to using a different A for each filter. In PointCNN <ref type="bibr" target="#b23">[23]</ref>, the extra features generated by the geometry lifting induce a larger kernel (1/4 more weights with default parameters <ref type="bibr" target="#b23">[23]</ref>), thus an increased memory footprint. To overcome this issue, <ref type="bibr" target="#b23">[23]</ref> also chooses to factorize the kernel as the product of two smaller matrices. Notice that this kernel separation trick can be implemented in any method that uses explicit kernel weights. Although we could as well, we do not need to resort to that trick in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our method: estimating a feature-kernel alignment</head><p>Our own convolution method is also based on Equation (4). However, contrary to preceding approaches, we do not use kernel points. Instead, we estimate a soft alignment matrix A based on the coordinates of neighboring points in N[n]. Our convolutional layer is illustrated on <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Neighborhood normalization. To be globally invariant to translation, all coordinates of the points of N[n] are expressed in the local coordinates system of n. This is particularly important for scene segmentation: the network should behave the same way for similar objects at different spatial locations. Please notice that it is not the case in RSConv <ref type="bibr" target="#b27">[27]</ref> where absolute coordinates are used, making it appropriate for shape analysis, not for scene processing.</p><p>N[n] is typically defined as the k-nearest neighbors (k-NNs) of n, or as all points in a ball around n. Both definitions have pros and cons. Using k-NNs is relatively fast, but the radius of the encompassing ball is (potentially highly) variable. As observed in <ref type="bibr" target="#b47">[47]</ref>, it may degrade spatial consistency compared to using a ball with a fixed radius. But searching within a radius is slower and yields (potentially widely) different numbers of neighbors, requiring strategies to deal with variable sizes, e.g., large tensor sizes and size tracking as in <ref type="bibr" target="#b47">[47]</ref>.</p><p>We propose an intermediate approach based on the k nearest neighbors, with a form a rescaling. As opposed to <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b35">35]</ref>, we do not normalize the neighborhood to a unit sphere regardless of its actual size in the original space. We estimate a normalization radius r t of the neighborhood at the layer level using the exponential moving average of Eq. (9) computed at training time, where t is the update step, m is a momentum parameter andr t is the average neighborhood radius of the current batch. Let q be the support point associated to n, and p i the i-th point of N[n]. The points (p i ) i actually used for the estimating A are the points (p i ) i centered and normalized using q and r t as follows:</p><formula xml:id="formula_6">r t =r t * m + r t?1 * (1 ? m),<label>(5)</label></formula><formula xml:id="formula_7">p i = (p i ? q)/r t .<label>(6)</label></formula><p>At inference time, this normalization ensures that all neighborhood are processed at the same scale while on average, neighborhoods are mapped to the unit ball.</p><p>Gating mechanism on distance to support point. While solving the problem of the neighborhood scale, this normalization strategy does not prevent points far away from the support point (the neighborhood center) to influence negatively the result. One could use hard-thresholding on the distance based on the estimated normalization radius r to filter these points, but this approach may cut too much information from the neighborhood, particularly in the case of high variance in neighborhood radii. Instead, we propose a gating mechanism to reduce, if needed, the effect of such faraway points. Given (p i ) i as defined in Equation <ref type="formula" target="#formula_0">(10)</ref>, the spatial gate weight s = (s i ) i satisfies</p><formula xml:id="formula_8">s i = ?(? ? ?||p i || 2 ),<label>(7)</label></formula><p>where ?(?) is the sigmoid function, ? is the cutoff distance (50% of the maximal value) and ? parametrize the slope of the transition between 0 (points filtered out) and 1 (points kept). Both ? and ? are learned layer-wise.  take the neighborhood into account, and thus may ignore useful information such as the local normal or curvature. To address point permutation invariance, PointNet <ref type="bibr" target="#b33">[33]</ref> uses a max-pooling operation. Likewise, we use a three-layer point-wise MLP with max-pooling after the first two layers. To reduce the influence of outliers, max-pooling inputs are weighted with s. The output is then concatenated to the point-wise features and given as input to the next fully-connected layer. This series of computations is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref> in the block called "alignment matrix estimation."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimation of</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Efficient point sampling with space quantization</head><p>Networks architectures for point cloud processing operates at full resolution through the entire network <ref type="bibr" target="#b50">[50]</ref>, or have an encoder/decoder structure <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">23]</ref> similar to networks used in image processing, e.g., U-Net <ref type="bibr" target="#b38">[38]</ref>. While the former maintain a maximum of information through the network, the later are usually faster as convolutions are applied to smaller point sets. However, decreasing the size of the point cloud requires to select the support points, i.e., the points at the center of the neighborhoods used in the convolution.</p><p>PointNet++ <ref type="bibr" target="#b35">[35]</ref> introduces farthest point sampling, an iterative sampling procedure where the next sampled point is the farthest from the already picked points. The main advantage of this sampling is to ensure a somewhat spatially uniform distribution which favors extreme points (e.g., at wing extremities for planes) as they are usually important for shape recognition. However, it requires to keep track of distances between all pairs of points, which is costly and increases the computation time, in particular when dealing with large point clouds.</p><p>In ConvPoint <ref type="bibr" target="#b2">[3]</ref>, the point-picking strategy only takes into account seen and unseen points, without distance consideration. Points are randomly picked among points that were not previously seen (picked points and points in the neighborhood of these points). While being much faster than farthest point sampling, it appears to be less efficient (see experiments in Section 6). In particular, the sampling is dependent of the neighborhood size: the sampling is done outside the neighborhoods of the previously picked support points. A very small neighborhood size reduces the method to a pure random sampling.</p><p>Space quantization. We propose an alternative approach that ensures a better sampling than <ref type="bibr" target="#b2">[3]</ref> while being much faster than <ref type="bibr" target="#b35">[35]</ref>. The procedure is illustrated on <ref type="figure" target="#fig_2">Fig. 3</ref>. We discretize the space using a regular voxel grid. Each point is associated to the grid cube it falls in. In each non-empty grid cube, one point is selected. We continue with the non-selected points and a voxel size divided by two, and repeat the process until the desired number of sampled points is reached or exceeded. In the later case, some points selected at the last iteration are discarded at random to reduce the cardinality of Q, the set selected points.</p><p>Quantization step estimation. Our approach is voxel-size dependent. On the one hand, a coarse grid leads to many iterations in the selection procedure, at the expense of computation time. On the other hand, a fine grid reduces to random sampling. Finding the optimal voxel size could be achieved using a exhaustive search (for |Q| filled voxels at a single quantization step), but it is very slow. Instead, we propose to estimate the voxel size via a rule of thumb derived by considering a simple configuration where a plane is intersecting a cube of unit length divided by a voxel grid of size a ? a ? a. If the plane is axis aligned, it intersects a 2 voxels. A sensible sampling would pick a support point in each intersected voxel, i.e., |Q| = a 2 . This indicates that letting the length v = 1/a of a voxel be proportional to 1/ |Q| is a reasonable choice for the voxel size. We found experimentally that choosing the diagonal length of the bounding box of the point cloud, denoted hereafter by diag, as factor of proportionality is usually a good choice (see Section 6.2). The voxel size is thus set to v = diag/ |Q|.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we evaluate our convolutional layer on shape classification, part segmentation and semantic segmentation, reaching the state of the art regarding task metrics while being efficient regarding computation time and memory usage.</p><p>Network architectures. In our experiments, we use a simple yet effective residual network for classification and semantic segmentation. We mimic the architecture of <ref type="bibr" target="#b47">[47]</ref>, except that ours is designed for k-NN convolution, i.e., we do not need to add phantom points and features to equalize the size of data tensor due to a variable number of points in radius search. The network has an encoder-decoder structure. The encoder is composed of an alternation of residual blocks maintaining the resolution and residual blocks with down-sampling. The decoder is a stack of fully-connected and nearest-neighbor upsampling layers. The classification network is the encoder of the previously described network followed by a global average pooling. For large scale semantic segmentation, we use either input modality dropout <ref type="bibr" target="#b47">[47]</ref> or dual network fusion <ref type="bibr" target="#b2">[3]</ref>, as indicated in tables.</p><p>Experimental setup. Our formulation (and code) allows a variable input size, but in order to use optimization with mini-batches, with train the networks with fixed input sizes. As every operations of FKAConv are differentiable, all parameters are optimized via gradient descent (including the spatial gating parameters ?'s and ?'s). Finally, we use a standard cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Benchmark results</head><p>Shape classification. The classification task is evaluated on ModelNet40 <ref type="bibr" target="#b54">[54]</ref>. As the spatial pooling process is stochastic, multiple predictions with the same point cloud might lead to different outcomes. We aggregate 16 predictions for each point cloud and select the most predicted shape (we use a similar approach for part segmentation). On the classification task <ref type="figure" target="#fig_0">(Table 1(a)</ref>), we present average (and best) results over five runs. For fair comparison, we train with 1024 (resp. 2048) points. We rank first (resp. second) among the method trained with 1024 (resp. 2048) points. We mainly observe that increasing the number points of reduces the standard deviation of the performances.</p><p>Part segmentation. On ShapeNet <ref type="bibr" target="#b57">[57]</ref>, the network is trained with 2048 input points and 50 outputs (one for each part). The loss and scores are computed per object category (16 object categories with 2-to 6-part labels). The results are presented in <ref type="table" target="#tab_2">Table 1</ref>(b). We rank among the best methods: top-2 or top-5 depending on the metric used, i.e., mean class intersection over union (mcIoU) or instance average intersection over union (mIoU); we are only 0.3 point mcIoU and 0.7 point mIoU behind the best method. It is interesting to notice that we are as good as or better than several methods for which the convolution falls into our formalism, such as ConvPoint <ref type="bibr" target="#b2">[3]</ref> or SPLATNet <ref type="bibr" target="#b43">[43]</ref>.</p><p>Semantic segmentation Three datasets are used for semantic segmentation corresponding to three different use cases. S3DIS <ref type="bibr" target="#b0">[1]</ref> is an indoor dataset acquired with an RGBD camera. The evaluation is done using a 6-fold cross validation. NPM3D <ref type="bibr" target="#b40">[40]</ref> is an outdoor dataset acquired in four sites using a lidar-equipped car. Finally, Semantic8 <ref type="bibr" target="#b14">[14]</ref> contains 30 lidar scenes acquired statically. NPM3D and Semantic8 are datasets with hidden test labels. Scores in the tables are reported from the official evaluation servers.</p><p>We use 8192 input points but, as subsampling the whole scene produces a significant loss of information, we select instead points in vertical pillars with a square footprint of 2 m for S3DIS, and 8 m for NPM3D and Semantic8. The center point of the pillar is selected randomly at training time and using a sliding window at test time. If a point is seen several times, the prediction scores are summed and the most probable class is selected afterward.</p><p>The results are presented in <ref type="figure">Fig. 4</ref> and <ref type="table" target="#tab_3">Table 2</ref>. We use S3DIS (Table 2(a)) to study the impact of the training strategy. As underlined in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b47">47]</ref>, direct learning with colored points yields a model relying too much on color information, at the expense of geometric information. We train three models. The first is the baseline model trained with color information, the second uses color dropout as in <ref type="bibr" target="#b47">[47]</ref>, and the third is a dual model with a fusion module <ref type="bibr" target="#b2">[3]</ref>. We observe that fusion gives the best results. In practice, the model trained with modality dropout tends to select one of the two modalities, either color or geometry, depending on what modality gives the best results. On the contrary, the fusion technique uses two networks each trained with a different modality, resulting in a lot larger network, but ensuring that the information of both modalities is taken into account.</p><p>Our network is second on S3DIS, first on NPM3D and third on Semantic8. On S3DIS, it is the best approach for 3 out of 13 categories and it performs well on the remaining ones. We are only outperformed by KPConv, which is based on radius search. On NPM3D, we reach an average intersection over union (av. IoU) of 82.7, which is 0.7 point above the second best method. On Semantic8, we place third according to average IoU, and first on overall accuracy among the published and arXiv methods. We obtain the best scores in 3 out 8 categories (the top-3 for 6 categories out of 8). More interestingly, we exceed the scores of ConvPoint [3] on 5 categories. The only downside is the very low score on the category of artefacts. One possible explanation could be that the architecture used in this paper (the residual network) is not suitable to learn a reject class (the artefact class is mainly all the points that do not belong to the 7 other classes, i.e., pedestrians but also 1 Note: We report here only the published methods at the time of writing. *In the official benchmark, the entry corresponding to our method is called LightConvPoint, which refers to the framework used for our implementation.</p><p>scanning outliers). It is future work to train the ConvPoint network with our convolution layer to support this hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Support point sampling: discretization parameter.</head><p>The rule of thumb in Equation <ref type="formula" target="#formula_9">(8)</ref> was derived in a simplistic case: a point cloud sampled from an axis aligned plane crossing a regular voxel grid. In practice, planar surfaces are very common, particularly in semantic segmentation (walls, floors, etc.), but are not a good model for most of the object of the scenes (chairs, cars, vegetation, etc.). To validate Eq. (8), we compute the optimal quantization parameter (i.e., the parameter with the largest value leading to the desired number of support points in a single quantization) computed using a dichotomic search on the parameter space and compare it to the derived expression. <ref type="figure">Figure 5</ref> presents the results of the experiment. For each point cloud, the optimal voxel size is represented by a semi-transparent disk (blue for ShapeNet, orange for S3DIS) and can be compared to the derived expression (red curve). In our setting, a curve under the colored disks is not desired; it is an over-quantization. We prefer a curve above these disks, possibly leading to extra iterations, but not affecting performance. We observe that Equation <ref type="formula" target="#formula_9">(8)</ref> provides a good estimate of the voxel size, especially for S3DIS which is a dataset containing a lot of planes.  For ShapeNet, we observe a higher variance, due to the great variability of shapes. Because of numerous objects that cannot be modeled well by planes in ShapeNet, we slightly overestimate the voxel size, leading only to one spurious iteration, which only slightly slows down the operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Support point sampling: computation times.</head><p>To assess our sampling approach, we run two experiments. First, in <ref type="table" target="#tab_5">Table 3</ref>(a), we compare the sampling time as a function of the size of the input point cloud. The number of support points is half the input point cloud size, and the number of neighbors is 16. The scores are averaged over 5000 random points clouds sampled in a cube. We also report the ShapeNet scores to relate the performance and the computation times. We compare our sampling strategy with farthest point sampling <ref type="bibr" target="#b35">[35]</ref>, with iterative neighborhood rejection <ref type="bibr" target="#b2">[3]</ref> and with a random baseline. As farthest point sampling <ref type="bibr" target="#b35">[35]</ref> is the reference of several state-of-the-art methods, we give the gain relatively to this method in percentage. Our quantized sampling is almost as fast as random sampling and much more efficient than farthest point sampling. In fact, our sampling has almost a linear complexity, compared to farthest point sampling, that has a quadratic complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Inference time and memory consumption</head><p>We present in <ref type="table" target="#tab_5">Table 3</ref>(b) the performance of our convolution layer and compare it to other convolutional layers. All computation times and memory usage are given for the segmentation network architecture and for one point cloud. The measures were done with 8192 points in each point cloud and a batch size of 16 (except for PCCN** for which the batch size is reduce to 4 to fit in the 11 GB GPU memory). Computational times are given per point cloud in milliseconds, and memory usage is reported in gigabytes.</p><p>We observe that our computation times at inference are very similar to those of ConvPoint <ref type="bibr" target="#b2">[3]</ref>, which is expected as it falls into our same general formulation. The same would probably be observed for a k-NN version of the KPConv <ref type="bibr" target="#b47">[47]</ref>. Then, we remark that PointCNN <ref type="bibr" target="#b23">[23]</ref> and PCCN <ref type="bibr" target="#b50">[50]</ref> are up to twice slower for inference. PCCN uses the separable kernel trick to improve memory performance (cf. Section 3.0.3). In this form, it is similar to N out (N out being the number of filters of the layer) parallel instances of our layer with one kernel element, i.e., it is equivalent to estimating a different A for each f ? {1, . . . , F }. We also report in <ref type="table" target="#tab_5">Table 3</ref>(b) the performance for PCCN**, which is the the purely continuous convolution described in PCCN <ref type="bibr" target="#b50">[50]</ref>, but without the separable kernel trick.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Filter visualization</head><p>Our method FKAConv was derived from the discrete convolution on regular grids. The behavior of our 3D filters should thus be comparable to their 2D counterparts. In <ref type="figure">Figure 6</ref>, we present the outputs of early and deep filters for the classification network on ModelNet40. For easier visualization, the features at coarse scales (high level / deep features) have been upsampled at the full point-cloud resolution. We notice that early layers produce features based on surface orientation. This is consistent with the small receptive field of early layers, that yields fine-scale features. On the contrary, deep layers produces shape-related features detecting objects parts, such as people heads or airplane bodies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Low-level features</head><p>High-level features <ref type="figure">Figure 6</ref>: FKAConv filter response for different input shapes on ModelNet40. Low-level features are extracted from the first layer (4 filters), and high-level features from the fourth layer (5 filters). The colormap represents the filter response for the shape, from blue (low response) to red (high response).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We presented a formulation for convolution on point clouds that unifies a range of existing convolutional layers and suggests a new point convolution approach. The core of the method is the estimation of an alignment matrix between the input points and the kernel. We also introduced an alternative point sampling strategy to farthest point sampling by using a progressive voxelization of the input space. While being almost as efficient as farthest point sampling, it is nearly as fast as random sampling. With these conceptually simple and easy to implement ideas, we obtained state-of-the-art results on several classification and semantic segmentation benchmarks among methods based on k-NN search, while being among the fastest and most memory-efficient approaches.</p><p>We present complementary information about the FKAConv paper. In Section A, we detail the network architectures used for classification and semantic segmentation. In Section B, we briefly describe the datasets used for experiments. In Section C, we discuss the use of a learned normalization of the support point neighborhoods. Finally, in Section D, we provide more qualitative results on the semantic segmentation test datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Network details</head><p>As mentioned in Section 5.1 of the paper, the networks we use in our experiments are based on the residual network architecture in KPConv <ref type="bibr" target="#b47">[47]</ref>, for which we replace the convolution by FKAConv and the neighborhood construction by our point sampling with space quantization. We detail here the main components.</p><p>A.0.1 Residual block.</p><p>The residual block in <ref type="figure" target="#fig_4">Fig. 7 (a)</ref> is the main module of our networks. This block is made of an FKAConv layer placed between two linear layers. The residual connection has one optional linear layer and one optional max-pooling layer. The optional linear layer is used only when the number of input channels is different from the number of output channels, and the optional max-pooling layer is used if the cardinality of the support points is different from the cardinality of the input points.</p><p>A.0.2 Classification and segmentation networks.</p><p>The two networks for these tasks are presented in <ref type="figure" target="#fig_4">Fig. 7 (b)</ref>. They share the same encoder structure, i.e., a FKAConv layer and 9 residual modules with a progressive reduction of the point cloud size.</p><p>? The classification network has an extra point-wise fully-connected layer (or unary convolutional) with its output dimension equal to the number of classes. The final prediction is done by averaging the scores of the 8 final support points.</p><p>? The segmentation network has an encoder-decoder structure. The decoder is a stack of 5 unary layers with, nearest-neighbor up-sampling between each layer. We use skip connections from the encoder to the decoder: the target points for up-sampling are the support points at the corresponding scale in the encoder, and the features from the encoder and the decoder are concatenated at each scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.0.3 Fusion network.</head><p>The fusion module presented in <ref type="figure" target="#fig_4">Fig. 7 (c)</ref> is identical to the module from the official repository of Conv-Point <ref type="bibr" target="#b2">[3]</ref>, except that it uses our proposed convolution. It is made of 3 layers: 2 FKAConv layers and 1 unary layer. The features from the penultimate layer of both segmentation networks are concatenated and given as input to the first FKAConv layer. The output of the second FKAConv layer is then concatenated with the predictions of the two segmentation networks and given to the unary layer.</p><p>A.0.4 Parameters of the convolution.</p><p>In order to keep the setup simple, we use the same parameters for all FKAConv layers. The neighborhood size of the support points is fixed to 16 and we use 16 kernels. In the encoder, each odd residual block but the first reduces the number of support points from |P| to 512, 128, 32, and finally 8.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Datasets</head><p>We evaluate our convolution on three different tasks: object classification, part segmentation and semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.0.1 Classification</head><p>is evaluated on ModelNet40 <ref type="bibr" target="#b54">[54]</ref>. It contains 12,311 point clouds sampled from CAD models of 40 different categories. 9843 shapes are used for training, 2468 for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.0.2 Part segmentation</head><p>is done on Shapenet <ref type="bibr" target="#b57">[57]</ref>. It is composed of 16 object category, each category being annotated with 2 to 6 part labels. As a pre-processing, all models are first normalized to the unit sphere. In our implementation, the network has 50 outputs (one for each part) and the loss and scores are computed per object category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.0.3 Semantic segmentation</head><p>is evaluated on 3 different indoor and outdoor datasets: S3DIS <ref type="bibr" target="#b0">[1]</ref>, NPM3D <ref type="bibr" target="#b40">[40]</ref> and Semantic8 <ref type="bibr" target="#b14">[14]</ref>.</p><p>? S3DISS <ref type="bibr" target="#b0">[1]</ref> is a subset of the 2D-3D-S dataset for semantic segmentation of building interiors. The data are acquired over 6 building floors with an RGBD camera. Each points is annotated with one of 13 labels: 12 semantic labels (floors, tables, chairs, etc.) and 1 label for a "clutter" class, mostly including office supplies. The evaluation is done using a 6-fold cross validation.</p><p>? NPM3D <ref type="bibr" target="#b40">[40]</ref> is a lidar dataset for large-scale outdoor semantic segmentation. Points were acquired in 4 sites using a car equipped with lidar. 10 classes of urban entities are labeled, such as impervious surface, poll or pedestrian.</p><p>? Semantic8 <ref type="bibr" target="#b14">[14]</ref> is the main dataset in the Semantic3D benchmark suite. It contains 30 lidar scenes, 15 for training and 15 for testing. Over 4 billion points are labeled with 8 classes such as building, vegetation and car, and a challenging class for scanning artifacts. The test set is particularly difficult as it covers several diverse scenes such as city streets, villages or old castles.</p><p>C K-nearest neighbors search with learned neighborhood normalization</p><p>As described in Section 4 of the paper, we normalize the size of point neighborhoods. After recalling the principle of this normalization, we analyze here its effect empirically, for different datasets and for different layers in our networks.</p><p>To normalize the neighborhoods, we estimate an average neighborhood radius r t using an exponential average of the actual neighborhood radiusr t seen at training time in the successive batches indexed by t (see Equation <ref type="bibr" target="#b8">(9)</ref> below where m is the momentum parameter). The point coordinates (p i ) 1?i?k of the k nearest neighbors of a support point q are then centered and normalized as specified in Equation <ref type="formula" target="#formula_0">(10)</ref>, yielding normalized points (p i ) 1?i?k :</p><formula xml:id="formula_10">r t =r t * m + r t?1 * (1 ? m),<label>(9)</label></formula><formula xml:id="formula_11">p i = (p i ? q)/r t .<label>(10)</label></formula><p>We also proposed a gating mechanism to reduce, if needed, the negative effect of faraway points. Given the centered and normalized points (p i ) i computed above, the spatial gate weight s = (s i ) i satisfies:</p><formula xml:id="formula_12">s i = ?(? ? ?||p i || 2 ),<label>(11)</label></formula><p>where ?(?) is the sigmoid function, and ?, ? are parameters to learn.</p><p>C.0.1 Average neighborhood radius r t .</p><p>First, we study the estimated neighborhood size r t at each layer of the encoder after training. This size is the averaged radius of the smallest sphere centered on the support point and encompassing the 16 nearest neighbors. We present in <ref type="figure">Fig. 8 (a)</ref> the evolution of this radius as a function of the layer's depth, for different datasets.</p><p>We observe that this radius is directly linked to the size of the bounding box of the input point cloud in the 3D space, i.e., to the size of the point cloud pillars (vertical infinite cylinders of diameter 8 meters for Semantic8 and NPM3D, and 2 meters for S3DIS) or to the size of the ShapeNet's CAD models.</p><p>C.0.2 Weighting function.</p><p>In <ref type="figure">Figs. 8 (b-c)</ref>, we plot the values of parameters ? and ? as a function of the depth of the FKAConv layers. First, we observe that these parameters, which are optimized with the network, take values that are different from the initial values that are set at the training initialization, i.e., ? = 1 and ? = 1. Second, the values of ? and ? are similar after training on Semantic8 and NPM3D. This could be expected as both datasets share common characteristics: outdoor urban scenes, same pillar size. Third, we observe the same global variations of the curves on all datasets: ? tends to decrease with the depth while ? tends to increase. As a result, the transition of the gating function becomes wider in deeper layers, i.e., deeper layers tend to take into account more far away points.</p><p>We illustrate this phenomenon on <ref type="figure">Figs. 8 (d-e)</ref>, where we represent the weighting function after optimization for the first and for the seventh FKAConv layer of the network. For comparison purpose, we normalize the curves by setting the weight to 1 at distance 0. The black curve is the initial function, before optimization.</p><p>At the first layer, only the neighboring points that are very close to the support point are taken into consideration to estimate matrix A. We hypothesize that, in the absence of noise in data, a small neighborhood is sufficient to estimate local geometric features.</p><p>On the contrary, in the 7 th layer, all the neighboring points are taken into consideration to compute A. At this stage, the number of support points is small and each point carries features that are discriminating for the task. The network considers all available information, including points that are faraway from the support points.</p><p>C.0.3 Influence on performance.</p><p>To quantify the impact of the neighborhood normalization and gating mechanism, we trained a segmentation network using five different configurations.</p><p>? Baseline (no normalization). We do not normalize the neighborhood coordinates. The network sees different neighborhood sizes.</p><p>? Baseline (normalization to the unit ball). Each neighborhood is normalized into the unit sphere, regardless of its original size.</p><p>? Learned normalization + fixed gating at r. The radius used for normalization is estimated using the proposed exponential moving average. The gating mechanism is replaced by hard-thresholding: features of all points outside the ball with the learned radius r are set to zero.  Influence of the distance to support point on feature weight (d) for the first FKAConv layer.</p><p>(e) for the 7 th residual block. <ref type="figure">Figure 8</ref>: Behavior of the learned neighborhood normalization and feature weighting across the segmentation network for various datasets.</p><p>? Learned normalization + fixed gating at 2r. Same as above but using a radius of 2r for hardthresholding.</p><p>? Our approach. The radius used for normalization and the gating function are learned as proposed. The results, reported in <ref type="table" target="#tab_8">Table 4</ref>, show a slight improvement using our approach with respect to the baselines: the mean class intersection over union (mcIoU) increases by 0.2 point and the instance average intersection over union (mIoU) by 0.1 point. This gain may seem small, but it is significant on this dataset as the performance are close to saturation in the leaderboard.</p><p>We also observe that the learned gating is an important factor of the success of our approach. Fixed gating leads to a performance drop (see <ref type="table" target="#tab_8">Table 4</ref>). This is due to the fact that hard-thresholding suppresses too much information, particularly in the late stages of the network where the neighborhoods are less regular and more subject to size variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Qualitative results of segmentation</head><p>Finally, we provide more visual results illustrating our semantic segmentation predictions on datasets NPM3D ( <ref type="figure">Fig. 9</ref>), Semantic8 ( <ref type="figure" target="#fig_0">Fig. 10</ref>) and S3DIS <ref type="figure" target="#fig_0">(Fig. 11</ref>). <ref type="figure">Figure 9</ref>: Visual results of our predictions on the test scenes of the NPM3D dataset. The input data is colored according to lidar intensity, from blue (low intensity) to red (high intensity) through green, yellow and orange. Colored, predicted, semantic segments are immediately on the right to lidar images. The segmentation results are obtained with the fusion model (intensity + geometry). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Kernel-input alignment for grid inputs (a,b) and point clouds (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>FKAConv convolutional layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Point sampling with space quantization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Visual results of semantic segmentation: S3DIS (1 st row), NPM3D (2 nd row) and Semantic3D (3 rd row). Ground truth of test data publicly unavailable for last two. y=1/sqrt(|Q|) |Q| Empirical validation of voxel size estimation: ShapeNet (blue), S3DIS (orange). Each dot is the empirical optimal voxel size obtained by dichotomic search. The red line is the voxel size defined as the inverse square root of the number of support points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Network architectures used for semantic segmentation and classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>LCP layer indexAverage size of the support point neighborhood (a) Average neighborhood radius at each layer.LCP layer index alpha LCP layer index Beta (b) ? learned at each layer. (c) ? learned at each layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 :</head><label>11</label><figDesc>Visual results of our predictions on the S3DIS dataset obtained with the fusion model (RGB + geometry).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Classification and part segmentation benchmarks.</figDesc><table><row><cell></cell><cell>(a) ModelNet40</cell><cell cols="2">(b) ShapeNet</cell></row><row><cell>Methods Mesh or voxels Subvolume [34] MVCNN [44] Points DGCNN [52] PointNet [33] PointNet++ [35] PointCNN [23] ConvPoint [3] KPConv [47] Ours FKAConv</cell><cell>Num. points --1024 1024 1024 1024 2048 2048 1024 92.3?0.2 (92.5) 89.6?0.3 (89.9) OA AA 89.2 -90.1 -92.2 90.2 89.2 86.2 90.7 -92.2 88.1 92.5 89.6 92.9 -Average?std. (best run) 2048 92.5?0.1 (92.5) 89.5?0.1 (89.7)</cell><cell>Method PointNet++ [35] SubSparseCN [12] SPLATNet [43] SpiderCNN [55] SO-Net [22] PCNN [2] KCNet [42] SpecGCN [49] RSNet [17] DGCNN [52] SGPN [51] PointCNN [23] ConvPoint [3] KPConv [47] FKAConv (Ours)</cell><cell>mcIoU mIoU 81.9 85.1 83.3 86.0 83.7 85.4 81.7 85.3 81.0 84.9 81.8 85.1 82.2 83.7 -85.4 81.4 84.9 82.3 85.1 82.8 85.8 84.6 86.1 83.4 85.8 85.1 86.4 84.8 85.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Semantic segmentation benchmarks. wall beam col. wind. door chair table book. sofa board clut. Pointnet [33] Knn 47.6 88.0 88.7 69.3 42.4 23.1 47.5 51.6 42.0 54.1 38.2 9.6 29.4 35.2 RSNet [17] -56.5 92.5 92.8 78.6 32.8 34.4 51.6 68.1 60.1 59.7 50.2 16.4 44.9 52.We report here only the published methods at the time of writing.</figDesc><table><row><cell>(a) S3DIS</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Computation time and memory consumption. Time and memory consumption for a segmentation network, with 8192 points.</figDesc><table><row><cell cols="4">(a) Computation times for different sampling strategies. Method Sampling time (ms)</cell><cell>ShapeNet</cell><cell>(b) Convolution</cell><cell cols="2">Training</cell><cell>Test</cell><cell></cell></row><row><cell></cell><cell>1k pts</cell><cell>5k pts</cell><cell>10k pts</cell><cell>(mIoU)</cell><cell>Layer</cell><cell cols="2">Time Memory</cell><cell cols="2">Time Memory</cell></row><row><cell>Random</cell><cell>1.66</cell><cell>8.6</cell><cell>18.6</cell><cell>84.4</cell><cell></cell><cell>(ms)</cell><cell>(GB)</cell><cell>(ms)</cell><cell>(GB)</cell></row><row><cell>(baseline)</cell><cell>(-60%)</cell><cell>(-89%)</cell><cell>(-94%)</cell><cell></cell><cell>ConvPoint [3]</cell><cell>85.7</cell><cell>10.1</cell><cell>65</cell><cell>2.9</cell></row><row><cell>ConvPoint [3]</cell><cell>2.60</cell><cell>25.4</cell><cell>88.2</cell><cell>84.6</cell><cell>ConvPoint*</cell><cell>12.2</cell><cell>4.3</cell><cell>4.29</cell><cell>1.6</cell></row><row><cell></cell><cell>(-37%)</cell><cell>(-68%)</cell><cell>(-71%)</cell><cell></cell><cell>PointCNN* [23]</cell><cell>33.6</cell><cell>3.5</cell><cell>6.23</cell><cell>1.7</cell></row><row><cell>Farthest [35]</cell><cell>4.12</cell><cell>79.8</cell><cell>310.2</cell><cell>84.7</cell><cell>PCCN* [50]</cell><cell>31.1</cell><cell>4.9</cell><cell>10.2</cell><cell>2.3</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(-)</cell><cell></cell><cell>PCCN** (bs4)</cell><cell>64.2</cell><cell>6.4</cell><cell>19.7</cell><cell>2.6</cell></row><row><cell>FKAConv sampling</cell><cell>1.93</cell><cell>10.3</cell><cell>20.4</cell><cell>84.6</cell><cell>FKAConv (Ours)</cell><cell>19.1</cell><cell>5.6</cell><cell>4.9</cell><cell>1.4</cell></row><row><cell></cell><cell>(-53%)</cell><cell>(-87%)</cell><cell>(-93%)</cell><cell></cell><cell cols="3">*: reimplemented in our framework.</cell><cell></cell><cell></cell></row><row><cell cols="4">(time for n inputs points, n/2 support points,</cell><cell></cell><cell cols="4">**: original formulation without separation trick,</cell><cell></cell></row><row><cell cols="4">16 neighbors, averaged over 5000 iterations).</cell><cell></cell><cell cols="4">differs from code used for experiments in [50].</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Ic (resp. Oc) is the number of input (resp. output) channels.</figDesc><table><row><cell></cell><cell>I c to I c /2</cell><cell>I c /2 to I c /2</cell><cell>I c /2 to O c</cell><cell>Unary conv. + BN + ReLU</cell></row><row><cell></cell><cell></cell><cell></cell><cell>No ReLU</cell><cell>ReLU</cell><cell>FKAConv + BN + ReLU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Optional depending on</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Max Pool.</cell><cell>Residual Block</cell><cell>stride and/or number of input/output channels</cell></row><row><cell>|P| pts</cell><cell cols="3">32 pts 512 ch. (a) Residual block. 512 pts 128 ch. 128 pts 256 ch.</cell><cell>8 pts 1024 ch.</cell><cell>Segmentation head</cell></row><row><cell>64 ch.</cell><cell></cell><cell></cell><cell></cell><cell>Classification</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Average pooling</cell><cell>head</cell></row><row><cell></cell><cell cols="2">Unary conv. + BN + ReLU</cell><cell cols="2">FKAConv + BN + ReLU</cell><cell>Residual block</cell><cell>Upsampling</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(b) Classification and segmentation networks.</cell></row><row><cell cols="2">Segmentation</cell><cell></cell><cell></cell></row><row><cell cols="2">network 1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Unary conv.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>+ BN + ReLU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>FKAConv +</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>BN + ReLU</cell></row><row><cell cols="2">Segmentation</cell><cell></cell><cell></cell></row><row><cell cols="2">network 2</cell><cell></cell><cell></cell></row></table><note>(c) Fusion module.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Impact of the neighborhood normalization and gating on ShapeNet. = 1 if d(p, q) &lt; rt, and s i = 0 otherwise. 83.8 84.8 s i = 1 if d(p, q) &lt; 2rt, and s i = 0 otherwise. 84.0 85.2 (p denotes a point in the neighborhood of q, and rt is the estimated radius in Equation (1).)</figDesc><table><row><cell>Method</cell><cell>mcIoU mIoU</cell></row><row><cell>Baseline (no normalization)</cell><cell>84.3 85.3</cell></row><row><cell>Baseline (normalization to unit ball)</cell><cell>84.6 85.6</cell></row><row><cell>Learned normalization + fixed gating</cell><cell></cell></row><row><cell>s i Our method (learned normalization and gating)</cell><cell>84.8 85.7</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3D semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1534" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Point convolutional neural networks by extension operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGRAPH, ACM Transaction on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ConvPoint: Continuous convolutions for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="24" to="34" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SnapNet: 3D point cloud semantic labeling with 2D deep segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guerry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="189" to="198" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Flexible, high performance convolutional neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Cire?an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1237" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Edge-convolution point net for semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Contreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Geoscience and Remote Sensing Symposium (IGARSS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5236" to="5239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.6070</idno>
		<title level="m">Spatially-sparse convolutional neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3D semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning rich features from RGB-D images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">net: A new large-scale point cloud classification benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Semantic3d</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences (ISPRS Annals), volume IV-1-W1</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast semantic segmentation of 3D point clouds with strongly varying density. ISPRS Annals of Photogrammetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing &amp; Spatial Information Sciences (ISPRS Annals)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pointwise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="984" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recurrent slice networks for 3D segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2626" to="2635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep projective 3D semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Lawin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tosteberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Analysis of Images and Patterns (CAIP)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="95" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SO-Net: Self-organizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G. Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9397" to="9406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">PointCNN: Convolution on X-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="820" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hierarchical depthwise graph convolutional neural network for 3D semantic segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8152" to="8158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dynamic points agglomeration for hierarchical point sets learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7546" to="7555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8895" to="8904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Point-voxel CNN for efficient 3D deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="965" to="975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Global context reasoning for semantic segmentation of 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2931" to="2940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">VoxNet: A 3D convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mind the gap: modeling local and global context in (road) networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Montoya-Zegarra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladick?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition (GCPR)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="212" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">PointNet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view CNNs for object classification on 3D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">PointNet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5105" to="5114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">3D graph neural networks for RGDB semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5199" to="5208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">OctNet: Learning deep 3D representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osman Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3577" to="3586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Roynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03583</idno>
		<title level="m">Classification of point cloud scenes with multiscale voxel deep network</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Paris-Lille-3D: A large and high-quality ground-truth urban point cloud dataset for automatic segmentation and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Roynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research (IJRR)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="545" to="557" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mining point cloud local structures by kernel correlation and graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">SPLATNet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3D shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Semantic classification of 3D point clouds with multiscale spherical neighborhoods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="390" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">KPConv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Fast point cloud registration using semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Gilani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M S</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Suter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Digital Image Computing: Techniques and Applications (DICTA)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Local spectral graph convolution for point set feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Samari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2589" to="2597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">SGPN: Similarity group proposal network for 3D point cloud instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2569" to="2578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dynamic graph CNN for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions On Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">PointConv: Deep convolutional networks on 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">SpiderCNN: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Modeling point clouds with self-attention and Gumbel subset sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3323" to="3332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3D shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">210</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">ShellNet: Efficient point cloud convolutional neural networks using concentric shells statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1607" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">PointWeb: Enhancing local neighborhood features for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5565" to="5573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">VoxelNet: End-to-end learning for point cloud based 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
