<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The 2021 Image Similarity Dataset and Challenge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Electrical Engineering</orgName>
								<orgName type="laboratory">VRG</orgName>
								<orgName type="institution">Czech Technical University</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><surname>Pizzi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zo?</forename><surname>Papakipos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lowik</forename><surname>Chanussot</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radenovic</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Jenicek</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Electrical Engineering</orgName>
								<orgName type="laboratory">VRG</orgName>
								<orgName type="institution">Czech Technical University</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Maximov</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Technical University Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Technical University Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Elezi</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Technical University Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Chum</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Electrical Engineering</orgName>
								<orgName type="laboratory">VRG</orgName>
								<orgName type="institution">Czech Technical University</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><forename type="middle">Canton</forename><surname>Ferrer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The 2021 Image Similarity Dataset and Challenge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces a new benchmark for largescale image similarity detection. This benchmark is used for the Image Similarity Challenge at NeurIPS'21 (ISC2021). The goal is to determine whether a query image is a modified copy of any image in a reference corpus of size 1 million. The benchmark features a variety of image transformations such as automated transformations, hand-crafted image edits and machine-learning based manipulations. This mimics real-life cases appearing in social media, for example for integrity-related problems dealing with misinformation and objectionable content. The strength of the image manipulations, and therefore the difficulty of the benchmark, is calibrated according to the performance of a set of baseline approaches. Both the query and reference set contain a majority of "distractor" images that do not match, which corresponds to a real-life needle-in-haystack setting, and the evaluation metric reflects that. We expect the DISC21 benchmark to promote image copy detection as an important and challenging computer vision task and refresh the state of the art. Code and data are available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Assessing whether an image is an edited copy of another source image or whether two images are edited copies of the same source image is a central task in the context of preserving information legitimacy and integrity, especially in social media <ref type="bibr" target="#b15">[20]</ref>.</p><p>The context is a system that tracks an image to determine its source, across platforms, through edits and re-encodings. There are many systems-level components of such a tracking system. The fundamental computer vision task is to determine whether a part of an image has been copied from another image.</p><p>The differences between the two images are due to various kinds of image degradation, encoding artifacts, image editing and manipulations. Thus, it is a 2D only transformation. The simplest variation is image re-encoding or resizing, generating near-exact copies, while more complex changes include cropping, color variations and collages with other images.</p><p>This topic has gathered some attention over decades and it has been deemed as a mature or even solved problem. However, most of the state-of-the-art solutions tend to deliver unsatisfactory results with dealing with large-scale corpora. Real-life scenarios of usergenerated content involve billions to trillions of images. With this idea in mind, this paper proposes the image similarity dataset and competition where we present a sufficiently large and difficult dataset to extrapolate algorithms to this operation scale. We call the dataset Dataset for ISC 2021, or DISC21. The dataset has been constructed with practical and commonly used types of manipulations between the query image and the target ones involving geometric, hand-made and even deepfake alterations. In this way, DISC21 provides a proxy measurement of algorithms in a real operation scenario (i.e. provenance) used in tasks like misinformation treatment, objectionable content detection and many others.</p><p>Publications on the subject of image and video copy detection are rare and not well known. This is because (1) the task is considered easy by the computer vision community and (2) the task is often adversarial, so organizations that use copy detection want to keep the techniques as obscure as possible. From a research point of view, it is useful to raise awareness in community that a "solved problem" like copy detection is not really solved at scale and in an adversarial setting. Also, there are subtle image matching problems that are not studied widely in the community and that are research problems in their own right. For example: determine what area of an image has been copied <ref type="bibr" target="#b63">[67,</ref><ref type="bibr" target="#b5">10]</ref>, or what transformation was applied to an image <ref type="bibr" target="#b66">[70]</ref>.</p><p>On the other end of the semantic spectrum, it is striking that most recent unsupervised image embedding extractors (SimCLR <ref type="bibr" target="#b4">[9]</ref>, Momentum contrast <ref type="bibr" target="#b16">[21]</ref>, Deepcluster <ref type="bibr" target="#b2">[7]</ref>) are trained exactly to do copy detection. Indeed, they use invariance to data augmentation as their main training criterion. Therefore they are directly optimizing for image copy detection. Selfsupervised learning methods have not been widely applied to the task of image copy detection and we hope this dataset and competition will be an occasion to ex-plore this direction.</p><p>The paper is organized as follows. Section 2 describes the industrial context of the task. Section 3 summarizes works on copy detection and related tasks. In Section 4 the process to build the dataset is introduced, together with the evaluation metrics. Section 5 reports baseline results on the dataset. Section 6 summarizes the rules of the ISC2021 competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation</head><p>Copy detection is widely used by Internet services, both to implement novel product features such as reverse image search, as well as to prevent the spread of content that have a negative social impact.</p><p>Use cases. Platforms use copy detection to apply moderator judgements to new copies of violating content, allowing them to moderate content more quickly, and at a larger scale than would be possible using manual content moderation. Image fingerprints, such as PhotoDNA from Microsoft, are used throughout the industry to identify images that depict child exploitation and abuse <ref type="bibr">[3]</ref>. Similar techniques can be applied to identify other types of violent or self-harm imagery.</p><p>Copy detection enables rapid response to viral or emergent offensive content, such as taking down copies of a live stream of the Christchurch shooting <ref type="bibr" target="#b25">[30]</ref>.</p><p>This allows taking action more quickly than classification models can be updated and deployed. Copy detection is also central to problem domains that require human judgement to identify. Facebook uses copy detection to identify misinformation by matching images to existing images that are associated with fact checking references [4].</p><p>In parallel, copy detection is used to find unauthorized copies of copyrighted media. For this task, copyrighted media to detect are identified by copyright holders.</p><p>On the negative side, one has to be aware that this technology could be used to implement extensive media censorship in some countries.</p><p>A solved problem? Image copy detection has received some attention over the years and may even be considered a solved problem. However, two new aspects of the problem make the problem especially hard to solve: scale and new attacks.</p><p>Scale. Copy detection algorithms are often deployed at large scale, both in terms of query volume and database size. For example, billions of images are uploaded to Facebook systems per day. The efficiency of matching algorithms is critical to making large scale copy detection practical. In particular, it is important to retrieve matches within a limited candidate volume.</p><p>In applied copy detection systems, the majority of query images do not have matches in the index. Thus, overall efficiency of the overall system is primarily determined by its efficiency processing non-match queries (e.g. the number of candidates retrieved).</p><p>The consequence is that copy detection typically operates in the high precision (and low recall) regime. Since the content to be detected is low-prevalence (needle in haystack), when moving from a small-scale experimental setting like this competition to a production setting, the weight of the false positives count increases much faster than the number of false negatives. Because of the flood of false positives due to scale, the only practical setting is to action only on high-confidence matches.</p><p>Retroactive matching, which aims to find existing copies of newly identified violating images, poses additional scale challenges. Retroactive matching databases can contain hundreds of billions to trillions of image fingerprints.</p><p>Difficult transforms. Changes in user behavior have made copy detection more challenging. Popular social media apps like Instagram and TikTok offer rich options to edit images and videos, expanding the set of transformations accessible to typical users. As users have moved more online activities to mobile devices, mobile screenshots and screen captures have become common ways to share media. Mobile screenshots often capture the original content in a small region of the image, capturing additional unrelated content (such as interface elements or comment text). These changes have extended the set of common transformations that copy detection systems must be robust to.</p><p>Moreover, the setting of the copy detection task is inherently adversarial. A user can upload content and gets immediate feedback from the platform about whether the content was blocked. Then the user can modify it slightly and retry the upload: this is a copy detection attack. In a production system. We briefly describe a copy detection system deployed at Facebook, as an example of large scale applied copy detection. As images are uploaded to Facebook products, multiple copy detection fingerprints are extracted, including conventional image hashes and deep learned global embedding models <ref type="bibr">[2]</ref>. We search over a database of images associated with moderator actions, using a distributed approximate nearest neighbor search system based on FAISS <ref type="bibr" target="#b22">[27]</ref>. This system searches for billions of images each day against a database of hundreds of millions of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related work</head><p>In this section we review prior work that is related to the DISC21 and the related existing datasets along with the corresponding research competitions, when there are any.  1: Image-pair similarity defined at different granularity levels in a nested way with the innermost level being a more restrictive one than the level above. The scope of the DISC21 dataset and competition correspond to the green area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Tasks and methods</head><p>Two images can be considered, i.e. labeled as, similar with criteria defined at different granularity levels as these are shown in <ref type="figure">Figure 1</ref> with examples in <ref type="figure">Figure 2</ref>. The most restrictive case corresponds to the exact duplicates, where images are bit-wise the same. Therefore, comparing images is trivial and reduces down to to comparing bit strings which is applied at a large scale via file comparison and hashing, e.g. with MD5 hashes. Near-exact duplicates correspond to changes that are nearly human imperceptible such as compression distortion. Edited copies, which are the main focus of DISC21, correspond to an image pair where both images are edited version of the same source image. Then, there exist the cases of images showing the same instance, e.g. a particular object, or two objects belonging to the same category, corresponding to the task of instance-level recognition (ILR) or categorylevel recognition (CLR), respectively.</p><p>We review approaches for Copy Detection (CD), but also copy retrieval due to overlap of approaches, in the domain of images and videos. In retrieval, the objective is to rank all database images with respect to the similarity to a query image. There is no need for the similarity score to be comparable across different queries; standard retrieval evaluation metrics perform on a per query basis. In detection though, the objective is to provide a list of detected copies, typically obtained by thresholding a confidence values that a query image is a copy of a source image. The confidence value should be comparable across queries; an appropriate evaluation metric should perform across all queries jointly. We additionally review approaches for ILR, as a neighboring level in the definitions of image similarity, and discuss cases for the tasks of retrieval, classification, and detection. ILR approaches are often directly transferable to copy detection and retrieval, which is also the case in the set of baseline approaches discussed in Section 5. Besides, there is an overlap between CD and ILR, where the image is e.g. printed out and captured by a camera: in that case there is a common source image but it is still a 3D image capture.</p><p>Video copy detection. The literature for copy detection is richer in the video domain than the image domain. Early approaches rely on appearancebased global representation for the whole video <ref type="bibr" target="#b52">[57,</ref><ref type="bibr" target="#b69">73,</ref><ref type="bibr" target="#b30">35,</ref><ref type="bibr" target="#b51">56]</ref>, reducing the task to similarity estimation in high dimensional spaces. Such approach is less effective for detecting partial copies. Frame-level matching is performed with the use of local descriptors only after a first filtering stage <ref type="bibr" target="#b69">[73]</ref>, or efficient local descriptor indexing and geometric matching <ref type="bibr" target="#b10">[15]</ref>, or with compact aggregated representations <ref type="bibr" target="#b11">[16]</ref>. As a postprocessing step, the temporal consistency of the result by frame-level matching is checked in the form of Hough temporal transform <ref type="bibr" target="#b11">[16]</ref> or cast as a network flow problem <ref type="bibr" target="#b57">[62]</ref>. Frame-level and temporal matching are jointly performed in the work of Poullot et al <ref type="bibr" target="#b44">[49]</ref>. Deep learning for video copy detection is used in recent approaches for video representation without temporal information <ref type="bibr" target="#b28">[33]</ref>, frame and temporal matching <ref type="bibr" target="#b17">[22]</ref> or spatio-temporal representations <ref type="bibr" target="#b27">[32]</ref>.</p><p>Image copy detection: The road path of global descriptors <ref type="bibr" target="#b24">[29,</ref><ref type="bibr" target="#b64">68]</ref> is followed for image copy detection too, which Douze et al <ref type="bibr" target="#b9">[14]</ref> perform a performance evaluation. Following the foot-steps of classical approaches in many other computer vision tasks, image copy detection is well handled by the use of local descriptors, the Bag-of-Words models and spatial constraints <ref type="bibr" target="#b72">[76,</ref><ref type="bibr" target="#b71">75]</ref>. Deep networks are only used in straight-forward ways, e.g. training in a Siamese way <ref type="bibr" target="#b70">[74]</ref>, which we interpret as loss of interest by the research community due to the same category  Real-world imaging process that generates image pairs that are considered similar at different granularity levels.  lack of new, challenging, large benchmarks that reflect real world applications. There is a more specialized line of work in detecting copy-move forgeries <ref type="bibr" target="#b63">[67,</ref><ref type="bibr" target="#b5">10]</ref>, an edit function where part of the image is copied and pasted into the same image, or estimation of the applied transformation <ref type="bibr" target="#b66">[70]</ref>.</p><p>Instance-level recognition. Instance-level recognition has attracted a lot of attention and many relevant approaches exist in the literature, especially for the task of retrieval. These include purely global descriptors <ref type="bibr" target="#b9">[14]</ref>, local descriptor indexing and matching <ref type="bibr" target="#b54">[59,</ref><ref type="bibr" target="#b18">23]</ref>, local descriptor aggregation into global descriptors <ref type="bibr" target="#b20">[25,</ref><ref type="bibr" target="#b40">45]</ref>, spatial verification <ref type="bibr" target="#b41">[46]</ref> and query expansion <ref type="bibr" target="#b6">[11]</ref>, among many others. Deep learning approaches are currently dominating in this task with both global <ref type="bibr" target="#b0">[5,</ref><ref type="bibr" target="#b48">53,</ref><ref type="bibr" target="#b47">52,</ref><ref type="bibr" target="#b32">37]</ref> and local descriptors <ref type="bibr" target="#b34">[39,</ref><ref type="bibr" target="#b53">58,</ref><ref type="bibr" target="#b60">65]</ref>. Recently, more attention is paid on ILR for classification at large scale <ref type="bibr" target="#b34">[39,</ref><ref type="bibr" target="#b60">65]</ref>, which bears similarities to the task of our work. Testing examples do not necessarily come from the same categories as the training ones, as some form of open set recognition, and classification is in practice performed <ref type="bibr" target="#b34">[39,</ref><ref type="bibr" target="#b60">65]</ref> by verification that a query and database image pair come from the same class. Another example departing from instance-level retrieval is the work of Furon and J?gou <ref type="bibr" target="#b13">[18]</ref>, where the focus is on detection of the relevant image pairs, which is also close to the setup of this work. <ref type="table" target="#tab_1">Table 1</ref> summarizes existing datasets that are related to the benchmark we are setting up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Datasets and competitions</head><p>Video datasets. Most copy detection datasets are in the video domain with some well established examples like VCDB <ref type="bibr" target="#b21">[26]</ref> and CCWeb <ref type="bibr" target="#b69">[73]</ref>. It is noteworthy that video copy detection is not a superset of image copy detection: video datasets focus on temporal degradations, such video edits, speed-up, slow-down, and, therefore, less on challenging frame-level degradations. In fact, due to the high information content of videos, it is easier to recognize copied videos than images <ref type="bibr" target="#b10">[15]</ref>.</p><p>Image datasets. Many of the prior approaches for image copy detection perform evaluation on private datasets, while the most well known publicly available dataset, Copydays <ref type="bibr" target="#b9">[14]</ref> is very small scale. There are other datasets that focus in the domain of logos, such as Belgalogos <ref type="bibr" target="#b23">[28]</ref>, which is a very specialized case because of the very limited number of templates to recognize. Dataset creation for copy detection at large scale is a tedious process, especially if user generated edited copies are included which also form the most challenging cases.</p><p>ILR datasets exist at much larger scale, but when relying on crowd-sourced labels <ref type="bibr" target="#b68">[72]</ref> to achieve that the result might include some noise. In this work, we introduce a dataset for image copy detection that (a) significantly exceeds the scale of previous ones, reaching the scale of ILR datasets, (b) contains a large amount of image copies including many user generated ones, and (c) includes a large number of distractor queries which is well aligned with the nature of a detection task.</p><p>Competitions There is no previous example of competition on copy detection on images. There was a video copy detection track in the TrecVid series of competitions between 2008 <ref type="bibr" target="#b10">[15]</ref> and 2012 <ref type="bibr" target="#b36">[41]</ref>. The Google Landmarks Challenge is a recent example for ILR at large-scale [1] that is attracting a lot of attention, i.e. more than hundred of actively participating teams on Kaggle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset</head><p>The dataset is composed of three parts: the reference set, also called database, which consists of the set of the original (source) images which are then transformed into the edited copies, the query set, which consists of the set of transformed images, and the training set which is of similar spirit to the reference set. There are two types of queries, depending on whether their source image is part of the reference set. For distractor queries, the source is not in the reference set: there no copy to detect for those. The dataset is a small-scale example of the task that is performed in a real-world industrial context. Even though Facebook, for example, processes billions of images per day, we restrict the size of the dataset to 1 million. On the other hand, the transformations that are used to generate the edited copies are harder than what is typically seen in a real-world context, which makes DISC21 a more interesting and attractive computer vision benchmark. We additionally make sure that specialized approaches, such as optical character recognition (OCR) and face recognition do not significantly improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data sources</head><p>The reference set is based on two sources: the YFCC100M [63] dataset 1 , and the DeepFake Detection Challenge (DFDC) dataset <ref type="bibr" target="#b8">[13]</ref>. The images are chosen so that they can be redistributed without legal implications. We choose images from YFCC100M which are under permissive creative commons license types and which do not contain any identifiable human; some images depict humans in the distance such that they are not identifiable, corresponding to an area no larger than 0.5% of the whole image area. Nevertheless, social media images contain many images of humans. Therefore, to make the dataset more realistic, we further add a smaller number of images which depict humans from a previous competition organized by Facebook, the DeepFake Detection Challenge (DFDC). The images we use are frames from DFDC videos of paid actors who gave their permission for their video recordings to be used. Around half of the DFDC images that we use are processed with a face swapping algorithm ("deepfake") <ref type="bibr" target="#b31">[36]</ref> to manipulate the faces before applying image copy transformations. In this way, the transformed image and the original image cannot be easily matched using face verification techniques. Pre-processing. Face detection is performed on the DFDC source images in order to keep a close crop, with some randomness, of the faces. We do this because most DFDC videos, therefore the images too, are taken at a distance from the actors, while images on photo sharing sites are more often selfies.</p><p>Both data sources contain pairs of similar images, where the types of similarity cover the whole range, from exact duplicates to the same object category. To reduce the ambiguity of the ground-truth, we remove the most obvious images pairs from the reference set that correspond to the same instance but look like a case of edited copies. There is a chicken-and-egg problem here because duplicates have to be identified to be removed, which is the purpose of the competition. We decided to use an in-house descriptor with a relatively narrow threshold to remove duplicates. Examples of removed pairs are shown in <ref type="figure" target="#fig_4">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Transformations</head><p>We do not use transformations of uniform difficulty, but instead target a wide range of difficulties, from easy near-exact duplicates to cases that are hard to assess by a human. To ensure that the dataset is neither too easy nor too hard, during the collection of the dataset, we monitor its difficulty by running a set of existing baseline approaches on it. The dataset is calibrated so that the performance of the different baselines ranges from 10 to 40%. In this way, there are several hard transformations included so that the dataset can attract and encourage research effort for years before becoming obsolete.</p><p>Images are transformed via either manual ( <ref type="figure">Figure 4</ref>) or automatic ( <ref type="figure" target="#fig_7">Figure 5</ref>) edits. All transformations start from a source image that has to be altered to a certain degree. Some transformations require a secondary image from the YFCC100M to serve as an overlay or a background. In that case, we make sure that this secondary image is not included in the reference set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manual transformations.</head><p>Manual transformations are performed by external image editors with a photo editing software. We chose GIMP as the software because it is free, available on all types of computers, and has a very large toolbox of image manipulation functions. The instructions provided to the editors are as follows:</p><p>? Editors are provided with one source image and optionally a secondary image that they can use to make collages</p><p>? Each edit should use 2 to 5 different tools in GIMP</p><p>? All editing tools are acceptable including geometrical deformations, color transformations, brush strokes, image filters, etc.</p><p>? Diversity is important; tools, as well as their parameters, should be changed as often as possible</p><p>? Once editors are familiarized with the task, the mean time to edit an image should be around 3 minutes</p><p>? The result should be distinctive enough so that the source image is still recognizable to the editor Automatic transformations.</p><p>Automatic transformations are applied using AugLy <ref type="bibr" target="#b37">[42]</ref>, a data augmentations library created at Facebook AI. The automatic transformations are classified into the following broad categories:</p><p>? Overlays: text and emoji ? embedding the image into the GUI of a social network application</p><p>Some images have only one automatic transformation applied, while some others have multiple chained together. The transformations and parameters are picked at random. The distribution and strength of the transformations is roughly based on what is witnessed in the production setting at Facebook, but with a stronger emphasis on difficult transformations.</p><p>In the second phase of the competition we added other transformations and made the existing ones more difficult. Appendix A details these transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Dataset structure</head><p>DISC21 consists of the following parts: the reference image set, the training image set, and two query image sets. We create two query sets in order to mimic the real-world setting, where there is a drift over time in the applied transformations, by adding a few additional transformations to the second set. This setting is aligned with the two phases of the competition (see <ref type="bibr">Section 6)</ref> and is intended to penalize methods that overfit to a fixed set of transformations, i.e. the ones in Phase I.</p><p>? Reference image set: 1 million images without any transformation. The YFCC100M dataset is publicly available, but we will re-host the chosen subset.</p><p>? Training image set: 1 million images collected in the same way as the reference set.  ? Development query image set (Phase I): 10,000 images from the reference set mixed with 40,000 distractor images that are not part of the reference set, that have been edited in various ways. Distractor image queries have no matching counterpart in the set of 1 million reference images.</p><p>? Test query image set (Phase II): 10,000 images from the reference set mixed with 40,000 distractor images. All these images are transformed in various ways to form the query images. This set, compared to the one of Phase I, is generated by including a few additional transformations and the transformation parameters are slightly modified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation metric</head><p>The output of an image copy detection algorithm is a set of pairs, where each pair is formed by a query image and a candidate source image from the reference set, and is accompanied by a confidence value. Not all queries need to be part of this list as there are many distractor queries. We use micro Average Precision to measure performance, which is described in the following and only after precision and recall measures are discussed.</p><p>Precision and recall. Detected pairs for all queries are considered simultaneously. Setting a threshold on the confidence amounts to taking a hard decision whether there is a detection or not. Then, the precision and recall of the detected pairs are evaluated according to the ground-truth. Note that detected pairs for distractor queries are always wrong detections as the corresponding source image is not part of the reference set. By varying the threshold, one can adjust the trade-off between precision and recall, as in <ref type="figure">Figure 6</ref>. Precision and recall measures can be extrapolated using basic arithmetic to larger reference sets than do  <ref type="figure">Figure 6</ref>: Example precision-recall plot for copy detection with 50,000 queries and a reference set of 1 million images. The total number of queries that are edited copies of images in the reference set is 10,000. A quick read of the plot shows that one threshold value on the scores obtains a recall of 60% for 80% precision. This means that 6,000 copies are properly detected (true positives) but 1,500 copies are incorrectly detected (false positives). The corresponding metric that we use for the benchmark is the area under the curve, ?AP = 0.62781 in this case.</p><p>not have additional positive pairs of images. For example, extrapolating precision equal to 0.8 and recall equal 0.6 measured on our 1 million image reference set to 10 million images, then, for a given threshold, the recall does not change as there are no additional positives, but the number of false positives increases by a factor 10. Therefore, the new precision drops to around 0.29. Incidentally, the low precision explains why at a large scale, image copy detection algorithms operate mostly in a high-precision and low-recall regime.</p><p>micro Average precision. We measure the overall performance, for all possible thresholds, by the average precision. It is equivalent to the area under the precision-recall curve when all pairs are taken into account and is also known as micro-AP (?AP ). This measure has been used for instance recognition <ref type="bibr" target="#b39">[44,</ref><ref type="bibr">1]</ref>. It is computed as</p><formula xml:id="formula_0">?AP = N i=1 p(i)?r(i) ? [0, 1],<label>(1)</label></formula><p>where p(i) is the precision at position i of the sorted list of pairs, ?r(i) is the difference of recall between position i and i ? 1, and N is the total number of returned pairs for all queries. Note that any detected pair for a distractor query will decrease the AP score. Thus, all queries are evaluated jointly by merging returned pairs for all queries and sorting them according to confidence; a single precision-recall curve is generated. This is different than mean-Average-Precision (mAP), also known as macro-AP <ref type="bibr" target="#b39">[44]</ref>, where AP is computed per query separately and then averaged over all queries. Confidence values matter with micro-AP, while only the ranking per query matters with macro-AP. The former better reflects the objectives of image copy detection, while the latter is typically used in retrieval tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Discussion</head><p>The design of the dataset aims at striking a tradeoff between realism and ease-of-use as a computer vision benchmark. This simplification introduces biases in the dataset that we discuss here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Biases.</head><p>A good fraction of social media images contain text and faces. Specialized techniques that focus on these features, such as OCR matching and face recognition, might be helpful on small benchmark datasets, but would not advance copy detection in the general case. We design the benchmark to be insensitive to these specialized recognition techniques in the following ways. Firstly, text is used often as a transformation; relying on text recognition will yield false positives. Secondly, we include face-specific transformations that render the face unrecognizable or change the person's identity, i.e. "deep fakes" <ref type="bibr" target="#b8">[13]</ref>.</p><p>Another bias is that each source image is transformed only once, which does not follow a real-world setting where user uploads can become viral. This design choice maintains the sensitivity of the benchmark and prevents from determining copy detection performance primarily by the performance on the few source images that are copied most of the times.</p><p>Another bias is that the source images are not transformed. This is realistic in a copyright context where we have access to the original images, but less realistic for other cases. However, it simplifies the setup.</p><p>The final bias is that the dataset focuses on too strong transforms. This focus will favor the emergence of expensive and complex approaches that are not required to handle the vast majority of production use cases. The rationale for this is that we often observe that it is useful to have a "topline" method in terms of accuracy. Other methods can then be derived from it by degrading its parameters.</p><p>Annotation errors. We do our best to provide a clean dataset and ground-truth. However, since the dataset is partially generated automatically and not validated manually, there could be annotation errors. These errors could be matching images in the reference set or query images that are not recognizable because the source content has disappeared completely. We do believe that the impact of these errors is low and most importantly will not incur a bias between different methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Baselines</head><p>We report results with a number of baseline methods. Popular and successful image retrieval methods were selected and adapted from publicly available code.  For methods that rely only on pairwise global descriptor matching, we indicate the descriptor dimension -if this dimension is below 256 they are eligible to submit to track 2 of the competition. The reported times are feature extraction times per image on the specified hardware. This excludes the matching phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Representative methods</head><p>We choose three baseline methods from existing state of the art, and adapted them minimally for the challenge. We present by increasing complexities.</p><p>GIST. The GIST <ref type="bibr" target="#b35">[40]</ref> is a very simple descriptor that is attractive because it is cheap to extract, works on low-resolution images and does not require any training. It was evaluated in <ref type="bibr" target="#b9">[14]</ref> for image copy detection and found to work reasonably well for JPEG encoding transformations and small crops. It is extracted by (1) resizing the image to 32?32 pixels (2) splitting the image in 3 channels and a 4?4 grid of cells, and (3) building a gradient orientation histogram in each cell. The output is a 960-dimensional vector. This descriptor can be matched and used for the competition's track 1. For Track 2, the descriptor has to be reduced to 256 dimensions (see Section 6.2). We do this using a PCA, followed by L2 normalization. Note that searching in the full dimension or applying PCA with whitening does not benefit the GIST descriptor.</p><p>Multigrain. The multigrain descriptor <ref type="bibr" target="#b1">[6]</ref> is an early example of global image embedding based largely on data augmentation. Multigrain is a Resnet50 model trained on Imagenet. In addition to the standard classification head, the multigrain model has an additional head that does a generalized max-pooling (GeM) <ref type="bibr" target="#b47">[52]</ref> of the last activation map to generate an image embedding. The input resolution is set to 512 pixels in the largest dimension and the GeM parameter p = 7. The image embedding enters a contrastive loss term that forces multiple data augmentations of the image to map to the same embedding. The GeM embedding has 2048 dimensions. It is reduced by PCA and whitening to 1500 dimensions and normalized. The resulting score is the inner product of the query and reference embeddings.</p><p>HOW deep local descriptors with ASMK. In HOW <ref type="bibr" target="#b60">[65]</ref>, individual activations of the CNN are treated as local descriptors. Even though the activations are treated as local descriptors at inference time, the network is trained with image level annotations only. During inference, the local descriptors are vectorquantized into visual words. Each local descriptor is assigned to a visual word and has a signature relative to that visual word. An image is then represented as a set of visual words, each with a signature aggregated for all descriptors associated with that word. These aggregated signatures are then binarized, following the ASMK approach <ref type="bibr" target="#b59">[64]</ref>. An inverted-file structure is used for efficient similarity estimation between a query image and each image of the reference set. We use the public source code and the publicly available network (ResNet50) that is trained on a large training set of popular landmarks and buildings. We use default values for all hyper-parameters for inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similarity normalization</head><p>The baseline approaches provide a way to estimate the image-to-image visual similarity (confidence value); query image to reference image in particular, for all reference images. It is important for copy detection, as also captured by the evaluation metric, that the similarity values are comparable across different queries. We consider the similarity normalization introduced in <ref type="bibr" target="#b19">[24]</ref>, which is later re-used at training time for word embeddings under the name CSLS <ref type="bibr" target="#b7">[12]</ref>. Let s(q, r) be the similarity between query image q and reference image r. The normalized similarity is given by subtracting the similarity score a known non-matching image from the training set defined a? s(q, r) = s(q, r) ? ?s(q, t n )</p><p>(2)</p><p>where t n is the n-th most similar image to the query image from the training set. Note that since the normalization factor is estimated on the training set and not on the reference set the final confidence score is independent of other reference images. It is also independent of other query images. The requirements are part of the rules for the research competition as described in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baseline results</head><p>The matching results on DISC21 for the three baseline methods are reported in <ref type="table" target="#tab_4">Table 2</ref> without and with similarity normalization. The 500k pairs with the highest confidence score are used to estimate the evaluation metric. Hyper-parameters (?,n) for the normalization are set equal to (0.5, 10) for GIST and to (2, 10) for the other two methods.</p><p>Computing GIST descriptors on 1 million images takes less than 20 minutes on a fast machine. Hence, the achieved performance is a good baseline for a cheap and relatively inaccurate descriptor. It detects the lightest image edits in the collection. Interestingly, reducing the dimension of the vector to 256 by PCA improves the results, so using the full 960-dimensional descriptor has no advantage. Multigrain performs better than GIST, while HOW+ASMK offers a small performance increase with a significantly larger query computational cost though.</p><p>From the results, it is obvious that the score normalization has a strong impact on the ?AP . This is because the methods that we used as baselines are mostly developed for ranking results to individual queries. We expect that techniques that explicitly optimize the estimation will make that normalization unnecessary. <ref type="figure" target="#fig_9">Figure 7</ref> shows typical examples where each of the baseline descriptors produces reliable matches (&gt; 90% precision). GIST can handle light color changes and small crops. HOW+ASMK is particularly good at matching strong crops and cluttered scenes, a domain classically dominated by local descriptor methods. False positive results (not showed here) suggest that especially HOW and Multigrain are too invariant to aspect changes like different viewpoints of 3D objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">The competition</head><p>The competition is intended to be as open as possible to individual, academic and industrial participants.</p><p>The official submission site is managed by Driven Data, via the website https://www.drivendata.org/ competitions/79/. It provides the dataset, the evaluation script with partial ground truth, the leaderboard and the submission site.</p><p>The competition rules and precise file formats are described on the website, this section is intended as a readable digest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Participants</head><p>A participant is a group of people that can submit results to the competition site. We disallow (1) people to move from one group to another and (2) private communication between groups -any information provided by one group should be made available to all other groups as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open vs. closed-source Participants can choose to register either as open-source or as closed-source.</head><p>For the open-source version, participants promise to release the source code prior to the final submission. For the closed-source version, participants do not need to submit source code, but they will not be considered for the prizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Tracks</head><p>There are two tracks in the competition, participants can choose to compete in either or both of the tracks. Track 1. Participants submit lists of matching images as a CSV file. The image matches are reported as a triplet of (query image, reference image, score). The source and reference images are the image file names. The score is a floating point number that evaluates how confident the match is (higher is more confident). The submission is evaluated by computing the micro-AP measure on the result list (see Section 4.4).</p><p>Track 2. Participants submit global image descriptor vectors for the query images and the reference images as a HDF5 file. The image descriptors are in 256 dimensions at most. The matches are performed using L2 distance on the evaluation server. Then the submission is evaluated in the same way as for Track 1. This setting is more constrained, therefore we expect the track 2 results to always be lower or equal to the track 1 results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Phases</head><p>The competition will take place in three phases.</p><p>Phase I: development. (between June and October). The participants can register, tinker with the dataset, use the provided ground truth, and submit to the development phase leaderboard.</p><p>The organizers provide the reference set, and the development query set. Ground-truth correspondences of the queries with the reference images are provided only for the 25,000 of the query images. For the other 25,000 images, the ground-truth is not provided but they are taken into account for the evaluation server.</p><p>The "open-source" participants should submit the code of their method prior to the evaluation phase. In addition, all Track 2 participants should submit the final descriptors for the reference images.</p><p>Phase II: evaluation. (48h in October). The test query set is delivered to the users, without the groundtruth. Users are allowed 3 submissions of search results on the final queries.</p><p>Phase III: verification. (November) The organizers verify that the code submitted in advance reproduces the reported results. The participants are expected to reply to requests from the organizers to assist in this validation.</p><p>The results will be announced at the NeurIPS competition workshop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Provided material</head><p>The organizers provide the following code to participants.</p><p>Evaluation script. This script is the one used for the track 1 and track 2 evaluation, computing the global AP metric. It can be run locally by participants on the partial ground truth on 25,000 query images.</p><p>Baseline methods. The code for baseline methods (Section 5) is provided. It is intended for validation and as a starting point for participants, on which they can build to develop their methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Acceptable methods</head><p>The scoring of a query image w.r.t. a reference image should be independent of other query images and reference images. This means that even if the dataset had a single query image and a single reference image, the score of the image pair would be the same. The intent of this rule is to avoid (1) that algorithms overfit to the reference set, for example by building a gigantic classifier with 1M outputs that predicts the matches, (2) that algorithms use irrelevant dataset statistics like the fact that there is at most one query image per reference image. This rules out methods based on query expansion <ref type="bibr" target="#b6">[11]</ref> or neighborhood graphs <ref type="bibr" target="#b45">[50]</ref>.</p><p>The training set is provided as a statistical twin of the reference set, it can be used to do all kinds of training tasks without risk of overfitting to the reference set.</p><p>We ask participants to provide some metainformation on their runs, like the processing time and memory usage. This is just for information purposes, higher resource consumption is not penalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>The DISC21 dataset aims at probing the state of the art of image de-duplication. We paid particular attention to the size of the dataset and calibration of the difficulty of transformations. We also did our best to ascertain that there are no loopholes that participants could take advantage of. We hope that many participants will take part in the challenge and that this will re-fuel the interest in the near-duplicate detection task. In the long run, we will make sure that the data will remain available on the website https://github.com/ facebookresearch/isc2021 so that progress can be measured over a few years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>[1] Google landmark recognition challenge.</p><p>https://www.kaggle.com/c/ landmark-recognition-challenge. Accessed: 2021-06-03.</p><p>[2] Here's how we're using ai to help detect misinformation. https://ai.facebook.com/blog/ heres-how-were-using-ai-to-help-detect-misinformatio Accessed: 2021-06-03.</p><p>[3] Microsoft PhotoDNA. https://www.microsoft. com/en-us/photodna. Accessed: 2021-06-03.</p><p>[4] Using ai to detect covid-19 misinformation and exploitative content.</p><p>https://ai.facebook.com/blog/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional transformations for the final phase</head><p>After the development phase of the competition, it appeared that the top submissions were stronger than we anticipated (?AP &gt;90% in the matching track). Therefore we decided to make the final phase transformations harder than those of the development phase. In addition, as decided beforehand, we included a fraction of new transformations, to simulate the realistic setting where the attacks are not completely known at training time and avoid overfitting to the development phase attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Automatic transformations</head><p>The automatic transformations are composed of a sequence of elementary AugLy transformations <ref type="bibr" target="#b37">[42]</ref> sampled randomly. We increased the likelihood of applying more transformations sequentially: whereas in the development set we applied 1-4 transformations to create each query image with increasing probabilities for higher numbers, for the test set we applied 1-5 and skewed the probabilities even more heavily so that it was much more likely to apply 3-5 transformations than 1-2.</p><p>We added some new transformations from AugLy to the set we applied:</p><p>? Overlays: overlaying stripes onto the image ? Pixel-level: adding random noise, "legofy" (a type of pixelization which replaces a number of adjacent pixels with a lego block of the average color)</p><p>? Spatial: vertical flip, adding other images in a collage around the image</p><p>We sampled harder parameters for some transformations compared to the dev set including blur, encoding quality and pixelization. For example, for blur the radius is sampled in <ref type="bibr" target="#b0">[5,</ref><ref type="bibr" target="#b5">10]</ref> for the development phase and in <ref type="bibr" target="#b0">[5,</ref><ref type="bibr" target="#b10">15]</ref> in the final phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Air-gap</head><p>We asked manual editors to perform "air-gap" transformations, where the picture is displayed or printed out, and re-captured with a camera or mobile phone. It is a quite common image attack, that is hard to reproduce automatically or even with an image editor like the GIMP. <ref type="figure" target="#fig_11">Figure 8</ref> shows an example air-gap augmentation (the zebra). The effect is a combination of over-exposure, geometric framing and blurriness on the picture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Adversarial transformations</head><p>Since is is likely that most submissions to the challenge are based on an image analysis with neural nets, we developed an attack that specifically targets those. This is inspired by previous works that show that even Air-gap transform  imperceptible changes to images can change the classification accuracy <ref type="bibr" target="#b56">[61,</ref><ref type="bibr" target="#b14">19,</ref><ref type="bibr" target="#b50">55]</ref> or a retrieval system <ref type="bibr" target="#b62">[66]</ref>. Some of these methods can even be applied with only black-box access to the models <ref type="bibr" target="#b38">[43]</ref>. However, this does not help here because we don't have access to the participant's models, even in black box. Instead, we start from a hypothesis of what model f the participants could be using and attack that model in whitebox mode. The goal is to slightly perturb an image so that when it is transformed its features will not be matching those of the original image.</p><p>Optimization. Our approach is based on the techniques used in watermarking <ref type="bibr" target="#b12">[17]</ref> because they offer a fine control of the distortion on the image. The attack starts from the source image x and produces an image y by distorting x. It minimizes the following loss that combines a pixel difference term (that must be minimized) and a feature difference term (that should be  </p><formula xml:id="formula_1">L = w x ? y 2 ? (1 ? w) f (x) ? f (T (y)) 2 ,<label>(3)</label></formula><p>where w ? [0, 1] is a weight factor and T is a random transformation applied to the image (we chose w = 1/2 for all experiments). A different transformation T is sampled at each iteration of the optimization. It is intended to make the optimization less sensitive to other image transformations that could be applied. The optimization does not operate on the model parameters or on y directly but on the pixels of a latent image z that is subsequently normalized y = N (SSIM(z, x), x, p) (4)</p><p>The first normalization step SSIM <ref type="bibr" target="#b67">[71]</ref> makes sure the changes w.r.t. the original image x occur preferably on edges (high frequency) of the image content. The second normalization step N scales the difference image between z and x so that the PSNR between the two remains above a target PSNR p:</p><formula xml:id="formula_2">N (z, x, p) = x + ?(p) z ? x (z ? x)<label>(5)</label></formula><p>where ?(p) is a decreasing function of p. ?(p) = 10 K?p 20</p><p>where K = 10 log 10 (255)+10 log 10 N pix , N pix being the number of pixels in the image. We use 100 iterations of the Adam optimizer with a learning rate of 0.05 to propagate back to the latent image pixels z to minimize the loss L. <ref type="figure" target="#fig_11">Figure 8</ref> shows a few examples of attacked images with target PSNRs of 20. A transformation with PSNR=20 is very visible, while 40 is perceivable for trained eyes.</p><p>Impact of adversarial attacks. We evaluate the adversarial attacks on the baseline methods of the challenge. We use several levels of PSNR and pretrained models.  <ref type="table">Table 3</ref>: Impact on adversarial attacks targeting different feature extraction models on the baseline methods of the challenge. The performance is measured as ?AP on a subset of the development queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retrieval method</head><p>We measure the ?AP on the subset of development queries that have a match. This is the simplest setting of the development kit of the challenge, without any score normalization. The SSCD model <ref type="bibr" target="#b43">[48]</ref> is a resnet50 trained with self-supervised learning (similar to <ref type="bibr" target="#b4">[9]</ref>) on the DISC2021 training set. <ref type="table">Table 3</ref> shows the results. Multigrain is very sensitive to attacks from all models. It is based on a resnet50 model trained on imagenet so it is not surprising that the resnet50-based attack is the most efficient. The local descriptor method HOW <ref type="bibr" target="#b60">[65]</ref> is much less sensitive to adversarial attacks, despite being based on the same resnet50 model. SSCD is trained with a specific entropy loss term that probably makes it less sensitive to attacks, especially from other models. Even the SSCD based attack (the only white box attack in these experiments) has an impact of less than 0.1 on the AP. It is interesting to notice that the behavior of the resnet50 attack is closer to Dino <ref type="bibr" target="#b3">[8]</ref> than to VGG16, despite the fact that Dino is trained in a self-supervised way on YFCC100M instead of Imagenet. The SSCD attack is less efficient on the other approaches i.e. it is not a good attack model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure</head><label></label><figDesc>Figure 1: Image-pair similarity defined at different granularity levels in a nested way with the innermost level being a more restrictive one than the level above. The scope of the DISC21 dataset and competition correspond to the green area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 2: Real-world imaging process that generates image pairs that are considered similar at different granularity levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Examples of image pairs that are removed from the reference set prior to forming the dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>text overlay + drawing DFDC image 1757 1 paste salient part of image + pad + overlay text Flickr / Shreveport-Bossier handwriting + blur non-salient part of image DFDC image 1128 7 perspective transform + circle salient part of image Flickr / senov paste salient part of background + pixelization Flickr / roland Figure 4: Examples of source images and their manually transformed counterparts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>?</head><label></label><figDesc>Color transformations: changes in brightness or saturation, grayscale, Instagram color filters and Augmented Reality effects ? Pixel-level transformations: blur, color palette with dithering, JPEG encoding, edge enhance, pixelization, pixel shuffling ? Spatial transformations: crop, rotation, horizontal flip, padding, aspect ratio change, perspective transform, overlay onto background image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Example pairs of source images (left) and their automatically transformed counterparts (right). The transformations are calibrated to match the range of difficulties encountered in a real-world setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Examples of difficult image pairs that each of the baseline methods are able to match in a regime of above 90% precision. Left: source image, right: image match.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Flickr</head><label></label><figDesc>up the quattro.... Legofy Flickr / lamouridanielle stripes + adversarial attack, resnet34, PSNR=20 Flickr / Vlad &amp; Marina Butsky Adversarial attack, vgg19, PSNR=20 Flickr / Rusty Clark</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Example transformations for the final phase of the competition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>Adversarial attack optimization. maximized):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Datasets that are publicly available and are related to our dataset and competition. Comp.: whether there was a corresponding research competition or not. QueriesD: distractor queries. U: user-generated trans- formations. S: synthesized transformations.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Performance measured with ?AP for the baseline methods on the development query set of DISC21.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This data source contains significantly less full images of text than what is seen typically on social media platforms.Flickr / m-takagi Flickr / sybarite48</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural codes for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Slesarev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="584" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05509</idno>
		<title level="m">Multigrain: a unified image embedding for classes and instances</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hinton. A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An evaluation of popular copymove forgery detection approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Christlein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Riess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Riess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Angelopoulou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1841" to="1854" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Total recall: Automatic query expansion with a generative feature model for object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04087</idno>
		<title level="m">Word translation without parallel data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dolhansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bitton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pflaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Howes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Ferrer</surname></persName>
		</author>
		<title level="m">The deepfake detection challenge dataset. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Evaluation of gist descriptors for web-scale image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sandhawalia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Amsaleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CIVR</title>
		<meeting>CIVR</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An imagebased approach to video copy detection with spatio-temporal post-filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="257" to="266" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Compact video description for copy detection with precise temporal alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="522" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Watermarking images in self-supervised latent spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09581</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Using extreme value theory for image detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno>RR- 8244</idno>
	</analytic>
	<monogr>
		<title level="j">INRIA</title>
		<imprint>
			<date type="published" when="2013-02" />
		</imprint>
	</monogr>
	<note type="report_type">Research Report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ozertem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saeidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Silvestri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<title level="m">Preserving integrity in online social networks. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning spatial-temporal features for video copy detection by the combination of cnn and rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="21" to="29" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hamming embedding and weak geometric consistency for large scale image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exploiting descriptor distances for precise image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INRIA</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Aggregating local image descriptors into compact codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">VCDB: a large-scale database for partial copy detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="357" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<title level="m">Billion-scale similarity search with gpus. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Logo retrieval with a contrario visual query expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MM</title>
		<meeting>ACM MM</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="581" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Content-based image copy detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="169" to="184" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Inside the team at facebook that dealt with the christchurch shooting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Klonick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Yorker</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fivr: Fine-grained incident video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kordopatis-Zilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kompatsiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2638" to="2652" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visil: Fine-grained spatiotemporal video similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kordopatis-Zilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kompatsiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6351" to="6360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Near-duplicate video retrieval with deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kordopatis-Zilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kompatsiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV Workshop</title>
		<meeting>ICCV Workshop</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="347" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Muscle-vcd-2007: a live benchmark for video copy detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Law-To</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Video histogram: A novel video signature for efficient web video duplicate detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MMM</title>
		<meeting>MMM</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="94" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ciagan: Conditional identity anonymization generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maximov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Elezi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5447" to="5456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bags of local convolutional features for scalable instance search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mohedano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICMR</title>
		<meeting>ICMR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scalable recognition with a vocabulary tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stewenius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Large-scale image retrieval with attentive deep local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Trecvid 2012 -an overview of the goals, tasks, data, evaluation mechanisms and metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qu?ot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Augly: Data augmentations for robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Papakipos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bitton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.06494</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Asia conference on computer and communications security</title>
		<meeting>the 2017 ACM on Asia conference on computer and communications security</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A family of contextual measures of similarity between distributions with application to image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Renders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2358" to="2365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Large-scale image retrieval with compressed Fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poirier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Lost in quantization: Improving particular object retrieval in large scale image databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A self-supervised descriptor for image copy detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pizzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Ravindra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>not on ArXiV yet</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Temporal matching kernel with explicit feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poullot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsukatani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MM</title>
		<meeting>ACM MM</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="381" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hello neighbor: Accurate object retrieval with k-reciprocal nearest neighbors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gammeter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Quack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="777" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Revisiting oxford and paris: Large-scale image retrieval benchmarking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Finetuning CNN image retrieval with no human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Visual instance retrieval with deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ITE Transactions on Media Technology and Applications</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="251" to="258" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Event retrieval in large video collections with circulant temporal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2459" to="2466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Dunkelman. A simple explanation for the existence of adversarial examples with small hamming distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Safran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ronen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10861</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Towards effective indexing for very large video sequence database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGMOD</title>
		<meeting>ACM SIGMOD</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="730" to="741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Uqlips: a real-time near-duplicate video clip detection system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB</title>
		<meeting>VLDB</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1374" to="1377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Local features and visual words emerge in activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sim?oni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multiple feature hashing for real-time large scale near-duplicate video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MM</title>
		<meeting>ACM MM</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="423" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Scalable detection of partial near-duplicate videos by visual-temporal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MM</title>
		<meeting>ACM MM</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="145" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Yfcc100m: the new data in multimedia research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">To aggregate or not to aggregate: Selective match kernels for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning and aggregating deep local descriptors for instancelevel recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jenicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<biblScope unit="page" from="460" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Targeted mismatch adversarial attack: Query with a flower to retrieve the tower</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5037" to="5046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Comofod-new database for copy-move forgery detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tralic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zupancic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grgic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ELMAR</title>
		<meeting>ELMAR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A survey of the image copy detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Cybernetics and Intelligent Systems</title>
		<meeting>Conference on Cybernetics and Intelligent Systems</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="738" to="743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Instre: a new benchmark for instance-level object retrieval and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">37</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Communications, and Applications</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Detecting photoshopped faces by scripting photoshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10072" to="10081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on image processing</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="600" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Google landmarks dataset v2 -A large-scale benchmark for instance-level recognition and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Practical elimination of near-duplicates from web video search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MM</title>
		<meeting>ACM MM</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="218" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Image copy detection based on convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Spatial coding for large scale partial-duplicate web image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MM</title>
		<meeting>ACM MM<address><addrLine>Firenze, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Effective and efficient global context verification for image copy detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="63" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
