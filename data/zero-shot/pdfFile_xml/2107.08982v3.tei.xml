<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">InsPose: Instance-Aware Networks for Single-Stage Multi-Person Pose Estimation ACM Reference Format</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 20-24, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahu</forename><surname>Shi</surname></persName>
							<email>shidahu@hikvision.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
							<email>weixing@mail.xjtu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yu</surname></persName>
							<email>yuxiaodong7@hikvision.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Tan</surname></persName>
							<email>tanwenming@hikvision.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Ren</surname></persName>
							<email>renye@hikvision.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
							<email>pushiliang.hri@hikvision.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahu</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Tan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Ren</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><forename type="middle">Pu</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Hikvision Research Institute Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Software Engineering Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Hikvision Research Institute Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Hikvision Research Institute Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Hikvision Research Institute Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Hikvision Research Institute Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">InsPose: Instance-Aware Networks for Single-Stage Multi-Person Pose Estimation ACM Reference Format</title>
					</analytic>
					<monogr>
						<title level="m">MM &apos;21</title>
						<meeting> <address><addrLine>Chengdu, Sichuan Province, China</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">October 20-24, 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3474085.3475447</idno>
					<note>KEYWORDS Pose Estimation, Conditional Convolutions, Neural Networks * Corresponding author. 2021. InsPose: Instance-Aware Networks for Single-Stage Multi-Person Pose Estimation. In Proceedings of the 29th ACM Int&apos;l Conference on Multimedia (MM &apos;21), Oct. 20-24, 2021, Chengdu, Sichuan Province, China. ACM, New York, NY, USA, 9 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ? Computer vision</term>
					<term>Machine learning</term>
					<term>Computer graphics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-person pose estimation is an attractive and challenging task. Existing methods are mostly based on two-stage frameworks, which include top-down and bottom-up methods. Two-stage methods either suffer from high computational redundancy for additional person detectors or they need to group keypoints heuristically after predicting all the instance-agnostic keypoints. The singlestage paradigm aims to simplify the multi-person pose estimation pipeline and receives a lot of attention. However, recent singlestage methods have the limitation of low performance due to the difficulty of regressing various full-body poses from a single feature vector. Different from previous solutions that involve complex heuristic designs, we present a simple yet effective solution by employing instance-aware dynamic networks. Specifically, we propose an instance-aware module to adaptively adjust (part of) the network parameters for each instance. Our solution can significantly increase the capacity and adaptive-ability of the network for recognizing various poses, while maintaining a compact end-to-end trainable pipeline. Extensive experiments on the MS-COCO dataset demonstrate that our method achieves significant improvement over existing single-stage methods, and makes a better balance of accuracy and efficiency compared to the state-of-theart two-stage approaches. The code and models are available at https://github.com/hikvision-research/opera.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Multi-person pose estimation aims to identify all the instances and detect the keypoints of each person simultaneously. It is a fundamental task and has a wide range of applications such as motion recognition <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b37">38]</ref>, person re-identification <ref type="bibr" target="#b19">[20]</ref>, pedestrian tracking <ref type="bibr" target="#b43">[44]</ref>, and athletic training assistance <ref type="bibr" target="#b36">[37]</ref>.</p><p>Multi-person pose estimation methods are usually based on two-stage frameworks including the bottom-up and top-down approaches. Bottom-up methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref> first detect all the potential keypoints in the image in an instance-agnostic manner, and then perform a grouping process to get the final instance-aware poses. Usually, the grouping process is heuristic in which many tricks are involved and does not facilitate end-to-end learning. On the other hand, top-down methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40]</ref> first detect each individual by a bounding-box and then transfer the task to an easier single-person pose estimation problem. While avoiding the heuristic grouping process, top-down methods come with the price of high computational complexity as mentioned in <ref type="bibr" target="#b25">[26]</ref>. Especially, the running time of top-down methods depends on the number of instances contained in the image. Similarly, top-down methods are also not learned in an end-to-end fashion.</p><p>Recently, the single-stage pose estimation paradigm <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39]</ref> is proposed and receives a lot of attention. This single-stage framework receives great interest since 1) it directly learns a mapping from the input image to the desired multi-person poses and is endto-end trainable, and 2) it can consolidate object detection and keypoint localization in a simple and unified framework, and thus 3) presents a more compact pipeline and attractive simplicity and efficiency over two-stage methods.</p><p>A straightforward way for single-stage pose estimation is adapting the bounding-box regression head of the single-stage object detectors <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b41">42]</ref>. However, such a naive approach expects a single feature vector (namely, a point feature) to regress the precise locations of all the keypoints. While a point feature may be sufficient for bounding box regression to some extent, it is difficult to encode rich pose information considering that keypoints have much more geometric variations than the regular box corners. As a result, the pose estimation accuracy of this approach is unsatisfactory <ref type="bibr" target="#b32">[33]</ref>. To deal with this problem, SPM <ref type="bibr" target="#b25">[26]</ref> proposes a progressive joint representation by dividing body joints into hierarchies induced from articulated kinematics. CenterNet <ref type="bibr" target="#b42">[43]</ref> proposes a hybrid solution by first regressing joint locations and then refining with the help of additional heatmaps. DirectPose <ref type="bibr" target="#b32">[33]</ref> proposes a keypoint alignment module with the aim of aligning the point feature to the features around target joints. However, they all involve heuristic designs and still result in less accurate performance compared to the state-of-the-art bottom-up method <ref type="bibr" target="#b5">[6]</ref>, leaving room for further optimization.</p><p>In this paper, we present a direct and simple framework for singlestage multi-person pose estimation, termed InsPose. The core of our method is an instance-aware module to adaptively adjust (part of) the network parameters for each instance, as illustrated in <ref type="figure">Figure 1</ref>. Instead of using a standard keypoint network (called, KP-Nets) with fixed parameters for predicting all instances of various poses, we borrow the idea from recent dynamic neural networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b40">41]</ref> to generate instance-aware parameters. Specifically, these KP-Nets are dynamically generated with respect to the different spatial locations of the input. Afterwards, they are applied to the whole feature maps to directly predict the full-body poses, get rid of the need for additional handcrafted processing. KP-Nets can better encode all kinds of pose variants since each one of them only focuses on an individual instance. Moreover, each KP-Net only consists of three 1 ? 1 convolutions, leading to a compact and efficient architecture.</p><p>The overview of our method is illustrated in <ref type="figure" target="#fig_1">Figure 3</ref> and we summarize our main contributions as below.</p><p>? We propose a simple single-stage network for multi-person pose estimation. It is end-to-end trainable and avoids many heuristic designs such as keypoints grouping <ref type="bibr" target="#b5">[6]</ref>, progressive pose representation <ref type="bibr" target="#b25">[26]</ref>, delicate feature aggregation <ref type="bibr" target="#b32">[33]</ref>, or pose anchors <ref type="bibr" target="#b38">[39]</ref>. ? The proposed model has a dynamic network that consists of instance-aware KP-Nets, which is different from previous methods that use a fixed set of convolutional filters for all the instances. ? We conduct extensive experiments on the COCO benchmark where our method outperforms state-of-the-art single-stage methods significantly and achieves a better trade-off between accuracy and efficiency compared to state-of-the-art twostage approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Multi-Person Pose Estimation</head><p>With the development of deep neural networks, modern pose estimation methods can be divided into three categories: top-down methods, bottom-up methods, and recent single-stage methods.</p><p>Top-down Methods. Top-down methods decouple the multiperson pose estimation task into two sub-tasks: multi-person detection and single-person pose estimation. The person detection is first performed to predict a bounding-box for each instance in the image. Then the instance is cropped from the bounding box for single-person pose estimation. Instead of cropping the original image, Mask RCNN <ref type="bibr" target="#b10">[11]</ref> proposes to utilize extracted features to improve efficiency. A cascade pyramid network is proposed in CPN <ref type="bibr" target="#b4">[5]</ref>, aiming to regress difficult keypoints. Moreover, HRNet <ref type="bibr" target="#b31">[32]</ref> focuses on learning reliable high-resolution representations and maintains high-resolution representations through the network. In general, top-down methods have better performance but also have higher computational complexity since they have to repeatedly perform single-person pose estimation for each instance. Moreover, top-down methods are highly dependent on the performance of the detectors.</p><p>Bottom-up Methods. On the other hand, bottom-up methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b5">[6]</ref> first detect all keypoints in instance-agnostic fashion. Then keypoints are assembled into full-body poses by a grouping process. Openpose <ref type="bibr" target="#b2">[3]</ref> proposes to utilize part affinity fields to establish connections between keypoints of the same instance. Associative embedding <ref type="bibr" target="#b24">[25]</ref> produces a detection heatmap and a tagging heatmap for each body joint, and then groups body joints with similar tags into individual people. Based on the above grouping strategy, HigherHRNet <ref type="bibr" target="#b5">[6]</ref> proposes multi-resolution heatmap aggregation for more accurate human pose estimation. Bottom-up methods are usually more efficient because of their simpler pipeline of sharing convolutional computation. However, the grouping postprocess is heuristic and involves many tricks which often makes its performance inferior to top-down methods.</p><p>Single-Stage Methods. Both top-down and bottom-up methods are not end-to-end trainable and have their own limitations. The single-stage methods <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b38">[39]</ref> are proposed recently to avoid the aforementioned difficulties. These methods predict instance-aware keypoints directly from estimated root locations <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b42">[43]</ref> or follow the dense predication strategy <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b32">[33]</ref> for estimating a full-body pose from each spatial location. SPM <ref type="bibr" target="#b25">[26]</ref> proposes a structured pose representation to unify position information of person instances and body joints. DirectPose <ref type="bibr" target="#b32">[33]</ref> and Point-set anchors <ref type="bibr" target="#b38">[39]</ref> adopt deformable-like convolutions to refine the initial estimations, mitigating the difficulties of feature misalignment. The performance of DirectPose <ref type="bibr" target="#b32">[33]</ref> is poor even with a well-designed feature aggregation mechanism. Although SPM and Point-set anchors achieve competitive performance among singlestage methods, they still fall behind the state-of-the-art bottom-up methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dynamic Neural Networks</head><p>The dynamic neural network is an emerging research topic in deep learning. Unlike traditional neural networks which have a static model at the inference stage, dynamic networks can adapt their structures or parameters to different inputs, leading to advantages in terms of adaptiveness, capacity, efficiency, etc. A typical approach to parameter adaptation is adjusting the weights based on their input during inference. For example, conditionally parameterized convolution <ref type="bibr" target="#b40">[41]</ref> and dynamic convolutional neural network <ref type="bibr" target="#b3">[4]</ref> perform soft attention on multiple convolutional kernels to produce an adaptive ensemble of parameters without noticeably increasing the computational cost. On the other hand, weight prediction <ref type="bibr" target="#b8">[9]</ref> directly generates (a subset of) instance-wise parameters with an independent model at test time. For instance, dynamic filter networks <ref type="bibr" target="#b15">[16]</ref> build a generation network to predict weights for a convolutional layer. Recently, CondInst <ref type="bibr" target="#b33">[34]</ref> employs the dynamic filters to generate binary masks for each instance and achieves promising performance in the instance segmentation field.</p><p>In this work, we leverage this idea to produce instance-aware dynamic convolutional weights to solve the challenging multi-person pose estimation problem, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. In parallel to our work, Mao et. al <ref type="bibr" target="#b23">[24]</ref> proposed a fully convolutional multi-person pose estimator. Our method and <ref type="bibr" target="#b23">[24]</ref> both present a single-stage multi-person pose estimation framework using dynamic instanceaware convolutions, which achieve better accuracy/efficiency tradeoff than other state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>In this section, we first introduce the overall architecture of our framework. Next, we elaborate on the proposed instance-aware dynamic network, which is capable of implementing the multi-person pose estimation task. Then we describe the disk offset prediction branch to reduce the discretization error. Finally, the loss function and the inference pipeline of our model are summarized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Let ? R ? ?3 be an input image of height and width , multi-person pose estimation (a.k.a. keypoint detection) aims at estimating human posesP of all the person instances in . The ground-truths are defined as</p><formula xml:id="formula_0">P = { 1 , 2 , . . . , } =1 ,<label>(1)</label></formula><p>where is the number of persons in the image, is the number of keypoints (i.e., left shoulder, right elbow), and = ( , ) denotes coordinates of the th keypoint from the th person. are the feature maps of the backbone network (e.g., ResNet-50). P 3 to P 7 are the FPN feature maps as in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35]</ref>, which are used for final predictions. ? denotes the height and width of feature maps, and is the number of keypoints for each instance. Classification head is used to classify the locations on the feature maps into "person" or "not person". The parameters of KP-Nets are dynamically generated with respect to different locations of the input. Then the learned parameters are split and reshaped as the weights and biases of convolutions in each KP-Net which is applied to predict keypoint maps for the particular instance. Disk offset branch predicts the instance-agnostic local offset to recover the discretization error caused by down-sampling.</p><p>Multi-Person Pose Representation. In this work, our core idea is that for an image with instances, different KP-Nets will be dynamically generated, and each KP-Net will contain the characteristics of the target instance in its filter kernels. As shown in <ref type="figure">Figure 1</ref>, when the KP-Net for person is applied to an input, it will produce a keyoint map ? R ? ? , where is the downsampling ratio of the map. Then the maximal activation in th channel of (denoted as ) indicates the location of the th keypoint for person , which can be formulated as</p><formula xml:id="formula_1">= ?,? = arg max ,<label>(2)</label></formula><p>To recover the discretization error caused by the down-sampling of keypoint maps, we additionally propose a disk offset prediction module. It predicts an instance-agnostic local offset ? R ? ?2 for each category of keypoints. Then we pick the offset vector (? , ? ) corresponding to the above keypoint location?, the final keypoint location on the input image plane can be defined a?</p><formula xml:id="formula_2">= (?+ ? ) ? , (?+ ? ) ?<label>(3)</label></formula><p>Network Architecture. We build the proposed InsPose on the popular anchor-free object detector FCOS <ref type="bibr" target="#b34">[35]</ref> due to its simplicity and flexibility, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. We make use of ResNet <ref type="bibr" target="#b12">[13]</ref> or HRNet <ref type="bibr" target="#b31">[32]</ref> as the backbone network. Then feature pyramid networks (FPN) <ref type="bibr" target="#b20">[21]</ref> is employed to produce 5 feature pyramid levels { 3 , 4 , 5 , 6 , 7 }, whose down-sampling ratios are 8, 16, 32, 64, 128, respectively.</p><p>As shown in previous works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref>, each location on the FPN's feature maps is considered as a positive sample if it is associated with a ground-truth instance. Otherwise the location is regarded as a negative sample (i.e., background). Specifically, suppose that the feature maps with a down-sampling ratio of can be represented as ? R ? ? . A location ( , ) on the feature maps can be mapped back onto the input image as + ? 2 ?, + ? 2 ? . As described in previous works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>, if the mapped location falls in the center region of an instance, the location is assigned to be responsible for that instance. The center region is defined by the box ? , ? , + , + , where , denotes the pseudo-box (minimum enclosing rectangles of the keypoints) center of the instance. denotes the radius of positive sample regions and is a constant being 1.5 as in FCOS <ref type="bibr" target="#b34">[35]</ref>. In the topmost cropped image, orange dots represent feature points while the dotted circle represents the disk region corresponding to the nose. Each position within the disk predicts an offset vector (i.e., green arrow), which is utilized to improve the keypoint localization accuracy. We only draw the keypoints of nose, left_elbow, right_elbow, left_knee, right_knee for clarity. <ref type="figure" target="#fig_1">Figure 3</ref>, on each feature level of the FPN, some functional heads (in the dashed box) are applied to make instancerelated predictions. For example, the classification head predicts the class (i.e., person or not person) of the instance associated with the location. Note that the parameters of these heads are shared between FPN levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Instance-Aware Dynamic Networks</head><p>Dynamic KP-Nets. To predict the parameters , of the KP-Net with respect to the location ( , ) of the input feature map, we concatenate all the parameters in each KP-Net (i.e., weights and biases) together as a -D vector, where is the total number of the parameters. Similar to CondInst <ref type="bibr" target="#b33">[34]</ref>, the KP-Net is a very compact FCN architecture, which has three 1?1 convolutions, each having 8 channels and using ReLU as the activation function except for the last one. No normalization layer such as batch normalization <ref type="bibr" target="#b14">[15]</ref> is adopted here. The last layer has (e.g., = 17 in COCO <ref type="bibr" target="#b22">[23]</ref> dataset) output channels and generates the final keypoint map for the instance. In particular, the KP-Net has 313 parameters in total, which consists of conv1 ((8 + 2) ? 8 + 8), conv2 (8 ? 8 + 8) and conv3 <ref type="bibr">(8 ? 17 + 17)</ref>. As mentioned before, the dynamically generated KP-Net contains information about the instance at the location. Thus, it will ideally only fire on the pixels of the instance's keypoints, as shown in <ref type="figure">Figure 1</ref>.</p><p>Shared Inputs. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, there is a keypoint feature branch connected to FPN level 3 with a down-sampling ratio of 8, which provides the shared feature map F ? R ? ? (i.e., = 8) for the final pose estimation. It first stacks four 3 ? 3 convolutions with 128 channels before the last layer. Then, to reduce the number of parameters in the KP-Net, the last layer of the keypoint feature branch decreases the number of channels from 128 to 8 (i.e., <ref type="bibr">= 8)</ref>. Considering that the relative coordinate map can provide a strong cue for distinguishing different instances, we therefore combine F with a relative coordinate map R , ? R ? ?2 , which are relative coordinates to the location ( , ) (i.e., where the parameters , are generated). Afterwards, the combinatio? F , ? R ? ?( +2) is fed into the KP-Nets to predict the keypoint map, which is omitted in <ref type="figure" target="#fig_1">Figure 3</ref> for simplicity.</p><p>Optimization. The loss function of the instance-aware KP-Nets module can be formulated as:</p><formula xml:id="formula_3">, = 1 ?? , 1 { * , &gt;0} F , ; , , M * , ,<label>(4)</label></formula><p>where FCN denotes the fully convolutional dynamic networks, which consists of the generated instance-aware parameters , at location ( , ). The classification label * , of location ( , ) is 1 if the location is associated with an ground-truth person and is 0 for indicating background. 1 { * , &gt;0} is the indicator function, being 1 if * , &gt; 0 and 0 otherwise. is the total number of locations where * , &gt; 0. As described before,F , is the combination of the keypoint feature F and the relative coordinate map R , .</p><p>As mentioned before, the resolution of the predicted keypoint map is 1 of the input image's resolution. Thus, for each visible ground-truth keypoint of an instance, we compute its low-resolution equivalent^= ? * ?, ? * ? . The training target is a one-hot ? binary mask where only a single pixel at location^is labeled as foreground. And M * , ? {0, 1} ? ? represents the one-hot masks of the person instance associated with location ( , ). is the softmax cross-entropy loss, which encourages a single keypoint to be detected. These operations are omitted in Equation (4) for clarification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Disk Offset Estimation</head><p>Suppose that ? R ? ? is the predicted keypoint map for the th person. The location of maximal activation in the th channel of indicates the location of the th keypoint of person , which is defined as?= (?,?). We can get the final keypoint location by mapping back?onto input image plane as ((?+ 0.5) ? , (?+ 0.5) ? ). However, there is a discretization error in this naive position mapping due to the down-sampling operation. In order to compensate for the discretization error, we predict a instanceagnostic local offset ? R ? ?2 for each category of keypoints. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, the disk offset branch takes the FPN feature maps 3 as input. Afterwards, three 3 ? 3 conv layers are applied for the final disk offset prediction. There are 2 output channels in the offset prediction, indicating the displacements in both the horizontal and vertical directions. Let be the 2-D position in the feature plane, and D = { : || ? || ? } be a disk of radius centered around , where represents the 2-D position of the th keypoint of the th person. As illustrated in <ref type="figure" target="#fig_2">Figure 4</ref>, for each position within the disk region D , the 2-D offset vector which points from the image position to keypoint is generated. During training, we penalize the disk offset regression errors with the 1 loss. The supervision is only performed in the disk region. During inference, the position of maximum response (?,?) in the keypoint map is shifted by the corresponding offset vector (? , ? ). Then the final result can be formulated as ((?+? )? , (?+? )? ). This well-designed module can further improve pose estimation performance, as shown in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training and Inference</head><p>Training and Loss Fucntion. Given the fact that multi-task learning facilitates the training of models, we adopt a multi-person heatmap prediction as an auxiliary task, which is omitted in <ref type="figure" target="#fig_1">Figure  3</ref> for simplicity. The heatmap prediction task takes as input the FPN feature maps 3 . Afterwards, two 3 ? 3 convolutions with channel being 256 are applied here. Finally, another 3 ? 3 convolution layer with the output channel being is appended for the final heatmap prediction, where is the number of keypoint categories in the dataset. As shown in the following experiments, this joint learning procedure can boost the performance of our model. Note that the heatmap-based task is only used for auxiliary supervision during training and is removed when testing.</p><p>Formally, the overall loss function of InsPose can be formulated as:</p><formula xml:id="formula_4">= + + + ?<label>(5)</label></formula><p>where is the focal loss function <ref type="bibr" target="#b21">[22]</ref> for the classification branch, is the loss of instance-aware KP-Nets module as defined in Equation <ref type="formula" target="#formula_3">(4)</ref>, is L1 loss for disk offset regression, ? is a variant of focal loss <ref type="bibr" target="#b18">[19]</ref> for the auxiliary heatmap branch.</p><p>Inference. The inference of InsPose is simple and straightforward. Given an input image, we forward it through the network to obtain the outputs including classification confidence , and the generated parameters , of the KP-Net at location ( , ). We first use a confidence threshold of 0.1 to filter out predictions with low confidence. Then we select the top (i.e., = 500) scoring person instances and their corresponding parameters. Then the N groups of pose estimation results can be obtained through Equations <ref type="formula" target="#formula_1">(2)</ref> and <ref type="bibr" target="#b2">(3)</ref>. At last, We do non-maximum suppression (NMS) using the minimum enclosing rectangles of estimated keypoints of the instances. The top 100 instance keypoints in each image are kept for evaluation.</p><p>It is worth noting that our framework can conveniently be adapted to simultaneous bounding-box and keypoint detection by attaching a bounding-box head, which shares the preceding convolutions of the parameter generating branch for KP-Nets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We evaluate our proposed InsPose on the large-scale COCO benchmark dataset <ref type="bibr" target="#b22">[23]</ref>, which contains more than 250 instances where each person has 17 annotated keypoints. Consistent with previous methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39]</ref>, we use the COCO train2017 split (57 images) for training and val2017 split (5 images) as validation for our ablation study. The main results are reported on the test-dev split (20 images) for comparison with other methods. We use the standard COCO metrics including AP, AP 50 , AP 75 , AP , AP to evaluate the performance of pose estimation.</p><p>Implementation Details: Unless specified, ablation studies are conducted with ResNet-50 <ref type="bibr" target="#b12">[13]</ref> and FPN <ref type="bibr" target="#b20">[21]</ref>. The models are trained with stochastic gradient descent (SGD) with a minibatch of 16 images. The initial learning rate is set to 0.01. Weight decay and momentum are set as 0.0001 and 0.9, respectively. Specifically, the model is trained for 36 epochs and the initial learning rate is divided by 10 at epoch 27 and 33 in ablation experiments. For the main results on test-dev split, we train the models for 100 epochs and the initial learning rate is divided by 10 at epoch 75 and 90. ImageNet <ref type="bibr" target="#b7">[8]</ref> pre-trained ResNet <ref type="bibr" target="#b12">[13]</ref> is employed to initialize our backbone networks. We initialize the newly added layers as described in <ref type="bibr" target="#b21">[22]</ref>. When training, the images are resized to have their shorter sides in [640, 800] and their longer sides less or equal to 1333, and then they are randomly horizontally flipped with the probability of 0.5. No extra data augmentation like rotation <ref type="bibr" target="#b25">[26]</ref> or random cropping <ref type="bibr" target="#b32">[33]</ref> is used during training. When testing, the input images are resized to have their shorter sides being 800 and their longer sides less or equal to 1333.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation Experiments</head><p>Baseline. First, we conduct the experiments through employing instance-aware dynamic networks to produce keypoint maps where the maximal activation of each channel is chosen as the keypoint. As shown in <ref type="table" target="#tab_0">Table 1</ref> (1st row), the naive framework can obtain competitive performance (60.3% in AP ). As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, we apply the local offset predicted by the disk offset branch to recover the discretization error of keypoint maps. As shown in <ref type="table" target="#tab_0">Table  1</ref> (2nd row), the performance is improved from 60.3% to 62.1% in AP , which demonstrate the effectiveness of disk offset branch. Therefore, in the sequel, we will use the disk offset branch for all the following experiments. By jointly learning with a heatmap prediction task, the performance of our model can be further improved from 62.1% to 63.1%, as shown in <ref type="table" target="#tab_0">Table 1</ref> (3rd row). Note that the heatmap prediction is only used during training to provide the multi-task regularization and is pruned during inference. We show some visualization results of InsPose in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>Architecture of KP-Net. In this section, we discuss the architecture of the KP-Nets in InsPose. Our baseline is the KP-Net of three 1 ? 1 convolutions with 8 channels considering the high capacity of conditional convolutions, which contains a total of 313 parameters. As shown in <ref type="table" target="#tab_1">Table 2</ref> (3rd row), it achieves 63.1% in AP . Then, we conduct experiments by varying the depth of the KP-Net. As shown in <ref type="table" target="#tab_1">Table 2</ref>, the depth being 1 achieves inferior performance as the KP-Net is actually equivalent to a linear mapping, which has overly weak capacity in this case. With the depth increases, the performance is improved gradually due to the higher capacity of the KP-Net. However, the performance drops slightly when the depth increases further mainly due to the reason that it is harder to optimize more dynamic parameters.</p><p>Resolution of Keypoint Maps and Disk Offset Predictions. Intuitively, the resolution of the above two predictions is of great importance to the final performance due to pose estimation being a position-sensitive task. We investigate this in the experiment. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, the resolution of the origin keypoint score maps and disk offset predictions is same as the resolution of P3 features, which is <ref type="bibr">1 8</ref> of the input image resolution. As shown in <ref type="table" target="#tab_2">Table 3</ref>, our baseline model (2nd row in the table) achieves 63.1% in AP . If both of the two predictions is downsampled to the <ref type="bibr">1 16</ref> of the input image resolution (1st row in the table), the performance drops slightly (from 63.1% to 62.6%). In paticular, AP drops remarkably by 1.3% (from 58.5% to 57.2%), which indicating that low-resolution predictions harms the performance of small-scale persons. Additionally, upsampling both of the predictions by a factor of 2 and 4 (using bilinear interpolation), the performance is degraded by 1.1% and 16.6, respectively. This is probably due to the relatively low-quality annotations of keypoints, which causes a growing negative impact on the training of the model as the output resolution increases.</p><p>Combining with Bounding Box Detection. By simply attaching a bounding-box regression head, we can simultaneously detect bounding-boxes and keypoints of the instances. When testing, NMS is applied to the detected bounding-boxes instead of the minimum enclosing rectangles of keypoints. Here we confirm it by  the experiment. As shown in <ref type="table" target="#tab_3">Table 4</ref>, our framework can achieve a reasonable person detection performance, which is competitive with the Faster R-CNN detector in Mask R-CNN <ref type="bibr" target="#b10">[11]</ref> (52.5% vs. 50.7%). Meanwhile, the keypoint performance drops a little due to different positive/negative label assignments between the detectionboxes and pseudo-boxes at training phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparisons with State-of-the-art Methods</head><p>In this section, we evaluate the proposed single-stage multi-person pose estimator on MS-COCO test-dev and compare it with other state-of-the-art methods. As shown in <ref type="table" target="#tab_4">Table 5</ref>, without any bells and whistles (e.g., multi-scale and flipping testing, the refining in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25]</ref>, and any other tricks), the proposed framework achieves 65.4% and 66.3% in AP on COCO test-dev split, with ResNet-50 and ResNet-101 as the backbone, respectively. With multi-scale testing, our framework can reach 71.0% in AP with HRNet-w32. Compared to Single-stage Methods: Our method significantly outperforms existing single-stage methods, such as Direct-Pose <ref type="bibr" target="#b32">[33]</ref>, CenterNet <ref type="bibr" target="#b42">[43]</ref>, SPM <ref type="bibr" target="#b25">[26]</ref> and Point-Set Anchors <ref type="bibr" target="#b38">[39]</ref>. The performance of our method is +3.2% and +3.0% higher as compared to DirectPose <ref type="bibr" target="#b32">[33]</ref> with ResNet-50 and ResNet-101, respectively. InsPose with HRNet-w32 even outperforms the latest single-stage pose estimator Point-Set Anchors <ref type="bibr" target="#b38">[39]</ref> with HRNet-w48, 71.0% vs. 68.7% in AP . Note that our model is anchor-free and Point-Set Anchors need 27 carefully chosen pose anchors per location.</p><p>Compared to Bottom-up Methods: Our method outperforms the state-of-the-art bottom-up methods, such as CMU-Pose <ref type="bibr" target="#b2">[3]</ref>, AE <ref type="bibr" target="#b24">[25]</ref>, PersonLab <ref type="bibr" target="#b26">[27]</ref>, PifPaf <ref type="bibr" target="#b17">[18]</ref>, HigherHRNet <ref type="bibr" target="#b5">[6]</ref> and the latest DGCN <ref type="bibr" target="#b28">[29]</ref>. With single-scale testing, it achieves significant improvement over HigherHRNet, 69.3% vs. 66.4% in AP , using the same backbone HRNet-w32. Besides that, our performance with Compared to Top-down Methods: With the same backbone ResNet-50, the proposed method outperforms previous strong baseline Mask R-CNN (65.4% vs. 62.7% in AP ). Our model is still behind other top-down methods, since they mitigate the scale variance of persons by cropping and resizing the detected person into the same scale. As noted before, two-stage methods take an additional person detector and employ a singe person pose estimator to each cropped image, resulting in high computation overhead. In contrast, our method is much simpler and faster since we eliminate the redundant person detector.</p><p>Inference Time Comparison: We measure the inference time of all variants of our method and other methods on the same hardware if possible. As shown in <ref type="table" target="#tab_4">Table 5</ref>, InsPose with HRNet-w32 can infer faster than Point-Set Anchors <ref type="bibr" target="#b38">[39]</ref> (0.15s vs. 0.27s per image). And it also runs significantly faster than state-of-the-art top-down methods, such as HRNet <ref type="bibr" target="#b31">[32]</ref> (&gt;2.34s per image) and RSN <ref type="bibr" target="#b1">[2]</ref> (&gt;3.06s per image). Note that the time of additional person detector used in HRNet and RSN is not included here. Moreover, we notice that the grouping post-processing in the bottom-up method HigherHRNet <ref type="bibr" target="#b5">[6]</ref> (0.60s per image) is time-consuming and results in inferior speed compared to InsPose. Overall, InsPose makes a good trade-off between the accuracy and efficiency, benefiting from its simple single-stage framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we present a single-stage multi-person pose estimation method, termed InsPose. It directly maps a raw input image to the desired instance-aware poses, get rid of the need for the grouping post-processing in bottom-up methods or the boundingbox detection in top-down approaches. Specifically, we propose an instance-aware module to adaptively adjust (part of) the network parameters for each instance. Our method can significantly increase the capacity and adaptive-ability of the network for recognizing various poses, while maintaining a compact end-to-end trainable pipeline. We also propose a disk offset branch to recover the discretization error due to down-sampling, boosting the keypoint detection performance further. We conduct extensive experiments on the MS-COCO dataset where our method achieves significant improvement over existing single-stage methods and performs comparably with state-of-the-art two-stage methods in terms of accuracy and efficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Visualization results of the proposed InsPose on MS-COCO val2017. InsPose can directly detect a wide range of poses, containing viewpoint change, occlusion, motion blur, multiple persons. Magenta, blue, and orange dots represent nose, keypoints of left body, and keypionts of right body, respectively. Note that some small-scale person do not have ground-truth keypoint annotations in the training set of MS-COCO, thus they might be missing when testing. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The overall architecture of InsPose. C 3 , C 4 , C 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of disk offset estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation experiments on COCO val2017. "Disk offset": the predicted local offset for recovering the discretization error caused by the keypoint map stride. "Heatmap": using heatmap to assist training.</figDesc><table><row><cell></cell><cell>Disk offset</cell><cell>Heatmap</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP</cell><cell>AP</cell><cell>AR</cell><cell>AR 50</cell><cell>AR 75</cell></row><row><cell>Our InsPose</cell><cell></cell><cell></cell><cell>60.3</cell><cell>85.4</cell><cell>65.5</cell><cell>55.0</cell><cell>68.4</cell><cell>68.6</cell><cell>91.0</cell><cell>73.7</cell></row><row><cell>Our InsPose</cell><cell>?</cell><cell></cell><cell>62.1</cell><cell>86.0</cell><cell>67.5</cell><cell>57.6</cell><cell>69.1</cell><cell>70.2</cell><cell>91.4</cell><cell>75.3</cell></row><row><cell>Our InsPose</cell><cell>?</cell><cell>?</cell><cell>63.1</cell><cell>86.2</cell><cell>68.5</cell><cell>58.5</cell><cell>70.1</cell><cell>70.9</cell><cell>91.2</cell><cell>76.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation experiments on COCO val2017 with different depth of KP-Nets. "depth": the number of conv. layers in the KP-Nets.</figDesc><table><row><cell>depth</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP</cell><cell>AP</cell></row><row><cell>1</cell><cell>54.7</cell><cell>81.1</cell><cell>57.4</cell><cell>50.5</cell><cell>62.2</cell></row><row><cell>2</cell><cell>62.4</cell><cell>85.9</cell><cell>67.9</cell><cell>58.0</cell><cell>69.2</cell></row><row><cell>3</cell><cell>63.1</cell><cell>86.2</cell><cell>68.5</cell><cell>58.5</cell><cell>70.1</cell></row><row><cell>4</cell><cell>61.7</cell><cell>85.9</cell><cell>67.2</cell><cell>57.1</cell><cell>68.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation experiments on COCO val2017 with different resolution of keypoint score maps and disk offset predictions. "res. ratio": the resolution ratio of the above two predictions to the input image.</figDesc><table><row><cell>res. ratio</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP</cell><cell>AP</cell></row><row><cell>1/16</cell><cell>62.6</cell><cell>85.8</cell><cell>68.2</cell><cell>57.2</cell><cell>70.5</cell></row><row><cell>1/8</cell><cell>63.1</cell><cell>86.2</cell><cell>68.5</cell><cell>58.5</cell><cell>70.1</cell></row><row><cell>1/4</cell><cell>62.0</cell><cell>83.9</cell><cell>67.5</cell><cell>57.8</cell><cell>69.1</cell></row><row><cell>1/2</cell><cell>45.4</cell><cell>73.6</cell><cell>47.0</cell><cell>39.1</cell><cell>54.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="7">: Ablation experiments on COCO val2017 for InsPose</cell></row><row><cell cols="7">with person bounding-box detection. The proposed frame-</cell></row><row><cell cols="7">work can achieve reasonable person detection results (52.5%</cell></row><row><cell cols="7">in box AP). As a reference, the Faster R-CNN person detector</cell></row><row><cell cols="6">in Mask R-CNN [11] achieves 50.7% in box AP.</cell><cell></cell></row><row><cell cols="2">w/ bbox AP</cell><cell>AP</cell><cell cols="3">AP 50 AP 75 AP</cell><cell>AP</cell></row><row><cell></cell><cell>-</cell><cell>63.1</cell><cell>86.2</cell><cell>68.5</cell><cell>58.5</cell><cell>70.1</cell></row><row><cell>?</cell><cell>52.5</cell><cell>62.5</cell><cell>86.2</cell><cell>68.1</cell><cell>58.3</cell><cell>70.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison with state-of-the-art methods on MS COCO test-dev dataset. * and ? denote multi-scale testing and using additional refinement, respectively. We compare the average inference time per image of our methods with other state-of-theart single-stage and two-stage methods. The time is counted with single-scale testing on a single NVIDIA TITAN X GPU.HRNet-w32 is higher than DGCN<ref type="bibr" target="#b28">[29]</ref> with ResNet-152, 69.3% vs. 67.4% in AP .</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP</cell><cell>AP</cell><cell>Times[s]</cell></row><row><cell></cell><cell cols="2">Top-down Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mask-RCNN [11]</cell><cell>ResNet-50</cell><cell>62.7</cell><cell>87.0</cell><cell>68.4</cell><cell>57.4</cell><cell>71.1</cell><cell>0.08</cell></row><row><cell>CPN [5]</cell><cell>ResNet-Inception</cell><cell>72.1</cell><cell>91.4</cell><cell>80.0</cell><cell>68.7</cell><cell>77.2</cell><cell>&gt; 0.65</cell></row><row><cell>HRNet [32]</cell><cell>HRNet-w32</cell><cell>74.9</cell><cell>92.5</cell><cell>82.8</cell><cell>71.3</cell><cell>80.9</cell><cell>&gt; 2.34</cell></row><row><cell>ECSI [31]</cell><cell>ResNet-152</cell><cell>74.3</cell><cell>91.8</cell><cell>81.9</cell><cell>70.7</cell><cell>80.2</cell><cell>-</cell></row><row><cell>RSN [2]</cell><cell>2 ? RSN-50</cell><cell>75.5</cell><cell>93.6</cell><cell>84.0</cell><cell>73.0</cell><cell>79.6</cell><cell>&gt; 3.06</cell></row><row><cell></cell><cell cols="2">Bottom-up Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CMU-Pose  *   ? [3]</cell><cell>3CM-3PAF (102)</cell><cell>61.8</cell><cell>84.9</cell><cell>67.5</cell><cell>57.1</cell><cell>68.2</cell><cell>-</cell></row><row><cell>AE  *   ? [25]</cell><cell>Hourglass-4 stacked</cell><cell>65.5</cell><cell>87.1</cell><cell>71.4</cell><cell>61.3</cell><cell>71.5</cell><cell>0.52</cell></row><row><cell>PersonLab [27]</cell><cell>ResNet-101</cell><cell>65.5</cell><cell>87.1</cell><cell>71.4</cell><cell>61.3</cell><cell>71.5</cell><cell>-</cell></row><row><cell>PifPaf [18]</cell><cell>ResNet-152</cell><cell>66.7</cell><cell>-</cell><cell>-</cell><cell>62.4</cell><cell>72.9</cell><cell>0.26</cell></row><row><cell>HigherHRNet [6]</cell><cell>HRNet-w32</cell><cell>66.4</cell><cell>87.5</cell><cell>72.8</cell><cell>61.2</cell><cell>74.2</cell><cell>0.60</cell></row><row><cell>DGCN [29]</cell><cell>ResNet-152</cell><cell>67.4</cell><cell>88.0</cell><cell>74.4</cell><cell>63.6</cell><cell>73.0</cell><cell>&gt; 0.26</cell></row><row><cell></cell><cell cols="2">Single-stage Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DirectPose [33]</cell><cell>ResNet-50</cell><cell>62.2</cell><cell>86.4</cell><cell>68.2</cell><cell>56.7</cell><cell>69.8</cell><cell>-</cell></row><row><cell>DirectPose [33]</cell><cell>ResNet-101</cell><cell>63.3</cell><cell>86.7</cell><cell>69.4</cell><cell>57.8</cell><cell>71.2</cell><cell>-</cell></row><row><cell>CenterNet [43]</cell><cell>Hourglass-104</cell><cell>63.0</cell><cell>86.8</cell><cell>69.6</cell><cell>58.9</cell><cell>70.4</cell><cell>0.16</cell></row><row><cell>SPM  *   ? [26]</cell><cell>Hourglass-8 stacked</cell><cell>66.9</cell><cell>88.5</cell><cell>72.9</cell><cell>62.6</cell><cell>73.1</cell><cell>-</cell></row><row><cell>Point-Set Anchors  *  [39]</cell><cell>HRNet-w48</cell><cell>68.7</cell><cell>89.9</cell><cell>76.3</cell><cell>64.8</cell><cell>75.3</cell><cell>0.27</cell></row><row><cell>Our InsPose</cell><cell>ResNet-50</cell><cell>65.4</cell><cell>88.9</cell><cell>71.7</cell><cell>60.2</cell><cell>72.7</cell><cell>0.08</cell></row><row><cell>Our InsPose  *</cell><cell>ResNet-50</cell><cell>67.1</cell><cell>89.7</cell><cell>74.0</cell><cell>62.6</cell><cell>73.8</cell><cell>-</cell></row><row><cell>Our InsPose</cell><cell>ResNet-101</cell><cell>66.3</cell><cell>89.2</cell><cell>73.0</cell><cell>61.2</cell><cell>73.9</cell><cell>0.10</cell></row><row><cell>Our InsPose  *</cell><cell>ResNet-101</cell><cell>67.8</cell><cell>90.3</cell><cell>74.9</cell><cell>64.3</cell><cell>73.4</cell><cell>-</cell></row><row><cell>Our InsPose</cell><cell>HRNet-w32</cell><cell>69.3</cell><cell>90.3</cell><cell>76.0</cell><cell>64.8</cell><cell>76.1</cell><cell>0.15</cell></row><row><cell>Our InsPose  *</cell><cell>HRNet-w32</cell><cell>71.0</cell><cell>91.3</cell><cell>78.0</cell><cell>67.5</cell><cell>76.5</cell><cell>-</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pose-native Network Architecture Search for Multi-person Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="592" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning delicate local representations for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxiong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binyi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erjin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="455" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dynamic Convolution: Attention Over Convolution Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5386" to="5395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">RSGNet: Relation based Skeleton Graph Network for Crowded Scenes Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianli</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predicting Parameters in Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Shakibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2148" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2334" to="2343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="34" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pay Attention Selectively and Comprehensively: Pyramid Gating Network for Human Pose Estimation without Pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenru</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaizhu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shufei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2364" to="2371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pifpaf: Composite fields for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11977" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
	<note>Feature pyramid networks for object detection</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">FCPose: Fully Convolutional Multi-Person Pose Estimation With Dynamic Instance-Aware Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weian</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9034" to="9043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Associative embedding: Endto-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05424</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Singlestage multi-person pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6951" to="6960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="269" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4929" to="4937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dgcn: Dynamic graph convolutional network for efficient multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwei</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11924" to="11931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multiperson pose estimation with enhanced channel-wise and spatial information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenqi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5674" to="5682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Directpose: Direct end-to-end multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07451</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-08-23" />
			<biblScope unit="page" from="282" to="298" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I 16</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An approach to pose-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="915" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">AI Coach: Deep Human Pose Estimation and Analysis for Personalized Athletic Training Assistance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianke</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="374" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">RGB-D-based human motion recognition with deep learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="page" from="118" to="139" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Point-set anchors for object detection, instance segmentation and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="527" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ngiam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04971</idno>
		<title level="m">Condconv: Conditionally parameterized convolutions for efficient inference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9759" to="9768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">APT: Accurate outdoor pedestrian tracking with smartphones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guihai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings IEEE INFOCOM</title>
		<imprint>
			<biblScope unit="page" from="2508" to="2516" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
