<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">sharpDARTS: Faster and More Accurate Differentiable Architecture Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hundt</surname></persName>
							<email>ahundt@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jain</surname></persName>
							<email>vjain@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
							<email>ghager1@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">sharpDARTS: Faster and More Accurate Differentiable Architecture Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract xml:lang="hu">
<div xmlns="http://www.tei-c.org/ns/1.0"> arXiv:1903.09900v1 [cs.CV]  </div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural Architecture Search (NAS) has been a source of dramatic improvements in neural network design, with recent results meeting or exceeding the performance of hand-tuned architectures. However, our understanding of how to represent the search space for neural net architectures and how to search that space efficiently are both still in their infancy.</p><p>We have performed an in-depth analysis to identify limitations in a widely used search space and a recent architecture search method, Differentiable Architecture Search (DARTS). These findings led us to introduce novel network blocks with a more general, balanced, and consistent design; a better-optimized Cosine Power Annealing learning rate schedule; and other improvements. Our resulting sharpDARTS search is 50% faster with a 20-30% relative improvement in final model error on CIFAR-10 when compared to DARTS. Our best single model run has 1.93% (1.98?0.07) validation error on CIFAR-10 and 5.5% error (5.8?0.3) on the recently released CIFAR-10.1 test set. To our knowledge, both are state of the art for models of similar size. This model also generalizes competitively to ImageNet at 25.1% top-1 (7.8% top-5) error.</p><p>We found improvements for existing search spaces but does DARTS generalize to new domains? We propose Differentiable Hyperparameter Grid Search and the HyperCuboid search space, which are representations designed to leverage DARTS for more general parameter optimization. Here we find that DARTS fails to generalize when compared against a human's one shot choice of models. We look back to the DARTS and sharpDARTS search spaces to understand why, and an ablation study reveals an unusual generalization gap. We finally propose Max-W regularization to solve this problem, which proves significantly better than the handmade design. Code will be made available.  kernels and stride 1. All convolutions are either 1x1 convolutions or Depthwise Separable Convolutions <ref type="bibr" target="#b2">[3]</ref>. The c mid hyperparameter can be either proportional to c out or set to an arbitrary constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Neural Architecture Search (NAS) promises to automatically and efficiently optimize a complex model based on representative data so that it will generalize, and thus make accurate predictions on new examples. Recent NAS results have been impressive, notably in computer vision, but the search process took large amounts of computing infrastructure <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b12">13]</ref>. These methods were followed up with a multiple order of magnitude increase in search efficiency <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b13">14]</ref>. However, key questions remain: What makes a search space worth exploring? Is each model visited in a search space getting a fair shot?</p><p>We investigate how to improve the search space of DARTS <ref type="bibr" target="#b13">[14]</ref>, which is one of several based on NASNet <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11]</ref>, propose Differentiable Hyperparameter Grid Search with DARTS, and draw conclusions that apply across architecture search spaces. To summarize, we make the following contributions: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Architecture search is the problem of optimizing the structure of a neural network to more accurately solve another underlying problem. In essence, the design steps of a neural network architecture that might otherwise be done by an engineer or graduate student by hand are instead automated and optimized as part of a well defined search space of reasonable layers, connections, outputs, and hyperparameters. In fact, architecture search can itself be defined in terms of hyperparameters <ref type="bibr" target="#b11">[12]</ref> or as a graph search problem <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b23">24]</ref>. Furthermore, once a search space is defined various tools can be brought to bear on the problem including Bayesian optimization <ref type="bibr" target="#b15">[16]</ref>, other neural networks <ref type="bibr" target="#b0">[1]</ref>, reinforcement learning, evolution <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20]</ref>, or a wide variety of optimization frameworks. A survey for the topic of Neural Architecture Search (NAS) is available at <ref type="bibr" target="#b5">[6]</ref>.</p><p>Differentiable Architecture Search (DARTS) <ref type="bibr" target="#b13">[14]</ref> defines a search space in terms of parameters ?, weights w i = sof tmax(? i ), and operation layers op i which are made differentiable in the form output i = w i * op i (input i ). ProxylessNAS <ref type="bibr" target="#b1">[2]</ref> works in a similar manner on a MobileNetv2 <ref type="bibr" target="#b22">[23]</ref> based search space. It only loads two architectures at a time, updating w based on relative changes between them. This saves GPU memory so that ImageNet size datasets and architectures load directly, but at the cost of shuttling whole architectures between the GPU and main memory.</p><p>Depthwise Separable Convolutions (SepConv) are a common building block of these searches, and were described as part of the Xception <ref type="bibr" target="#b2">[3]</ref>   improved efficiency on a per-parameter basis compared to its predecessor Inception-v3 <ref type="bibr" target="#b24">[25]</ref>. It was subsequently used to great effect in MobileNetV2 <ref type="bibr" target="#b22">[23]</ref>. A SepConv is where an initial convolution is defined in which the number of groups is equal to the number of input channels, followed by a single 1x1 convolution with a group size of 1. This type of convolution tends to have roughly equivalent or better performance than a standard Conv layer with fewer operations and lower memory utilization. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b2">3]</ref> Furthermore, so-called "bottleneck" layers or blocks have proven useful to limiting the size and improving the accuracy of neural network models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Augmentation is another fundamental tactic when optimizing the efficiency of neural networks. For example, on CIFAR-10 Cutout <ref type="bibr" target="#b4">[5]</ref> randomly sets 16x16 squares in an input image to zero; and AutoAugment <ref type="bibr" target="#b3">[4]</ref> is demonstrated on PyramidNet <ref type="bibr" target="#b8">[9]</ref>, where it applies reinforcement learning to optimize parameter choices for a set of image transforms, and to the odds of applying each transform during training.</p><p>However, improvements via the methods above are only valuable if the results are reproducible and generalize to new data, issues which are a growing concern throughout academia. Recht et al. <ref type="bibr" target="#b21">[22]</ref> have investigated such concerns, creating a new CIFAR-10.1 test set selected from the original tiny images dataset from which CIFAR-10 was itself selected. While the rank ordering of evaluated models remains very consistent, their work demonstrates a significant drop in accuracy on the test set relative to the validation set Operations in the sharpDARTS search space</p><formula xml:id="formula_0">Name K S D CMid CMidMult none ---- - AvgPool3x3 1 1 1 - - MaxPool3x3 1 1 1 - - SkipConnect 1 1 1 - - SepConv3x3 3 1 1 - 1 DilConv3x3 3 1 2 - 1 FloodConv3x3 3 1 1 - 4 DilFloodConv3x3 3 1 2 - 4 ChokeConv3x3</formula><p>3 1 1 32 -DilChokeConv3x3 3 2 2 32 - Operations in the sharpDARTS search space</p><formula xml:id="formula_1">Name K S D CMid CMidMult none ---- - AvgPool3x3 1 1 1 - - MaxPool3x3 1 1 1 - - SkipConnect 1 1 1 - - SepConv3x3 3 1 1 - 1 DilConv3x3 3 1 2 - 1 FloodConv3x3 3 1 1 - 4 DilFloodConv3x3 3 1 2 - 4 ChokeConv3x3</formula><p>3 1 1 32 -DilChokeConv3x3 3 2 2 32 -  for all models. Variation implicit in the training process, models, and the dataset itself must be carefully considered when reading results; and so a slightly better score does not guarantee better generalization. For reference, on CIFAR-10.1 "a conservative confidence interval (Clopper-Pearson at confidence level 95%) for accuracy 90% has size about ?1% with n = 2,000 (to be precise, [88.6%, 91.3%])"[?].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Search Space and Training Methods</head><p>We seek better generalization of architecture search using fewer resources.</p><p>We begin by analyzing DARTS[?], particularly its search space implementation and training regimen. The search space covers a variety of core operations or "blocks" which include pooling, skip connections, no layer, separable convolutions and dilated convolutions. Upon deeper analysis, we note that the DilConv operation contains 2 convolution layers while SepConv contains 4, thus comparing blocks with di?erent scales. This scale imbalance matters, and Sec. 4.2, 4.3, and 4.1 will explore why, but first we will go through the design choices and data leading up to that conclusion. Therefore, we start with our SharpSepConv block, which helps to correct the aforementioned imbalance, and the Cosine Power Annealing learning rate schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Sharp Separable Convolution Block</head><p>We first define the SharpSepConv block which consists of 2 Separable Convolutions and 2 bottlenecks to balance the number of layers and to capitalize on these components' computational e ciency, as we described in Related Work (Sec. 2). All of the parameters of SharpSepConv are visualized in <ref type="figure" target="#fig_1">Fig. 1</ref> and the block is defined using the PyTorch 1.0[?] framework in <ref type="figure" target="#fig_3">Fig. 3</ref>. We will refer to these figures and the definitions in the <ref type="table">Table 2</ref> caption throughout the remaining text.</p><p>The SharpSepConv block permits variation in the dilation parameters and the number of filters contained in the middle layers, both relative to the input and in absolute terms, without changing the number of convolutions. In e?ect, C mid adds fixed-size bottlenecks directly into the architecture search space which can have a large impact on the number of AddMult operations in an architecture and thus the computational e ciency. Furthermore, C mid mult makes it possible to incorporate an additional reduction or increase in network size as needed within individual cells as can be seen in <ref type="figure" target="#fig_2">Fig. 2</ref>.</p><p>Using SharpSepConv and other operations we create the sharpDARTS search space defined in <ref type="table" target="#tab_0">Table 1</ref>. As the ablation study in Sec. 4.2 and the figures in <ref type="table">Table  2</ref> show, SharpSepConv operations substantially contribute to the improvements inherent to the final sharp-DARTS and SharpSepConvDARTS architectures. for all models. Variation implicit in the training process, models, and the dataset itself must be carefully considered when reading results; and so a slightly better score does not guarantee better generalization. For reference, on CIFAR-10.1 "a conservative confidence interval (Clopper-Pearson at confidence level 95%) for accuracy 90% has size about ?1% with n = 2,000 (to be precise, [88.6%, 91.3%])" <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Search Space and Training Methods</head><p>We seek better generalization of architecture search using fewer resources.</p><p>We begin by analyzing DARTS <ref type="bibr" target="#b13">[14]</ref>, particularly its search space implementation and training regimen. The search space covers a variety of core operations or "blocks" which include pooling, skip connections, no layer, separable convolutions and dilated convolutions. Upon deeper analysis, we note that the DilConv operation contains 2 convolution layers while SepConv contains 4, thus comparing blocks with different scales. This scale imbalance matters, and Sec. 4.2, 4.3, and 4.1 will explore why, but first we will go through the design choices and data leading up to that conclusion. Therefore, we start with our SharpSepConv block, which helps to correct the aforementioned imbalance, and the Cosine Power Annealing learning rate schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Sharp Separable Convolution Block</head><p>We first define the SharpSepConv block which consists of 2 Separable Convolutions and 2 bottlenecks to balance the number of layers and to capitalize on these components' computational efficiency, as we described in Related Work (Sec. 2). All of the parameters of SharpSepConv are visualized in <ref type="figure" target="#fig_1">Fig. 1</ref> and the block is defined using the PyTorch 1.0 <ref type="bibr" target="#b17">[18]</ref> framework in <ref type="figure" target="#fig_3">Fig.  3</ref>. We will refer to these figures and the definitions in the <ref type="table">Table 2</ref> caption throughout the remaining text.</p><p>The SharpSepConv block permits variation in the dilation parameters and the number of filters contained in the middle layers, both relative to the input and in absolute terms, without changing the number of convolutions. In effect, C mid adds fixed-size bottlenecks directly into the architecture search space which can have a large impact on the number of AddMult operations in an architecture and thus the computational efficiency. Furthermore, C mid mult makes it possible to incorporate an additional reduction or increase in network size as needed within individual cells as can be seen in <ref type="figure" target="#fig_2">Fig. 2</ref>.</p><p>Using SharpSepConv and other operations we create the sharpDARTS search space defined in <ref type="table" target="#tab_0">Table 1</ref>. As the ablation study in Sec. 4.2 and the figures in <ref type="table">Table  2</ref> show, SharpSepConv operations substantially contribute to the improvements inherent to the final sharp-DARTS and SharpSepConvDARTS architectures. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cosine Power Annealing Cosine Annealing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Annealing Methods -sharpDARTS -CIFAR-10</head><p>Percentage of Total Runtime, Lower is Better Time Between Validation Accuracy Improvements -  and Cosine Power Annealing (Eq. 2 with p = 10) learning rate schedules for ImageNet. The bottom chart's logarithmic scale demonstrates how the cosine term is most prominent during early and late epochs, while the exponential term p smooths decay through the middle epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cosine Power Annealing</head><p>Cosine Annealing <ref type="bibr" target="#b14">[15]</ref> is a method of adjusting the learning rate over time, reproduced below:</p><formula xml:id="formula_2">? t = ? i min + 1 2 (? i max ? ? i min ) 1 + cos ? T cur T i<label>(1)</label></formula><p>Where ? i min and ? i max are the minimum and maximum learning rates, respectively; T i is the total number of epochs; T cur is the current epoch; and i is the index into a list of these parameters for a sequence of warm restarts in which ? i max typically decays. This schedule has been widely adopted and it is directly implemented in PyTorch 1.0 <ref type="bibr" target="#b17">[18]</ref> without warm restarts, where i = 0. The Cosine Annealing schedule works very well and is employed by DARTS. However, as can be seen in <ref type="figure" target="#fig_5">Fig.  4</ref>, the average time between improvements is above 2% of the total runtime between epochs 300 and 700. This is an imbalance in the learning rate, which is an artifact of the initial slow decay rate of Cosine Annealing followed by the rapid relative decay in learning rate. We mitigate this imbalance by introducing a power curve parameter p into the algorithm which we call Cosine Power Annealing:</p><formula xml:id="formula_3">? t = ? i min +(? i max ?? i min ) p 1 2 1+cos ? Tcur T i +1 ? p p 2 ? p<label>(2)</label></formula><p>The introduction of the normalized exponential term p permits tuning of the curve's decay rate such that it maintains a high learning rate for the first few epochs, while simultaneously taking a shallower slope during the final third of epochs. These two elements help reduce the time between epochs in which validation accuracy improves. A comparison is provided in <ref type="figure" target="#fig_5">Fig. 4</ref> and 5. In our implementation we also define a special case for the choice of p = 1 such that it falls back to standard cosine annealing. This algorithm is also compatible with decaying warm restarts, but we leave that schedule out of scope for the purposes of this paper.  <ref type="table">Table 2</ref>: Results for the CIFAR-10 dataset and the CIFAR-10.1 <ref type="bibr" target="#b21">[22]</ref> test set, bold lines are trained with AutoAugment <ref type="bibr" target="#b3">[4]</ref> and Cosine Power Annealing (Eq. 2) where we set p = 2, ? 0 max = 0.025, ? 0 min = 1e-8, and T0 = 2000 epochs of training for our best results. Our figures with a range in the bottom section are defined as median ? range/2 and include at least 3 runs with Cosine Power Annealing. Dagger ? indicates genotypes available in our accompanying repository. Non-bold numbers have 1000 epochs of Cosine Power Annealing and AutoAugment disabled to better match figures from past work. The code and this table is derived from DARTS <ref type="bibr" target="#b13">[14]</ref>. The GradOrder column is 1 for standard gradients, 2 for Hessians on DARTS based algorithms. All algorithms utilize CutOut <ref type="bibr" target="#b4">[5]</ref> except for ProgressiveNAS. A "genotype" simply defines an architecture as a graph. It has a list of layers to use, and the previous node, which it should use as an input. A "primitive" is an operation aka block of layers that can be chosen during the search. A "node" takes the output of two instances of an (input, primitive) pair as input and adds them, as seen in <ref type="figure" target="#fig_2">Fig. 2</ref>. DARTS+SSC, no search compares the genotype published by DARTS with SharpSepConv layers in the primitives. SharpSepConvDARTS is the same as DARTS + SharpSepConv but uses a genotype determined by a new search incorporating the DARTS primitives with SharpSepConv blocks. sharpDARTS uses the architecture search space described in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR</head><p>ProxylessNAS <ref type="bibr" target="#b1">[2]</ref> is in second at 2.08% val error. We also show a statistically significant <ref type="bibr" target="#b21">[22]</ref> improvement over ShakeShake64d <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b6">7]</ref> which is the best available CIFAR-10.1 model, with 7.0?1.2 test error.</p><p>GPipe AmoebaNet-B <ref type="bibr" target="#b10">[11]</ref> remains the best at any scale with 1% val error. It is scaled up from the original AmoebaNet-B and we expect other models to scale in a similar way. This truly massive model cannot load on typical GPUs due to 557M parameters and billions of   <ref type="table" target="#tab_7">Table 3</ref>, and this translates to relative improvements over DARTS of 7% in top-1 error, 13% in top-5 error, and 80% in search time. Our ImageNet model uses the genotype of the CIFAR-10 search and follows the same cell based architecture of <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19]</ref> with different operations in our search space. We apply random cropping to 224x224, random horizontal flipping, AutoAugment <ref type="bibr" target="#b3">[4]</ref>, normalization to the dataset mean and std deviation, and finally Cutout <ref type="bibr" target="#b4">[5]</ref> with a cut length of 112x112. Training of final models was done on 2x GTX 2080Ti in 16 bit mixed precision mode and takes 4-6 days, which is 8-12 GPU-days, depending on the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Towards Better Generalization</head><p>DARTS search improved results over random search by 19% <ref type="table">(Table 2)</ref>, and by adding our training regimen and search space improvements we get an additional 30% relative improvement over DARTS. Manual changes like those made to the SharpSepConv block and to AmoebaNet-B for GPipe <ref type="bibr" target="#b10">[11]</ref> are not represented in any search space, and yet they directly lead to clear improvements in accuracy. So why aren't they accounted for? Let's assume that it is possible to encode all of these elements and more into a single, broader search space in which virtually every neural network graph imaginable is encoded by hyperparameters. To even imagine tackling a problem of this magnitude, we must first ask ourselves an important question: Does DARTS even generalize to other search domains designed with this challenge in mind? In this section, we provide a preliminary exploration to begin answering these questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Differentiable Grid Search</head><p>We introduce Differentiable Hyperparameter Grid Search which is run on a HyperCuboid Search Space parameterized by an n-tuple, such as HC = (?, d n , d r , p), where ? is an arbitrary set of hyperparameters; d n is the number layers in one block; d r is the number of layer strides; and p is an arbitrary set of choices, primitives in this case. In the HyperCuboid Search Space many possible paths pass through each node, and all final sequential paths in the Directed Acyclic Graph of architecture weights are of equal length. The number of architecture weights w in the HyperCuboid graph is the product of the size of the tuple elements; and the number of hyperparameters, primitives, and tuple size can vary in this design. We test a specific HyperCuboid called MultiChannelNet with a tuple (filter scale pairs, normal layer depth, reduction layer depth, primitives), visualized at a small w <ref type="figure" target="#fig_1">i,1,1 = HC(c, 0, 1, p)  w i,1,0 = HC(c, 0, 0, p)  w i,0,1 = HC(c, 0, 1, p</ref>  There is a statistically significant <ref type="bibr" target="#b21">[22]</ref> improvement from the original Scalar DARTS to our proposed Max-W Weight Regularization DARTS. We also see the hypothesized increase in accuracy, parameters, and AddMult (+?) flops described in Sec .8] where final linear paths have 14 nodes, since "Add" nodes are excluded. In MultiChannelNet, graph nodes determine which filter input, filter output, and layer type should be utilized. Larger weights w imply a better choice of graph node and we therefore search for an optimal path through a sequence of primitive weights which maximizes the total path score.</p><p>We construct a basic handmade model in one shot with SharpSepConv and 32 filters, doubling the number of output filters at layers of stride 2 up to the limit of 256. We also ran an automated DARTS search for 60 epochs and about 16 GPU-Hours to find an optimal model. To our surprise, the handmade model outperforms DARTS by over 2%, as <ref type="table" target="#tab_8">Table 4</ref> indicates. Why might this be? To answer this we return to our original NASNet based search space to look for discrepancies in an ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>Figures for our ablation study are in <ref type="table">Table 2</ref> which also has a description of our preprocessing changes in the caption. We will analyze the 3 main model configurations below: (1) DARTS+SSC directly re-places all convolution primitives in DARTS <ref type="bibr" target="#b13">[14]</ref> with a SharpSepConv layer where the block parameters, primitives, and the genotype are otherwise held constant; we see a 10% relative improvement over DARTS val err (2.55% vs 2.83%) with only 5% more AddMult operations and no additional augmentation. (2) SharpSep-ConvDARTS is the same as item 1 but with a 1st order gradient DARTS search; we see relative improvements of 13% in val err (2.45% vs 2.83%) and 80% in search time. (3) sharpDARTS <ref type="table" target="#tab_0">(Table 1)</ref> has slightly different primitives including flood where middle channels expand 4x, choke with a fixed 32 middle filters, and one of each conv with a dilation of 2.</p><p>Our sharpDARTS model <ref type="figure" target="#fig_2">(Fig. 2)</ref> with our improved training regimen achieved an absolute best error of 2.27%. Without training enhancements the 1st order gradient search of sharpDARTS has similar accuracy to 1st order DARTS and is definitively more efficient with 32% fewer parameters, 31% fewer AddMult operations, and lower memory requirements. However, the absolute accuracy of sharpDARTS is marginally lower than the original DARTS, and also suffers from a larger disparity on ImageNet <ref type="table" target="#tab_7">(Table 3</ref>). This is startling for two reasons: (1) Substituting the SharpSepConv block improves accuracy in DARTS+SSC and SharpSepCon-vDARTS. (2) The sharpDARTS search space still contains all primitives needed to represent both the final DARTS+SSC and SharpSepConvDARTS model genotypes perfectly.</p><p>We've replicated a discrepancy in DARTS behavior across two different search spaces, so the most likely remaining possibility must be a limitation in the DARTS search method itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Max-W Regularization</head><p>GPipe <ref type="bibr" target="#b10">[11]</ref> shows AmoebaNet-B models improving in accuracy as they scale to &gt;500M parameters and billions of AddMult flops <ref type="figure" target="#fig_2">(Fig. 2, 3)</ref>. These models are from a search space similar to those used by DARTS, among others <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27]</ref>. Assume that the GPipe scaling principle holds for similar training configurations and search spaces, and one might expect that DARTS would tend towards larger models throughout the search process. However, during the early epochs of training, DARTS reliably produces models composed entirely of max pools and skip connects. These are among the smallest primitives in the DARTS architecture search space with respect to parameters and Ad-dMult operations. Higher capacity layers are chosen later in the search process, as visualized in the animation included with the original DARTS source code 2 . We experimented with removing max pool layers and the undesired behavior simply shifts to skip connects.</p><p>We posit that the DARTS Scalar Weighting output i = w i * op i (input i ) tends towards the layers with the maximum gradient, and thus models will consist of smaller layers than appropriate. Such bias during early phases of training is inefficient with respect to optimal accuracy, even if models might eventually converge to larger, more accurate models after a long period of training. Therefore, we hypothesize that subtracting the maximum weight in a given layer will regularize weight changes via Max-W Weighting:</p><formula xml:id="formula_4">output i = (1 ? max(w) + w i ) * op i (input i ) (3)</formula><p>Intuitively, consider the architecture parameters ?, weights w = sof tmax(?)|w ? [0, 1], highest score index i max = argmax(w), and the highest score weight w max = w[i max ] at some arbitrary time during training. If we apply Max-W weighting to the layer corresponding with w max , we will have (1 ? w max + w max ) * op i (input i ) which reduces to output i = 1 * op i (input i )|i = i max . In this case the ? underlying w max will remain unchanged, but this is not true for other values w i |i = i max . Here it will be incumbent on non-maximum layers w i to outperform the highest score weight and grow their value ? i . As values other than w max grow, w max will naturally drop in accordance with the behavior of sof tmax. This has the net effect of reducing bias corresponding to the highest score layer op i paired with w[i max ].</p><p>Our search with Max-W weighting on MultiChan-nelNet found a model which is both larger and significantly more accurate than both the original DARTS Scalar search models and a hand designed model ( <ref type="table" target="#tab_8">Table 4</ref>). These results indicate that our initial hypothesis holds and Max-W DARTS (Eq. 3) is an effective approach to regularization when compared to standard Scalar DARTS. Specific models will be released with the code. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Future Work</head><p>Our investigation also indicates several other areas for future work. The SharpSepConvDARTS and sharp-DARTS search spaces might also benefit from Max-W regularization, so it is an interesting topic for an additional ablation study. MultiChannelNet and sharp-DARTS indicate that the final DARTS model <ref type="bibr" target="#b13">[14]</ref> did not fully converge to the optimum due to the scalar weighting bias (Sec. 4.2, 4.3). We suspect other DARTS algorithms, such as ProxylessNAS <ref type="bibr" target="#b1">[2]</ref>, suffer from this same bias. We have also shown how Max-W Regularization correctly chooses larger models when those are more accurate, however this means a search with Max-W DARTS currently exceeds mobile-scale on the DARTS and sharpDARTS search spaces. Adding a resource cost based on time and memory to each node might make it possible to directly optimize costaccuracy tradeoffs with respect to a specific budget. Arbitrary multi-path subgraphs respecting this budget could be chosen by iterative search with a graph algorithm like network simplex <ref type="bibr" target="#b16">[17]</ref>. Other alternatives include a reinforcement learning algorithm or differentiable metrics like the latency loss in ProxylessNAS <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper we met or exceeded state of the art mobile-scale architecture search performance on CIFAR-10, CIFAR-10.1 and ImageNet with a new SharpSepConv block. We introduced the Cosine Power Annealing learning rate schedule, which is more often at an optimal learning rate than Cosine Annealing alone, and demonstrated an improved sharp-DARTS training regimen. Finally, we introduced Differentiable Hyperparameter Grid Search with a Hy-perCuboid search space to reproduce bias within the DARTS search method, and demonstrated how Max-W regularization of DARTS corrects that imbalance.</p><p>Finally, Differentiable Hyperparameter Search and HyperCuboids might be evaluated more broadly in the computer vision space and on other topics such as recurrent networks, reinforcement learning, natural language processing, and robotics. For example, SharpSepConv is manually designed so a new Hyper-Cuboid might be constructed to empirically optimize the number, sequence, type, layers, activations, normalization, and connections within a block. Perhaps a future distributed large scale model might run Differentiable Hyperparameter Search over hundreds of hyperparameters which embed a superset of search spaces, making it possible to efficiently and automatically find new models for deployment to any desired application.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Our SharpSepConv Block configured with 3x3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>(Top) High-level structure of NASNet[27] style architectures like sharpDARTS. Normal cells are stride 1 and Reduction cells are stride 2 (Left) The SharpDARTS and SharpSepConvDARTS normal cell. (Right) The Sharp-Darts and SharpSepConvDARTS reduction cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The complete PyTorch 1.0 [?] definition of the SharpSepConv block visualized in Fig. 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3 Figure 3 :</head><label>33</label><figDesc>The complete PyTorch 1.0 [18] definition of the SharpSepConv block visualized in Fig. 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>A comparison of annealing methods. Cosine Power Annealing is more frequently at an optimal learning rate.These 1000 epoch examples of CIFAR-10 sharpDARTS SGD training were selected to compare progress. Here final validation accuracy differs by 0.04%, but in the typical case Cosine Power Annealing has better performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>A comparison of Cosine Annealing<ref type="bibr" target="#b14">[15]</ref> (Eq. 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>A HyperCuboid with shape HC = (filter scales, normal layers, reduction layers, layer types) = ((2x2)x2x2x2) for Differentiable Hyperparameter Grid Search (Sec. 4.1). The data is ordered (channels, width, height). An example single-path model is highlighted with red arrows. Here you can see a small scale n-tuple coordinate system which might serve as the conceptual basis for future large scale search spaces. (Sec. 4) Weights/Path Par. +? Val Err % Test Err % Scalar 0.8M 23M 8.62?0.16 17.2?0.8 Handmade 1.0M 25M 6.50?0.18 13.9?0.4 Max-W 0.9M 31M 5.44?0.12 12.4?1.0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>. 4.3 and 4.1. Final training is ?0.3 days on an RTX 2080Ti. ((2x2)x2x2x2) scale in Fig. 6 with SharpSepConv and MaxPool primitives. Here ? = 2 n ?2 m |n, m ? [5, 6] are combinations of possible input and output filter scales. Our actual search has dimension ((4x4)x3x3x2)|n, m ? [5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>2</head><label></label><figDesc>DARTS[14] model search animation for reduce cells: https:// git.io/fjfTC normal cells: https://git.io/fjfbT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>1 .</head><label>1</label><figDesc>Define the novel SharpSepConv block with a more consistent structure of model operations and an adaptable middle filter count in addition to the sharpDARTS architecture search space. This leads to highly parameter-efficient results which match or beat state of the art performance for mobile-scale architectures on CIFAR-10, CIFAR-10.1, and ImageNet with respect to Accuracy, Ad-dMult operations, and GPU search hours.</figDesc><table><row><cell>2. Introduce the Cosine Power Annealing learn-</cell></row><row><cell>ing rate schedule for tuning between Cosine</cell></row><row><cell>Annealing[15] and exponential decay, which main-</cell></row><row><cell>tains a more optimal learning rate throughout the</cell></row><row><cell>training process.</cell></row><row><cell>3. Introduce Differentiable Hyperparameter Grid</cell></row><row><cell>Search and the HyperCuboid search space for effi-</cell></row><row><cell>ciently evaluating arbitrary discrete choices.</cell></row></table><note>4. Demonstrate the low-capacity bias of DARTS search on two search spaces, and introduce Max-W Weight Regularization to correct the problem.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>architecture, which</figDesc><table><row><cell></cell><cell>Input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Stem 1 Stem 0</cell><cell>Normal Cell</cell><cell>Normal Cell</cell><cell>?</cell><cell>Reduction Cell</cell><cell>?</cell><cell>Linear AvgPool</cell></row><row><cell></cell><cell cols="3">SharpSepConvDARTS Normal Cell</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>MaxPool 3x3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>SharpSepCo</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>c_{k-1}</cell><cell>nv SharpSepCo</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>nv ResizingMax</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Pool</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>c_{k-2}</cell><cell>MaxPool 3x3 SharpSepCo</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>nv SharpSepCo</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>nv</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Available operations for connections within cells</figDesc><table><row><cell>in the sharpDARTS search space. Columns K, S, and D</cell></row><row><cell>are kernel, stride, and dilation, respectively. All "Conv"</cell></row><row><cell>operations are configurations of SharpSepConv, shown in</cell></row><row><cell>Fig. 1 and 3.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Available operations for connections within cells</figDesc><table><row><cell>class SharpSepConv(nn.Module):</cell></row><row><cell>def __init__(</cell></row><row><cell>self, C_in, C_out, kernel_size, stride,</cell></row><row><cell>padding=1, dilation=1, affine=True,</cell></row><row><cell>C_mid_mult=1, C_mid=None):</cell></row><row><cell>super(SharpSepConv, self).__init__()</cell></row><row><cell>cmid = int(C_out * C_mid_mult)</cell></row><row><cell>cmid = C_mid if C_mid else cmid</cell></row><row><cell>self.op = nn.Sequential(</cell></row><row><cell>nn.ReLU(inplace=False),</cell></row><row><cell>nn.Conv2d(C_in, C_in, kernel_size, stride,</cell></row><row><cell>padding, dilation, groups=C_in, bias=0),</cell></row><row><cell>nn.Conv2d(C_in, cmid, kernel_size=1,</cell></row><row><cell>padding=0, bias=0),</cell></row><row><cell>nn.BatchNorm2d(cmid, affine=affine),</cell></row><row><cell>nn.ReLU(inplace=False),</cell></row><row><cell>nn.Conv2d(cmid, cmid, kernel_size,</cell></row><row><cell>stride=1, padding=(kernel_size-1)//2,</cell></row><row><cell>dilation=1, groups=cmid, bias=0),</cell></row><row><cell>nn.Conv2d(cmid, C_out, kernel_size=1,</cell></row><row><cell>padding=0, bias=0),</cell></row><row><cell>nn.BatchNorm2d(C_out, affine=affine))</cell></row><row><cell>def forward(self, x):</cell></row><row><cell>return self.op(x)</cell></row></table><note>in the sharpDARTS search space. Columns K, S, and D are kernel, stride, and dilation, respectively. All "Conv" operations are configurations of SharpSepConv, shown in Fig. 1 and 3.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Mobile ImageNet Architecture Comparison, table updated from DARTS<ref type="bibr" target="#b13">[14]</ref>. Lower values are better in all columns. A dash indicates data was either not available or not applicable.</figDesc><table /><note>totals is because the slower sharpDARTS search finds a faster final model. ImageNet: Our top SharpSep- ConvDARTS model achieved 25.1% top-1 and 7.8% top-5 error, which is competitive with other state of the art mobile-scale models depicted in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>CIFAR-10 and CIFAR-10.1 results for model</figDesc><table><row><cell>paths from our MultiChannelNet (Fig. 6) example of</cell></row><row><cell>the HyperCuboid Search Space.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This research was conducted on 5 Nvidia GPU types: Titan X, GTX 1080 Ti, GTX 1080, Titan XP, and the RTX 2080Ti.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">SMASH: one-shot model architecture search through hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
		<idno>abs/1708.05344</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<ptr target="https://arxiv.org/abs/1805.09501" />
		<title level="m">Learning Augmentation Policies from Data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improved Regularization of Convolutional Neural Networks with Cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1708.04552" />
		<imprint>
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Hendrik</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1808.05377" />
		<title level="m">Neural Architecture Search: A Survey. ArXiv e-prints</title>
		<imprint>
			<date type="published" when="2018-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Shake-Shake regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gastaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Network decoupling: From regular to depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05517</idno>
		<ptr target="https://arxiv.org/abs/1808.05517" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Pyramidal Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gpipe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06965</idno>
		<ptr target="https://arxiv.org/abs/1811.06965" />
		<title level="m">Efficient Training of Giant Neural Networks using Pipeline Parallelism</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Paxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1810.11714" />
		<title level="m">The CoSTAR Block Stacking Dataset: Learning with Workspace Constraints</title>
		<imprint>
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards automatically-tuned neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mendoza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Automatic Machine Learning</title>
		<editor>F. Hutter, L. Kotthoff, and J. Vanschoren</editor>
		<meeting>the Workshop on Automatic Machine Learning<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-24" />
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A polynomial time primal network simplex algorithm for minimum cost flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Orlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="129" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<title level="m">Efficient Neural Architecture Search via Parameters Sharing. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4092" to="4101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01548</idno>
		<ptr target="https://arxiv.org/abs/1802.01548" />
		<title level="m">Regularized Evolution for Image Classifier Architecture Search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2902" to="2911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00451</idno>
		<ptr target="https://arxiv.org/abs/1806.00451" />
		<title level="m">Do CIFAR-10 Classifiers Generalize to CIFAR-10? arXiv preprint</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">MobileNetV2: Inverted Residuals and Linear Bottlenecks. CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convolutional neural fabrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4053" to="4061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Neural architecture search with reinforcement learning. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
