<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beta Embeddings for Multi-Hop Logical Reasoning in Knowledge Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
							<email>hyren@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Beta Embeddings for Multi-Hop Logical Reasoning in Knowledge Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One of the fundamental problems in Artificial Intelligence is to perform complex multi-hop logical reasoning over the facts captured by a knowledge graph (KG). This problem is challenging, because KGs can be massive and incomplete. Recent approaches embed KG entities in a low dimensional space and then use these embeddings to find the answer entities. However, it has been an outstanding challenge of how to handle arbitrary first-order logic (FOL) queries as present methods are limited to only a subset of FOL operators. In particular, the negation operator is not supported. An additional limitation of present methods is also that they cannot naturally model uncertainty. Here, we present BETAE, a probabilistic embedding framework for answering arbitrary FOL queries over KGs. BETAE is the first method that can handle a complete set of first-order logical operations: conjunction (?), disjunction (?), and negation (?). A key insight of BETAE is to use probabilistic distributions with bounded support, specifically the Beta distribution, and embed queries/entities as distributions, which as a consequence allows us to also faithfully model uncertainty. Logical operations are performed in the embedding space by neural operators over the probabilistic embeddings. We demonstrate the performance of BETAE on answering arbitrary FOL queries on three large, incomplete KGs. While being more general, BETAE also increases relative performance by up to 25.4% over the current state-of-the-art KG reasoning methods that can only handle conjunctive queries without negation.</p><p>Our model is able to handle arbitrary first-order logic queries in an efficient and scalable manner. We perform experiments on standard KG datasets and compare BETAE to prior approaches [9, 10] that can only handle EPFO queries. Experiments show that our model BETAE is able to achieve state-of-the-art performance in handling arbitrary conjunctive queries (including ?, ?) with a relative</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Reasoning is a process of deriving logical conclusion or making predictions from available knowledge/facts. Knowledge can be encoded in a knowledge graph (KG), where entities are expressed as nodes and relations as edges. Real-world KGs, such as Freebase <ref type="bibr" target="#b0">[1]</ref>, Yago <ref type="bibr" target="#b1">[2]</ref>, NELL <ref type="bibr" target="#b2">[3]</ref>, are large-scale as well as noisy and incomplete. Reasoning in KGs is a fundamental problem in Artificial Intelligence. In essence, it involves answering first-order logic (FOL) queries over KGs using operators existential quantification (?), conjunction (?), disjunction (?), and negation (?).</p><p>To find answers, a given FOL query can be viewed as a computation graph which specifies the steps needed. A concrete example of the computation graph for the query "List the presidents of European countries that have never held the World Cup" is shown in <ref type="figure">Fig. 1</ref>. The query can be represented as a conjunction of three terms: "Located(Europe,V)", which finds all European countries; "?Held(World Cup,V)", which finds all countries that never held the World Cup; and "President(V,V ? )", which finds presidents of given countries. In order to answer this query, one first locates the entity "Europe" and then traverses the KG by relation "Located" to identify a set of European countries. Similar operations are needed for the entity "World Cup" to obtain countries that hosted the World Cup. One then needs to complement the second set to identify countries that have never held the World Cup and intersect the complement with the set of European countries. The final step is to apply the relation Intrs.</p><p>Pres. Query <ref type="figure">Figure 1</ref>: BETAE answers first-order logic queries that include ?, ?, ? and ? logical operators. (A): A given query "List the presidents of European countries that have never held the World Cup" can be represented by its computation graph where each node represents a set of entities and each edge represents a logical operation. (B): BETAE models each node of the computation graph as a Beta distribution over the entity embedding space and each edge of the computation graph transforms the distribution via a projection, negation, or intersection operation. BETAE applies a series of logical operators that each transform and shape the Beta distribution. The answer to the query are then entities that are probabilistically close to the embedding of the query (e.g., embedding of "Macron" is closer to the query embedding and the embedding of "Rebelo de Sousa").</p><p>"President" to the resulting intersection set to find the list of country presidents, which gives the query answer.</p><p>KG reasoning presents a number of challenges. One challenge is the scale of KGs. Although queries could be in principle answered by directly traversing the KG, this is problematic in practice since multi-hop reasoning involves an exponential growth in computational time/space. Another challenge is incompleteness, where some edges between entities are missing. Most real-world KGs are incomplete and even a single missing edge may make the query unanswerable.</p><p>Previous methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> aim to address the above challenges by using embeddings and this way implicitly impute the missing edges. Methods also embed logical queries into various geometric shapes in the vector space <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. The idea here is to design neural logical operators and embed queries iteratively by executing logical operations according to the query computation graph ( <ref type="figure">Fig. 1</ref>). An advantage of these approaches is that they do not need to track all the intermediate entities, and that they can use the nearest neighbor search <ref type="bibr" target="#b12">[13]</ref> in the embedding space to quickly discover answers. However, these methods only support existential positive first-order (EPFO) queries, a subset of FOL queries with existential quantification (?), conjunction (?) and disjunction (?), but not negation (?). Negation, however, is a fundamental operation and required for the complete set of FOL operators. Modeling negation so far has been a major challenge. The reason is that these methods embed queries as closed regions, e.g., a point <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> or a box <ref type="bibr" target="#b9">[10]</ref> in the Euclidean space, but the complement (negation) of a closed region does not result in a closed region. Furthermore, current methods embed queries as static geometric shapes and are thus unable to faithfully model uncertainty.</p><p>Here we propose Beta Embedding (BETAE), a method for multi-hop reasoning over KGs using full first-order logic (FOL). We model both the entities and queries by probabilistic distributions with bounded support. Specifically, we embed entities and queries as Beta distributions defined on the [0, 1] interval. Our approach has the following important advantages: (1) Probabilistic modeling can effectively capture the uncertainty of the queries. BETAE adaptively learns the parameters of the distributions so that the uncertainty of a given query correlates well with the differential entropy of the probabilistic embedding. (2) We design neural logical operators that operate over these Beta distributions and support full first-order logic: ?, ?, ? and most importantly ?. The intuition behind negation is that we can transform the parameters of the Beta distribution so that the regions of high probability density become regions of low probability density and vice versa.  <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> and neural architectures <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. Besides vector embeddings, KG2E <ref type="bibr" target="#b16">[17]</ref> and TransG <ref type="bibr" target="#b17">[18]</ref> both model the uncertainties of the entities and relations on KGs by using the Gaussian distributions and mixture models. However, their focus is link prediction and it is unclear how to generalize these approaches to multi-hop reasoning with logical operators. In contrast, our model aims at multi-hop reasoning and thus learns probabilistic embeddings for complex queries and also designs a set of neural logical operators over the probabilistic embeddings. Another line of work models the uncertainty using order embeddings <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>, distributions <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> and Quantum logic <ref type="bibr" target="#b25">[26]</ref>. The difference here is that our goal is to model the logical queries and their answers, which goes beyond modeling the inclusion and entailment between a pair of concepts in KGs.</p><p>Multi-hop Reasoning on KGs. Another line of related work is multi-hop reasoning on KGs. This includes (1) answering multi-hop logical queries on KGs, which is most relevant to our paper, and (2) using multi-hop rules or paths to improve the performance of link prediction. Previous methods that answer queries <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> can only model a subset of FOL queries, while our method can handle arbitrary FOL queries with probabilistic embeddings. Rule and path-based methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> pre-define or achieve these multi-hop rules in an online fashion that require a modeling of all the intermediate entities on the path, while our main focus is to directly embed and answer a complex FOL query without the need to model the intermediate entities, which leads to more scalable algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>Knowledge Graph (KG) G is heterogeneous graph structure that consists of a set of entities V and a set of relation types R, G = (V, R). Each relation type r ? R is a binary function r : V ? V ? {True, False} that indicates (directed) edges of relation type r between pairs of entities.</p><p>We are interested in answering first-order logic (FOL) queries with logical operations including conjunction (?), disjunction (?), existential quantification (?) and negation (?) 1 . We define valid FOL queries in its disjunctive normal form (DNF), i.e., disjunction of conjunctions. Definition 1 (First-order logic queries). A first-order logic query q consists of a non-variable anchor entity set V a ? V, existentially quantified bound variables V 1 , . . . , V k and a single target variable V ? , which provides the query answer. The disjunctive normal form of a logical query q is a disjunction of one or more conjunctions.</p><formula xml:id="formula_0">q[V ? ] = V ? . ?V 1 , . . . , V k : c 1 ? c 2 ? ... ? c n 1.</formula><p>Each c represents a conjunctive query with one or more literals e. c i = e i1 ? e i2 ? ? ? ? ? e im .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Each literal e represents an atomic formula or its negation.</p><formula xml:id="formula_1">e ij = r(v a , V ) or ? r(v a , V ) or r(V , V ) or ? r(V , V ), where v a ? V a , V ? {V ? , V 1 , . . . , V k }, V ? {V 1 , . . . , V k }, V = V , r ? R.</formula><p>Computation Graph: As shown in <ref type="figure">Fig. 1</ref>, we can derive, for a given query, its corresponding computation graph by representing each atomic formula with relation projection, merging by intersection and transforming negation by complement. This directed graph demonstrates the computation process to answer the query. Each node of the computation graph represents a distribution over a set of entities in the KG and each edge represents a logical transformation of this distribution. The computation graphs of FOL queries can be viewed as heterogeneous trees, where each leaf node corresponds to a set of cardinality 1 that contains a single anchor entity v a ? V a (note that one anchor entity may appear in multiple leaf nodes) and the root node represents the unique target variable, which is the set of answer entities. The mapping along each edge applies a certain logical operator:</p><p>1. Relation Projection: Given a set of entities S ? V and relation type r ? R, compute adjacent entities</p><formula xml:id="formula_2">? v?S A r (v) related to S via r: A r (v) ? {v ? V : r(v, v ) = True}. 2. Intersection: Given sets of entities {S 1 , S 2 , . . . , S n }, compute their intersection ? n i=1 S i . 3. Complement/Negation: Given a set of entities S ? V, compute its complement S ? V \ S.</formula><p>We do not define a union operator for the computation graph, which corresponds to disjunction. However, this operator is not needed, since according to the De Morgan's laws, given sets of entities</p><formula xml:id="formula_3">{S 1 , . . . , S n }, ? n i=1 S i is equivalent to ? n i=1 S.</formula><p>In order to answer a given FOL query, we can follow the computation graph and execute logical operators. We can obtain the answers by looking at the entities in the root node. We denote the answer set as q , which represents the set of entities on G that satisfy q, i.e., v ? q ?? q[v] = True. Note that this symbolic traversal of the computation graph is equivalent to traversing the KG, however, it cannot handle noisy or missing edges in the KG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Probabilistic Embeddings for Logical Reasoning</head><p>To answer queries in a large and incomplete KG, we first introduce our model BETAE, which embeds both entities and queries as Beta distributions. Then we define probabilistic logical operators for relation projection, intersection and negation. These operate on the Beta embeddings which allow us to support arbitrary FOL queries. Finally, we describe our training objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Beta Embeddings for Entities and Queries</head><p>In order to model any FOL query, the desirable properties of the embedding include: (1) the embedding can naturally model uncertainty; (2) we can design logical/set operators (conjunction/intersection and especially negation/complement) that are closed. The closure property is important for two reasons: (i) operators can be combined in arbitrary ways; (ii) the representation remains at a fixed space/time complexity and does not grow exponentially as additional operators are applied.</p><p>We propose to embed both the entities and queries into the same space using probabilistic embeddings with bounded support. With a bounded support, the negation/complement can be accordingly defined, where we follow the intuition to switch high-density regions to low density and vice versa ( <ref type="figure" target="#fig_1">Fig. 2)</ref>. Specifically, we look at the [0, 1] interval and adopt the Beta distribution. A Beta distribution Beta(?, ?) has two shape parameters, and our method relies on its probability density function (PDF):</p><formula xml:id="formula_4">p(x) = x ??1 (1?x) ??1 B(?,?)</formula><p>, where x ? [0, 1] and B(?) denotes the beta function. The uncertainty of a Beta distribution can be measured by its differential entropy:</p><formula xml:id="formula_5">H = ln B(?, ?) ? (? ? 1)[?(?) ? ?(? + ?)] ? (? ? 1)[?(?) ? ?(? + ?)], where ?(?) represents the digamma function.</formula><p>For each entity v ? V, which can be viewed as a set with a single element, we assign an initial Beta embedding with learnable parameters. We also embed each query q with a Beta embedding, which is calculated by a set of probabilistic logical operators (introduced in the next section) following the computation graph. Note that BETAE learns high-dimensional embeddings where each embedding consists of multiple independent Beta distributions, capturing a different aspect of a given entity or a query: S = [(? 1 , ? 1 ), . . . , (? n , ? n )], where n is a hyperparameter. We denote the PDF of the i-th Beta distribution in S as p S,i . Without loss of generality and to ease explanation, we shall assume that each embedding only contains one Beta distribution: S = [(?, ?)], and we denote its PDF as p S .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Probabilistic Logical Operators</head><p>In order to answer a query using the computation graph, we need probabilistic logical operators for the Beta embedding. Next, we describe the design of these logical operators used in computation graphs, which include relation projection P, intersection I and negation N . As discussed before, union can be implemented using intersection and complement. Each operator takes one or more Beta embeddings as input and then transforms them into a new Beta embedding.</p><p>Probabilistic Projection Operator P: In order to model the relation projection from one distribution to another, we design a probabilistic projection operator P that maps from one Beta embedding S to another Beta embedding S given the relation type r. We then learn a transformation neural network for each relation type r, which we implement as a multi-layer perceptron (MLP):</p><formula xml:id="formula_6">S = MLP r (S)<label>(1)</label></formula><p>The goal here is that for all entities S covered by the input distribution, we can achieve the embedding distribution that covers entities</p><formula xml:id="formula_7">S = ? v?S A r (v), where A r (v) ? {v ? V : r(v, v ) = True}.</formula><p>Importantly, projection operation represents a relation traversal from one (fuzzy) set of entities to another (fuzzy) set, and may yield a huge number of results, yet here we represent it with a single fixed-size Beta embedding, making BETAE scalable.</p><p>Probabilistic Intersection Operator I: Given n input embeddings {S 1 , . . . , S n }, the goal of probabilistic intersection operator I is to calculate the Beta embedding S Inter that represents the intersection of the distributions (i.e., the intersection of the distributions defining fuzzy input sets of entities). We model I by taking the weighted product of the PDFs of the input Beta embeddings:</p><formula xml:id="formula_8">p SInter = 1 Z p w1 S1 . . . p wn Sn ,<label>(2)</label></formula><p>where Z is a normalization constant and w 1 , . . . , w n are the weights with their sum equal to 1.</p><p>To make the model more expressive, we use the attention mechanism and learn w 1 , . . . , w n through a MLP Att that takes as input the parameters of S i and outputs a single attention scalar:</p><formula xml:id="formula_9">w i = exp(MLP Att (S i )) j exp(MLP Att (S j ))<label>(3)</label></formula><p>Since S i is a Beta distribution [(? i , ? i )], the weighted product p SInter is a linear interpolation of the parameters of the inputs. We derive the parameters of S Inter to be [( w i ? i , w i ? i )]:</p><formula xml:id="formula_10">p SInter (x) ? x wi(?i?1) (1 ? x) wi(?i?1) = x wi?i?1 (1 ? x) wi?i?1<label>(4)</label></formula><p>Our approach has three important advantages ( <ref type="figure" target="#fig_1">Fig. 2):</ref> (1) Taking a weighted product of the PDFs demonstrates a zero-forcing behavior <ref type="bibr" target="#b32">[33]</ref> where the effective support of the resulting Beta embedding S Inter approximates the intersection of the effective support of the input embeddings (effective support meaning the area with sufficiently large probability density <ref type="bibr" target="#b32">[33]</ref>). This follows the intuition that regions of high density in p SInter should have high density in the PDF of all input embeddings {p S1 , . . . , p Sn }.  Proposition 1 shows that our design of the probabilistic intersection operator and the probabilistic negation operator achieves two important properties that obey the rules of real logical operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Learning Beta Embeddings</head><p>Distance: Assume we use a n-dimensional Beta embedding for entities and queries, which means that each embedding consists of n independent Beta distributions with 2n number of parameters. Given an entity embedding v with parameters</p><formula xml:id="formula_11">[(? v 1 , ? v 1 ), . . . , (? v n , ? v n )]</formula><p>, and a query embedding q with parameters [(? q 1 , ? q 1 ), . . . , (? q n , ? q n )], we define the distance between this entity v and the query q as the sum of KL divergence between the two Beta embeddings along each dimension:</p><formula xml:id="formula_12">Dist(v; q) = n i=1 KL(p v,i ; p q,i ),<label>(5)</label></formula><p>where p v,i (p q,i ) represents the i-th Beta distribution with parameters ? v i and ? v i (? q i and ? q i ). Note that we use KL(p v,i ; p q,i ) rather than KL(p q,i ; p v,i ) so that the query embeddings will "cover" the modes of all answer entity embeddings <ref type="bibr" target="#b33">[34]</ref>.</p><p>Training Objective: Our objective is to minimize the distance between the Beta embedding of a query and its answers while maximizing the distance between the Beta embedding of the query and other random entities via negative sampling <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref>, which we define as follows:</p><formula xml:id="formula_13">L = ? log ? (? ? Dist(v; q)) ? k j=1 1 k log ? Dist(v j ; q) ? ? ,<label>(6)</label></formula><p>where v ? q belongs to the answer set of q, v j / ? q represents a random negative sample, and ? denotes the margin. In the loss function, we use k random negative samples and optimize the average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion on Modeling Union: With the De Morgan's laws (abbreviated as DM)</head><p>, we can naturally model union operation S 1 ? S 2 with S 1 ? S 2 , which we can derive as a Beta embedding. However, according to the Theorem 1 in <ref type="bibr" target="#b9">[10]</ref>, in order to model any queries with the union operation, we must have a parameter dimensionality of ?(M ), where M is of the same order as the number of entities <ref type="bibr" target="#b9">[10]</ref>. The reason is that we need to model in the embedding space any subset of the entities. Q2B <ref type="bibr" target="#b9">[10]</ref> overcomes this limitation by transforming queries into a disjunctive normal form (DNF) and only deals with union at the last step. Our DM modeling of union is also limited in this respect since the Beta embedding can be at most bi-modal and as a result, there are some union-based queries that BETAE cannot model in theory. However, in practice, union-based queries are constrained and we do not need to model all theoretically possible entity subsets. For example, a query "List the union of European countries and tropical fruits." does not make sense; and we further learn high-dimensional Beta embeddings to alleviate the problem. Furthermore, our DM modeling is always linear w.r.t the number of union operations, while the DNF modeling is exponential in the worst case (with detailed discussion in Appendix B). Last but not least, BETAE can safely incorporate both the DNF modeling and DM modeling, and we show in the experiments that the two approaches work equally well in answering real-world queries.</p><p>Inference: Given a query q, BETAE directly embeds it as q by following the computation graph without the need to model intermediate entities. To obtain the final answer entities, we rank all the entities based on the distance defined in Eq. 5 in constant time using Locality Sensitive Hashing <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we evaluate BETAE on multi-hop reasoning over standard KG benchmark datasets. Our experiments demonstrate that: (1) BETAE effectively answers arbitrary FOL queries. (2) BETAE outperforms less general methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> on EPFO queries (containing only ?, ? and ?) that these methods can handle. (3) The probabilistic embedding of a query corresponds well to its uncertainty.   <ref type="table">Table 1</ref>: MRR results (%) of BETAE, Q2B and GQE on answering EPFO (?, ?, ?) queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Setup</head><p>Our experimental setup is focused on incomplete KGs and thus we measure performance only over answer entities that require (implicitly) imputing at least one edge. More precisely, given an incomplete KG, our goal is to obtain non-trivial answers to arbitrary FOL queries that cannot be discovered by directly traversing the KG. We use three standard KGs with official training/validation/test edge splits, FB15k <ref type="bibr" target="#b3">[4]</ref>, FB15k-237 <ref type="bibr" target="#b34">[35]</ref> and NELL995 <ref type="bibr" target="#b26">[27]</ref> and follow <ref type="bibr" target="#b9">[10]</ref> for the preprocessing.</p><p>Evaluation Protocol: We follow the evaluation protocol in <ref type="bibr" target="#b9">[10]</ref>. We first build three KGs: training KG G train , validation KG G valid , test KG G test using training edges, training+validation edges, training+validation+test edges, respectively. Our evaluation focuses on incomplete KGs, so given a test (validation) query q, we are interested in discovering non-trivial answers q test \ q val ( q val \ q train ). That is, answer entities where at least one edge needs to be imputed in order to create an answer path to that entity. For each non-trivial answer v of a test query q, we rank it against non-answer entities V\ q test . We denote the rank as r and calculate the Mean Reciprocal Rank (MRR): 1 r ; and, Hits at K (H@K): 1[r ? K] as evaluation metrics. Queries: We base our queries on the 9 query structures proposed in Query2Box (Q2B) <ref type="bibr" target="#b9">[10]</ref> and make two additional improvements. First, we notice that some test queries may have more than 5,000 answers. To make the task more challenging, we thus regenerate the same number of validation/test queries for each of the 9 structures, keeping only those with answers smaller than a threshold. We list the statistics of the new set of queries in <ref type="table" target="#tab_5">Table 6</ref> (in Appendix C). We evaluate BETAE on both the queries in Q2B and our new realistic queries, which are more challenging since they use the same training queries without any enforcement on the maximum number of answers for a fair comparison. Second, from the 9 structures we derive 5 new query structures with negation. As shown in <ref type="figure" target="#fig_4">Fig. 3</ref>, in order to create realistic structures with negation, we look at the 4 query structures with intersection (2i/3i/ip/pi) and perturb one edge to perform set complement before taking the intersection, resulting in 2in/3in/inp/pni/pin structures. Additional information about query generation is given in Appendix C.</p><p>As summarized in <ref type="figure" target="#fig_4">Fig. 3</ref>, our training and evaluation queries consist of the 5 conjunctive structures (1p/2p/3p/2i/3i) and also 5 novel structures with negation (2in/3in/inp/pni/pin). Furthermore, we also evaluate model's generalization ability which means answering queries with logical structures that the model has never seen during training. We further include ip/pi/2u/up for evaluation.  Baselines: We consider two state-of-the-art baselines for answering complex logical queries on KGs: Q2B <ref type="bibr" target="#b9">[10]</ref> and GQE <ref type="bibr" target="#b8">[9]</ref>. GQE embeds both queries and entities as point vectors in the Euclidean space; Q2B embeds the queries as hyper-rectangles (boxes) and entities as point vectors so that answers will be enclosed in the query box. Both methods design their corresponding projection and intersection operators, however, neither can handle the negation operation since the complement of a point/box in the Euclidean space is no longer a point/box. For fair comparison, we assign the same dimensionality to the embeddings of the three methods 2 . Note that since the baselines cannot model negation operation, the training set for the baselines only contain queries of the 5 conjunctive structures. We ran each method for 3 different random seeds after finetuning the hyperparameters. We list the hyperparameters, architectures and more details in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Modeling Arbitrary FOL Queries</head><p>Modeling EPFO (containing only ?, ? and ?) Queries: First we compare BETAE with baselines that can only model queries with conjunction and disjunction (but no negation). <ref type="table">Table 1</ref> shows the MRR of the three methods. BETAE achieves on average 9.4%, 5.0% and 7.4% relative improvement MRR over previous state-of-the-art Q2B on FB15k, FB15k-237 and NELL995, respectively. We refer the reader to <ref type="table" target="#tab_9">Tables 9 and 10</ref> in Appendix E for the H@1 results. Again, on EPFO queries BETAE achieves better performance than the two baselines on all three datasets.</p><p>DNF vs. DM: As discussed in Sec. 4.3, we can model queries with disjunction in two ways: (1) transform them into disjunctive normal form (DNF); (2) represent disjunction with conjunction and negation using the De Morgan's laws (DM). We evaluate both modeling schemes <ref type="figure">(Table 1(right)</ref>). DNF modeling achieves slightly better results than DM since it is able to better represent disjunction with multi-modal embeddings. However, it also demonstrates that our DM modeling provides a nice approximation to the disjunction operation, and generalizes really well since the model is not trained on 2u and up queries. Note that BETAE is very flexible and can use and improve both modeling approaches while the baselines can only use DNF since they cannot model the negation operation.</p><p>Modeling Queries with Negation: Next, we evaluate our model's ability to model queries with negation. We report both the MRR and H@10 results in <ref type="table" target="#tab_3">Table 2</ref>. Note that answering queries with negation is challenging since only a small fraction of the training queries contain negation. As shown in <ref type="table" target="#tab_6">Table 7</ref> (Appendix), during training, the number of 2in/3in/inp/pin/pni queries is 10 times smaller than the number of conjunctive queries. Overall, BETAE generalizes well and provides the first embedding-based method that can handle arbitrary FOL queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Modeling the Uncertainty of Queries</head><p>We also investigate whether our Beta embeddings are able to capture uncertainty. The uncertainty of a (fuzzy) set can be characterized by its cardinality. Given a query with answer set q , we aim to calculate the correlation between the differential entropy of the Beta embedding p q and the cardinality of the answer set | q |. For comparison, Q2B embeds each query as a box, which can also model the uncertainty of the query by expanding/shrinking the box size. We consider two types of statistical correlations: Spearman's rank correlation coefficient (SRCC), which measures the statistical dependence between the rankings of two variables; and Pearson's correlation coefficient (PCC), which measures the linear correlation of the two variables. <ref type="table">Table 3</ref> and <ref type="table">Table 11</ref> (in Appendix E) show that BETAE achieves up to 77% better correlation than Q2B. We conclude that BETAE with Beta embeddings is able to capture query uncertainty. Furthermore, note that BETAE naturally learns this property without any regularization to impose the correlation during training.  <ref type="table">Table 3</ref>: Spearman's rank correlation between learned embedding (differential entropy for BETAE, box size for Q2B) and the number of answers of queries. BETAE shows up to 77% relative improvement.  <ref type="table">Table 4</ref>: ROC-AUC score of BETAE for all the 12 query structures on classification of queries with/without answers on the NELL dataset.</p><p>Modeling Queries without Answers: Since BETAE can effectively model the uncertainty of a given query, we can use the differential entropy of the query embedding as a measure to represent whether the query is an empty set (has no answers). For evaluation, we randomly generated 4k queries without answers and 4k queries with more than 5 answers for each of the 12 query structures on NELL. Then we calculate the differential entropy of the embeddings of each query with a trained BETAE and use this to classify whether a query has answers. As a result, we find an ROC-AUC score of 0.844 and list the ROC-AUC score of each query structure in <ref type="table">Table 4</ref>. These results suggest that BETAE can naturally model queries without answers, since (1) we did not explicitly train BETAE to optimize for correlation between the differential entropy and the cardinality of the answer set; (2) we did not train BETAE on queries with empty answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented BETAE, the first embedding-based method that could handle arbitrary FOL queries on KGs. Given a query, BETAE embeds it into Beta distributions using probabilistic logical operators by following the computation graph in a scalable manner. Extensive experimental results show that BETAE significantly outperforms previous state-of-the-art, which can only handle a subset of FOL, in answering arbitrary logical queries as well as modeling the uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>BETAE gives rise to the first method that handles all logical operators in large heterogeneous KGs. It will greatly increase the scalability and capability of multi-hop reasoning over real-world KGs and heterogenous networks.</p><p>One potential risk is that the model may make undesirable predictions in a completely random KG, or a KG manipulated by adversarial and malicious attacks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. Recent progress on adversarial attacks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> have shown that manipulation of the KG structure may effectively deteriorate the performance of embedding-based methods. And this may mislead the users and cause negative impact. We will continue to work on this direction to design more robust KG embeddings. Alternatively, this issue can also be alleviated through human regularization of real-world KGs.   Generation of Queries with Negation: For the additional queries with negation, we derive 5 new query structures from the 9 EPFO structures. Specifically, as shown in <ref type="figure" target="#fig_4">Fig. 3</ref>, we only consider query structures with intersection for the derivation of queries with negation. The reason is that queries with negation are only realistic if we take negation with an intersection together. Consider the following example, where negation is not taken with intersection, "List all the entities on KG that is not European countries.", then both "apple" and "computer" will be the answers. However, realistic queries will be like "List all the countries on KG that is not European countries.", which requires an intersection operation. In this regard, We modify one edge of the intersection to further incorporate negation, thus we derive 2in from 2i, 3in from 3i, inp from ip, pin and pni from pi. Note that following the 9 EPFO structures, we also enforce that all queries with negation have at most 100 answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Experimental Details</head><p>We implement our code using Pytorch. We use the implementation of the two baselines GQE <ref type="bibr" target="#b8">[9]</ref> and Q2B <ref type="bibr" target="#b9">[10]</ref>  Each single experiment is run on a single NVIDIA GeForce RTX 2080 TI GPU, and we run each method for 300k iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Experimental Results</head><p>Here we list some additional experimental results. <ref type="table">Table 1</ref> the MRR results of the three methods on answering EPFO queries. Our methods show a significant improvement over the two baselines in all three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>We show in</head><p>We show in <ref type="table">Table 10</ref> the MRR results of the three methods on answering EPFO queries in the dataset proposed in <ref type="bibr" target="#b9">[10]</ref>, where the queries may have more than 5,000 answers. Our method is still better than the two baselines.</p><p>We show in <ref type="table">Table 11</ref> the Pearson correlation coefficient between the learned embedding and the number of answers of queries. Our method is better than the baseline Q2B in measuring the uncertainty of the queries.   <ref type="table">Table 10</ref>: MRR results (%) on queries from <ref type="bibr" target="#b9">[10]</ref>, where we show that we are also able to achieve higher performance than baselines Q2B and GQE on all three KGs.  <ref type="table">Table 11</ref>: Pearson correlation coefficient between learned embedding (differential entropy for BETAE, box size for Q2B) and the number of answers of queries (grouped by different query type). Ours achieve higher correlation coefficient.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of our probabilistic intersection operator I (left) and probabilistic negation operator N (right). I transforms the input distribution by taking the weighted product of the PDFs; N transforms the input distribution by taking the reciprocal of its parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 2 )</head><label>2</label><figDesc>As shown in Eq. 4, the probabilistic intersection operator I is closed, since the weighted product of PDFs of Beta distributions is proportional to a Beta distribution. (3) The probabilistic intersection operator I is commutative w.r.t the input Beta embeddings following Eq. 2.Probabilistic Negation Operator N : We require a probabilistic negation operator N that takes Beta embedding S as input and produces an embedding of the complement N (S) as a result. A desired property of N is that the density function should reverse in the sense that regions of high density in p S should have low probability density in p N (S) and vice versa(Fig. 2). For the Beta embeddings, this property can be achieved by taking the reciprocal of the shape parameters ? and ?: N ([(?, ?)]) = [( 1 ? , 1 ? )]. As shown inFig. 2, the embeddings switch from bell-shaped unimodal density function with 1 &lt; ?, ? to bimodal density function with 0 &lt; ?, ? &lt; 1. Proposition 1. By defining the probabilistic logical operators I and N , BETAE has the following properties (with proof in Appendix A):1. Given Beta embedding S, S is a fixed point of N ? N : N (N (S)) = S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 .</head><label>2</label><figDesc>Given Beta embedding S, we have I({S, S, . . . , S}) = S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Top: Training and evaluation queries represented with their graphical structures, an abbreviation of the computation graph. Naming convention: p projection, i intersection, n negation, u union. Bottom: Query structures with negation used in our experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>423 0.552 0.564 0.594 0.610 0.598 0.535 0.711 0.595 0.354 0.447 0.639</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>0.766 0.793 0.909 0.933 0.868 0.798 0.865 0.93 0.801 0.809 0.848</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>236 0.403 0.433 0.404 0.385 0.403 0.403 0.515 0.514 0.255 0.354 0.455</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>(3) Our neural modeling of ? and ? naturally corresponds to the real operations and captures several properties of first-order logic. For example, applying the negation operator twice will return the same input. (4) Using the De Morgan's laws, disjunction ? can be approximated with ? and ?, allowing BETAE to handle a complete set of FOL operators and thus supporting arbitrary FOL queries. increase of the accuracy by up to 25.4%. Furthermore, we also demonstrate that BETAE is more general and is able to accurately answer any FOL query that includes negation ?. Project website with data and code can be found at http://snap.stanford.edu/betae.</figDesc><table><row><cell>2 Related Work</cell></row></table><note>Uncertainty in KG Embeddings. Previous works on KG embeddings assign a learnable vector for each entity and relation with various geometric intuitions</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>MRR and H@10 results (%) of BETAE on answering queries with negation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Knowledge graph dataset statistics as well as training, validation and test edge splits. .2 18.3 12.5 18.9 23.8 15.9 14.6 19.8 21.6 16.9 FB15k-237 1.7 17.3 24.3 6.9 4.5 17.7 10.4 19.6 24.3 16.3 13.4 19.5 21.7 18.2 NELL995 1.6 14.9 17.5 5.7 6.0 17.4 11.9 14.9 19.0 12.9 11.1 12.9 16.0 13.0</figDesc><table><row><cell>Dataset</cell><cell>1p</cell><cell>2p</cell><cell>3p</cell><cell>2i</cell><cell>3i</cell><cell>ip</cell><cell>pi</cell><cell>2u</cell><cell>up</cell><cell>2in</cell><cell>3in</cell><cell>inp</cell><cell>pin</cell><cell>pni</cell></row><row><cell>FB15k</cell><cell cols="5">1.7 19.6 24.4 8.0 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Average number of answers of test queries in our new dataset.</figDesc><table><row><cell>Queries</cell><cell>Training</cell><cell></cell><cell cols="2">Validation</cell><cell></cell><cell>Test</cell></row><row><cell>Dataset</cell><cell cols="2">1p/2p/3p/2i/3i 2in/3in/inp/pin/pni</cell><cell>1p</cell><cell>others</cell><cell>1p</cell><cell>others</cell></row><row><cell>FB15k</cell><cell>273,710</cell><cell>27,371</cell><cell cols="4">59,097 8,000 67,016 8,000</cell></row><row><cell>FB15k-237</cell><cell>149,689</cell><cell>14,968</cell><cell cols="4">20,101 5,000 22,812 5,000</cell></row><row><cell>NELL995</cell><cell>107,982</cell><cell>10,798</cell><cell cols="4">16,927 4,000 17,034 4,000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Number of training, validation, and test queries generated for different query structures.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>in https://github.com/hyren/query2box. We finetune the hyperparameters for the three methods including number of embedding dimensions from {200, 400, 800} and the learning rate from {1e ?4 , 5e ?3 , 1e ?3 }, batch size from {128, 256, 512}, and the negative sample size from {32, 64, 128}, the margin ? from {20, 30, 40, 50, 60, 70}. We list the hyperparameters of each model in theTable 8. Additionally, for our BETAE, we finetune the structure of the probabilistic projection operator MLP r and the attention module MLP Att . For both modules, we implement a three-layer MLP with 512 latent dimension and ReLU activation. embedding dim learning rate batch size negative sample size margin Hyperparameters used for each method.</figDesc><table><row><cell>GQE</cell><cell>800</cell><cell>0.0005</cell><cell>512</cell><cell>128</cell><cell>30</cell></row><row><cell>Q2B</cell><cell>400</cell><cell>0.0005</cell><cell>512</cell><cell>128</cell><cell>30</cell></row><row><cell>BETAE</cell><cell>400</cell><cell>0.0005</cell><cell>512</cell><cell>128</cell><cell>60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>FB15k BETAE 52.0 17.0 16.9 43.5 55.3 32.3 19.3 28.1 17.0 16.9 17.4 31.3</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>1p</cell><cell>2p</cell><cell>3p</cell><cell>2i</cell><cell>3i</cell><cell>pi</cell><cell>ip</cell><cell cols="3">2u DNF DM DNF DM up</cell><cell>avg</cell></row><row><cell></cell><cell>Q2B</cell><cell cols="8">52.0 12.7 7.8 40.5 53.4 26.7 16.7 22.0</cell><cell>-</cell><cell>9.4</cell><cell>-</cell><cell>26.8</cell></row><row><cell></cell><cell>GQE</cell><cell cols="2">34.2 8.3</cell><cell cols="6">5.0 23.8 34.9 15.5 11.2 11.5</cell><cell>-</cell><cell>5.6</cell><cell>-</cell><cell>16.6</cell></row><row><cell></cell><cell cols="3">BETAE 28.9 5.5</cell><cell cols="5">4.9 18.3 31.7 14.0 6.7</cell><cell>6.3</cell><cell>6.1</cell><cell>4.6</cell><cell>4.8 13.4</cell></row><row><cell>FB15k-237</cell><cell>Q2B</cell><cell cols="2">28.3 4.1</cell><cell cols="5">3.0 17.5 29.5 12.3 7.1</cell><cell>5.2</cell><cell>-</cell><cell>3.3</cell><cell>-</cell><cell>12.3</cell></row><row><cell></cell><cell>GQE</cell><cell cols="2">22.4 2.8</cell><cell cols="4">2.1 11.7 20.9 8.4</cell><cell>5.7</cell><cell>3.3</cell><cell>-</cell><cell>2.1</cell><cell>-</cell><cell>8.8</cell></row><row><cell></cell><cell cols="3">BETAE 43.5 8.1</cell><cell cols="5">7.0 27.2 36.5 17.4 9.3</cell><cell>6.9</cell><cell>6.0</cell><cell>4.7</cell><cell>4.7 17.8</cell></row><row><cell>NELL995</cell><cell>Q2B</cell><cell cols="2">23.8 8.7</cell><cell cols="5">6.9 20.3 31.5 14.3 10.7</cell><cell>5.0</cell><cell>-</cell><cell>6.0</cell><cell>-</cell><cell>14.1</cell></row><row><cell></cell><cell>GQE</cell><cell cols="2">15.4 6.7</cell><cell cols="5">5.0 14.3 20.4 10.6 9.0</cell><cell>2.9</cell><cell>-</cell><cell>5.0</cell><cell>-</cell><cell>9.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>H@1 results (%) of BETAE, Q2B and GQE on answering EPFO (?, ?, ?) queries. 42.1 37.8 52.9 64.0 41.5 22.9 48.8 26.9 44.6 Q2B 67.1 38.0 27.5 49.2 62.8 36.2 19.2 49.0 28.9 42.0 GQE 54.6 30.5 22.2 37.7 48.4 24.8 14.7 33.8 24.7 32.4 FB15k-237 BETAE 39.1 24.2 20.4 28.1 39.2 19.4 10.6 22.0 17.0 24.4 Q2B 40.3 22.8 17.5 27.5 37.9 18.5 10.5 20.5 17.4 23.6 GQE 35.0 19.0 14.4 22.0 31.2 14.6 8.8 15.0 14.6 19.4 NELL995 BETAE 53.0 27.5 28.1 32.9 45.1 21.8 10.4 38.6 19.6 30.7 Q2B 41.8 22.9 20.8 28.6 41.2 19.9 12.3 26.9 15.5 25.5 GQE 32.8 19.3 17.9 23.1 31.9 16.2 10.3 17.3 13.1 20.2</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>1p</cell><cell>2p</cell><cell>3p</cell><cell>2i</cell><cell>3i</cell><cell>pi</cell><cell>ip</cell><cell>2u</cell><cell>up</cell><cell>avg</cell></row><row><cell></cell><cell cols="2">BETAE 65.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FB15k</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that we do not consider FOL queries with universal quantification (?) in this paper. Queries with universal quantification do not apply in real-world KGs since no entity connects with all the other entities.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">If GQE has embeddings of dimension 2n, then Q2B has embeddings of n since it needs to model both the center and offset of a box, and BETAE also has n beta distributions since each has two parameters, ? and ?.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>We thank Shengjia Zhao, Rex Ying, Jiaxuan You, Weihua Hu, Tailin Wu and Pan Li for discussions, and Rok Sosic for providing feedback on our manuscript. Hongyu Ren is supported by the Masason Foundation Fellowship. Jure Leskovec is a Chan Zuckerberg Biohub investigator. We also gratefully acknowledge the support of DARPA under Nos. </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Proof for Proposition 1</head><p>We restate the proposition 1 and its proof here. Proposition 2. Given the probabilistic logical operators I and N defined in Sec. 4.2, BETAE has the following properties:</p><p>1. Given Beta embedding S, S is a fixed point of N ? N : N (N (S)) = S.</p><p>2. Given Beta embedding S, we have I({S, S, . . . , S}) = S.</p><p>Proof. For the first property, the probabilistic negation operator N takes the reciprocal of the parameters of the input Beta embeddings. If we apply N twice, it naturally equals the input Beta embeddings. For the second property, the probabilistic intersection operator I takes the weighted product of the PDFs of the input Beta embeddings, and according to Eq. 4, the parameters of the output Beta embeddings are linear interpolation of the parameters of the input Beta embeddings. Then we naturally have S = I({S, . . . , S}).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Computation Complexity of DM and DNF</head><p>Here we discuss the computation complexity of representing any given FOL query using the De Morgan's laws (DM) and the disjunctive normal form (DNF). Given a FOL query q, representing q with DNF may in the worst case creates exponential number of atomic formulas. For example, transforming a valid FOL query (q 11 ? q 12 ) ? (q 21 ? q 22 ) ? ? ? ? (q n1 ? q n2 ) leads to exponential explosion, resulting in a query with 2 n number of formulas in the DNF. For DM, since we could always represent a disjunction operation with three negation operation and one conjunction operation:</p><p>, which is a constant. Hence, the DM modeling only scales linearly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Query Generation and Statistics</head><p>Generation of EPFO (with ?, ? and ?) Queries: Following <ref type="bibr" target="#b9">[10]</ref>, we generate the 9 EPFO query structures in a similar manner. Given the three KGs, and its training/validation/test edge splits, which is shown in <ref type="table">Table 5</ref>, we first create G train , G valid , G test as discussed in Sec. 5.1. Then for each query structure, we use pre-order traversal starting from the target node/answer to assign an entity/relation to each node/edge iteratively until we instantiate every anchor nodes (the root of the query structure). After the instantiation of a query, we could perform post-order traversal to achieve the answers of this query. And for validation/test queries, we explicitly filter out ones that do not exist non-trivial answers, i.e., they can be fully answered in G train /G valid . Different from the dataset in <ref type="bibr" target="#b9">[10]</ref>, where the maximum number of test queries may exceed 5,000, we set a bar for the number of answers one query has, and additionally filter out unrealistic queries with more than 100 answers. We list the average number of answers the new test queries have in <ref type="table">Table 6</ref> and the number of training/validation/test queries in <ref type="table">Table 7</ref>. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD international conference on Management of data (SIGMOD)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International World Wide Web Conference</title>
		<meeting>the International World Wide Web Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Toward an architecture for never-ending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Hruschka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quaternion knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Embedding logical queries on knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Query2box: Reasoning over knowledge graphs in vector space using box embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Traversing knowledge graphs in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Chains of reasoning over entities, relations, and text using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>in European Chapter of the Association for Computational Linguistics (EACL</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbors: towards removing the curse of dimensionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirtieth annual ACM symposium on Theory of computing</title>
		<meeting>the thirtieth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive convolution for multi-relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to represent knowledge graphs with gaussian embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transg: A generative model for knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Order-embeddings of images and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to predict denotational probabilities for modeling entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Improved representation learning for predicting commonsense ontologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00549</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Probabilistic embedding of knowledge graphs with box lattice measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Smoothing the geometry of probabilistic box embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Boratko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Word representations via gaussian embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical density order embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Quantum embedding of knowledge for reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ikbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vishwakarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Subramaniam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deeppath: A reinforcement learning method for knowledge graph reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-hop knowledge graph reasoning with reward shaping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">V</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Embedding uncertain knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zaniolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding with iterative guidance from soft rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Jointly embedding knowledge graphs and logical rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Entity context and relational paths for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06757</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Information-geometric set embeddings (igse): From sets to probability distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nielsen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12463</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adversarial attacks on neural networks for graph data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Z?gner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akbarnejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adversarial attack on graph structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
