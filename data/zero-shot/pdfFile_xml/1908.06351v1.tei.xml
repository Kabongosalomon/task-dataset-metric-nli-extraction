<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Anomaly Detection in Video Sequence with Appearance-Motion Correspondence</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trong-Nguyen</forename><surname>Nguyen</surname></persName>
							<email>nguyetn@iro.umontreal.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Montreal</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><forename type="middle">Meunier</forename><surname>Diro</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Montreal</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Anomaly Detection in Video Sequence with Appearance-Motion Correspondence</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Anomaly detection in surveillance videos is currently a challenge because of the diversity of possible events. We propose a deep convolutional neural network (CNN) that addresses this problem by learning a correspondence between common object appearances (e.g. pedestrian, background, tree, etc.) and their associated motions. Our model is designed as a combination of a reconstruction network and an image translation model that share the same encoder. The former sub-network determines the most significant structures that appear in video frames and the latter one attempts to associate motion templates to such structures. The training stage is performed using only videos of normal events and the model is then capable to estimate frame-level scores for an unknown input. The experiments on 6 benchmark datasets demonstrate the competitive performance of the proposed approach with respect to state-ofthe-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>This supplementary material provides these contents:</p><p>? ROC curves of our frame-level scores on the CUHK Avenue and UCSD Ped2 datasets, and Precision-Recall (PR) curves on the traffic datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Anomaly detection in video sequences is a necessary functionality for surveillance systems. Because abnormal events rarely occur in real-world videos, this task is significantly time-consuming and may require a large amount of resource (e.g. people) to perform manual checking. A method than can automatically determine potential frames of anomalous events is thus crucial.</p><p>Our model is a combination of a convolutional autoencoder (Conv-AE) and a U-Net with skip connections <ref type="bibr" target="#b39">[39]</ref> that share the same encoder sub-network. Other related works employed either an AE or a U-Net to perform the anomaly detection in different ways. Hasan et al. <ref type="bibr" target="#b10">[11]</ref> estimate regularity score for frames in video sequences according to reconstruction models. Their two AEs (with and without convolutional layers) work on two different inputs: hand-crafted features (HOG and HOF with trajectory-based properties <ref type="bibr" target="#b49">[49]</ref>) and concatenation of 10 consecutive frames along the temporal axis. The reconstruction error is used to indicate their regularity score. Unlike that work, the input of our Conv-AE is a single frame and the temporal factor is considered in the other stream via U-Net. The purpose of our Conv-AE is to learn only regular appearance structures.</p><p>On the contrary, Ravanbakhsh et al. <ref type="bibr" target="#b36">[37]</ref> employ the U-Net structure proposed in <ref type="bibr" target="#b16">[17]</ref> to translate an input from video frame to a corresponding optical flow and vice versa. We argue that the use of two CNNs with the same structure may be redundant and an appropriate modification and/or combination would improve the model ability. Compared with <ref type="bibr" target="#b36">[37]</ref>, our network keeps the stream translating a video frame to an optical flow (but using our proposed structure instead of <ref type="bibr" target="#b16">[17]</ref>) while replaces the other U-Net by a Conv-AE that shares the encoding flow.</p><p>Inspired by the good performance of the video prediction model in <ref type="bibr" target="#b31">[32]</ref>, Liu et al. <ref type="bibr" target="#b24">[25]</ref> present a model that uses a U-Net structure to predict a frame from a number of recent ones and then estimates the corresponding optical flow. The model is optimized according to the difference between the outputted and original versions of video frame as well as the optical flow together with an adversarial loss. Our work also predicts an optical flow but directly from a single frame in order to determine the association between a scene appearance and its typical motion. Since a fixed procedure of optical flow estimation (FlowNet <ref type="bibr">[8]</ref>) is embedded inside the network in <ref type="bibr" target="#b24">[25]</ref>, the selection of such method is thus limited because the estimator has to be fully differentiable to perform an end-to-end training. Our model, however, has a stream that directly estimates a mapping from input frame to optical flow. We only use a pretrained estimator for ground truth calculation and the model signal does not propagate through it during the training as well as inference stages.</p><p>Our main contributions are summarized as follows:</p><p>? We design a CNN that combines a Conv-AE and a U-Net, in which each stream has its own contribution for the task of detecting anomalous frames. The model can be trained end-to-end.</p><p>? We integrate an Inception module modified from <ref type="bibr" target="#b48">[48]</ref> right after the input layer to reduce the effect of network's depth since this depth is considered as a hyper-parameter that requires a careful selection.</p><p>? We propose a patch-based scheme estimating framelevel normality score that reduces the effect of noise which appears in the model outputs.</p><p>? Experiments on 6 benchmark datasets demonstrate the potential of our model with competitive performance compared with state-of-the-art methods. We also provide discussions for these datasets that should be useful for future works.</p><p>The remainder of this paper is organized as follows: a summary of related studies is given in Section 2; Section 3 describes the details of our method; experiments and discussions for the 6 benchmark datasets are presented in Section 4; and Section 5 concludes this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>We briefly describe the principal categories that lead to very different approaches for anomaly detection in video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Trajectory</head><p>The diversity of possible anomalous events is the main challenge of the anomaly detection problem. Some researchers simplify this issue by explicitly specifying anomalies (e.g. <ref type="bibr" target="#b45">[45]</ref>) or particular relevant attributes that can be used effectively for anomaly detection, in which the most common one is motion trajectory. These studies aim to learn patterns of object trajectories determined from normal events <ref type="bibr" target="#b33">[34,</ref><ref type="bibr">3,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b53">53]</ref>. There are four main stages in the methodology including object detection, tracking, trajectory-based feature extraction and classification/detection. The advantages of methods in this category are the simple implementation and fast execution. However, their effectiveness may significantly degrade when working on videos with cluttered background since the trajectory determination depends on the result of object detection and tracking. Moreover, trajectory anomalies do not cover the whole spectrum of anomalies in video surveillance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Sparse coding</head><p>Instead of explicitly defining and estimating specific anomaly attributes, other researchers consider an input sequence of frames as a collection of small 3D patches. Concretely, a number of consecutive frames are concatenated along the temporal axis and then split into same-size 3D patches according to a window sliding on the image plane. In the inference stage, each 3D patch extracted from unknown inputs is represented as a sparse combination of training samples of normal events. The reconstruction error is considered as the score supporting the final decision. Such sparsity-based methods have achieved state-of-the-art performances <ref type="bibr">[6,</ref><ref type="bibr" target="#b55">55]</ref>. The main drawback is the high computational cost in finding combination coefficients due to sparse representation. Some studies thus attempt to reduce the complexity by modifying the learning algorithms and/or data structures <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28]</ref>. Beside window-based split, 3D patches are also determined using keypoint detectors <ref type="bibr">[5]</ref> while other researchers attempt to learn the relation between training patches according to their distribution <ref type="bibr" target="#b29">[30]</ref> or graph-based representation <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Deep learning</head><p>Since deep learning models currently achieve top performance in a wide range of vision applications such as image classification <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b12">13]</ref>, object detection <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b11">12]</ref> and image captioning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, many CNNs have been proposed to deal with the problem of anomaly detection in videos. Typical structures of image reconstruction and translation are usually employed and the difference between their output and ground truth is used to indicate the frame-level score <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b24">25]</ref>. Some researchers apply pretrained classification models (such as VGG <ref type="bibr" target="#b41">[41]</ref>) to extract useful features from input videos <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b15">16]</ref>. Results of object detection and/or foreground estimation are also used for the determination of anomalous events in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b51">51]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed method</head><p>An overview of our model is visualized in <ref type="figure" target="#fig_7">Figure 1</ref>. The model includes two processing streams. The first one is performed via a Conv-AE to learn common appearance spatial structures in normal events. The second stream is to determine an association between each input pattern and its corresponding motion represented by an optical flow of 3 channels (xy displacements and magnitude). The skip connections in U-Net are useful for image translation since it directly transforms low-level features (e.g. edge, image patch) from original domains to the decoded ones. Such connections are not employed in the appearance stream because the network may let the input information go through these connections instead of emphasizing underlying attributes via the bottleneck.</p><p>Our model does not use any fully-connected layer, so it can theoretically work on images of any resolution. In order to simplify the model as well as make it be appropriate for possible further extensions, we fixed the size of input layer as 128 ? 192 ? 3. The image size is set to a ratio of 1:1.5 instead of 1:1 as in related works (e.g. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b24">25]</ref>) in order to preserve the aspect of objects in surveillance videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Inception module</head><p>The Inception module was originally proposed to let a CNN decide its filter size (in a few layers) automatically <ref type="bibr" target="#b47">[47]</ref>. A number of convolutional operations with various filter resolutions are performed in parallel and the ob-   <ref type="figure" target="#fig_7">Figure 1</ref>. Overview of our model structure together with the spatial resolution of feature maps in each block (i.e. a sequence of layers with the same output shape). The number of channels corresponding to each layer in each block is also presented (in parentheses). The input and two output layers have the same size of 128?192?3. There are three clusters of layers: common encoder (left), appearance decoder (top right) and motion decoder (bottom right). Each concatenation is performed along the channel axis right before operating the next deconvolution. The model input is a single video frame It and the outputs from the two decoders are a reconstructed frame?t and an optical flowFt predicting the motion between It and It+1. Best viewed in color.</p><formula xml:id="formula_0">I t? t F t</formula><p>tained feature maps are then concatenated along the channel axis. The use of this module in our work can be explained under an alternative perspective as follows. The proposed network has an encoder-decoder structure with bottleneck. A very deep architecture may eliminate the features that are helpful for decoding. On the contrary, a shallow network takes the risk of missing high-level abstractions. Therefore, we apply an Inception module to let the model select its appropriate convolutional operations.</p><p>This work focuses on surveillance videos acquired from a fixed position. Given a convolutional layer with a predefined receptive field (i.e. filter size) right after the input layer, the information abstraction would be different for the same object captured at various distances. This property is propagated for next layers, we thus expect the model to early determine low-level features by putting the Inception module right after the input layer. We remove the maxpooling in this module since the input is a regular video frame instead of a collection of feature maps. Our Inception module is modified from <ref type="bibr" target="#b48">[48]</ref> including 4 streams of convolutions of filter sizes 1 ? 1, 3 ? 3, 5 ? 5 and 7 ? 7. Each convolutional layer of filter larger than 1 ? 1 is factorized into a sequence of layers with smaller receptive fields in order to reduce the computational cost as suggested in <ref type="bibr" target="#b48">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Appearance convolutional autoencoder</head><p>Our Conv-AE supports the detection of strange (abnormal) objects within input frames by learning common appearance templates in normal events. This sub-network consists of the encoder and the top decoder without any skip connection as shown in <ref type="figure" target="#fig_7">Figure 1</ref>. The encoder is constructed by a sequence of blocks including triple layers: convolution, batch-normalization (BatchNorm) and leaky-ReLU activation <ref type="bibr" target="#b28">[29]</ref>. The first block (right after the Inception module) does not contain BatchNorm layer as suggested in <ref type="bibr" target="#b16">[17]</ref> for our U-Net task in Section 3.3. Instead of using pooling layer to reduce the resolution of feature maps, we apply strided convolution. Such parametric operation is expected to support the network finding an informative way to downsample the spatial resolution of feature maps as well as learning the further upsampling in decoding stage <ref type="bibr" target="#b43">[43]</ref>.</p><p>The decoder is also a sequence of layer blocks that increases the spatial resolution while reduces the number of feature maps after each deconvolution layer. A dropout layer (with p drop = 0.3) is attached before the ReLU activation in each block as a regularization that reduces the risk of overfitting during the training stage <ref type="bibr" target="#b44">[44]</ref>.</p><p>Since the Conv-AE is to learn common appearance patterns of normal events, we consider the l 2 distance between the input image I and its reconstruction?. The model thus forces to produce an image with similar intensity for each pixel. The intensity loss is estimated as</p><formula xml:id="formula_1">L int (I,?) = I ?? 2 2 (1)</formula><p>A drawback of using only l 2 loss is the blur in the output, we thus add a constraint that attempts to preserve the original gradient (i.e. the sharpness) in the reconstructed image. The gradient loss is defined as the difference between absolute gradients along the two spatial dimensions as</p><formula xml:id="formula_2">L grad (I,?) = d?{x,y} g d (I) ? g d (?) 1 (2)</formula><p>where g d denotes the image gradient along the d-axis. The final loss function of the appearance Conv-AE is formed as a summation of the intensity and gradient losses.</p><formula xml:id="formula_3">L appe (I,?) = L int (I,?) + L grad (I,?)<label>(3)</label></formula><p>This loss combination has been reported to give good performance for the task of video prediction <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b24">25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Motion prediction U-Net</head><p>Beside the appearance of strange object structures, unusual motions of typical objects would also be appropriate to provide an assessment of a video frame. Recall that each block in the encoder is to emphasize spatial abstractions of common objects within training frames. Our U-Net sub-network thus focuses on learning the association between such patterns and corresponding motions. The ground truth optical flow employed in this work is estimated by a pretrained FlowNet2 <ref type="bibr" target="#b14">[15]</ref>. Compared with related models, the optical flow outputted from FlowNet2 is not only much smoother but also preserves motion discontinuities with sharper boundaries. The motion stream is expected to associate typical motions to common appearance objects while ignoring the static background patterns.</p><p>The decoder of our U-Net has the same structure as the Conv-AE except for the skip connections. These concatenations are to combine the feature maps upsampled from a higher level of abstraction with the ones containing lowlevel details. The use of leaky-ReLU activation in the encoder also keeps weak responses that may be informative for the translation in the decoder.</p><p>Unlike the Conv-AE in Section 3.2, the loss between an outputted optical flow and its ground truth is measured by l 1 distance. There are two main reasons for this. First, the FlowNet2 model is formed as a fusion of multiple networks providing optical flows from coarse (noisy) to fine (smooth), the result might thus contain noise or even amplify noisy regions during the smoothing procedure. Second, because the selection of optical flow estimation is not limited to FlowNet2, the training ground truth obtained from other algorithms might therefore possibly have small patches of wrong and/or noisy motion measure. In order to reduce the effect of such outliers when learning the motion association, we apply l 1 distance loss</p><formula xml:id="formula_4">L f low F t ,F t = F t ?F t 1<label>(4)</label></formula><p>where F t is the ground truth optical flow estimated from two consecutive frames I t and I t+1 , andF t is the output of our U-Net given I t . In summary, this stream attempts to predict instant motions of objects appearing in the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Additional motion-related objective function</head><p>Beside the distance-based loss L f low , we also add another loss that penalizes the underlying distribution of predicted optical flow to be similar to ground truth. The generative adversarial network (GAN) <ref type="bibr">[10]</ref> was originally introduced to allow a CNN learning an implicit distribution of patterns. The model consists of a generator that creates fake samples from noise and a discriminator that attempts to distinguish such outputs from the real patterns. Many modified GAN versions have been proposed for the task of data generation. The discriminator also plays the role of a regularization in many models. Inspired by <ref type="bibr" target="#b31">[32]</ref> where using a GAN loss is reported to provide better results compared with employing only distance-based ones, we apply such strategy as an additional objective function.</p><p>Our generator is the entire network in <ref type="figure" target="#fig_7">Figure 1</ref> while the discriminator conditionally performs the classification on predicted optical flow. A visualization of our discriminator Binary classification <ref type="figure" target="#fig_8">Figure 2</ref>. The architecture of our discriminator. The input layer of shape 128 ? 192 ? 6 is fed by the concatenation of a video frame and its optical flow (that is either ground truth or outputted from the U-Net). The output layer is sigmoid activation of 512 feature maps of spatial resolution 16 ? 24. Best viewed in color.</p><p>architecture is shown in <ref type="figure" target="#fig_8">Figure 2</ref>. Notice that the discriminator is not employed in the inference stage. Although the recent study <ref type="bibr" target="#b24">[25]</ref> employed a Least Square GAN <ref type="bibr" target="#b30">[31]</ref> and achieved state-of-the-art performance in detecting anomalous video frames, our model follows the strategy of typical conditional GAN (cGAN) where both the ground truth video frame and its corresponding optical flow are fed into the discriminator. There are two reasons leading to this decision. First, the cGAN theoretically avoids the problem of mode collapse in vanilla GAN since ground truth information (i.e. labels, real samples) is fed into the discriminator. The model is thus expected to efficiently learn the distribution of training samples. Second, cGAN is appropriate for a CNN of image translation as demonstrated in <ref type="bibr" target="#b16">[17]</ref>. Finally, the adversarial loss is directly computed on the last layer containing activated feature maps in the discriminator. This calculation is different from <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25]</ref> where a convolutional layer is employed to collapse previous feature channels into a 2D map. The common sense of our model and the two others is the structural penalization where the classification is performed according to image patches instead of the whole image. However, we strictly constrain patches at feature-level so that each feature map must attempt to provide a classification result. This design is inspired from the study <ref type="bibr">[4]</ref> demonstrating that each convolutional channel attends to particular semantic patterns.</p><p>Given an input video frame I and its associated optical flow F obtained from FlowNet2, the proposed network in <ref type="figure" target="#fig_7">Figure 1</ref> (the generator denoted as G) produces a reconstructed frame? and a predicted optical flowF , while the discriminator D estimates a probability that the optical flow associated to I is the ground truth F . The GAN objective function consists of two loss functions:</p><formula xml:id="formula_5">L D (I, F,F ) = 1 2 x,y,c ?logD(I, F ) x,y,c + 1 2 x,y,c ?log[1 ? D(I,F ) x,y,c ]<label>(5)</label></formula><formula xml:id="formula_6">L G (I,?, F,F ) = ? G x,y,c ?logD(I,F ) x,y,c + ? a L appe (I,?) + ? f L f low (F,F )<label>(6)</label></formula><p>where x, y and c respectively indicate the spatial position and the corresponding channel of a unit in the feature maps outputted from D, and ? values are the weights associated to partial losses within our proposed model. Our GAN is optimized by alternately minimizing the two GAN losses.</p><p>In our experiments (see Section 4), we assigned 0.25 for ? G , 1 for ? a and 2 for ? f . This GAN aims to emphasize the efficiency of motion prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Anomaly detection</head><p>Our model aims to provide a score of normality for each frame. In related studies, such scores are usually quantities measuring the similarity between a ground truth and the reconstructed/predicted output. There are two common scores employed in CNN approaches: L p distance and Peak Signal to Noise Ratio (PSNR). The normality of each video frame is decided by comparing its score with a threshold. It is obvious that an anomalous event occurring within a small image region may be missed due to the summation and/or average operations over all pixel positions. We hence propose another score estimation scheme considering only a small patch instead of the entire frame.</p><p>First, we define partial scores individually estimated on the two model streams sharing the same patch position as</p><formula xml:id="formula_7">S I (P ) = 1 |P | i,j?P (I i,j ?? i,j ) 2 S F (P ) = 1 |P | i,j?P (F i,j ?F i,j ) 2<label>(7)</label></formula><p>where P indicates an image patch and |P | is its number of pixels. Our frame-level score is then computed as a weighted combination of the two partial scores as follows:</p><formula xml:id="formula_8">S = log[w F S F (P )] + ? S log[w I S I (P )]<label>(8)</label></formula><p>where w F and w I are the weights calculated according to the training data, ? S is to control the contribution of partial scores to the summation, andP is the patch providing the highest value of S F in the considering frame, i.e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P ? argmax</head><p>P slides on frame</p><formula xml:id="formula_9">S F (P )<label>(9)</label></formula><p>The weights w F and w I are estimated as the inverse of average scores obtained on the training data of n images:</p><formula xml:id="formula_10">? ? ? ? ? ? ? w F = 1 n n i=1 S Fi (P i ) ?1 w I = 1 n n i=1 S Ii (P i ) ?1<label>(10)</label></formula><p>This helps to normalize the two scores on the same scale. The size of P was set to 16 ? 16 in our experiments. Typically, such patches are determined by a sliding window. In realistic implementation, it can be performed using a convolutional operation with a filter of size 16 ? 16. ? S was empirically set to 0.2 since the model focuses on motion prediction efficiency. Finally, we perform a normalization on frame-level scores in each evaluated video as suggested in related studies such as <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b24">25]</ref>. Our final frame-level score i?</p><formula xml:id="formula_11">S t = S t max(S 1..m )<label>(11)</label></formula><p>where t is the frame index in a video containing m frames. The score estimated from a frame of abnormal event is expected to be higher compared with the ones of normal event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We performed experiments on various benchmark datasets of anomaly detection including CUHK Avenue <ref type="bibr" target="#b25">[26]</ref>, UCSD Ped2 <ref type="bibr" target="#b23">[24]</ref>, Subway Entrance Gate and Exit Gate <ref type="bibr">[1]</ref>, Traffic-Belleview and Traffic-Train <ref type="bibr" target="#b52">[52]</ref>. Their training data contain only normal events. Some examples of normal and abnormal frames in the first 4 datasets are shown in <ref type="figure" target="#fig_1">Figure 3</ref>. The first two datasets are provided with frame-level ground truth, we thus employ area under curve (AUC) of the receiver operating characteristic (ROC) curve measured according to frame-level scores outputted from the proposed model to indicate the performance. The next two Subway datasets are evaluated on event-level that requires some additional operations described below. The last two datasets are evaluated according to the average precision (AP) since the precision-recall (PR) curve was usually used for their assessment <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b51">51]</ref>. We used the FlowNet2 pretrained on FlyingThing3D <ref type="bibr" target="#b32">[33]</ref> and ChairsS-DHom <ref type="bibr" target="#b14">[15]</ref> datasets as the ground truth optical flow estimator. The GAN was trained using Adam algorithm <ref type="bibr" target="#b20">[21]</ref> where the initial learning rates were set to 2 ? 10 ?4 for the generator G and 2 ? 10 ?5 for the discriminator D. The description, experimental results and a discussion corresponding to each evaluation are presented in the remaining of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Avenue</head><p>Ped2 Conv-AE <ref type="bibr" target="#b10">[11]</ref> 0.702 0.900 Discriminative learning <ref type="bibr">[7]</ref> 0.783 -Hashing filters <ref type="bibr" target="#b54">[54]</ref> -0.910 Unmask late fusion <ref type="bibr" target="#b15">[16]</ref> 0.806 0.822 AMDN (double fusion) <ref type="bibr" target="#b51">[51]</ref> -0.908 ConvLSTM-AE <ref type="bibr" target="#b26">[27]</ref> 0.770 0.881 DeepAppearance <ref type="bibr" target="#b42">[42]</ref> 0.846 -FRCN action <ref type="bibr" target="#b13">[14]</ref> -0.922 TSC <ref type="bibr" target="#b27">[28]</ref> 0.806 0.910 Stacked RNN <ref type="bibr" target="#b27">[28]</ref> 0.817 0.922 AbnormalGAN <ref type="bibr" target="#b36">[37]</ref> -0.935 GrowingGas <ref type="bibr" target="#b46">[46]</ref> -0.941 Future frame prediction <ref type="bibr" target="#b24">[25]</ref> 0.851 0.954 Our proposed method 0.869 0.962 <ref type="table" target="#tab_5">Table 1</ref>. Frame-level performance (AUC) of anomaly detection on the CUHK Avenue and UCSD Ped2 datasets. The methods are ordered according to the year of publication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">CUHK Avenue and UCSD Ped2</head><p>The Avenue dataset consists of 30652 frames that are split into 16 clips for training and 21 clips for testing. This dataset was captured in a campus avenue and contains various types of anomaly such as unusual action (e.g. running), wrong moving direction and abnormal object (e.g. bicycle). This also provides some challenges for evaluation such as slight camera shake and the occurrence of a few outliers.</p><p>The UCSD anomaly dataset includes two subsets Ped1 and Ped2 acquired from static cameras overlooking pedestrian walkways. The anomalies are the appearance of nonpedestrian object (e.g. vehicle) and strange pedestrian motion. The difference between the two subsets is the walking direction (toward and away from the camera in Ped1, parallel to the camera plane in Ped2). We select only the Ped2 dataset for two reasons. First, our optical flow estimator (FlowNet2) does not work well on very small and thin pedestrians appearing too far from the camera. Nevertheless, examples of people walking towards and away from the camera are available in the CUHK Avenue dataset allowing to evaluate performance in this situation. Second, we observed that some events were labeled as normality in the training data but were considered as anomalous in the test data (e.g. people walking on grass). Therefore, the Ped2 dataset (16 training and 12 testing clips) was used in our experiments.</p><p>The frame-level assessment results in <ref type="table" target="#tab_5">Table 1</ref> show that our model outperforms all other recent methods in the task of anomaly detection. Examples of reconstructed frames and predicted optical flows obtained from the appearance and motion streams are given in <ref type="figure" target="#fig_3">Figure 4</ref>. Considering the first example, the truck was reconstructed as a collection  of pedestrian patterns since it is a new object observed by the model. The corresponding predicted motion was thus completely different from the ground truth. The processing of the bicycle on the right image edge was also similar. The second scene shows that the model still worked well on a crowded scene with many pedestrians and an anomalous  object having similar intensities with the background. In the next two Avenue frames, the model expected slower moving speed and another motion direction as observed in the training data. In addition, notice that the reconstructed man's trouser color was slightly different from the input frame while the back ground was well restored. This demonstrates that the model reasonably determined the low-significance relation between the color of a pattern and its movement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Subway Entrance and Exit gates</head><p>This dataset contains videos capturing the entrance gate and exit gate of a subway station. Their lengths are respectively 96 and 43 minutes. The anomalous events in these two videos are wrong direction (e.g. passenger exits through the entrance gate), no payment, loitering, irregular interaction (e.g. a person walks awkwardly to avoid another) and miscellaneous (e.g. sudden changing of walking speed).</p><p>We performed the evaluation according to the ground truth of events with the training and test sets provided in <ref type="bibr" target="#b19">[20]</ref>, in which the normal events in the first 15 minutes of the Entrance Gate video and 5 minutes of the Exit Gate were used in training stage. Notice that the experiments were performed individually for the two videos.</p><p>Since the dataset does not provide the frame-level ground truth, we employ the assessment scheme in <ref type="bibr" target="#b10">[11]</ref> to determine anomalous events in the experiments. In detail, the persistence algorithm <ref type="bibr" target="#b21">[22]</ref> is applied on the sequence of scores to locate local maxima, in which each maximum point indicates an anomalous event. In order to reduce the effect of possible noisy detected extrema, nearby events are combined to provide only an anomalous one.</p><p>Our event-based assessment results are presented in Table 2. It shows that our model detected most anomalous events but also generated more false alarm than other recent studies. By taking a closer look at these false alarms, we determined that some events denoted as normal in the test set can be considered as anomaly under other circumstances. A visualization of some false alarms and missed anomaly detections in the Entrance dataset is given in <ref type="figure" target="#fig_4">Figure 5</ref>. <ref type="figure" target="#fig_4">Figure 5</ref> shows that the normality decision of movement stopping and loitering was unstable since the cases (a)-(e) were missed while (f)-(h) were wrongly detected. There are two possible reasons: (1) the use of maximum localization as in <ref type="bibr" target="#b10">[11]</ref> is not ideal when the anomaly score smoothly and/or slowly changes, and (2) the training set (according to <ref type="bibr" target="#b19">[20]</ref>) contains loitering event [caused by the man in (b) and (e)]. The ambiguity in ground truth annotation is also shown in the event (h) where a loitering man appeared on the right side but was not labeled as anomaly. In the event (i), the model predicted that the man would go through the left gate but he suddenly changed to the right one (the color indicates the motion direction). Since this action does not occur in the training data, the model determined it as an anomalous event. Regarding the last example (j), the motion stream expected the passenger to go to the train because most people at this location move to the left side in the training data. In other words, the model may forget training patterns moving to the right side. In this case, using sparse coding approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b25">26]</ref> can be appropriate since the effect of the frequency of training patterns is reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Traffic-Belleview and Traffic-Train</head><p>The Traffic-Belleview dataset was acquired by a surveillance camera looking at the traffic on a road intersection from a high viewpoint. In the training data (300 frames), vehicles only run on the main street. The appearance and movement of vehicles from/to left or right roads is defined as anomaly in the test set containing a total of 2618 frames. The video is gray-scale and has a low quality.</p><p>Unlike the previous benchmark datasets, the Traffic-Train can be considered as the most challenging dataset since the lighting conditions vary drastically together with camera jitter. The camera was mounted in a train and people movement is defined as anomaly. The training and test sets consist of 800 and 4160 frames, respectively.</p><p>Our average precision of frame-level assessment is presented in <ref type="table">Table 3</ref>. <ref type="figure" target="#fig_6">Figure 6</ref> shows examples of problems that the model encountered when dealing with the traffic datasets as well as illustrates the change of lighting conditions in the Train dataset. In <ref type="figure" target="#fig_6">Figure 6(b)</ref>, the predicted motion was very noisy and the passenger at the frame center was missed in the error map. The effect of optical flow estimator is illustrated in <ref type="figure" target="#fig_6">Figure 6</ref>(c) where two cars were combined to be a big blob. This bad estimation significantly affected the error map though the three cars running on other way were correctly determined. The results may thus be im- Method Belleview Train GANomaly <ref type="bibr">[2]</ref> 0.735 0.194 AEs + local feature <ref type="bibr" target="#b34">[35]</ref> 0.748 0.171 AEs + global feature <ref type="bibr" target="#b34">[35]</ref> 0.776 0.216 ALOCC D(X) <ref type="bibr" target="#b40">[40]</ref> 0.734 0.182 ALOCC D(R(X)) <ref type="bibr" target="#b40">[40]</ref> 0.805 0.237 Our proposed method 0.751 0.490 SSIM on appearance stream 0.830 0.798 <ref type="table">Table 3</ref>. The average precision of frame-level anomaly detection on the Traffic-Belleview and Traffic-Train datasets.</p><formula xml:id="formula_12">(f) (g) (h) (i) (j) (a) (b) (c) (d) (e)</formula><p>proved by choosing another optical flow estimator or tuning the pretrained FlowNet2 by a more appropriate dataset.</p><p>As an attempt to reduce the effect of such factors, we estimated another frame-level score without the support of motion as in section 3.5. Concretely, we used the Structural Similarity Index (SSIM) <ref type="bibr" target="#b50">[50]</ref> to compute the similarity between an input frame and its reconstruction provided by the appearance stream. Compared with other common measures such as MSE or PSNR, SSIM can work well on jitter images where pixel by pixel comparison is not appropriate. <ref type="table">Table 3</ref> shows that this modification improved the anomaly detection results, especially with the Train dataset.</p><p>Further details including ROC and PR curves, visualization of some feature maps and evaluation results of each single stream are provided in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presents an anomaly detection approach that exploits the correspondence between pattern appearances and their motions. The model is designed as a combination of two streams. The first one attempts to reconstruct the appearance according to its auto-encoder architecture while the second stream uses a U-Net structure to predict the in-  stant motion given an input video frame. By sharing the same encoder, the model is forced to learn the correspondence. A patch-based scheme of anomaly score estimation is proposed to reduce the effect of noise in model outputs. Experiments on 6 benchmark datasets demonstrated the potential of our method. Detailed discussions are also presented to provide improvement suggestions for further works.</p><p>? Experimental results of using either appearance reconstruction stream or motion prediction stream for score estimation.</p><p>? Impact of integrating motion stream and patch-based score estimation.</p><p>? Visualization of some feature maps in different blocks obtained in our experiments.</p><p>? Reconstructed frames and predicted motions after some training epochs. <ref type="figure" target="#fig_7">Figure 1</ref> shows the color coding used in visualization of our optical flow in the main paper. This color coding is similar to <ref type="bibr">[5]</ref> where the color indicates motion direction and the saturation corresponds to the pixel displacement.    <ref type="bibr">[4]</ref>, hashing filters <ref type="bibr">[10]</ref>, AMDN double fusion <ref type="bibr">[9]</ref>, sparse dictionary <ref type="bibr">[6]</ref>, discriminative learning <ref type="bibr">[3]</ref>, GANomaly <ref type="bibr">[1]</ref>, autoencoder with global features <ref type="bibr">[7]</ref> and ALOCC <ref type="bibr">[8]</ref>. The ROC curves of the first 5 mentioned studies are provided in their original papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Flow field color coding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Evaluation curves on 4 datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental results on single streams</head><p>As indicated in the main paper, our frame-level score is estimated as a weighted combination of two partial scores</p><formula xml:id="formula_13">S = log[w F S F (P )] + ? S log[w I S I (P )]<label>(1)</label></formula><p>where S F (P ) and S I (P ) are respectively partial scores calculated from the motion and appearance streams, w F and w I are corresponding weights computed from the training data, ? S is a hyperparameter controlling the contribution of partial scores to the summation, andP is the patch providing the highest value of S F in the considering frame.</p><p>In this section, we present the evaluation results in the cases of using only one of the two partial scores as the frame-level score indicator (see <ref type="figure" target="#fig_1">Figure 3</ref>). Both AUC and average precision (AP) measures are also provided for a convenient comparison with other studies. Note that the AUC and AP values are not comparable though there is a connection between ROC and PR spaces, and they are both affected by the balance of the two classes in each dataset <ref type="bibr">[2]</ref>. <ref type="figure" target="#fig_1">Figure 3</ref> shows that the combination of the two partial scores improved the detection ability since its AUC and AP increased compared with individual measures. For the Subway datasets, this combination reduced the risk of false detection, but the number of detected anomalous events was also slightly decreased (Subway Entrance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Impact of motion stream and patch-based</head><p>score estimation for anomaly detection <ref type="table" target="#tab_5">Table 1</ref> shows the experimental results obtained on the 6 benchmark datasets using patch-based normality assessment and SSIM on appearance stream. We also remove the motion stream and the motion-oriented discriminator (Sections 3.3 &amp; 3.4 in main paper) for the assessment of motion impact. SSIM was suggested due to the errors in op-   Using motion significantly improved results of the first 4 datasets while SSIM on appearance stream was just slightly reduced for the others (i.e. 0.830 vs. 0.832 for Belleview, and 0.798 vs. 0.808 for Traffic-Train).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Feature maps</head><p>A visualization of some feature maps given an input frame for each dataset is shown in <ref type="figure" target="#fig_3">Figure 4</ref>. Each example is represented by 4 rows of images. We illustrate two feature maps (grouped in a red bounding box) for each layer block, except for the Inception module where 4 feature maps are  shown for the 1 ? 1, 3 ? 3, 5 ? 5 and 7 ? 7 convolutional filters. The first two rows include the input frame, activation maps resulting from the Inception module and subsequent blocks of the shared encoder. The third and fourth rows respectively consist of feature maps in the decoder of motion and appearance streams. The value of units in each map was normalized to provide a good visualization. <ref type="figure" target="#fig_3">Figure 4</ref> shows that our motion stream attempts to emphasize the image edges to provide a smooth optical flow (because FlowNet2 <ref type="bibr">[5]</ref> was used as the ground truth motion estimator) while the other one tends to reconstruct appearance textures. By observing all feature maps provided by the Inception module, we found that 7 ? 7 convolutional filters extracted informative details only on the CUHK Avenue, Subway Entrance and Traffic-Belleview datasets (best viewed when the feature map is enlarged). It demonstrated the reasonable use of Inception module right after the input layer to let the network automatically decides its appropriate low-level filter sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Model optimization during training phase</head><p>In this section, we show the outputs of the proposed model after some training epochs given the same input for each dataset. The number of training epochs and batch size are presented in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>In <ref type="figure" target="#fig_4">Figure 5</ref>, the correspondence between a reconstructed frame and its predicted motion can be clearly observed. A sharper frame would be obtained together with a motion with more details (e.g. epochs 2 vs. 4 in the UCSD Ped2 experiment) as the number of epochs increases. It also demonstrates that the model encountered difficulty in optimizing the two streams on the Traffic-Train dataset due to the sud-den change of lighting and camera jitter. However, the overall structure of the acquired scene was still preserved (e.g. poles and seats). The use of SSIM on the input frame and its reconstruction hence improved the anomaly detection results (presented in the main paper).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>128</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Examples of normal (top) and abnormal (bottom) frames in the CUHK Avenue, UCSD Ped2, Exit Gate, and Entrance Gate (from left to right) datasets. Anomalous events are highlighted including a man picking a bag, bicycle appearance, and loitering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) The appearance of a truck and a bicycle. (Ped2) (b) A bicycle is running in a low contrast region. (Ped2) (c) A man is running. (Avenue) (d) A man is tossing papers. (Avenue)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>(Best viewed in color) Results on the Ped2 and Avenue datasets. Each example consists of 3 image columns that are input frame and its optical flow (left), reconstructed frame and predicted motion (middle), and the frame superimposed by the motion error map below (right). The flow field color coding is the same as<ref type="bibr" target="#b14">[15]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Examples of missed detections (a)-(e) and false alarms (f)-(j) in our experiments on the Entrance dataset. Each example consists of 4 images that are (from top to bottom) the input frame, ground truth optical flow, predicted motion and the corresponding motion error map. The missed detections are: (a)-(c) movement stopping, (d) loitering, and (e) loitering (man) and movement stopping (woman). The false alarms are: (f)-(g) movement stopping, (h) loitering, (i) changing gate, and (j) passenger going near the railway. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) The change of lighting in the Traffic-Train dataset. (b) Passengers moving in the stopping train.(c) Cars turning to the left way.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>(Best viewed in color) Some testing results on the two traffic datasets. Each example consists of 6 images as inFigure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 1 :</head><label>1</label><figDesc>The color coding used for visualizing our optical flow in the main paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 2 :</head><label>2</label><figDesc>method, AUC = 0.962 FRCN action, AUC = 0.922 Hashing filters, AUC = 0.910 Double fusion, AUC = 0method, AUC = 0.869 Sparse dict., AUC = 0.809 Disc. learning, AUC = 0Top: ROC curves on the Ped2 and Avenue datasets. Bottom: PR curves on the Belleview and Train datasets. The corresponding Area Under Curve (AUC) and Average Precision (AP) are also provided. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 2 displays</head><label>2</label><figDesc>ROC and PR curves of our frame-level scores obtained in the experiments. Some state-of-the-art methods are also added into the figure to provide a visual comparison. These methods consist of FRCN action</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 3 :</head><label>3</label><figDesc>Evaluation results of our model using only the appearance reconstruction (Conv-AE), the motion prediction (U-Net) and their combination. The frame-level AUROC and Average Precision scores are provided for the Ped2, Avenue, Belleview and Train datasets. The numbers of true positive detections (i.e. true positive) and false alarms are presented for the Entrance and Exit datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Our results of anomaly detection on the Subway datasets. In the ground truth, the numbers of abnormal events in the Entrance and Exit are respectively 66 and 19. The term TP indicates the number of true positive detections while FA is the counting of false alarms. The methods are listed in temporal order.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Experimental results using patch-based normality assessment and SSIM on appearance stream. tical flow measurement (camera jitter in Traffic-Train and low-quality frames in Belleview). Without motion stream, the model becomes a reconstruction auto-encoder of single frame, and the results on the first 5 datasets still demonstrate the efficiency of the proposed patch-based normality score.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 :</head><label>2</label><figDesc>Number of training epochs and batch size in our experiments. These values were selected according to the number of training images in each dataset and the memory capacity of our hardware (Intel i7-7700K, 16 GB memory, GTX 1080).</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust real-time unusual event detection using multiple fixed-location monitors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilan</forename><surname>Shimshoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Reinitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="555" to="560" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ganomaly: Semisupervised anomaly detection via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samet</forename><surname>Akcay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ACCV 2018</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning object motion patterns for anomaly detection and improved object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslan</forename><surname>Basharat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Gritai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Video anomaly detection and localization using hierarchical feature representation and gaussian process regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yie-Tarng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Hsien</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="2909" to="2917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sparse reconstruction cost for abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="3449" to="3456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A discriminative framework for anomaly detection in large videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allison</forename><surname>Del Giorno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="334" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sparse subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="2790" to="2797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmudul</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint detection and recounting of abnormal events by learning deep generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Hinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin&amp;apos;ichi</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="3639" to="3647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unmasking the abnormal events in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorina</forename><surname>Radu Tudor Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="2914" to="2922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="5967" to="5976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Densecap: Fully convolutional localization networks for dense captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="4565" to="4574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="664" to="676" />
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Observe locally, infer globally: A space-time mrf for detecting abnormal activities with incremental updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaechul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="2921" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Extracting and filtering minima and maxima of 1d functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeara</forename><surname>Kozlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tino</forename><surname>Weinkauf</surname></persName>
		</author>
		<ptr target="https://www.csc.kth.se/?weinkauf/notes/persistence1d.html." />
		<imprint>
			<date type="published" when="2019-02-15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Anomaly detection and localization in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="32" />
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Future frame prediction for anomaly detection a new baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Abnormal event detection at 150 fps in matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013-12" />
			<biblScope unit="page" from="2720" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Remembering history with convolutional lstm for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="439" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A revisit of sparse coding based anomaly detection in stacked rnn framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Deep Learning for Audio, Speech and Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Viral Bhalodia, and Nuno Vasconcelos. Anomaly detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="1975" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="2813" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1511.05440</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Somboon Hongeng, and Ramakant Nevatia. Event detection and analysis from video streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rard</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Bremond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="873" to="889" />
			<date type="published" when="2001-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dynamic video anomaly detection and localization using sparse denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medhini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sowmya</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="13173" to="13195" />
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Trajectory-based anomalous event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Piciarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Micheloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gian</forename><forename type="middle">Luca</forename><surname>Foresti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2008-11" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1544" to="1554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Abnormal event detection in videos using generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdyar</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucio</forename><surname>Marcenaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Regazzoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="1577" to="1581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>C. Cortes, N. D. Lawrence, D. D</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015</title>
		<editor>Nassir Navab, Joachim Hornegger, William M. Wells, and Alejandro F. Frangi</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adversarially learned one-class classifier for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Khalooei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep appearance features for abnormal behavior detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorina</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tudor</forename><surname>Radu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alexe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sebastiano Battiato, Giovanni Gallo, Raimondo Schettini, and Filippo Stanco</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="779" to="789" />
		</imprint>
	</monogr>
	<note>Image Analysis and Processing -ICIAP 2017</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Striving for simplicity: The all convolutional net. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6806</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Real-world anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waqas</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="6479" to="6488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Online growing neural gas for anomaly detection in changing surveillance scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="187" to="201" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013-12" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Conrad</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">Rahim</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Detecting anomalous events in videos by learning deep representations of appearance and motion. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="117" to="127" />
		</imprint>
	</monogr>
	<note>Image and Video Understanding in Big Data</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Anomalous behaviour detection using spatiotemporal oriented energies, subset inclusion histogram comparison and event-driven processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zaharescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Kostas Daniilidis, Petros Maragos, and Nikos Paragios</title>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="563" to="576" />
		</imprint>
	</monogr>
	<note>Computer Vision -ECCV 2010</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning semantic scene models by object classification and trajectory clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="1940" to="1947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Video anomaly detection based on locality sensitive hashing filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Sakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Compositional Models and Structured Learning for Visual Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="302" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Online detection of unusual events in videos via dynamic sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="3313" to="3320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Ganomaly: Semisupervised anomaly detection via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samet</forename><surname>Akcay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ACCV 2018</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The relationship between precision-recall and roc curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Goadrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning, ICML &apos;06</title>
		<meeting>the 23rd International Conference on Machine Learning, ICML &apos;06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A discriminative framework for anomaly detection in large videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allison</forename><surname>Del Giorno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="334" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Joint detection and recounting of abnormal events by learning deep generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Hinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin&amp;apos;ichi</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="3639" to="3647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Abnormal event detection at 150 fps in matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013-12" />
			<biblScope unit="page" from="2720" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Dynamic video anomaly detection and localization using sparse denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medhini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sowmya</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="13173" to="13195" />
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Adversarially learned one-class classifier for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Khalooei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Detecting anomalous events in videos by learning deep representations of appearance and motion. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="117" to="127" />
		</imprint>
	</monogr>
	<note>Image and Video Understanding in Big Data</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Video anomaly detection based on locality sensitive hashing filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Sakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Compositional Models and Structured Learning</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="302" to="311" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Pattern Recognition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
