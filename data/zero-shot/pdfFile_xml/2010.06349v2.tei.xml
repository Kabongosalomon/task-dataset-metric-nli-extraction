<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Collaborative Video Object Segmentation by Multi-Scale Foreground-Background Integration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongxin</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
						</author>
						<title level="a" type="main">Collaborative Video Object Segmentation by Multi-Scale Foreground-Background Integration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Video Object Segmentation</term>
					<term>Convolutional Neural Networks</term>
					<term>Metric Learning !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper investigates the principles of embedding learning to tackle the challenging semi-supervised video object segmentation. Unlike previous practices that focus on exploring the embedding learning of foreground object (s), we consider background should be equally treated. Thus, we propose a Collaborative video object segmentation by Foreground-Background Integration (CFBI) approach. CFBI separates the feature embedding into the foreground object region and its corresponding background region, implicitly promoting them to be more contrastive and improving the segmentation results accordingly. Moreover, CFBI performs both pixel-level matching processes and instance-level attention mechanisms between the reference and the predicted sequence, making CFBI robust to various object scales. Based on CFBI, we introduce a multi-scale matching structure and propose an Atrous Matching strategy, resulting in a more robust and efficient framework, CFBI+. We conduct extensive experiments on two popular benchmarks, i.e., DAVIS, and YouTube-VOS. Without applying any simulated data for pre-training, our CFBI+ achieves the performance (J &amp;F) of 82.9% and 82.8%, outperforming all the other state-of-the-art methods. Code: https://github.com/z-x-yang/CFBI.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Video Object Segmentation (VOS) is a fundamental task in computer vision with many potential applications, including augmented reality <ref type="bibr" target="#b0">[1]</ref> and self-driving cars <ref type="bibr" target="#b1">[2]</ref>. In this paper, we focus on semi-supervised VOS, which targets on segmenting a particular object across the entire video sequence based on the object mask given at the first frame. The development of semi-supervised VOS can benefit many related tasks, such as video instance segmentation <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> and interactive video object segmentation <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>.</p><p>Early VOS works( <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>) rely on fine-tuning with the first frame in evaluation, which heavily slows down the inference speed. Recent works (e.g., <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>) aim to avoid fine-tuning and achieve better run-time. In these works, STMVOS <ref type="bibr" target="#b12">[13]</ref> introduces memory networks to learn to read sequence information and outperforms all the fine-tuning based methods. However, STMVOS relies on simulating extensive frame sequences using large image datasets <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> for training. The simulated data significantly boosts the performance of STMVOS but makes the training procedure elaborate. Without simulated data, FEELVOS <ref type="bibr" target="#b11">[12]</ref> adopts a semantic pixel-wise embedding together with a global (between the first and current frames) and a local (between the previous and current frames) matching mechanism to guide the prediction. The matching mechanism is simple and fast, but the performance is not comparable with STMVOS.</p><p>Even though the efforts mentioned above have made significant progress, current state-of-the-art works pay little attention to the feature embedding of background region in videos and only focus on exploring robust matching strategies for the foreground object (s). Intuitively, it is easy to extract the foreground region from a video when precisely removing all the background. Moreover, modern Z. <ref type="bibr">Yang</ref>  video scenes commonly focus on many similar objects, such as the cars in car racing, the people in a conference, and the animals on a farm. For these cases, the contempt of integrating foreground and background embeddings traps VOS in an unexpected background confusion problem. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, if we focus on only the foreground matching like FEELVOS, a similar and same kind of object (sheep here) in the background is easy to confuse the prediction of the foreground object. Such an observation motivates us that the background should be equally treated compared with the foreground so that better feature embedding can be learned to relieve the background confusion and promote the accuracy of VOS. We propose a novel framework for Collaborative video object segmentation by Foreground-Background Integration (CFBI) based on the above motivation. Unlike the above methods, we not only extract the embedding and do match-arXiv:2010.06349v2 [cs.CV] <ref type="bibr" target="#b15">16</ref> May 2021 ing for the foreground target in the reference frame, but also for the background region to relieve the background confusion. In particular, our framework extracts two types of embedding, pixel-level and instance-level, for each video frame to cover different scales of features. Like FEELVOS, we employ pixel-level embedding to match all the objects' details with the same global &amp; local mechanism. However, the pixel-level matching is not sufficient and robust to match those objects with larger scales and may bring unexpected noises due to the pixel-wise diversity. Thus we introduce instance-level embedding to help the segmentation of largescale objects by using attention mechanisms. For the training process, we propose a balanced random-crop scheme to avoid biasing learned attributes to background attributes. These proposed strategies can effectively improve the quality of the learned collaborative embeddings for conducting VOS while keeping the network simple yet effective simultaneously. Based on CFBI, we further introduce an efficient multi-scale matching structure, resulting in a more robust framework, CFBI+. Within CFBI+, we propose an Atrous Matching (AM) strategy, which can significantly save computation and memory usage of matching processes. The use of AM makes CFBI+ not only more robust but also more efficient than CFBI.</p><p>We perform extensive experiments on DAVIS <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, and YouTube-VOS <ref type="bibr" target="#b20">[21]</ref> to validate the effectiveness of the proposed CFBI and CFBI+. Without any bells and whistles (such as the use of simulated data, fine-tuning, or postprocessing), CFBI+ outperforms all other state-of-the-art methods on the validation splits of DAVIS 2017 (ours, J &amp;F 82.9%) and YouTube-VOS (82.8%). Meanwhile, our multiobject inference speed is faster than previous state-of-the-art methods. We have made the code publicly available, and we hope our simple yet effective CFBI and CFBI+ will serve as two solid baselines and help ease the future research related to VOS. This paper is an extension of our previous conference version <ref type="bibr" target="#b21">[22]</ref>. The current work adds to the initial version in some significant aspects. First, we propose a plug-andplay Atrous Matching (AM) algorithm, which can significantly save computation and memory usage of matching processes. Second, based on the proposed AM, we design a multi-scale matching framework, resulting in a more strong and efficient VOS framework, CFBI+. Third, we incorporate considerable new experimental results, including ablation study, model setting, and visualization analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Semi-supervised Video Object Segmentation. Many previous methods for semi-supervised VOS rely on fine-tuning at test time. Among them, OSVOS <ref type="bibr" target="#b7">[8]</ref> and MoNet <ref type="bibr" target="#b22">[23]</ref> finetune the network on the first-frame ground-truth at test time. OnAVOS <ref type="bibr" target="#b8">[9]</ref> extends the first-frame fine-tuning by an online adaptation mechanism, i.e., online fine-tuning. Mask-Track <ref type="bibr" target="#b23">[24]</ref> uses optical flow to propagate the segmentation mask from one frame to the next. PReMVOS <ref type="bibr" target="#b9">[10]</ref> combines four different neural networks (including an optical flow network <ref type="bibr" target="#b24">[25]</ref>) using extensive fine-tuning and a merging algorithm. Despite achieving promising results, all these methods are seriously slowed down by fine-tuning during inference.</p><p>Some other recent works (e.g., <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b25">[26]</ref>) aim to avoid fine-tuning and achieve a better run-time. OSMN <ref type="bibr" target="#b10">[11]</ref> employs two networks to extract the instance-level information and make segmentation predictions, respectively. PML <ref type="bibr" target="#b26">[27]</ref> learns a pixel-wise embedding with the nearest neighbor classifier. Similar to PML, VideoMatch <ref type="bibr" target="#b27">[28]</ref> uses a soft matching layer that maps the pixels of the current frame to the first frame in a learned embedding space. Following PML and VideoMatch, FEELVOS <ref type="bibr" target="#b11">[12]</ref> extends the pixel-level matching mechanism by additionally matching between the current frame and the previous frame. Compared to fine-tuning methods, FEELVOS achieves a much higher speed, but there is still an accuracy gap. Like FEELVOS, RGMP <ref type="bibr" target="#b28">[29]</ref> and STMVOS <ref type="bibr" target="#b12">[13]</ref> does not require any finetuning. STMVOS, which leverages a memory network to store and read the information from past frames, outperforms all the previous methods. However, STMVOS and its following works (EGMN <ref type="bibr" target="#b29">[30]</ref> and KMNVOS <ref type="bibr" target="#b30">[31]</ref>) rely on an elaborate pre-training procedure using extensive simulated data generated from multiple datasets with pixel-level annotations. LWLVOS <ref type="bibr" target="#b31">[32]</ref> proposes to use an online fewshot learner during both training and testing stages. Without simulated data, LWLVOS is comparable with KMNVOS on YouTube-VOS, but generalizes worse than the above methods using simulated data on DAVIS.</p><p>In previous practices, learning foreground feature embedding has been well explored. OSMN proposed to conduct an instance-level matching, but such a matching scheme fails to consider the feature diversity among the details of the target's appearance and results in coarse predictions. PML and FEELVOS alternatively adopt the pixellevel matching by matching each pixel of the target, which effectively takes the feature diversity into account and achieves promising performance. Nevertheless, performing pixel-level matching may bring unexpected noises in the case of some pixels from the background are with a similar appearance to the ones from the foreground ( <ref type="figure" target="#fig_0">Fig. 1</ref>).</p><p>Thus, we propose a collaborative integration method by additionally learning background embedding. Furthermore, our CFBI utilizes both the pixel-level and instance-level embeddings to guide prediction. Attention Mechanisms. Recent works introduce the attention mechanism into convolutional networks (e.g., <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>). Following them, SE-Nets <ref type="bibr" target="#b34">[35]</ref> introduced a lightweight gating mechanism that focuses on enhancing the representational power of the convolutional network by modeling channel attention. Inspired by SE-Nets, CFBI uses an instance-level average pooling method to embed collaborative instance information from pixel-level embeddings. After that, we conduct a channel-wise attention mechanism to help guide prediction. Compared to OSMN, which employs an additional convolutional network to extract instancelevel embedding, our instance-level attention method is more efficient and lightweight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>The Overview of CFBI. To overcome or relieve the problems raised by previous methods and promote the foreground objects from the background, we present Collaborative video </p><formula xml:id="formula_0">t = 1 t = T ? 1 Softmax Pixel-level Embedding (1) (2) (3)<label>(4)</label></formula><p>Pixel-level Instance-level Backbone Backbone Backbone <ref type="figure" target="#fig_3">Fig. 2</ref>. An overview of CFBI. F-G denotes Foreground-Background. We use red and blue to indicate foreground and background separately. The deeper the red or blue color, the higher the confidence. Given the first frame (t = 1), previous frame (t = T ? 1), and current frame (t = T ), we firstly extract their pixel-wise embedding by using a backbone network. Second, we separate the first and previous frame embeddings into the foreground and background pixels based on their masks. After that, we use F-G pixel-level matching and instance-level attention to guide our collaborative ensembler network to generate a prediction.</p><p>object segmentation by Foreground-Background Integration (CFBI), as shown in <ref type="figure" target="#fig_3">Figure 2</ref>. We use red and blue to indicate foreground and background separately. First, beyond learning feature embedding from foreground pixels, our CFBI also considers embedding learning from background pixels for collaboration. Such a learning scheme will encourage the feature embedding from the target object and its corresponding background to be contrastive, promoting the segmentation results accordingly. Second, we further conduct the embedding matching from both pixel-level and instancelevel with the collaboration of pixels from the foreground and background. For the pixel-level matching, we improve the robustness of the local matching under various object moving rates. For the instance-level matching, we design an instance-level attention mechanism to augment the pixellevel matching efficiently. Moreover, to implicitly aggregate the learned foreground &amp; background and pixel-level &amp; instance-level information, we employ a collaborative ensembler to construct large receptive fields and make precise predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Collaborative Pixel-level Matching</head><p>For the pixel-level matching, we adopt a global and local matching mechanism similar to FEELVOS for introducing the guided information from the first and previous frames, respectively. Unlike previous methods <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b26">[27]</ref>, we additionally incorporate background information and apply multiple windows in the local matching, which is shown in the middle of <ref type="figure" target="#fig_3">Fig. 2</ref>. For incorporating background information, we firstly redesign the pixel distance of <ref type="bibr" target="#b11">[12]</ref> to further distinguish the foreground and background. Let B t and F t denote the pixel sets of background and all the foreground objects of frame t, respectively. We define a new distance between pixel p of the current frame T and pixel q of frame t in terms of their corresponding embedding, e p and e q , by</p><formula xml:id="formula_1">D(p, q) = 1 ? 2 1+exp(||ep?eq|| 2 +b B ) if q ? B t 1 ? 2 1+exp(||ep?eq|| 2 +b F ) if q ? F t ,<label>(1)</label></formula><p>where b B and b F are trainable background bias and foreground bias. We introduce these two biases to make our model be able further to learn the difference between foreground distance and background distance. Foreground-Background Global Matching. Let P t denote the set of all pixels (with a stride of 4) at time t and P t,o ? P t is the set of pixels at time t which belongs to the foreground object o. The global foreground matching between one pixel p of the current frame T and the pixels of the first reference frame (i.e., t = 1) is,</p><formula xml:id="formula_2">G o (p) = min q?P1,o D(p, q).<label>(2)</label></formula><p>Similarly, let P t,o = P t \P t,o denote the set of relative background pixels of object o at time t, and the global background matching is,</p><formula xml:id="formula_3">G o (p) = min q?P1,o D(p, q).<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Foreground-Background Multi-Local</head><p>Matching. In FEELVOS, the local matching is limited in only one fixed extent of neighboring pixels, but the offset of objects across two adjacent frames in VOS is variable, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><formula xml:id="formula_4">ID: 0a598e18a8 e</formula><p>slow moving rate</p><formula xml:id="formula_5">t = T t = T + 1 t = T + 1</formula><p>(a) Slow moving rate ID: 00f88c4f0a ID fast moving rate slo Thus, we propose to apply the local matching mechanism on different scales and let the network learn how to select an appropriate local scale, which makes our framework more robust to various moving rates of objects. Notably, we use the intermediate results of the local matching with the largest window to calculate on other windows. Thus, the increase of computational resources of our multi-local matching is negligible.</p><formula xml:id="formula_6">t = T t = T t = T + 1 (b) Fast moving rate</formula><p>Formally, let K = {k 1 , k 2 , ..., k n } denote all the neighborhood sizes and H(p, k) denote the neighborhood set of pixels that are at most k pixels away from p in both x and y directions, our foreground multi-local matching between the current frame T and its previous frame T ? 1 is</p><formula xml:id="formula_7">M L o (p, K) = {L o (p, k 1 ), L o (p, k 2 ), ..., L o (p, k n )}, (4) where L o (p, k) = min q?P p,k T ?1,o D(p, q) if P p,k T ?1,o = ? 1 otherwise .<label>(5)</label></formula><p>Here, P p,k T ?1,o := P T ?1,o ? H(p, k) denotes the pixels in the local window (or neighborhood). And our background multi-local matching is</p><formula xml:id="formula_8">M L o (p, K) = {L o (p, k 1 ), L o (p, k 2 ), ..., L o (p, k n )}, (6) where L o (p, k) = ? ? ? min q?P p,k T ?1,o D(p, q) if P p,k T ?1,o = ? 1 otherwise .<label>(7)</label></formula><p>Here similarly, P</p><formula xml:id="formula_9">p,k T ?1,o := P T ?1,o ? H(p, k)</formula><p>. In addition to the global and multi-local matching maps, we concatenate the pixel-level embedding feature and mask of the previous frame with the current frame feature. FEELVOS demonstrates the effectiveness of concatenating the previous mask. Following this, we empirically find that introducing the previous embedding can further improve the performance (J &amp;F ) by about 0.5%.</p><p>In summary, the output of our collaborative pixel-level matching is a concatenation of (1) the pixel-level embedding of the current frame, (2) the pixel-level embedding and mask of the previous frame, (3) the multi-local matching map and (4) the global matching map, as shown in the bottom box of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Res-Block Scale</head><formula xml:id="formula_10">1 ? 1 ? 4C e 1 ? 1 ? C H ? W ? C Collaborative</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instance-level Guidance Vector</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-linear</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Res-Block</head><p>Scale </p><formula xml:id="formula_11">1 ? 1 ? 4C e 1 ? 1 ? C 1 ? 1 ? C H ? W ? C</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Collaborative Instance-level Attention</head><p>As shown in the right of <ref type="figure" target="#fig_3">Fig 2,</ref> we further design a Collaborative instance-level attention mechanism to guide the segmentation for large-scale objects. After getting the pixel-level embeddings of the first and previous frames, we separate them into foreground and background pixels (i.e., P 1,o , P 1,o , P T ?1,o , and P T ?1,o ) according to their masks. Then, we apply channel-wise average pooling on each group of pixels to generate a total of four instance-level embedding vectors and concatenate these vectors into one collaborative instance-level guidance vector. Thus, the guidance vector contains the information from both the first and previous frames, and both the foreground and background regions.</p><p>In order to efficiently utilize the instance-level information, we employ an attention mechanism to adjust our Collaborative Ensembler (CE). We show a detailed illustration in <ref type="figure" target="#fig_4">Fig. 4</ref>. Inspired by SE-Nets <ref type="bibr" target="#b34">[35]</ref>, we leverage a fullyconnected (FC) layer (we found this setting is better than using two FC layers as adopted by SE-Net) and a non-linear activation function to construct a gate for the input of each Res-Block in the CE. The gate will adjust the scale of the input feature channel-wisely.</p><p>By introducing collaborative instance-level attention, we can leverage a full scale of foreground-background information to guide the prediction further. The information with a  <ref type="bibr" target="#b35">[36]</ref>. And then, we use the Feature Pyramid Network (FPN) <ref type="bibr" target="#b36">[37]</ref> to fuse the information from small scales to large scales and reduce the channel dimensions of three features. After this, we do all the matching processes of CFBI on each scale. The output of each scale will be sent to the consistent stage of Collaborative Ensembler (CE).</p><p>large (instance-level) receptive field is useful to relieve local ambiguities <ref type="bibr" target="#b37">[38]</ref>, which is inevitable with a small (pixelwise) receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Collaborative Ensembler (CE)</head><p>In the lower right of <ref type="figure" target="#fig_3">Fig. 2</ref>, we design a collaborative ensembler for making large receptive fields to aggregate pixel-level and instance-level information and implicitly learn the collaborative relationship between foreground and background.</p><p>Inspired by ResNets <ref type="bibr" target="#b38">[39]</ref> and Deeplabs <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b39">[40]</ref>, which both have shown significant representational power in image segmentation tasks, our CE uses a downsampleupsample structure, which contains three stages of Res-Blocks <ref type="bibr" target="#b38">[39]</ref> and an Atrous Spatial Pyramid Pooling (ASPP) <ref type="bibr" target="#b35">[36]</ref> module. The number of Res-Blocks in Stage 1, 2, and 3 are 2, 3, 3 in order. Besides, we employ dilated convolutional layers to improve the receptive fields efficiently. The dilated rates of the 3 ? 3 convolutional layer of Res-Blocks in one stage are separately 1, 2, 4 ( or 1, 2 for Stage 1). At the beginning of Stage 2 and Stage 3, the feature maps will be downsampled by the first Res-Block with a stride of 2. After these three stages, we employ an ASPP and a Decoder <ref type="bibr" target="#b35">[36]</ref> module to increase the receptive fields further, upsample the scale of feature and fine-tune the prediction collaborated with the low-level backbone features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">CFBI+: Towards Efficient Multi-scale Matching</head><p>High-quality matching maps are essential for CFBI to generate accurate predictions with sharp object boundaries, which is one of the critic factors for improving VOS's performance. However, it is costly in terms of both GPU memory and time to achieve a delicate and high-resolution matching map (e.g., with a stride of 4). Intuitively, there are two ways to accelerate matching processes. (1) Performing matching over low-resolution feature maps. Even the process will be much more light-weight, it is easy to miss many object details. (2) Reducing the channel dimensions while keeping using a high-resolution feature map. Even the amount of computation will decrease linearly with the channel dimensions, the accuracy of matching will also decrease.</p><p>Some recent works (e.g., <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b40">[41]</ref>) prove that multi-scale strategies can efficiently improve convolutional networks' performance. We consider that such strategies also benefit matching processes. Thus, we introduce an efficient multiscale matching structure into CFBI, resulting in a more robust framework, i.e., CFBI+. An overview of CFBI+ is shown in <ref type="figure" target="#fig_5">Fig. 5</ref>. Firstly, CFBI+ extracts three features with different scales (S = 4, 8, 16) from backbone. And then, we use the Feature Pyramid Network (FPN) <ref type="bibr" target="#b36">[37]</ref> to further fuse the information from small scales to large scales. After this, we do all the matching processes of CFBI on every scale. The output of each scale will be sent to each corresponding stage of CE.</p><p>Concretely, to harness the merits of the two accelerate matching processes as well as alleviate their disadvantages, we adopt an adaptive matching strategy for feature maps of different scales. CFBI+ progressively and linearly increases the channel dimensions from larger scales to smaller scales, which reduces the amount of calculation for matching on larger scales. Meanwhile, richer semantic information in smaller scales can successfully make up for the performance drop due to the reduction of channel dimension for larger scales. In this way, various coarse-to-fine information can help CFBI+ achieve better segmentation results.</p><p>Besides, only progressively and linearly increasing channel dimensions is not enough to make the multi-scale matching more efficient than single-scale, because the complexity of matching processes increases exponentially with the resolution of feature maps. For example, the calculation of G T,o (p) on S = 4 scale is 256 times of S = 16. Thus, we additionally propose an Atrous Matching (AM) strategy to save computation and memory usage of matching processes further. Introducing AM helps CFBI+ to be more efficient than CFBI.</p><p>Atrous Matching (AM). Algorithme?trous (or "atrous algorithm" in the following text), an algorithm for wavelet decomposition <ref type="bibr" target="#b41">[42]</ref>, played a key role in some recent convolutional networks <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b42">[43]</ref>. By adding a spatial interval during sampling, the atrous algorithm can reduce calculation while maintaining the same resolution. Intuitively, spatially close pixels always share similar semantic information. Hence, we argue that the atrous algorithm is also effective in matching processes. Removing part of similar pixels from referred pixels will not heavily drop performance but save much computation cost.</p><p>Let q x,y be the pixel at position (x, y) and let l be an atrous factor, we generalize the foreground global matching (Eq. 2) into an atrous form,</p><formula xml:id="formula_12">G l o (p) = min q?P l 1,o D(p, q),<label>(8)</label></formula><p>where</p><formula xml:id="formula_13">P l 1,o = {q x,y ? P 1,o , ?x, y ? {l, 2l, 3l, ...}}<label>(9)</label></formula><p>is a l-atrous object pixel set. We show an illustration in <ref type="figure">Fig. 6</ref>. Let x p and y p denote the position of pixel p, the atrous form of the foreground local matching (Eq. 5) is</p><formula xml:id="formula_14">L l o (p, k) = min q?P l,p,k T ?1,o D(p, q) if P l,p,k T ?1,o = ? 1 otherwise ,<label>(10)</label></formula><p>where</p><formula xml:id="formula_15">P l,p,k T ?1,o := P T ?1,o ? H l (p, k),<label>(11)</label></formula><p>and H l (p, k) = {q x,y ? H(p, k)?x ? {x p , x p ? l, x p ? 2l, ...}, y ? {y p , y p ? l, y p ? 2l, ...}} (12) is a l-atrous neighborhood set.</p><p>In the same way, we can also generalize Eq. 3, Eq. 7 Eq. 4, and Eq. 6 into atrous forms, i.e., G </p><formula xml:id="formula_16">, i.e., G l=1 o (p) ? G o (p) and L l=1 o (p, k) ? L o (p, k)</formula><p>. On the largest matching scale (S = 4) of CFBI+, we apply 2-atrous matching processes, which significantly improve the efficiency of CFBI+. Notably, AM is a plug-and-play algorithm and can also improve the efficiency of CFBI during the testing stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IMPLEMENTATION DETAILS</head><p>Following FEELVOS, we use the DeepLabv3+ <ref type="bibr" target="#b35">[36]</ref> architecture as the backbone for our network. However, our backbone is based on the dilated ResNet-101 <ref type="bibr" target="#b35">[36]</ref> instead of Xception-65 <ref type="bibr" target="#b43">[44]</ref> for saving computational resources. We apply batch normalization (BN) <ref type="bibr" target="#b44">[45]</ref> in our backbone and pre-train it on ImageNet <ref type="bibr" target="#b45">[46]</ref> and COCO <ref type="bibr" target="#b14">[15]</ref>. To make the training process more effective and consistent with the inference stage, we additionally adopt two tricks, i.e., Balanced Random-Crop, and Sequential Training: <ref type="figure">Fig. 7</ref>. When using normal random-crop, some red windows contain few or no foreground pixels. For reliving this problem, we propose balanced random-crop.</p><formula xml:id="formula_17">t = T + 1 t = 1 t = T ? 1 t = T ? (a) Normal t = T + 1 t = 1 t = T ? 1 t = T ? (b) Balanced</formula><p>? Balanced Random-Crop. As shown in <ref type="figure">Fig. 7</ref>, there is an apparent imbalance between the foreground and the background pixel number on VOS datasets. Such an issue usually makes the models easier to be biased to background attributes. In order to relieve this problem, we take a balanced random-crop scheme, which crops a sequence of frames (i.e., the first frame, the previous frame, and the current frame) by using a same cropped window and restricts the cropped region of the first frame to contain enough foreground information. The restriction method is simple yet effective. To be specific, the balanced random-crop will decide on whether the randomly cropped frame contains enough pixels from foreground objects or not. If not, the method will continually take the cropping operation until we obtain an expected one. ? Sequential Training. In the training stage, FEELVOS predicts only one step in one iteration, and the guidance masks come from the ground-truth data. RGMP and STMVOS use previous guidance information (mask or feature memory) in training, which is more consistent with the inference stage and performs better. The previous guidance masks are always generated by the network in the previous inference steps in the evaluation stage. Following RGMP, we train the network using a sequence of consecutive frames in each SGD iteration.</p><p>In each iteration, we randomly sample a batch of video sequences. For each video sequence, we randomly sample a frame as the reference frame and a continuous N + 1 frames as the previous frame and current frame sequence (with N frames). When predicting the first frame, we use the ground-truth of the previous frame Mean Update Parameters <ref type="figure">Fig. 8</ref>. An illustration of the sequential training. In each step, the previous mask comes from the previous prediction (the green lines) except for the first step, whose previous mask comes from the ground-truth (GT) mask (the blue line).</p><formula xml:id="formula_18">GT T GT T + 1 GT T + N Loss T Loss T + 1 Loss T + N Grad T Grad T + 1 Grad T + N ?? backward backward backward CFBI Frame 1 GT 1 Frame T CFBI Frame T + 1 CFBI Frame T + N Prediction T Prediction T + 1 Prediction T + N ?? Frame GT T ? 1 T ? 1 ?? ??</formula><p>as the previous mask. When predicting the following frames, we use the latest prediction to be the previous mask. We show an illustration in <ref type="figure">Fig. 8</ref>. In CFBI, the backbone is followed by one depth-wise separable convolution for extracting pixel-wise embedding (channels=100) with a stride of 4. We further downsample the embedding feature to a half size for the multi-local matching using bi-linear interpolation for saving GPU memory.</p><p>In CFBI+, the backbone is followed by FPN <ref type="bibr" target="#b36">[37]</ref> for extracting three pixel-wise embeddings (channels=32, 64, and 128) with strides of 4, 8, and <ref type="bibr" target="#b15">16</ref> For the collaborative ensembler, we apply Group Normalization (GN) <ref type="bibr" target="#b46">[47]</ref> and Gated Channel Transformation (GCT) <ref type="bibr" target="#b47">[48]</ref> to improving training stability and performance when using a small batch size. We initialize b B and b F to 0. In CFBI+, each matching scale has individual b B and b F .</p><p>During training, we firstly downsample all the videos to 480p resolution, which is the same as the DAVIS default setting. We adopt SGD with a momentum of 0.9 and apply a bootstrapped cross-entropy loss, which only considers the 15% hardest pixels. In addition, we apply flipping, scaling, and balanced random-crop as data augmentations. The scaling range is from 1.0 to 1.3 times, and the cropped window size is 465?465. During the training stage, we freeze the parameters of BN in the backbone. In the testing stage, all the videos are resized to be no more than 1.3 ? 480p resolution, which is consistent with our training stage. For the multiscale testing, we apply the scales of {1.0, 1.15, 1.3, 1.5} and {1.5, 1.7, 1.9} on YouTube-VOS, and DAVIS, respectively.</p><p>For YouTube-VOS experiments, we use a learning rate of 0.01 for 100, 000 steps with a batch size of 8 using 4 Tesla V100 GPUs. The current sequence's length is N = 3. The training time on YouTube-VOS is about 3 days. For training with only DAVIS, we use a learning rate of 0.006 for 50, 000 steps with a batch size of 6 videos using 2 GPUs. The current sequence's length is N = 3 as well. For training with both DAVIS and YouTube-VOS, we first train CFBI or CFBI+ on YouTube-VOS following the above setting. After that, we fine-tune the model on DAVIS. To avoid overfitting, we mix DAVIS videos with YouTube-VOS in a ratio of 1:2 during fine-tuning. Besides, we use a learning rate of 0.01 for 50, 000 steps with a batch size of 8 videos using 4 Tesla V100 GPUs. The current sequence's length is N = 5, which is slightly better than N = 3. We use PyTorch <ref type="bibr" target="#b52">[53]</ref> to implement our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>Following the previous state-of-the-art method <ref type="bibr" target="#b12">[13]</ref>, we evaluate our method on YouTube-VOS <ref type="bibr" target="#b20">[21]</ref>, DAVIS 2016 <ref type="bibr" target="#b18">[19]</ref> and DAVIS 2017 <ref type="bibr" target="#b19">[20]</ref>. For the evaluation on YouTube-VOS, we train our model on YouTube-VOS training split <ref type="bibr" target="#b20">[21]</ref>. For DAVIS, we train our model on the DAVIS-2017 training split <ref type="bibr" target="#b19">[20]</ref>. Furthermore, we provide DAVIS results using  <ref type="bibr" target="#b19">[20]</ref>. 600p : using 600p videos instead of 480p during inference. ? : timing extrapolated from single-object speed assuming linear scaling in the number of objects. both DAVIS 2017 and YouTube-VOS for training following some latest works <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. The evaluation metric is J score, calculated as the average IoU between the prediction and the ground truth mask, and F score, calculated as an average boundary similarity measure between the boundary of the prediction and the ground truth, and their average value (J &amp;F ). We evaluate our results on the official evaluation server or use official tools. <ref type="bibr" target="#b20">[21]</ref> is the latest large-scale dataset for multiobject video segmentation. Compared to the popular DAVIS benchmark, which consists of 120 videos, YouTube-VOS is about 37 times larger. In detail, YouTube-VOS contains 3471 videos in training split (65 categories), 507 videos in validation split (additional 26 unseen categories), and 541 videos in testing split (additional 29 unseen categories). Due to the existence of unseen object categories, the YouTube-VOS validation split is much suitable for measuring the generalization ability of VOS methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Compare with the State-of-the-art Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YouTube-VOS</head><p>As shown in <ref type="table" target="#tab_2">Table 1</ref>, we compare our method with the latest VOS methods on both Validation 2018 and Testing 2019 splits. Without using any bells and whistles, like fine-tuning at test time <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> or pre-training on larger augmented simulated data <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, our CFBI+ achieves an average score of 82.0%, which significantly outperforms all other methods in every evaluation metric. We can improve the performance of CFBI+ to 82.8% (CFBI+ 2? ) by using a stronger training schedule with a double batch size and learning rate. Especially, CFBI+'s multi-object inference speed is much faster than BoLT (1.36s).</p><p>Benefit from the multi-scale matching, our CFBI+ is robuster (82.0% vs.81.4%) and more efficient (0.25s vs.0.29s) than CFBI. Especially, CFBI+ uses only half of the training batch size to exceed CFBI 2? . Besides, the 82.8% result is significantly higher (1.4%) than KMNVOS, which follows STMVOS to use extensive simulated data for training. Without simulated data, the performance of STMVOS will drop a lot from 79.4% to 68.2%. Moreover, we can further boost the performance of CFBI+ to 83.3% by applying a multi-scale and flip strategy during the evaluation.</p><p>We also compare our method with two of the best results on Testing 2019 split, i.e., Rank 1 (EMN <ref type="bibr" target="#b51">[52]</ref>) and Rank 2 (MST <ref type="bibr" target="#b50">[51]</ref>) results in the 2nd Large-scale Video Object Segmentation Challenge. Without using model ensemble, simulated data or testing-stage augmentation, our CFBI+ (82.9%) significantly outperforms the Rank 1 result (81.8%) while maintaining an efficient multi-object speed of 4 FPS. Notably, the improvement of CFBI+ mainly comes from the unseen categories (78.9%J /86.8%F vs.77.3%J /84.7%F) instead of seen. Such a strong result further demonstrates CFBI+'s generalization ability and effectiveness. DAVIS 2017 <ref type="bibr" target="#b19">[20]</ref> is a multi-object extension of DAVIS 2016. The validation split of DAVIS 2017 consists of 59 objects in 30 videos. And the training split contains 60 videos. Compared to YouTube-VOS, DAVIS is much smaller and easy to be over-fitted.</p><p>As shown in <ref type="table" target="#tab_3">Table 2</ref>, CFBI+ exceeds KMNVOS and EGMN (82.9% vs.82.8%) without using simulated data. Moreover, CFBI+ achieves a faster multi-object inference speed (0.18s) than KMNVOS (0.24%). Different from KM-NVOS and EGMN, the backbone features of CFBI+ or CFBI is shared for all the objects in each frame, which leads to a more efficient multi-object inference. The augmentation in evaluation can further boost CFBI+ to a higher score of 84.5%.</p><p>We also evaluate our method on the DAVIS-2017 testing split, which is much more challenging than the validation split. On the testing split, we outperform KMNVOS (77.2%) by 0.8% under the setting proposed by STMVOS (i.e., evaluating on 600p resolution). When evaluating on the default 480p resolution of DAVIS, CFBI+ or CFBI is much better than the STMVOS using 600p resolution (75.6% or 75.0% vs.72.2%). These strong results further prove the generalization ability of CFBI+ and CFBI. The speed of CFBI+ is only comparable with CFBI when evaluating using a 480p resolution on DAVIS. The reason for this is that convolutional layers have a larger proportion of calculations when evaluating on a small resolution. And CFBI+ has more convolutional layers than CFBI because of introducing FPN. On larger resolution (e.g., 600p), CFBI+ is faster than CFBI (0.29s vs.0.35s). DAVIS 2016 <ref type="bibr" target="#b18">[19]</ref> contains 20 videos annotated with highquality masks each for a single target object. We compare our CFBI method with state-of-the-art methods in <ref type="table" target="#tab_5">Table 3</ref>. On the DAVIS-2016 validation split, our CFBI+ trained with the additional YouTube-VOS training split achieves an average score of 89.9%, which is slightly worse than KMNVOS 90.5%, a method using simulated data as mentioned before. Since the amount of data in DAVIS is tiny, using additional simulation data can help alleviate over-fitting. Compare to a much fair baseline (i.e., FEELVOS) whose setting is closer to ours, the proposed CFBI+ not only achieves a much better accuracy (89.9% vs.81.7%) but also maintains a much faster inference speed (0.17s vs.0.45s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>We analyze the ablation effect of each component proposed in CFBI on the DAVIS-2017 validation split. Following FEELVOS, we use only the DAVIS-2017 training split as training data for these experiments. Ablation of multi-scale matching. We evaluate the speed and performance of our methods on the YouTube-VOS validation split. S: the stride of feature maps. G: applying foreground-background global matching. L: applying foreground-background multi-local matching. l: atrous factor. Background Embedding. As shown in <ref type="table" target="#tab_6">Table 4</ref>, we first analyze the influence of removing the background embedding while keeping the foreground only. Without any background mechanisms, the result of our method heavily drops from 74.9% to 70.9%. This result shows that it is significant to embed both foreground and background features collaboratively. Besides, the missing of background information in the pixel-level matching or the instance-level attention will decrease the result to 73.0% or 72.3% separately. Thus, compared to instance-level attention, the pixel-level matching performance is more sensitive to the effect of background embedding. A possible reason for this phenomenon is that the possibility of existing some background pixels similar to the foreground is higher than some background instances. Finally, we remove the foreground and background bias, b F and b B , from the distance metric, and the result drops to 72.8%, which further shows that the distance between foreground pixels and the distance between background pixels should be separately considered. Atrous Matching. As shown in <ref type="table">Table 5</ref>, the performance of CFBI will decrease, and the speed will increase as the atrous factor (l) increases. Compared to the original matching, 2-atrous matching will significantly accelerate inference speed, but the performance will only slightly decrease. The speed will no longer be fast improved by further increasing l, and the performance will be heavily decreased. Compared 2-atrous multi-local matching, 2-atrous global matching has a nearly identical performance as the original global matching (81.3%vs.81.4%) but greatly accelerates the speed by 93%. In short, Atrous Matching can significantly improve the efficiency of matching processes, especially for global matching.</p><formula xml:id="formula_19">Name S=4 S=8 S=16 Avg t/s CFBI-S4 G L l=2 81.6 0.65 CFBI-S4-G2 G l=2 L l=2 81.6 0.27 CFBI-S8 G L 80.9 0.13 CFBI-S16 G L 78.3 0.11 CFBI G L 81.4 0.29 CFBI-G2 G l=2 L 81.3 0.15 CFBI+ G l=2 L l=2 G L G L 82.0 0.25</formula><p>Multi-scale Matching. <ref type="table" target="#tab_7">Table 6</ref> shows the ablation study of multi-scale matching. In CFBI experiments, the channel CFBI CFBI+ 0% 25% 50% 75% 100% Time: <ref type="figure">Fig. 9</ref>. Qualitative comparison between CFBI and CFBI+ on the DAVIS-2017 validation split. In the first video, CFBI fails to segment one hand of the right person (the white box), while CFBI+ generates an accurate boundary between two similar persons. In the second video, CFBI entirely loses two tiny objects (cellphones). In contrast, CFBI+ successfully predicts their masks.</p><p>dimension of pixel-wise features is 100. In CFBI+ experiments, the channel dimensions are 32, 64, and 128 for S = 4, S = 8, and S = 16, respectively. We first evaluate the difference between different matching scales. As shown, doing matching in larger scales leads to better performance but takes much more inference time. CFBI-S4 is much powerful than CFBI-S16 (81.6% vs.78.3%). However, CFBI-S16 is about 5 times faster than CFBI-S4. For better efficiency, CFBI brings the multi-local matching of CFBI-S4 from S = 4 to S = 8, which improves 124% speed and loses only 0.2% performance. If we want to do matching on larger scales, Atrous global matching (l = 2) is critic in saving computational resources (CFBI-G2 0.15s vs.CFBI 0.29s, CFBI-S4-G2 0.27s vs.CFBI-S4 0.65s) while losing little performance (CFBI-G2 81.3% vs.CFBI 81.4%, CFBI-S4-G2 81.6% vs.CFBI-S4 81.6%). Finally, by combining all the matching processes on three scales and progressively increasing their channel dimensions, CFBI+ achieves better performance (82.0%) while proposed atrous matching helps guarantee an efficient speed (0.25s). Qualitative Comparison. To further compare CFBI with CFBI+, we visualize some representative comparison results on the DAVIS-2017 validation split in <ref type="figure">Fig. 9</ref>. Benefit from the local matching on larger scales, CFBI+ can generate a more accurate boundary between similar targets. Moreover, CFBI+ is capable of predicting some tiny objects, which are difficult for CFBI. In addition, we show more results of CFBI+ under some of the hardest cases on the DAVIS-2017 testing split and YouTube-VOS in <ref type="figure" target="#fig_0">Fig. 10</ref>. CFBI generalize well on most of these cases, including similar objects, small objects, and occlusion.</p><p>Other Components. The ablation study of other proposed components is shown in <ref type="table" target="#tab_8">Table 7</ref>. Line 0 (74.9%) is the result of proposed CFBI, and Line 6 (68.3%) is our baseline method reproduced by us. Under the same setting, our CFBI significantly outperforms the baseline.</p><p>In line 1, we use only one local neighborhood window to conduct the local matching following the setting of FEELVOS, which degrades the result from 74.9% to 73.8%. It demonstrates that our multi-local matching module is more robust and effective than the single-local matching module of FEELVOS. Notably, the computational complexity of multi-local matching dominantly depends on the biggest local window size because we use the intermediate results of the local matching of the biggest window to calculate on smaller windows.</p><p>In line 2, we replace our sequential training by using ground-truth masks instead of network predictions as the previous mask. By doing this, the performance of CFBI drops from 74.9% to 73.3%, which shows the effectiveness of our sequential training under the same setting.</p><p>In line 3, we replace our collaborative ensembler with 4 depth-wise separable convolutional layers (and we keep applying instance-level attention before each separable convolutional layer). This architecture is the same as the dynamic segmentation head of <ref type="bibr" target="#b11">[12]</ref>. Compared to our collaborative ensembler, the dynamic segmentation head has much smaller receptive fields and performs 1.6% worse.</p><p>In line 4, we use normal random-crop instead of our balanced random-crop during the training process. In this situation, the performance drops by 2.1% to 72.8% as well. As expected, our balanced random-crop is successful in relieving the model form biasing to background attributes.</p><p>In line 5, we disable the use of instance-level attention as guidance information to the collaborative ensembler, which means we only use pixel-level information to guide the prediction. In this case, the result deteriorates even further to 72.7, which proves that instance-level information can further help the segmentation with pixel-level information.</p><p>In summary, we explain the effectiveness of each proposed component of CFBI and CFBI+. For VOS, it is necessary to embed both foreground and background features. Time: <ref type="figure" target="#fig_0">Fig. 10</ref>. Qualitative results of CFBI+ on the DAVIS-2017 testing split and YouTube-VOS 2018 validation split. These videos cover many of the most challenging VOS cases, including similar objects, small objects, occlusion, and blur. In the first four videos, CFBI+ performs well on similar objects, small objects, and occlusion. Especially for the third video, there are 10 similar targets (dancers) in total and many similar people in the background. Besides, all the targets continuously occlude each other. However, CFBI+ does not collapse under such a complicated case and correctly track every dancer. In the last video, we show one of the worst cases on the DAVIS-2017 testing split, where CFBI+ fails to segment all the motorbike parts. The blur caused by such a strong halo makes it difficult for CFBI+ to distinguish the motorbike's appearance.</p><p>Besides, the model will be more robust by combining pixellevel information and instance-level information. Notably, multi-scale pixel-level matching is much more potent than single-scale, and atrous matching is critical for improving matching efficiency. Apart from this, the proposed balanced random-crop and sequential training are useful but straightforward in improving training performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>This paper proposes a novel framework for video object segmentation by introducing collaborative foregroundbackground integration and achieves new state-of-the-art results on three popular benchmarks. Specifically, we impose the feature embedding from the foreground target and its corresponding background to be contrastive. Moreover, we integrate both pixel-level and instance-level embeddings to make our framework robust to various object scales while keeping the network simple and fast. In particular, our design of multi-scale matching can further improve VOS's performance, and our atrous matching can greatly improve the efficiency of matching processes. We hope CFBI or CFBI+ will serve as a solid baseline and help ease the future research of VOS and related areas, such as video object tracking and interactive video editing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>CI: collaborative integration. There are two foreground sheep (pink and blue) in the sequence. In the top line, the contempt of background matching leads to a confusion of sheep's prediction. In the bottom line, we relieve the confusion problem by introducing background matching (dot-line arrow).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The moving rate of objects across two adjacent frames is largely variable for different sequences. Examples are from YouTube-VOS<ref type="bibr" target="#b20">[21]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>The trainable part of the instance-level attention. Ce denotes the channel dimension of pixel-wise embedding. H, W , C denote the height, width, channel dimension of CE features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>DeepLabV3+Fig. 5 .</head><label>5</label><figDesc>An overview of CFBI+. S: the stride of feature maps. Firstly, CFBI+ extracts three features with different scales (S = 4, 8, 16) from backbone, ResNet101-DeepLabV3+</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>(a) l = 1 ( 3 Fig. 6 .</head><label>136</label><figDesc>original matching) (b) l = 2(c) l = An illustration of l-atrous object pixel set. Atrous matching improves computational efficiency by periodically filtering out referred object pixels without loss of resolution. (a) Yellow points indicate the referred object pixel set used in the original matching process. All the object (red dog) pixels are sampled. (b) (c) 2-atrous and 3-atrous object pixel set. Referred pixels are sampled out with a period of 2 or 3 pixels vertically and horizontally.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>l o (p), L l o (p, k), M L l o (p, K), and M L l o (p, K). Since the number of referred pixels is reduced l 2 times, AM's computational complexity is only 1/l 2 of original matching. Notably, AM is equivalent to original matching when l is equal to 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, Y. Wei and Y. Yang are with the ReLER Lab, Centre for Artificial Intelligence, University of Technology Sydney, Ultimo, NSW 2007, Australia. Email: zongxin.yang@student.uts.edu.au, {yunchao.wei, yi.yang}@uts.edu.au</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, respectively. The window sizes are {4, 8, 12, 16, 20, 24}, {2, 4, 6, 8, 10, 12}, and {4, 6, 8, 10} for three scales (stride= 4, 8, and 16).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 83.3 82.8 87.5 77.3 85.7 2.17</head><label>1</label><figDesc>The quantitative evaluation on YouTube-VOS<ref type="bibr" target="#b20">[21]</ref>. F, S, and * denote online fine-tuning, using simulated data during training and performing model ensemble in evaluation, respectively. t/s: time per frame in seconds. 2? : using double batch size and learning rate during training.</figDesc><table><row><cell>Seen</cell><cell>Unseen</cell></row></table><note>M S : using a multi-scale and flip strategy in evaluation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2</head><label>2</label><figDesc>The quantitative evaluation on DAVIS 2017</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3</head><label>3</label><figDesc>The quantitative evaluation on the DAVIS-2016 validation set<ref type="bibr" target="#b18">[19]</ref>. (Y) denotes additionally using YouTube-VOS for training.</figDesc><table><row><cell>Methods</cell><cell>F</cell><cell>S</cell><cell>Avg</cell><cell>J</cell><cell>F</cell><cell>t/s</cell></row><row><cell>OSMN [CVPR18] [11]</cell><cell></cell><cell></cell><cell>-</cell><cell>74.0</cell><cell></cell><cell>0.14</cell></row><row><cell>PML [CVPR18] [27]</cell><cell></cell><cell></cell><cell>77.4</cell><cell>75.5</cell><cell>79.3</cell><cell>0.28</cell></row><row><cell>VideoMatch [ECCV18] [28]</cell><cell></cell><cell></cell><cell>80.9</cell><cell>81.0</cell><cell>80.8</cell><cell>0.32</cell></row><row><cell>RGMP [CVPR18] [29]</cell><cell></cell><cell></cell><cell>68.8</cell><cell>68.6</cell><cell>68.9</cell><cell>0.14</cell></row><row><cell>RGMP [CVPR18] [29]</cell><cell></cell><cell></cell><cell>81.8</cell><cell>81.5</cell><cell>82.0</cell><cell>0.14</cell></row><row><cell>A-GAME [CVPR19] [49] (Y)</cell><cell></cell><cell></cell><cell>82.1</cell><cell>82.2</cell><cell>82.0</cell><cell>0.07</cell></row><row><cell>FEELVOS [CVPR19] [12] (Y)</cell><cell></cell><cell></cell><cell>81.7</cell><cell>81.1</cell><cell>82.2</cell><cell>0.45</cell></row><row><cell>OnAVOS [BMVC17] [9]</cell><cell></cell><cell></cell><cell>85.0</cell><cell>85.7</cell><cell>84.2</cell><cell>13</cell></row><row><cell>PReMVOS [ACCV18] [10]</cell><cell></cell><cell></cell><cell>86.8</cell><cell>84.9</cell><cell>88.6</cell><cell>32.8</cell></row><row><cell>STMVOS [ICCV19] [13] (Y)</cell><cell></cell><cell></cell><cell>89.3</cell><cell>88.7</cell><cell>89.9</cell><cell>0.16</cell></row><row><cell>KMNVOS [ECCV20] [31] (Y)</cell><cell></cell><cell></cell><cell>90.5</cell><cell>89.5</cell><cell>91.5</cell><cell>0.12</cell></row><row><cell>CFBI</cell><cell></cell><cell></cell><cell>86.1</cell><cell>85.3</cell><cell>86.9</cell><cell>0.16</cell></row><row><cell>CFBI (Y)</cell><cell></cell><cell></cell><cell>89.4</cell><cell>88.3</cell><cell>90.5</cell><cell>0.16</cell></row><row><cell>CFBI+ (Y)</cell><cell></cell><cell></cell><cell>89.9</cell><cell>88.7</cell><cell>91.1</cell><cell>0.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4</head><label>4</label><figDesc>Ablation of background embedding on the DAVIS-2017 validation split. P and I denote the pixel-level matching and instance-level attention, respectively. : removing the foreground and background bias.</figDesc><table><row><cell>P</cell><cell>I</cell><cell>Avg</cell><cell>J</cell><cell>F</cell></row><row><cell></cell><cell></cell><cell>74.9</cell><cell>72.1</cell><cell>77.7</cell></row><row><cell>*</cell><cell></cell><cell>72.8</cell><cell>69.5</cell><cell>76.1</cell></row><row><cell></cell><cell></cell><cell>73.0</cell><cell>69.9</cell><cell>76.0</cell></row><row><cell></cell><cell></cell><cell>72.3</cell><cell>69.1</cell><cell>75.4</cell></row><row><cell></cell><cell></cell><cell>70.9</cell><cell>68.2</cell><cell>73.6</cell></row><row><cell></cell><cell></cell><cell>TABLE 5</cell><cell></cell><cell></cell></row><row><cell cols="5">Ablation of atrous matching. We evaluate the speed and performance</cell></row><row><cell cols="5">of CFBI on the YouTube-VOS validation split using different atrous</cell></row><row><cell cols="5">matching factors (l). l = 1 is equivalent to original matching.</cell></row><row><cell>l</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell></cell><cell cols="3">Global Matching</cell><cell></cell></row><row><cell>Avg</cell><cell>81.4</cell><cell>81.3</cell><cell>80.7</cell><cell>79.9</cell></row><row><cell>t/s</cell><cell>0.29</cell><cell>0.15</cell><cell>0.13</cell><cell>0.12</cell></row><row><cell></cell><cell cols="3">Multi-local Matching</cell><cell></cell></row><row><cell>Avg</cell><cell>81.4</cell><cell>80.8</cell><cell>80.1</cell><cell>79.5</cell></row><row><cell>t/s</cell><cell>0.29</cell><cell>0.26</cell><cell>0.25</cell><cell>0.25</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6</head><label>6</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7</head><label>7</label><figDesc>Ablation of other components on the DAVIS-2017 validation split.</figDesc><table><row><cell></cell><cell>Ablation</cell><cell>Avg</cell><cell>J</cell><cell>F</cell></row><row><cell>0</cell><cell>Ours (CFBI)</cell><cell>74.9</cell><cell>72.1</cell><cell>77.7</cell></row><row><cell>1</cell><cell>w/o multi-local windows</cell><cell>73.8</cell><cell>70.8</cell><cell>76.8</cell></row><row><cell>2</cell><cell>w/o sequential training</cell><cell>73.3</cell><cell>70.8</cell><cell>75.7</cell></row><row><cell>3</cell><cell>w/o collaborative ensembler</cell><cell>73.3</cell><cell>70.5</cell><cell>76.1</cell></row><row><cell>4</cell><cell>w/o balanced random-crop</cell><cell>72.8</cell><cell>69.8</cell><cell>75.8</cell></row><row><cell>5</cell><cell>w/o instance-level attention</cell><cell>72.7</cell><cell>69.8</cell><cell>75.5</cell></row><row><cell>6</cell><cell>baseline (reproduced FEELVOS)</cell><cell>68.3</cell><cell>65.6</cell><cell>70.9</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work is partly supported by ARC DP200100938 and ARC DECRA DE190101315.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Video segmentation and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Instance-level segmentation for autonomous driving with deep densely connected mrfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="669" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5188" to="5197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dual embedding learning for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast user-guided video object segmentation by interaction-and-propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5247" to="5256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Memory aggregation networks for efficient interactive video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Memory aggregated cfbi+ for interactive video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="221" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Premvos: Proposalgeneration, refinement and merging for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ACCV</title>
		<imprint>
			<biblScope unit="page" from="565" to="580" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="6499" to="6507" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Feelvos: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical image saliency detection on extended cssd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="717" to="729" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Youtube-vos: A large-scale video object segmentation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03327</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Collaborative video object segmentation by foreground-background integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Monet: Deep motion exploitation for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1140" to="1148" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2663" to="2672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast and accurate online video object segmentation via tracking parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="7415" to="7424" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Blazingly fast video object segmentation with pixel-wise metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1189" to="1198" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Videomatch: Matching based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="54" to="70" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by reference-guided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S. Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="7376" to="7385" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Video object segmentation with episodic graph memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Kernelized memory network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning what to learn for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Lawin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Contextual priming for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="169" to="191" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A real-time algorithm for signal analysis with the help of the wavelet transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Holschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kronland-Martinet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tchamitchian</surname></persName>
		</author>
		<editor>Wavelets</editor>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="286" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Gated channel transformation for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno>2020. 7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A generative appearance model for end-to-end video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brissman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="8953" to="8962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Boltvos: Box-level tracking for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04552</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Motion-guided spatial time attention for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Enhanced memory network for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<idno>2017. 7</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
