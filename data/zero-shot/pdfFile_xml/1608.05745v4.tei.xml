<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RETAIN: An Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology ? Sutter Health</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Taha Bahadori</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology ? Sutter Health</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">A</forename><surname>Kulas</surname></persName>
							<email>jkulas3@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology ? Sutter Health</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Schuetz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology ? Sutter Health</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><forename type="middle">F</forename><surname>Stewart</surname></persName>
							<email>stewarwf@sutterhealth.org</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology ? Sutter Health</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
							<email>jsun@cc.gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology ? Sutter Health</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RETAIN: An Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accuracy and interpretability are two dominant features of successful predictive models. Typically, a choice must be made in favor of complex black box models such as recurrent neural networks (RNN) for accuracy versus less accurate but more interpretable traditional models such as logistic regression. This tradeoff poses challenges in medicine where both accuracy and interpretability are important. We addressed this challenge by developing the REverse Time AttentIoN model (RETAIN) for application to Electronic Health Records (EHR) data. RETAIN achieves high accuracy while remaining clinically interpretable and is based on a two-level neural attention model that detects influential past visits and significant clinical variables within those visits (e.g. key diagnoses). RETAIN mimics physician practice by attending the EHR data in a reverse time order so that recent clinical visits are likely to receive higher attention. RETAIN was tested on a large health system EHR dataset with 14 million visits completed by 263K patients over an 8 year period and demonstrated predictive accuracy and computational scalability comparable to state-of-the-art methods such as RNN, and ease of interpretability comparable to traditional models.</p><p>Recently, recurrent neural networks (RNN) have been successfully applied in modeling sequential EHR data to predict diagnoses <ref type="bibr" target="#b26">[27]</ref> and model encounter sequences <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref>. But, the gain in accuracy</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The broad adoption of Electronic Health Record (EHR) systems has opened the possibility of applying clinical predictive models to improve the quality of clinical care. Several systematic reviews have underlined the care quality improvement using predictive analysis <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18]</ref>. EHR data can be represented as temporal sequences of high-dimensional clinical variables (e.g., diagnoses, medications and procedures), where the sequence ensemble represents the documented content of medical visits from a single patient. Traditional machine learning tools summarize this ensemble into aggregate features, ignoring the temporal and sequence relationships among the feature elements. The opportunity to improve both predictive accuracy and interpretability is likely to derive from effectively modeling temporality and high-dimensionality of these event sequences.</p><p>Accuracy and interpretability are two dominant features of successful predictive models. There is a common belief that one has to trade accuracy for interpretability using one of three types of traditional models <ref type="bibr" target="#b5">[6]</ref>: 1) identifying a set of rules (e.g. via decision trees <ref type="bibr" target="#b23">[24]</ref>), 2) case-based reasoning by finding similar patients (e.g. via k-nearest neighbors <ref type="bibr" target="#b15">[16]</ref> and distance metric learning <ref type="bibr" target="#b32">[33]</ref>), and 3) identifying a list of risk factors (e.g. via LASSO coefficients <ref type="bibr" target="#b14">[15]</ref>). While interpretable, all of these models rely on aggregated features, ignoring the temporal relation among features inherent to EHR data. As a consequence, model accuracy is sub-optimal. Latent-variable time-series models, such as <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, account for temporality, but often have limited interpretation due to abstract state variables. <ref type="figure">Figure 1</ref>: Common attention models vs. RETAIN, using folded diagrams of RNNs. (a) Standard attention mechanism: the recurrence on the hidden state vector v i hinders interpretation of the model. (b) Attention mechanism in RETAIN: The recurrence is on the attention generation components (h i or g i ) while the hidden state v i is generated by a simpler more interpretable output. from use of RNNs is at the cost of model output that is notoriously difficult to interpret. While there have been several attempts at directly interpreting RNNs <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b7">8]</ref>, these methods are not sufficiently developed for application in clinical care.</p><p>We have addressed this limitation using a modeling strategy known as RETAIN, a two-level neural attention model for sequential data that provides detailed interpretation of the prediction results while retaining the prediction accuracy comparable to RNN. To this end, RETAIN relies on an attention mechanism modeled to represent the behavior of physicians during an encounter. A distinguishing feature of RETAIN (see <ref type="figure">Figure 1</ref>) is to leverage sequence information using an attention generation mechanism, while learning an interpretable representation. And emulating physician behaviors, RETAIN examines a patient's past visits in reverse time order, facilitating a more stable attention generation. As a result, RETAIN identifies the most meaningful visits and quantifies visit specific features that contribute to the prediction.</p><p>RETAIN was tested on a large health system EHR dataset with 14 million visits completed by 263K patients over an 8 year period. We compared predictive accuracy of RETAIN to traditional machine learning methods and to RNN variants using a case-control dataset to predict a future diagnosis of heart failure. The comparative analysis demonstrates that RETAIN achieves comparable performance to RNN in both accuracy and speed and significantly outperforms traditional models. Moreover, using a concrete case study and visualization method, we demonstrate how RETAIN offers an intuitive interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>We first describe the structure of sequential EHR data and our notation, then follow with a general framework for predictive analysis in healthcare using EHR, followed by details of the RETAIN method.</p><p>EHR Structure and our Notation. The EHR data of each patient can be represented as a timelabeled sequence of multivariate observations. Assuming we use r different variables, the n-th patient of N total patients can be represented by a sequence of T (n) tuples (t</p><formula xml:id="formula_0">(n) i , x (n) i ) ? R ? R r , i = 1, . . . , T (n) . The timestamps t (n) i</formula><p>denotes the time of the i-th visit of the n-th patient and T (n) the number of visits of the n-th patient. To minimize clutter, we describe the algorithms for a single patient and have dropped the superscript (n) whenever it is unambiguous. The goal of predictive modeling is to predict the label at each time step y i ? {0, 1} s or at the end of the sequence y ? {0, 1} s . The number of labels s can be more than one.</p><p>For example, in encounter sequence modeling (ESM) <ref type="bibr" target="#b10">[11]</ref>, each visit (e.g. encounter) of a patient's visit sequence is represented by a set of varying number of medical codes {c 1 , c 2 , . . . , c n }. c j is the j-th code from the vocabulary C. Therefore, in ESM, the number of variables r = |C| and input x i ? {0, 1} |C| is a binary vector where the value one in the j-th coordinate indicates that c j was documented in i-th visit. Given a sequence of visits x 1 , . . . , x T , the goal of ESM is, for each time step i, to predict the codes occurring at the next visit x 2 , . . . , x T +1 , with the number of labels s = |C|.</p><p>In case of learning to diagnose (L2D) <ref type="bibr" target="#b26">[27]</ref>, the input vector x i consists of continuous clinical measures. If there are r different measurements, then x i ? R r . The goal of L2D is, given an input sequence x 1 , . . . , x T , to predict the occurrence of a specific disease (s = 1) or multiple diseases (s &gt; 1). Without loss of generality, we will describe the algorithm for ESM, as L2D can be seen as a special case of ESM where we make a single prediction at the end of the visit sequence.</p><p>In the rest of this section, we will use the abstract symbol RNN to denote any recurrent neural network variants that can cope with the vanishing gradient problem <ref type="bibr" target="#b2">[3]</ref>, such as LSTM <ref type="bibr" target="#b20">[21]</ref>, GRU <ref type="bibr" target="#b8">[9]</ref>, and IRNN <ref type="bibr" target="#b25">[26]</ref>, with any depth (number of hidden layers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries on Neural Attention Models</head><p>Attention based neural network models are being successfully applied to image processing <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34]</ref>, natural language processing <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30]</ref> and speech recognition <ref type="bibr" target="#b11">[12]</ref>. The utility of the attention mechanism can be seen in the language translation task <ref type="bibr" target="#b1">[2]</ref> where it is inefficient to represent an entire sentence with one fixed-size vector because neural translation machines finds it difficult to translate the given sentence represented by a single vector.</p><p>Intuitively, the attention mechanism for language translation works as follows: given a sentence of length S in the original language, we generate h 1 , . . . , h S , to represent the words in the sentence. To find the j-th word in the target language, we generate attentions ? j i for i = 1, . . . , S for each word in the original sentence. Then, we compute the context vector c j = i ? j i h i and use it to predict the j-th word in the target language. In general, the attention mechanism allows the model to focus on a specific word (or words) in the given sentence when generating each word in the target language.</p><p>We rely on a conceptually similar temporal attention mechanism to generate interpretable prediction models using EHR data. Our model framework is motivated by and mimics how doctors attend to a patient's needs and explore the patient record, where there is a focus on specific clinical information (e.g., key risk factors) working from the present to the past. <ref type="figure" target="#fig_0">Figure 2</ref> shows the high-level overview of our model, where a central feature is to delegate a considerable portion of the prediction responsibility to the process for generating attention weights. This is intended to address, in part, the difficulty with interpreting RNNs where the recurrent weights feed past information to the hidden layer. Therefore, to consider both the visit-level and the variablelevel (individual coordinates of x i ) influence, we use a linear embedding of the input vector x i . That is, we define</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Reverse Time Attention Model RETAIN</head><formula xml:id="formula_1">v i = W emb x i , (<label>Step 1)</label></formula><p>where v i ? R m denotes the embedding of the input vector x i ? R r , m the size of the embedding dimension, W emb ? R m?r the embedding matrix to learn. We can alternatively use more sophisticated yet interpretable representations such as those derived from multilayer perceptron (MLP) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>. MLP has been used for representation learning in EHR data <ref type="bibr" target="#b9">[10]</ref>.</p><p>We use two sets of weights, one for the visit-level attention and the other for variable-level attention, respectively. The scalars ? 1 , . . . , ? i are the visit-level attention weights that govern the influence of each visit embedding v 1 , . . . , v i . The vectors ? 1 , . . . , ? i are the variable-level attention weights that focus on each coordinate of the visit embedding v 1,1 , v 1,2 , . . . , v 1,m , . . . , v i,1 , v i,2 , . . . , v i,m .</p><p>We use two RNNs, RNN ? and RNN ? , to separately generate ?'s and ?'s as follows, Step 5: Making prediction. Note that in Steps 2 and 3 we use RNN in the reversed time.</p><formula xml:id="formula_2">g i , g i?1 , . . . , g 1 = RNN ? (v i , v i?1 , . . . , v 1 ), e j = w ? g j + b ? , for j = 1, . . . , i ? 1 , ? 2 , . . . , ? i = Softmax(e 1 , e 2 , . . . , e i ) (Step 2) h i , h i?1 , . . . , h 1 = RNN ? (v i , v i?1 , . . . , v 1 ) ? j = tanh W ? h j + b ? for j = 1, . . . , i, (Step 3) , # , ) , * &amp; # &amp; ) &amp; * " # " ) " * $ # $ ) $ * + # + ) + * ' # ' ) ' * ? . * / * 5 ? ? ?</formula><formula xml:id="formula_3">where g i ? R p is the hidden layer of RNN ? at time step i, h i ? R q the hidden layer of RNN ? at time step i and w ? ? R p , b ? ? R, W ? ? R m?q and b ? ? R m are the parameters to learn.</formula><p>The hyperparameters p and q determine the hidden layer size of RNN ? and RNN ? , respectively. Note that for prediction at each timestamp, we generate a new set of attention vectors ? and ?. For simplicity of notation, we do not include the index for predicting at different time steps. In Step 2, we can use Sparsemax <ref type="bibr" target="#b27">[28]</ref> instead of Softmax for sparser attention weights.</p><p>As noted, RETAIN generates the attention vectors by running the RNNs backward in time; i.e., RNN ? and RNN ? both take the visit embeddings in a reverse order v i , v i?1 , . . . , v 1 . Running the RNN in reversed time order also offers computational advantages since the reverse time order allows us to generate e's and ?'s that dynamically change their values when making predictions at different time steps i = 1, 2, . . . , T . This ensures that the attention vectors are modified at each time step, increasing the computational stability of the attention generation process. <ref type="bibr" target="#b0">1</ref> Using the generated attentions, we obtain the context vector c i for a patient up to the i-th visit as follows,</p><formula xml:id="formula_4">c i = i j=1 ? j ? j v j , (<label>Step 4)</label></formula><p>where denotes element-wise multiplication. We use the context vector c i ? R m to predict the true label y i ? {0, 1} s as follows,</p><formula xml:id="formula_5">y i = Softmax(Wc i + b), (</formula><p>Step 5) where W ? R s?m and b ? R s are parameters to learn. We use the cross-entropy to calculate the classification loss as follows,</p><formula xml:id="formula_6">L(x 1 , . . . , x T ) = ? 1 N N n=1 1 T (n) T (n) i=1 y i log( y i ) + (1 ? y i ) log(1 ? y i )<label>(1)</label></formula><p>where we sum the cross entropy errors from all dimensions of y i . In case of real-valued output y i ? R s , we can change the cross-entropy in Eq. (1) to, for example, mean squared error.</p><p>Overall, our attention mechanism can be viewed as the inverted architecture of the standard attention mechanism for NLP <ref type="bibr" target="#b1">[2]</ref> where the words are encoded by RNN and the attention weights are generated by MLP. In contrast, our method uses MLP to embed the visit information to preserve interpretability and uses RNN to generate two sets of attention weights, recovering the sequential information as well as mimicking the behavior of physicians. Note that we did not use the timestamp of each visit in our formulation. Using timestamps, however, provides a small improvement in the prediction performance. We propose a method to use timestamps in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Interpreting RETAIN</head><p>Finding the visits that contribute to prediction are derived using the largest ? i , which is straightforward. However, finding influential variables is slightly more involved as a visit is represented by an ensemble of medical variables, each of which can vary in its predictive contribution. The contribution of each variable is determined by v, ? and ?, and interpretation of ? alone informs which visit is influential in prediction but not why.</p><p>We propose a method to interpret the end-to-end behavior of RETAIN. By keeping ? and ? values fixed as the attention of doctors, we analyze changes in the probability of each label y i,1 , . . . , y i,s in relation to changes in the original input x 1,1 , . . . , x 1,r , . . . , x i,1 , . . . , x i,r . The x j,k that yields the largest change in y i,d will be the input variable with highest contribution. More formally, given the sequence x 1 , . . . , x i , we are trying to predict the probability of the output vector y i ? {0, 1} s , which can be expressed as follows</p><formula xml:id="formula_7">p(y i |x 1 , . . . , x i ) = p(y i |c i ) = Softmax (Wc i + b)<label>(2)</label></formula><p>where c i ? R m denotes the context vector. According to Step 4, c i is the sum of the visit embeddings v 1 , . . . , v i weighted by the attentions ?'s and ?'s. Therefore Eq <ref type="formula" target="#formula_7">(2)</ref> can be rewritten as follows,</p><formula xml:id="formula_8">p(y i |x 1 , . . . , x i ) = p(y i |c i ) = Softmax W i j=1 ? j ? j v j + b<label>(3)</label></formula><p>Using the fact that the visit embedding v i is the sum of the columns of W emb weighted by each element of x i , Eq <ref type="formula" target="#formula_8">(3)</ref> can be rewritten as follows,</p><formula xml:id="formula_9">p(y i |x 1 , . . . , x i ) = Softmax W i j=1 ? j ? j r k=1 x j,k W emb [:, k] + b = Softmax i j=1 r k=1 x j,k ? j W ? j W emb [:, k] + b<label>(4)</label></formula><p>where x j,k is the k-th element of the input vector x j . Eq (4) can be completely deconstructed to the variables at each input x 1 , . . . , x i , which allows for calculating the contribution ? of the k-th variable of the input x j at time step j ? i, for predicting y i as follows,</p><formula xml:id="formula_10">?(y i , x j,k ) = ? j W(? j W emb [:, k])</formula><p>Contribution coefficient</p><formula xml:id="formula_11">x j,k Input value ,<label>(5)</label></formula><p>where the index i of y i is omitted in the ? j and ? j . As we have described in Section 2.2, we are generating ?'s and ?'s at time step i in the visit sequence x 1 , . . . , x T . Therefore the index i is always assumed for ?'s and ?'s. Additionally, Eq <ref type="bibr" target="#b4">(5)</ref> shows that when we are using a binary input value, the coefficient itself is the contribution. However, when we are using a non-binary input value, we need to multiply the coefficient and the input value x j,k to correctly calculate the contribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We compared performance of RETAIN to RNNs and traditional machine learning methods. Given space constraints, we only report the results on the learning to diagnose (L2D) task and summarize the encounter sequence modeling (ESM) in Appendix C. The RETAIN source code is publicly available at https://github.com/mp2893/retain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setting</head><p>Source of data: The dataset consists of electronic health records from Sutter Health. The patients are 50 to 80 years old adults chosen for a heart failure prediction model study. From the encounter records, medication orders, procedure orders and problem lists, we extracted visit records consisting of diagnosis, medication and procedure codes. To reduce the dimensionality while preserving the clinical information, we used existing medical groupers to aggregate the codes into input variables. The details of the medical groupers are given in the Appendix B. A profile of the dataset is summarized in <ref type="table" target="#tab_0">Table 1</ref>. Implementation details: We implemented RETAIN with Theano 0.8 <ref type="bibr" target="#b3">[4]</ref>. For training the model, we used Adadelta <ref type="bibr" target="#b34">[35]</ref> with the mini-batch of 100 patients. The training was done in a machine equipped with Intel Xeon E5-2630, 256GB RAM, two Nvidia Tesla K80's and CUDA 7.5.</p><p>Baselines: For comparison, we completed the following models.</p><p>? Logistic regression (LR): We compute the counts of medical codes for each patient based on all her visits as input variables and normalize the vector to zero mean and unit variance. We use the resulting vector to train the logistic regression. ? MLP: We use the same feature construction as LR, but put a hidden layer of size 256 between the input and output. ? RNN: RNN with two hidden layers of size 256 implemented by the GRU. Input sequences x 1 , . . . , x i are used. Logistic regression is applied to the top hidden layer. We use two layers of RNN of to match the model complexity of RETAIN. ? RNN+? M : One layer single directional RNN (hidden layer size 256) along time to generate the input embeddings v 1 , . . . , v i . We use the MLP with a single hidden layer of size 256 to generate the visit-level attentions ? 1 , . . . , ? i . We use the input embeddings v 1 , . . . , v i as the input to the MLP. This baseline corresponds to <ref type="figure">Figure 1a</ref>. ? RNN+? R : This is similar to RNN+? M but uses the reverse-order RNN (hidden layer size 256) to generate the visit-level attentions ? 1 , . . . , ? i . We use this baseline to confirm the effectiveness of generating the attentions using reverse time order.</p><p>The comparative visualization of the baselines are provided in Appendix D. We use the same implementation and training method for the baselines as described above. The details on the hyperparameters, regularization and drop-out strategies for the baselines are described in Appendix B.</p><p>Evaluation measures: Model accuracy was measured by:</p><p>? Negative log-likelihood that measures the model loss on the test set. The loss can be calculated by Eq (1). ? Area Under the ROC Curve (AUC) of comparing y i with the true label y i . AUC is more robust to imbalanced positive/negative prediction labels, making it appropriate for evaluation of classification accuracy in the heart failure prediction task.</p><p>We also report the bootstrap (10,000 runs) estimate of the standard deviation of the evaluation measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Heart Failure Prediction</head><p>Objective: Given a visit sequence x 1 , . . . , x T , we predicted if a primary care patient will be diagnosed with heart failure (HF). This is a special case of ESM with a single disease outcome at the end of the sequence. Since this is a binary prediction task, we use the logistic sigmoid function instead of the Softmax in Step 5.</p><p>Cohort construction: From the source dataset, 3,884 cases are selected and approximately 10 controls are selected for each case (28,903 controls). The case/control selection criteria are fully described in the supplementary section. Cases have index dates to denote the date they are diagnosed with HF. Controls have the same index dates as their corresponding cases. We extract diagnosis codes, medication codes and procedure codes in the 18-months window before the index date.</p><p>Training details: The patient cohort was divided into the training, validation and test sets in a 0.75:0.1:0.15 ratio. The validation set was used to determine the values of the hyper-parameters. See Appendix B for details of hyper-parameter tuning.  <ref type="figure">and RETAIN)</ref> for ESM would take considerably longer than L2D, because ESM modeling generates context vectors at each time step. RNN, on the other hand, does not require additional computation other than embedding the visit to its hidden layer to predict target labels at each time step. Therefore, in ESM, the training time of the attention models will increase linearly in relation to the length of the input sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Interpretation for Heart Failure Prediction</head><p>We evaluated the interpretability of RETAIN in the HF prediction task by choosing a HF patient from the test set and calculating the contribution of the variables (medical codes in this case) to diagnostic prediction. The patient suffered from skin problems, skin disorder (SD), benign neoplasm (BN), excision of skin lesion (ESL), for some time before showing symptoms of HF, cardiac dysrhythmia (CD), heart valve disease (HVD) and coronary atherosclerosis (CA), and then a diagnosis of HF ( <ref type="figure" target="#fig_1">Figure 3</ref>). We can see that skin-related codes from the earlier visits made little contribution to HF prediction as expected. RETAIN properly puts much attention to the HF-related codes that occurred in recent visits.</p><p>To confirm RETAIN's ability to exploit the sequence information of the EHR data, we reverse the visit sequence of <ref type="figure" target="#fig_1">Figure 3a</ref> and feed it to RETAIN. <ref type="figure" target="#fig_1">Figure 3b</ref> shows the contribution of the medical codes of the reversed visit record. HF-related codes in the past are still making positive contributions, but not as much as they did in <ref type="figure" target="#fig_1">Figure 3a</ref>. <ref type="figure" target="#fig_1">Figure 3b</ref> also emphasizes RETAIN's superiority to interpretable, but stationary models such as logistic regression. Stationary models often aggregate past information and remove the temporality from the input data, which can mistakenly lead to the same risk prediction for <ref type="figure" target="#fig_1">Figure 3a</ref> and 3b. RETAIN, however, can correctly digest the sequence information and calculates the HF risk score of 9.0%, which is significantly lower than that of <ref type="figure" target="#fig_1">Figure 3a</ref>. prediction (0.2165) of <ref type="figure" target="#fig_1">Figure 3c</ref> is lower than that of <ref type="figure" target="#fig_0">Figure 3a (0.2474)</ref>. This suggests that taking proper medications can help the patient in reducing their HF risk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Our approach to modeling event sequences as predictors of HF diagnosis suggest that complex models can offer both superior predictive accuracy and more precise interpretability. Given the power of RNNs for analyzing sequential data, we proposed RETAIN, which preserves RNN's predictive power while allowing a higher degree of interpretation. The key idea of RETAIN is to improve the prediction accuracy through a sophisticated attention generation process, while keeping the representation learning part simple for interpretation, making the entire algorithm accurate and interpretable. RETAIN trains two RNN in a reverse time order to efficiently generate the appropriate attention variables. For future work, we plan to develop an interactive visualization system for RETAIN and evaluating RETAIN in other healthcare applications.</p><p>A A method to use the timestamps As before, we use t (n) i to represent the timestamp of the i-th visit of the n-th patient. In the following, we suppress the superscript (n) to avoid cluttered notation. Note that the timestamp t i can be anything that provides the temporal information of the i-th visit: the number of days from the first visit, the number of days between two consecutive visits, or the number of days until the index date of some event such as heart failure diagnosis.</p><p>In order to use the timestamps, we modify Step 2 and Step 3 in Section 2.2 as follows:</p><formula xml:id="formula_12">g i , g i?1 , . . . , g 1 = RNN ? (v i , v i?1 , . . . , v 1 ), e j = w ? g j + b ? , for j = 1, . . . , i ? 1 , ? 2 , . . . , ? i = Softmax(e 1 , e 2 , . . . , e i ) h i , h i?1 , . . . , h 1 = RNN ? (v i , v i?1 , . . . , v 1 ) ? j = tanh W ? h j + b ? for j = 1, . . . , i, where v i = [v i , t i ]</formula><p>where we use v i , the concatenation of the visit embedding v i and the timestamp t i , to generate the attentions ? and ?. However, when obtaining the context vector c i as per Step 4, we use v i , not v i to match the dimensionality. The entire process could be understood such that we use the temporal information not to embed each visit, but to calculate the attentions for the entire visit sequence. This is consistent with our modeling approach where we lose the sequential information in embedding the visit with MLP, then recover the sequential information by generating the attentions using the RNN. By using the temporal information, specifically the log of the number of days from the first visit, we were able to improve the heart failure prediction AUC by 0.003 without any hyper-parameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Details of the experiment settings B.1 Hyper-parameter Tuning</head><p>We used the validation set to tune the hyper-parameters: visit embedding size m, RNN ? 's hidden layer size p, RNN ? 's hidden layer size q, L 2 regularization coefficient, and drop-out rates.</p><p>L 2 regularization was applied to all weights except the ones in RNN ? and RNN ? . Two separate drop-outs were used on the visit embedding v i and the context vector c i . We performed the random search with predefined ranges m, p, q ? {32, 64, 128, 200, 256}, L 2 ? {0.1, 0.01, 0.001, 0.0001}, dropout vi , dropout ci ? {0.0, 0.2, 0.4, 0.6, 0.8}. We also performed the random search with m, p and q fixed to 256.</p><p>The final value we used to train RETAIN for heart failure prediction is m, p, q = 128, dropout vi = 0.6, dropout ci = 0.6 and 0.0001 for the L 2 regularization coefficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Code Grouper</head><p>Diagnosis codes, medication codes and procedure codes in the dataset are respectively using International Classification of Diseases (ICD-9), Generic Product Identifier (GPI) and Current Procedural Terminology (CPT  ? LR: We use 0.01 L 2 regularization coefficient for the logistic regression weight.</p><p>? MLP: We use drop-out rate 0.6 on the output of the hidden layer. We use 0.0001 L 2 regularization coefficient for the hidden layer weight and the logistic regression weight.</p><p>? RNN: We use drop-out rate 0.6 on the outputs of both hidden layers. We use 0.0001 L 2 regularization coefficient for the logistic regression weight. The dimension size of both hidden layers is 256.</p><p>? RNN+? M : We use drop-out rate 0.4 on the output of the hidden layer and 0.6 on the output of the context vector i ? i v i . We use 0.0001 L 2 regularization coefficient for the hidden layer weight of the MLP that generates ?'s and the logistic regression weight. The dimension size of the hidden layers in both RNN and MLP is 256.</p><p>? RNN+? R : We use drop-out rate 0.4 on the output of the hidden layer and 0.6 on the output of the context vector i ? i v i . We use 0.0001 L 2 regularization coefficient for the hidden layer weight of the RNN that generates ?'s and the logistic regression weight. The dimension size of the hidden layers in both RNNs is 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Heart Failure Case/Control Selection Criteria</head><p>Case patients were 40 to 85 years of age at the time of HF diagnosis. HF diagnosis (HFDx) is defined as: 1) Qualifying ICD-9 codes for HF appeared in the encounter records or medication orders. Qualifying ICD-9 codes are displayed in <ref type="table" target="#tab_3">Table 3</ref>. 2) a minimum of three clinical encounters with qualifying ICD-9 codes had to occur within 12 months of each other, where the date of diagnosis was assigned to the earliest of the three dates. If the time span between the first and second appearances of the HF diagnostic code was greater than 12 months, the date of the second encounter was used as the first qualifying encounter. The date at which HF diagnosis was given to the case is denoted as HFDx. Up to ten eligible controls (in terms of sex, age, location) were selected for each case, yielding an overall ratio of 9 controls per case. Each control was also assigned an index date, which is the HFDx of the matched case. Controls are selected such that they did not meet the operational criteria for HF diagnosis prior to the HFDx plus 182 days of their corresponding case. Control subjects were required to have their first office encounter within one year of the matching HF case patient's first office visit, and have at least one office encounter 30 days before or any time after the case's HF diagnosis date to ensure similar duration of observations among cases and controls.</p><p>C Results on encounter sequence modeling Objective: Given a sequence of visits x 1 , . . . , x T , the goal of encounter sequence modeling is, for each time step i, to predict the codes occurring at the next visit x 2 , . . . , x T +1 . In this experiment, we focus on predicting the diagnosis codes in the encounter sequence, so we create a separate set of labels y 1 , . . . , y T that do not contain non-diagnosis codes such as medication codes or procedure codes. Therefore y i will contain diagnosis codes from the next visit x i+1 .</p><p>Dataset: We divide the entire dataset described in <ref type="table" target="#tab_0">Table 1</ref> into 0.75:0.10:0.15 ratio, respectively for training set, validation set, and test set.</p><p>Baseline: We use the same baseline models we used for HF prediction. However, since we are predicting 283 binary labels now, we replace the logistic regression function with the Softmax function. The drop-out and L 2 regularization policies remain the same.</p><p>For LR and MLP, at each step i, we aggregate maximum ten past input vectors 6 x i?9 , . . . , x i to create a pseudo-context vector c i . LR applies the Softmax function on top of c i . MLP places a hidden layer on top of c i then applies the Softmax function.</p><p>Evaluation metric: We use the negative log likelihood Eq (1) on the test set to evaluate the model performance. We also use Recall@k as an additional metric to measure the prediction accuracy.</p><p>? Recall@k: Given a sequence of visits x 1 , . . . , x T , we evaluate the model performance based on how accurately it can predict the diagnosis codes y 1 , . . . , y T . We use the average Recall@k, which is expressed as below,</p><formula xml:id="formula_13">1 N N n=1 1 T (n) T (n) i=1</formula><p>Recall@k( y i ), where Recall@k( y i ) = |argsort( y i )[: k] ? nonzero(y i )| |nonzero(y i )|</p><p>where argsort returns a list of indices that will decrementally sort a given vector and nonzero returns a list of indices of the coordinates with non-zero values. We use Recall@k because of its similar nature to the way a human physician performs the differential diagnostic procedure, which is to generate a list of most likely diseases for an undiagnosed patient, then perform medical practice until the true disease, or diseases are determined.</p><p>Prediction accuracy: <ref type="table" target="#tab_4">Table 4</ref> displays the prediction performance of RETAIN and the baselines. We use k = 5, 10 for Recall@k to allow a reasonable number of prediction trials, as well as cover complex patients who often receive multiple diagnosis codes at a single visit.</p><p>RNN shows the best prediction accuracy for encounter diagnosis prediction. However, considering the purpose of encounter diagnosis prediction, which is to assist doctors to provide quality care for the patient, black-box behavior of RNN makes it unattractive as a clinical tool. On the other hand, RETAIN performs as well as other attention models, only slightly inferior to RNN, provides full interpretation of its prediction behavior, making it a feasible solution for clinical applications.</p><p>The interesting finding in <ref type="table" target="#tab_4">Table 4</ref> is that MLP is able to perform as accurately as RNN+? M in terms of Recall@10. Considering the fact that MLP uses aggregated information of past ten visits, we can assume that encounter diagnosis prediction depends more on the frequency of disease occurrences rather than the order in which they occurred. This is quite different from the HF prediction task, where stationary models (LF, MLP) performed significantly worse than sequential models.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>TimeFigure 2 :</head><label>2</label><figDesc>Unfolded view of RETAIN's architecture: Given input sequence x 1 , . . . , x i , we predict the label y i . Step 1: Embedding, Step 2: generating ? values using RNN ? , Step 3: generating ? values using RNN ? , Step 4: Generating the context vector using attention and representation vectors, and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3c shows howSDFigure 3 :</head><label>3</label><figDesc>the contributions of codes change when selected medication data are used in the model. We added two medications from day 219: antiarrhythmics (AA) and anticoagulants (AC), both of which are used to treat cardiac dysrhythmia (CD). The two medications make a negative contributions, especially towards the end of the record. The medications decreased the positive contributions of heart valve disease and cardiac dysrhythmia in the last visit. Indeed, the HF risk (a) Temporal visualization of a patient's visit records where the contribution of variables for diagnosis of heart failure (HF) is summarized along the x-axis (i.e. time) with the y-axis indicating the magnitude of visit and code specific contributions to HF diagnosis. (b) We reverse the order of the visit sequence to see if RETAIN can properly take into account the modified sequence information. (c) Medication codes are added to the visit record to see how it changes the behavior of RETAIN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 4</head><label>44</label><figDesc>Graphical illustration of the baselines: (a) Logistic regression (LR), (b) Multilayer Perceptron (MLP), (c) Recurrent neural network (RNN), (d) RNN with attention vectors generated via an MLP (RNN+? M ), (e) RNN with attention vectors generated via an RNN (RNN+? R ). RETAIN is given in Figure 1b. D Illustration and comparison of the baselines illustrates the baselines used in the experiments and shows the relationship among them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of EHR dataset. (D:Diagnosis, R:Medication, P:Procedure)</figDesc><table><row><cell># of patients</cell><cell>263,683</cell><cell>Avg. # of codes in a visit</cell><cell>3.03</cell></row><row><cell># of visits</cell><cell>14,366,030</cell><cell>Max # of codes in a visit</cell><cell>62</cell></row><row><cell>Avg. # of visits per patient</cell><cell>54.48</cell><cell cols="2">Avg. # of Dx codes in a visit 1.83</cell></row><row><cell># of medical code groups</cell><cell cols="2">615 (D:283, R:94, P:238) Max # of Dx in a visit</cell><cell>42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Heart failure prediction performance of RETAIN and the baselines Logistic regression and MLP underperformed compared to the four temporal learning algorithms (Table 2). RETAIN is comparable to the other RNN variants in terms of prediction performance while offering the interpretation benefit.Note that RNN+? R model are a degenerated version of RETAIN with only scalar attention, which is still a competitive model as shown in table 2. This confirms the efficiency of generating attention weights using the RNN. However, RNN+? R model only provides scalar visit-level attention, which is not sufficient for healthcare applications. Patients often receives several medical codes at a single</figDesc><table><row><cell>Model</cell><cell>Test Neg Log Likelihood</cell><cell>AUC</cell><cell cols="2">Train Time / epoch Test Time</cell></row><row><cell>LR</cell><cell>0.3269 ? 0.0105</cell><cell>0.7900 ? 0.0111</cell><cell>0.15s</cell><cell>0.11s</cell></row><row><cell>MLP</cell><cell>0.2959 ? 0.0083</cell><cell>0.8256 ? 0.0096</cell><cell>0.25s</cell><cell>0.11s</cell></row><row><cell>RNN</cell><cell>0.2577 ? 0.0082</cell><cell>0.8706 ? 0.0080</cell><cell>10.3s</cell><cell>0.57s</cell></row><row><cell>RNN+? M</cell><cell>0.2691 ? 0.0082</cell><cell>0.8624 ? 0.0079</cell><cell>6.7s</cell><cell>0.48s</cell></row><row><cell>RNN+? R</cell><cell>0.2605 ? 0.0088</cell><cell>0.8717 ? 0.0080</cell><cell>10.4s</cell><cell>0.62s</cell></row><row><cell>RETAIN</cell><cell>0.2562 ? 0.0083</cell><cell>0.8705 ? 0.0081</cell><cell>10.8s</cell><cell>0.63s</cell></row><row><cell>Results:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>visit, and it will be important to distinguish their relative importance to the target. We show such a case study in section 4.3. Table 2 also shows the scalability of RETAIN, as its training time (the number of seconds to train the model over the entire training set once) is comparable to RNN. The test time is the number of seconds to generate the prediction output for the entire test set. We use the mini-batch of 100 patients when assessing both training and test times. RNN takes longer than RNN+? M because of its two-layer structure, whereas RNN+? M uses a single layer RNN. The models that use two RNNs (RNN, RNN+? R , RETAIN) 2 take similar time to train for one epoch. However, each model required a different number of epochs to converge. RNN typically takes approximately 10 epochs, RNN+? M and RNN+? R 15 epochs and RETAIN 30 epochs. Lastly, training the attention models (RNN+? M , RNN+? R</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Qualifying ICD-9 codes for heart failure B.3 Training Specifics of the Basline Models</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Encounter diagnosis prediction performance of RETAIN and the baselines</figDesc><table><row><cell>Model</cell><cell>Negative Likelihood</cell><cell>Recall@5 Recall@10</cell></row><row><cell></cell><cell></cell><cell>?</cell></row><row><cell></cell><cell></cell><cell>+ *</cell></row><row><cell></cell><cell></cell><cell>(e)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For example, feeding visit embeddings in the original order to RNN? and RNN ? will generate the same e1 and ?1 for every time step i = 1, 2, . . . , T . Moreover, in many cases, a patient's recent visit records deserve more attention than the old records. Then we need to have ej+1 &gt; ej which makes the process computationally unstable for long sequences.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The RNN baseline uses two layers of RNN, RNN+?R uses one for visit embedding and one for generating ?, RETAIN uses each for generating ? and ?</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp 4 http://www.wolterskluwercdi.com/drug-data/medi-span-electronic-drug-file/ 5 https://www.hcup-us.ahrq.gov/toolssoftware/ccs_svcsproc/ccssvcproc.jsp</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We also tried aggregating all past input vectors x1, . . . , xi, but the performance was slightly worse than using just ten.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiple object recognition with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SciPy</title>
		<meeting>SciPy</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The impact of ehealth on the quality and safety of health care: a systematic overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Car</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pagliari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Anandan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cresswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bokun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mckinstry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Procter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Majeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Med</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1000387</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Systematic review: impact of health information technology on quality, efficiency, and costs of medical care</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maglione</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mojica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Shekelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of internal medicine</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="742" to="752" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Distilling knowledge from deep networks with applications to healthcare domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Khemani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03542</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-layer representation learning for medical concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Bahadori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Searles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Coffey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Doctor ai: Predicting clinical events via recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Bahadori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05942</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Visualizing higher-layer features of a deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Montreal</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Predicting clinical events by combining static and dynamic information using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Esteban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Staeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02685</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Clinical predictors of progression to Alzheimer disease in amnestic mild cognitive impairment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Fleisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Sowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Gamst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Thal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T A D C</forename><surname>Study</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurology</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="1588" to="1595" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bringing cohort studies to the bedside: framework for a &apos;green button&apos; to support clinical decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">O</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sivaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Longhurst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Coiera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Comparative Effectiveness Research</title>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sequence learning with recurrent networks: analysis of internal representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Karamcheti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aerospace Sensing</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="449" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Costs and benefits of health information technology: new trends from the literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Goldzweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Towfigh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maglione</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Shekelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Health affairs</publisher>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="282" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04623</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1684" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Use of electronic health records in us hospitals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Desroches</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Donelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Ferris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shields</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blumenthal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">N Engl J Med</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02078</idno>
		<title level="m">Visualizing and understanding recurrent networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Use of diverse electronic medical record systems to identify genetic risk for type 2 diabetes within a genome-wide association study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Kho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMIA</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="212" to="218" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to Diagnose with LSTM Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wetzell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">From softmax to sparsemax: A sparse model of attention and multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Astudillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning individual and population level traits from clinical temporal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Penn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS, Predictive Models in Personalized Medicine workshop</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A probabilistic graphical model for individualizing prognosis in chronic, complex diseases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schulam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page">143</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Supervised patient similarity measure of heterogeneous patient records</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edabollahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="24" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Adadelta: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701,2012.LR0.0288</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
