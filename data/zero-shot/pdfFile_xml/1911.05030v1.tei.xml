<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">0-1 phase transitions in sparse spiked matrix estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Barbier</surname></persName>
							<email>.jbarbier@ictp.it</email>
							<affiliation key="aff0">
								<orgName type="department">International Center for Theoretical Physics</orgName>
								<address>
									<settlement>Trieste</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Macris</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">) Ecole Polytechnique F?d?rale de Lausanne</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">0-1 phase transitions in sparse spiked matrix estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider statistical models of estimation of a rank-one matrix (the spike) corrupted by an additive gaussian noise matrix in the sparse limit. In this limit the underlying hidden vector (that constructs the rank-one matrix) has a number of non-zero components that scales sub-linearly with the total dimension of the vector, and the signal strength tends to in nity at an appropriate speed. We prove explicit low-dimensional variational formulas for the asymptotic mutual information between the spike and the observed noisy matrix in suitable sparse limits. For Bernoulli and Bernoulli-Rademacher distributed vectors, and when the sparsity and signal strength satisfy an appropriate scaling relation, these formulas imply sharp 0-1 phase transitions for the asymptotic minimum mean-square-error. A similar phase transition was analyzed recently in the context of sparse high-dimensional linear regression (compressive sensing) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Low rank matrix estimation (or factorization) is an important problem with numerous applications in image processing, principal component analysis (PCA), machine learning, DNA microarray data, tensor decompositions, etc. These modern applications often require to look at the high-dimensional limit and sparse limits of the problem. Sparsity is often a crucial ingredient for the interpretability of high dimensional statistical models. In this context, it is of great importance to determine computational limits of estimation and to benchmark them by the fundamental information theoretical (i.e., statistical) limits. In this paper we concentrate on information theoretic limits for two probabilistic models, the so-called sparse spiked Wishart and Wigner matrix models.</p><p>In the simplest rank-one version one seeks a matrix U ? V constructed from high-dimensional hidden vectors U = (U 1 , . . . , U n ) ? R n and V = (V 1 , . . . , V m ) ? R m , m = ? n n, based on a noisy observed data matrix W with entries obtained as W ij ? N ( ? n /n U i V j , 1) for i = 1, . . . , n, j = 1, . . . , m and ? n &gt; 0 the signal strength. The hidden vectors have independent identically distributed (i.i.d.) components drawn from two di erent distributions. The high-dimensional limit means that we look at n, m ? +?, ? n ? ? &gt; 0. We suppose that V has on average ? V,n m non-zero component which scales sub-linearly for a sequence ? V,n ? 0 + . We will see that non-trivial estimation is only possible if ? n ? +? (whereas if ? V,n ? ? V &gt; 0, ? n ? ? &gt; 0 nite). The problem is to estimate U ? V given the data matrix W 1 . In the Bayesian setting, which is our concern here, it is supposed that the priors and hyper-parameters are all known. We will refer to this problem as the sparse spiked Wishart matrix model.</p><p>A popular version of this model, and one addressed here, corresponds to a xed standard gaussian distribution for U ? N (0, I n ) (I n is the n?n identity matrix) and a Bernoulli-Rademacher distribution for V i ? P V,n = (1?? V,n )? 0 + 1 2 ? V,n (? ?1 +? 1 ). This estimation problem is equivalent to the important "spiked covariance model" or "gaussian sparse-PCA" <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> which amounts to estimate a sparse binary matrix V ? V from samples generated by the normal law N (0, I n + ? n V ? V ). An even simpler and paradigmatic matrix estimation problem has a symmetric data matrix W with elements drawn as W ij ? N ( ? n /n X i X j , 1) for 1 ? i &lt; j ? n and X = (X 1 , . . . , X n ) ? R n with i.i.d. components, with n ? +? in the high-dimensional limit. Again, the sparse version corresponds to having a sub-linear number of non-zero components, i.e., ? n n with ? n ? 0 + , and nontrivial estimation is possible only for ? n ? +?. We call this model the sparse spiked Wigner matrix model. We will focus in particular on binary vectors generated from Bernoulli X i ? P X,n = Ber(? n ) or Bernoulli-Rademacher X i ? P X,n = (1 ? ? n )? 0 + 1 2 ? n (? ?1 + ? 1 ) distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Background</head><p>Much progress has been accomplished in recent years on spiked matrix models for non-sparse settings, by which we mean that the distributions P X , P U , P V are xed independent of n, m, and thus the number of non-zero components of X, V , even if "small", scales linearly with n. An interesting phenomenology of information theoretical (or statistical) as well as computational limits has been derived <ref type="bibr" target="#b4">[5]</ref> by heuristic methods of statistical physics of spin glass theory (the so-called replica method). In the asymptotic regime of n ? +? these limits take the form of sharp phase transitions. The rigorous mathematical theory of these phase transitions is now largely under control. On one hand, the approximate message passing (AMP) algorithm has been analyzed by state evolution <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. And on the other hand, the asymptotic mutual informations per variable between hidden spike and data matrices, have been rigorously computed in a series of works using various methods (cavity method, spatial coupling, interpolation methods) <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>. The information theoretic phase transitions are then signalled by singularities, as a function of the signal strength, in the limit of the mutual information per variable when n ? +?. The phase transition also manifests itself as a jump discontinuity in the minimum mean-square-error (MMSE) <ref type="bibr" target="#b1">2</ref> . Once the mutual information is known it is usually possible to deduce the MMSE. For example, in the simplest case of the spiked Wigner model, if I(X ? X; W ) is the mutual information between the spike X ? X and the data W , the MMSE(X ? X|W ) = E X ? X ? E[X ? X|W ] 2 F satis es the I-MMSE relation (such relations are derived in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, see also appendix 11) d d? n 1 n I(X ? X; W ) = 1 4n 2 MMSE(X ? X|W ) + O(n ?1 ) .</p><p>Closed form expressions for the asymptotic mutual information <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref> therefore allow to benchmark the fundamental information theoretical limits of estimation. See also <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref> for results on the limits of detecting the precense of a spike in a noisy matrix, rather than estimating it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Our contributions</head><p>In this paper we are exclusively interested in determining information theoretic phase transitions in regimes of sub-linear sparsity. We identify the correct scaling regimes of vanishing sparsity and diverging lim n + MMSE((X i X j ) i &lt; j |W)/(n ) 2 <ref type="figure">Figure 1</ref>: A sequence of suitably normalized mutual information and minimum mean-square-error (MMSE) curves as a function of ?/?c(?) for the symmetric matrix estimation model for Xi ? Ber(?). Here ?c(?) = 4| ln ?|/?. In the sparse limit ? ? 0 the MMSE curves approach a 0-1 phase transition with the discontinuity at ? = ?c(?). This corresponds to an angular point for the mutual information.</p><p>signal strength in which non-trivial information theoretic phase transitions occur. We use the adaptive interpolation method <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref> rst introduced in the non-sparse matrix estimation problems, to provide for the sparse limit, closed form expressions of the mutual information in terms of low-dimensional variational expressions (theorems 1 and 4 in section 2). That the adaptive interpolation method can be extended to the sparse limit is interesting and not a priori obvious. By the I-MMSE relation and the solution of the variational problems we then nd, for Bernoulli and Bernoulli-Rademacher distributions of the sparse signal, that the MMSE displays to a 0-1 phase transition and we determine the exact thresholds.</p><p>Let us describe the regimes studied and the information theoretical thresholds found here (precise statements are found in section 2). We rst note that for sub-linear sparsity, a phase transition appears only if the signal strength tends to in nity. For the Wigner case, for example, this can be seen from the following heuristic argument: the total signal-to-noise ratio (SNR) per non-zero component (i.e., SNR per observation (? n /n)? 2 n times the number of observations ?(n 2 ) divided by the number of non-zero components ? n n) scales as (? n /n)? 2 n n 2 /(? n n) = ? n ? n so that ? n ? +? is necessary in order to have enough energy to estimate the non-zero components. Our analysis shows that non-trivial phase transitions occur when ? n = ?(| ln ? n |/? n ) (Wigner case) and ? n = ?( | ln ? V,n |/? V,n ) (Wishart case) when ? n and ? V,n tend to zero slowly enough.</p><p>We study in particular the cases of binary signals, i.e., P X,n and P V,n equal to Ber(? n ) or Bernoulli-</p><formula xml:id="formula_0">Rademacher (1 ? ? n )? 0 + 1 2 ? n (? ?1 + ? 1 )</formula><p>. For these distributions we nd 0-1 phase transitions at the level of the MMSE as long as ? n ? 0 + and ? V,n ? 0 + not too fast. This is illustrated on gure 1 for the Wigner case with Bernoulli distribution. The left hand side shows that as ? n ? 0 + the (suitably normalized) mutual information approaches the broken line with an angular point at ?/? c (? n ) = 1 where ? c (? n ) = 4| ln ? n |/? n ; in the case of Bernoulli-Rademacher distribution the threshold is the same. On the right hand side the (suitably normalized) MMSE approaches a 0-1 curve: it tends to 1 for ?/? c (? n ) &lt; 1, develops a jump discontinuity at ?/? c (? n ) = 1, and takes the value 0 when ?/? c (? n ) &gt; 1. A similar 0-1 transition is found to hold for the MMSE of V ? V in the spiked covariance model with a threshold ? c (? V,n ) = 4| ln ? n |/(? n ? n ) (with ? n ? ?). This is illustrated on gure 2 in section 2. Note that these gures are obtained from the asymptotic prediction where rst n ? +? and then ? ? 0 + , so not in the sub-linear sparsity regime. Our analysis con rms that this picture with its sharp transition holds in the truly sparse (sub-linear) regime ? n ? 0 + with n ? +?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Related work</head><p>Spiked matrix ensembles have played a crucial role in the analysis of threshold phenomena in highdimensional statistical models for almost two decades. Early rigorous results are found in <ref type="bibr" target="#b23">[24]</ref> who determined by spectral methods the location of the information theoretic phase transition point in a spiked covariance model, and <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> for the Wigner case. More recently, the information theoretic limits and those of hypothesis testing have been derived, with the additional structure of sparse vectors, for large but nite sizes <ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref>. These estimates are consistent with our results. The additional feature that we provide here, is an asymptotic limit in which a sharp 0-1 phase transition is identi ed, with fully explicit formulas for the thresholds. Moreover closed form expressions for the mutual information are also determined.</p><p>The 0-1 transitions and formulas for the thresholds and mutual information were rst computed in <ref type="bibr" target="#b4">[5]</ref> using the heuristic replica method of spin-glass theory. However, it must be stressed that, not only this calculation is far from rigorous, but more importantly the limit n ? +? is rst taken for xed parameters ? n = ?, ? V,n = ? V , and the sparse limit ?, ? V ? 0 + is taken only after. Although the thresholds found in this way agree with our derivation of ? c (? n ), this is far from evident a priori. For example, it not clear if this sort of approach yields correct computational thresholds in the sparse limit <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>Similar phase transitions in sublinear sparse regimes for binary signals (Bernoulli or Bernoulli-Rademacher) have been studied in the context of linear estimation or compressed sensing <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> for support recovery. These works focus on the MMSE and prove the occurence of the 0-1 phase transition which they call an "all-or-nothing" phenomenon. We note that our approach is technically very di erent in that it determines the variational expressions for mutual informations and nds the transitions as a consequence.</p><p>A lot of e orts have been devoted to computational aspects of sparse PCA with many remarkable results <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref>. The picture that has emerged is that the information theoretic and computational phase transition regimes are not on the same scale and that the computational-to-statistical gap diverges in the limit of vanishing sparsity. Note that this is also seen within the context of state evolution for the AMP algorithm <ref type="bibr" target="#b4">[5]</ref>, but with the sparse limit taken after the n ? +? limit. It would be desirable to rigorously determine the thresholds of the AMP algorithm and the correct scaling regime of ? n ? +? and ? n ? 0 + or ? V,n ? 0 + where a computational phase transition is observed. We believe that techniques developed for compressed sensing with nite size samples <ref type="bibr" target="#b36">[37]</ref> could also apply here.</p><p>2 Sparse spiked matrix models: setting and main results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sparse spiked Wigner matrix model</head><p>We consider a sparse signal-vector X = (X 1 , . . . , X n ) ? R n with n i.i.d. components distributed according to P X,n = ? n p X + (1 ? ? n )? 0 . Here ? 0 is the Dirac mass at zero and (? n ) ? (0, 1] N is a sequence of weights. For the distribution p X we assume that : i) it is independent of n, ii) it has nite support in an interval [?S, S], iii) it has second moment equal to 1 (without loss of generality). One has access to the symmetric data matrix W ? R n?n with noisy entries</p><formula xml:id="formula_1">W = ? n n X ? X + Z , 1 ? i &lt; j ? n ,<label>(1)</label></formula><p>where ? n &gt; 0 controls the strength of the signal and the noise is i.i.d. gaussian Z ij ? N (0, 1) for i &lt; j and symmetric Z ij = Z ji .</p><p>We are interested in sparse regimes where ? n ? 0 + and ? n ? +?. While our results are more general (see appendix 4 and theorem 3) our main interest is in a regime of the form</p><formula xml:id="formula_2">? n = 4?| ln ? n | ? n , ? n = ?(n ?? ),<label>(2)</label></formula><p>for ?, ? ? R ?0 and ? small enough. We prove that in this regime a phase transition occurs as function of ?. The phase transition manifests itself as a singularity (more precisely a discontinuous rst order derivative) in the mutual information I(X ? X; W ) = H(W ) ? H(W |X ? X). Note that because the data W depends on X only through X ? X we have H(W |X ? X) = H(W |X) and therefore I(X ? X; W ) = I(X; W ). From now on we use the form I(X; W ). To state the precise result we de ne the potential function:</p><formula xml:id="formula_3">i pot n (q, ?, ?) ? ? 4 (q ? ?) 2 + I n (X; ?qX + Z) ,<label>(3)</label></formula><p>where I n (X; ? ?qX + Z) is the mutual information for a scalar gaussian channel, with X ? P X,n and Z ? N (0, 1). The mutual information I n is indexed by n because of its dependence on P X,n .</p><p>Theorem 1 (Sparse spiked Wigner model). Let the sequences ? n and ? n verify (2) with ? ? [0, 1/6) and ? &gt; 0. There exists C &gt; 0 independent of n such that</p><formula xml:id="formula_4">1 ? n | ln ? n | 1 n I(X; W ) ? inf q?[0,?n] i pot n (q; ? n , ? n ) ? C (ln n) 1/3 n (1?6?)/7 .<label>(4)</label></formula><p>The mutual information is thus given, to leading order, by a one-dimensional variational problem</p><formula xml:id="formula_5">I(X; W ) = n? n | ln ? n | inf q?[0,?n]</formula><p>i pot n (q; ? n , ? n ) + correction terms .</p><p>The factor ? n | ln ? n | is naturally related to the entropy (in nats) of the support of the signal ?n(? n ln ? n + (1 ? ? n ) ln(1 ? ? n )) which behaves like n? n | ln ? n | for ? n ? 0 + . In particular, for both the Bernoulli and Bernoulli-Rademacher distributions an analytical solution of the variational problem (given in appendix 9) shows that lim n?+? 1 n? n | ln ? n | I(X; W ) = ?I(? ? 1) + I(? ? 1) .</p><p>This is also seen numerically on gure 1 (for the Bernoulli case). The I-MMSE relation (see introduction) then shows that the suitably rescaled MMSE is simply given by a derivative w.r.t. ? and therefore displays a 0-1 phase transition at ? = 1 (or equivalently at the critical threshold ? c (? n ) = 4| ln ? n |/? n ) as depicted on the right hand side of gure 1. We do not claim that <ref type="bibr" target="#b4">(5)</ref> and the consequence for the MMSE are rigorously derived. However these results are "contained" in the variational expression for the mutual information and are "mere consequences" of a precise analysis of this one-dimensional variational problem. For more generic distributions than these two cases the situation is richer. Although one generically observes phase transitions in the same scaling regime, the limiting curves appear to be more complicated than the simple 0-1 shape and the jumps are not necessarily located at ? = 1. A classi cation of these transitions is an interesting problem that is out of the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sparse spiked Wishart model</head><p>The sparse spiked Wishart model is a non-symmetric version of the previous one. There are two distinct vectors U = (U 1 , . . . , U n ) ? R n and V = (V 1 , . . . , V m ) ? R m with dimensions of the same order of magnitude. We set m = ? n n and will let ? n ? ? &gt; 0 as n ? +?. The data matrix W ? R n?m is</p><formula xml:id="formula_7">W = ? n n U ? V + Z</formula><p>where ? n &gt; 0 and R n?m Z = (Z ij ) i,j , i = 1, . . . , n, j = 1, . . . , m, is a Wishart noise matrix with i.i.d. standard gaussian entries. Both the entries of U , V are i.i.d. and drawn from possibly sparse distributions. Speci cally</p><formula xml:id="formula_8">U i ? P U,n ? ? U,n p U + (1 ? ? U,n )? 0 and V i ? P V,n ? ? V,n p V + (1 ? ? V,n )? 0 .</formula><p>We assume that both p U and p V have nite support included in an interval [?S, S] and (without loss of generality) they both have unit second moment. Our main interest is in regimes of the form</p><formula xml:id="formula_9">? n ? ? &gt; 0 , ? U,n ? ? U &gt; 0 , ? V,n = ?(n ?? ), ? n = 4?| ln ? V,n | ? n ? V,n .<label>(6)</label></formula><p>This scaling allows to greatly simplify the analysis and is the proper scaling regime to observe the information theoretic phase transition. Many of our results hold in wider generality (see appendix 4). The main result is again a variational expression for the mutual information</p><formula xml:id="formula_10">I(U ? V ; W ) = I((U , V ); W ) = H(W ) ? H(W |U , V )</formula><p>between the spike (or signal-vectors) and the data matrix, in terms of a potential function:</p><formula xml:id="formula_11">i pot n (q U , q V , ?, ?, ? U , ? V ) = ?? 2 (q U ? ? U )(q V ? ? V ) + I n (U ; ??q V U + Z) + ?I n (V ; ?q U V + Z) (7)</formula><p>where I n (U ; ? ??q V U + Z) is the mutual information for a scalar gaussian channel, with U ? P U,n and Z ? N (0, 1), while I n (V ;</p><formula xml:id="formula_12">? ?q U V + Z) is with V ? P V,n .</formula><p>Our main result reads:</p><p>Theorem 2 (Sparse spiked Wishart model). Consider the scaling regime (6) with ? ? [0, 1/3). There exists a constant C &gt; 0 independent of n such that</p><formula xml:id="formula_13">1 ? V,n | ln ? V,n | 1 n I (U , V ); W ? inf q U ?[0,? U,n ] sup q V ?[0,? V,n ] i pot n q U , q V , ? n , ? n , ? U,n , ? V,n ? C (ln n) 1/3 n (4?12?)/18 .</formula><p>To leading order the mutual information is given by the solution of a two-dimensional variational problem. An analytical solution of this problem for the spiked covariance model U ? N (0, I n ) and</p><formula xml:id="formula_14">V i ? (1 ? ? V,n )? 0 + 1 2 ? V,n (? ?1 + ? 1 ) shows (appendix 9) 1 n ? V,n | ln ? V,n | I (U , V ); W = ? ?? + ?(1 ? ?)I(? ? 1) ?| ln ?| + correction terms .</formula><p>Here we see that the phase transition is washed out at leading order and only seen at higher order with a threshold at ? = 1, i.e., ? c (? V,n ) = {4| ln ? V,n |/(?? V,n )} 1/2 . Note that in the present regime ? V,n ? 0 and ? = ?(1) so the mutual information remains positive. The consequences of this formula for the MMSE are richer and more subtle than in the symmetric Wigner case. One can consider three MMSE's associated to the matrices V ? V , U ? U , or U ? V . All three MMSE's can be computed from the solution (q * U , q * V ) in the variational problem of theorem 2, as shown in <ref type="bibr" target="#b37">[38]</ref>. We have (?n</p><formula xml:id="formula_15">) ?2 MMSE(V ? V |W ) = E[V 2 1 ] 2 ? (q * V ) 2 , n ?2 MMSE(U ? U |W ) = E[U 2 1 ] 2 ? (q * U ) 2 and (nm) ?1 MMSE(U ? V |W ) = E[U 2 1 ]E[V 2 1 ] ? q * U q * V .</formula><p>We note that the last expression is equivalent to an I-MMSE relation, i.e., it can be obtained by di erentiating the mutual information (2/m)I((U , V ); W ) with respect to ? n . An application of these formulas to the analytical solutions of the variational problem (found in appendix 9) shows that with suitable rescaling (?n? V,n ) ?2 MMSE(V ? V |W ) displays the 0-1 phase transition. For the other two MMSE's one cannot expect to see such behavior because U is gaussian. Instead one nds asymptotically that these MMSE's (with suitable rescaling) tend to 1 when ? V,n ? 0 + . The transition at ? = 1 is a higher order e ect seen on higher order corrections. These results are illustrated with a numerical calculation depicted on gure 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Analysis by the adaptive interpolation: the Wigner case</head><p>In this section we provide the essential architecture for the proof of theorem 1 which relies on the adaptive interpolation method <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. The proof requires concentration properties for "free energies" and "overlaps" which are deferred to appendices 6 and 7. We will also employ various known information theoretic properties of gaussian channels (I-MMSE relation, concavity of the MMSE with respect to the SNR and input distribution etc). For the convenience of the reader these are presented and adapted to our setting in appendix 11.</p><p>An essentially similar analysis can be done for theorem 2 in the Wishart case, and is deferred to appendix 5. When no confusion is possible we use the notation E A 2 = E[ A 2 ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The interpolating model.</head><p>Let ? [s n , 2s n ], for a sequence tending to zero, s n = n ?? /2 ? (0, 1/2), for ? &gt; 0 chosen later on. Let q n : [0, 1] ? [s n , 2s n ] ? [0, ? n ] and set R n (t, ) ? + ? n t 0 ds q n (s, ) .</p><p>Consider the following interpolating estimation model, where t ? [0, 1], with accessible data (W ij (t)) i,j and (W i (t, )) i obtained through</p><formula xml:id="formula_16">? ? ? W ij (t) = W ji (t) = (1 ? t) ?n n X i X j + Z ij , 1 ? i &lt; j ? n , W (t, ) = R n (t, ) X +Z ,</formula><p>with standard gaussian noiseZ ? N (0, I n ), and Z ij = Z ji ? N (0, 1). The posterior associated with this model reads (here ? is the 2 norm)</p><formula xml:id="formula_17">dP n,t, (x|W (t),W (t, )) = 1 Z n,t, (W (t),W (t, )) n i=1 dP X,n (x i ) ?exp n i&lt;j (1 ? t) ? n n x 2 i x 2 j 2 ? (1 ? t) ? n n x i x j W ij (t) + R n (t, ) x 2 2 ? R n (t, )x ?W (t, ) .</formula><p>The normalization factor Z n,t, (. . . ) is also called partition function. We also de ne the mutual information density for the interpolating model</p><formula xml:id="formula_18">i n (t, ) ? 1 n I X; (W (t),W (t, )) .<label>(8)</label></formula><p>The (n, t, , R n )-dependent Gibbs-bracket (that we simply denote ? t for the sake of readability) is de ned for functions</p><formula xml:id="formula_19">A(x) = A A(x) t = dP n,t, (x|W (t),W (t, )) A(x) .<label>(9)</label></formula><p>Lemma 1 (Boundary values). The mutual information for the interpolating model veri es</p><formula xml:id="formula_20">i n (0, ) = 1 n I(X; W ) + O(? n s n ) , i n (1, ) = I n (X; {? n 1 0 dt q n (t, )} 1/2 X + Z) + O(? n s n ) .<label>(10)</label></formula><p>where I n (X; {? n 1 0 dt q n (t, )} 1/2 X + Z) is the mutual information for a scalar gaussian channel with input X ? P X,n and noise Z ? N (0, 1).</p><p>Proof. We start with the chain rule for mutual information:</p><formula xml:id="formula_21">i n (0, ) = 1 n I(X; W (0)) + 1 n I(X;W (0, )|W (0)).</formula><p>Note that I(X; W (0)) = I(X; W ) which is obvious. Moreover we claim 1 n I(X;W (0, )|W (0)) = O(? n s n ) which yields the rst identity in <ref type="bibr" target="#b9">(10)</ref>. This claim simply follows from the I-MMSE relation (appendix 11) and R n (0, ) =</p><formula xml:id="formula_22">d d 1 n I(X;W (0, )|W (0)) = 1 2n MMSE(X|W (0, ), W (0)) ? ? n 2 .<label>(11)</label></formula><p>The last inequality above is true because MMSE(X|W (0, ), W (0)) ? E X?E X 2 = nVar(X 1 ) ? n? n , as the components of X are i.i.d. from P X,n . Therefore 1 n I(X;W (0, )|W (0)) is ?n 2 -Lipschitz in ? [s n , 2s n ]. Moreover we have that I(X;W (0, 0)|W (0)) = 0. This implies the claim.</p><p>The proof of the second identity in (10) again starts from the chain rule for mutual information i n (1, ) = 1 n I(X;W (1, )) + 1 n I(X; W (1)|W (1, )) .</p><p>Note that I(X; W (1)|W (1, )) = 0 as W (1) does not depend on X. Moreover,</p><formula xml:id="formula_23">1 n I(X;W (1, )) = I n (X; R n (1, )X + Z) = I n (X; {? n 1 0 dt q n (t, )} 1/2 X + Z) + O(? n s n ) . because I n (X; ? ?X + Z) is a ?n 2 -Lipschitz function of ?, by an application of the I-MMSE relation (appendix 11) d d? I n (X; ? ?X + Z) = MMSE(X| ? ?X + Z)/2 ? Var(X)/2 ? ? n /2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fundamental sum rule.</head><p>Proposition 1 (Sum rule). The mutual information veri es the following sum rule:</p><formula xml:id="formula_24">1 n I(X; W ) = i pot n 1 0 dt q n (t, ); ? n , ? n + ? n 4 R 1 ? R 2 ? R 3 + O(? n s n ) + O ? n n<label>(12)</label></formula><p>with non-negative "remainders" that depend on (n, , R n )</p><formula xml:id="formula_25">? ? ? ? ? ? ? R 1 ? 1 0 dt q n (t, ) ? 1 0 ds q n (s, ) 2 , R 2 ? 1 0 dt E Q ? E Q t 2 t , R 3 ? 1 0 dt q n (t, ) ? E Q t 2 .<label>(13)</label></formula><p>where Q = 1 n x ? X is called the overlap. The constants in the O(? ? ? ) terms are independent of n, t, .</p><p>Proof. By the fundamental theorem of calculus i n (0, ) = i n (1, ) ? 1 0 dt d dt i n (t, ). Note that i n (0, ) and i n (1, ) are given by <ref type="bibr" target="#b9">(10)</ref>. The t-derivative of the interpolating mutual information is simply computed combining the I-MMSE relation with the chain rule for derivatives</p><formula xml:id="formula_26">d dt i n (t, ) = ? ? n 2 1 n 2 i&lt;j E (X i X j ? x i x j t ) 2 + ? n q n (t, ) 2 1 n E X ? x t 2 (14) = ? ? n 4 1 n 2 E X ? X ? x ? x t 2 F + ? n q n (t, ) 2 1 n E X ? x t 2 + O ? n n .<label>(15)</label></formula><p>The correction term in (15) comes from completing the diagonal terms in the sum i&lt;j in order to construct the matrix-MMSE, namely the rst term on the r.h.s. of <ref type="bibr" target="#b14">(15)</ref>. This expression can be simpli ed by application of the Nishimori identities (appendix 10 contains a proof of these general identities). Starting with the second term (a vector-MMSE)</p><formula xml:id="formula_27">1 n E X ? x t 2 = E X 2 + x t 2 ? 2X ? x t = 1 n E X 2 ? X ? x t = ? n ? E Q t ,<label>(16)</label></formula><p>were we used E X 2 = n? n and the Nishimori identity</p><formula xml:id="formula_28">E x t 2 = E[X ? x t ].</formula><p>By similar manipulations we obtain for the matrix-MMSE</p><formula xml:id="formula_29">1 n 2 MMSE(X ? X|W (t, ), W (t)) = 1 n 2 E X ? X ? x ? x t 2 F = ? 2 n ? E Q 2 t .<label>(17)</label></formula><p>From <ref type="formula" target="#formula_1">(10)</ref>, <ref type="bibr" target="#b14">(15)</ref>, <ref type="bibr" target="#b15">(16)</ref>, <ref type="bibr" target="#b16">(17)</ref> and the fundamental theorem of calculus we deduce</p><formula xml:id="formula_30">1 n I(X; W ) =I n X; {? n 1 0 dt q n (t, )} 1/2 X + Z + ? n 4 1 0 dt ? 2 n ? E Q 2 t ? 2q n (t, )(? n ? E Q t ) + O(? n s n ) + O ? n n .</formula><p>The terms on the r.h.s can be re-arranged so that the potential (3) appears, and this gives immediately the sum rule <ref type="bibr" target="#b11">(12)</ref>.</p><p>Theorem 1 follows from the upper and lower bounds proven below, and applied for s n = 1 2 n ?? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Upper bound: linear interpolation path.</head><p>Proposition 2 (Upper bound). We have</p><formula xml:id="formula_31">1 n I(X; W ) ? inf q?[0,?n] i pot n (q, ? n , ? n ) + O(? n s n ) + O ? n n . Proof. Fix q n (t, ) = q n ? [0, ? n ] a constant independent of , t. The interpolation path R n (t, )</formula><p>is therefore a simple linear function of time. From (13) R 1 cancels and since R 2 and R 3 are non-negative we get from Proposition (1)</p><formula xml:id="formula_32">1 n I(X; W ) ? i pot n (q, ? n , ? n ) + O(? n s n ) + O ? n n .</formula><p>Note that the error terms O(? ? ? ) are bounded independently of q n . Therefore optimizing the r.h.s over the free parameter q n ? [0, ? n ] yields the upper bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Lower bound: adaptive interpolation path.</head><p>We start with a de nition: the map ? R n (t, ) is called regular if it is a C 1 -di eomorphism whose jacobian is greater or equal to one for all t ? [0, 1].</p><p>Proposition 3 (Lower bound). Consider sequences ? n and ? n satisfying c 1 ? ? n ? n ? c 2 n ? for some constants positive constant c 1 , c 2 and ? ? [0, 1/2[. Then</p><formula xml:id="formula_33">1 n I(X; W ) ? inf q?[0,?n] i pot n (q, ? n , ? n ) + O(? n s n ) + O ? n n + O ? 4 n ? n ns 4 n 1/3 .<label>(18)</label></formula><p>Proof. First note that the regime (2) for the sequences ? n , ? n satis es the more general condition assumed in this lemma (this is the condition in theorem 3 of appendix 4). Assume for the moment that the map ? R n (t, ) is regular. Then, based on Proposition 11 and identity (41) (appendix 7), we have a bound on the overlap uctuation. Namely, for some numerical constant C ? 0 independent of n ? n s n 2sn</p><formula xml:id="formula_34">sn d R 2 = ? n s n 2sn sn d 1 0 dt E (Q ? E Q n,t,Rn(t, ) ) 2 n,t,Rn(t, ) ? C ? 4 n ? n ns 4 n 1/3 .<label>(19)</label></formula><p>Using this concentration result, and R 1 ? 0, and averaging the sum rule (12) over ? [s n , 2s n ] (recall the error terms are independent of ) we nd</p><formula xml:id="formula_35">1 n I(X; W ) ? 1 s n 2sn sn d i pot n 1 0 dt q n (t, ), ? n , ? n ? ? n 4 1 s n 2sn sn d 1 0 dt q n (t, ) ? E Q t 2 + O(? n s n ) + O ? n n + O ? 4 n ? n ns 4 n 1/3 .<label>(20)</label></formula><p>At this stage it is natural to see if we can choose q n (t, ) to be the solution of q n (t, ) = E Q t . Setting F n (t, R n (t, )) ? E Q n,t,Rn(t, ) , we recognize a rst order ordinary di erential equation</p><formula xml:id="formula_36">d dt R n (t, ) = F n (t, R n (t, )) with initial condition R n (0, ) = .<label>(21)</label></formula><p>As F n (t, R n (t, )) is C 1 with bounded derivative w.r.t. its second argument the Cauchy-Lipschitz theorem implies that (21) admits a unique global solution R *</p><formula xml:id="formula_37">n (t, ) = + t 0 ds q * n (s, ), where q * n : [0, 1] ? [s n , 2s n ] ? [0, ? n ].</formula><p>Note that any solution must satisfy q * n (t, ) ? [0, ? n ] because E Q n,t, ? [0, ? n ] as can be seen from a Nishimori identity (appendix 10) and <ref type="bibr" target="#b15">(16)</ref>.</p><p>We check that R * n is regular. By Liouville's formula the jacobian of the ow ? R * n (t, ) satis es</p><formula xml:id="formula_38">d d R * n (t, ) = exp t 0 ds d dR F n (s, R) R=R * n (s, )</formula><p>.</p><p>Applying repeatedly the Nishimori identity of Lemma 12 (appendix 10) one obtains (this computation does not present any di culty and can be found in section 6 of <ref type="bibr" target="#b12">[13]</ref>)</p><formula xml:id="formula_39">d dR F n (s, R) = 1 n n i,j=1 E ( x i x j n,s,R ? x i n,s,R x j n,s,R ) 2 ? 0<label>(22)</label></formula><p>so that the ow has a jacobian greater or equal to one. In particular it is locally invertible (surjective). Moreover it is injective because of the unicity of the solution of the di erential equation, and therefore it is a C 1 -di eomorphism. Thus ? R * n (t, ) is regular. With the choice R * n , i.e., by suitably adapting the interpolation path, we cancel R 3 . This yields</p><formula xml:id="formula_40">1 n I(X; W ) ? 1 s n 2sn sn d i pot n 1 0 dt q * n (t, ), ? n , ? n + O(? ? ? ) ? inf q?[0,?n] i pot n (q, ? n , ? n ) + O(? ? ? )</formula><p>where the O(? ? ? ) is a shorthand notation for the three error terms in <ref type="bibr" target="#b19">(20)</ref>. This the desired result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices 4 General results on the mutual information of sparse spiked matrix models</head><p>In this appendix we give a more general form of theorems 1 and 4 in section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Spiked Wigner model</head><p>Our analysis by the adaptive interpolation method works for any regime where the sequences ? n and ? n verify:</p><formula xml:id="formula_41">C ? ? n ? n = O(n ? ) for some constants ? ? [0, 1/2) and C &gt; 0 .<label>(23)</label></formula><p>Of course this contains the regime (2) as a special case. Our general result is a statement on the smallness of</p><formula xml:id="formula_42">?I Wig n ? 1 ? n | ln ? n | 1 n I(X; W ) ? inf q?[0,?n] i pot n (q, ? n , ? n ) .</formula><p>The analysis of section 3 leads to the following general theorem.</p><p>Theorem 3 (Sparse spiked Wigner model). Let the sequences ? n and ? n verify (23) and let ? &gt; 0. There exists a constant C &gt; 0 independent of n, such that the mutual information for the Wigner spike model veri es</p><formula xml:id="formula_43">?I Wig n ? C | ln ? n | max 1 n ? , ? n n? n , ? 4 n n 1?4? ? 2 n 1 + ? n ? 2 n 1/3 .</formula><p>In particular, choosing ? n = ?(| ln ? n |/? n ) (which is the appropriate scaling to observe a phase transition)</p><formula xml:id="formula_44">?I Wig n ? C max 1 n ? | ln ? n | , 1 n? 2 n , | ln ? n | n 1?4? ? 6 n 1/3 . If in addition we set ? n = ?(n ?? ), ? ? 0 (which is the regime (2)) we have ?I Wig n ? C max 1 n ? ln n , 1 n 1?2? , ln n n 1?4??6? 1/3</formula><p>. This bound vanishes as n grows if ? ? [0, 1/6) and ? ? (0, (1 ? 6?)/4]. The last bound is optimized (up to polylog factors) setting ? = (1 ? 6?)/7. In this case (again, when ? n = ?(| ln ? n |/? n ) and</p><formula xml:id="formula_45">? n = ?(n ?? )) ?I Wig n ? C (ln n) 1/3 n (1?6?)/7 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Spiked Wishart model</head><p>The following regime is of particular interest and is the one mostly studied in the literature given in the introduction on spiked covariance models:</p><formula xml:id="formula_46">? n ? ? &gt; 0 , ? U,n ? ? U &gt; 0 , ?(1/n) = ? V,n ? 0 + , ? n = ? | ln ? V,n | ? V,n .<label>(24)</label></formula><p>The notation ?(1/n) = ? V,n means that the sequence ? V,n vanishes at a rate slower than 1/n. The analysis of appendix 5 leads to the following general theorem on the smallness of</p><formula xml:id="formula_47">?I Wish n ? 1 ? V,n | ln ? V,n | 1 n I (U , V ); W ? inf q U ?[0,? U,n ] sup q V ?[0,? V,n ] i pot n q U , q V , ? n , ? n , ? U,n , ? V,n .</formula><p>Theorem 4 (Sparse spiked Wishart model). Under the scalings (24), there exists a constant C &gt; 0 independent of n such that the mutual information for the spiked Wishart model veri es for any ? &gt; 0</p><formula xml:id="formula_48">? n I Wish ? C max 1 n ? ? V,n | ln ? V,n | , | ln ? V,n | ?1/24 n 1/3?2? ? 11/12 V,n 1 n ? + ? V,n | ln ? V,n | 1/3 .</formula><p>We set ? V,n = ?(n ?? ). Optimizing over ? (up to polylog factors) such that n ?? &lt; ? V,n | ln ? V,n | yields ? = (4 ? 3?)/18. In this case the bound simpli es to</p><formula xml:id="formula_49">? n I Wish ? C (ln n) 1/3 n (4?12?)/18</formula><p>for some C &gt; 0. This bound vanishes if ? ? [0, 1/3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Proof of theorem 4 by the adaptive interpolation method</head><p>In this appendix we prove theorem 4 by the adapative interpolation method. The analysis is similar to the one of the Wigner case in section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.1</head><p>The interpolating model.</p><formula xml:id="formula_50">Let = ( U , V ) ? [s n , 2s n ] 2 for some sequence s n = 1 2 n ?? . Let q U,n : [0, 1] ? [s n , 2s n ] ? [0, ? U,n ] and similarly for q V,n . Set R U,n (t, ) ? U + ? n t 0 ds q U,n (s, ) , R V,n (t, ) ? V + ? n ? n t 0 ds q V,n (s, ) .</formula><p>Consider the following interpolating estimation model, where t ? [0, 1], with accessible data</p><formula xml:id="formula_51">? ? ? ? ? ? ? W (t) = (1 ? t) ?n n U ? V + Z , W U (t, ) = R V,n (t, ) U +Z U , W V (t, ) = R U,n (t, ) V +Z V ,<label>(25)</label></formula><formula xml:id="formula_52">with independent standard gaussian noiseZ U ,Z V ? N (0, I n ), Z = (Z ij ) ij with i.i.d. Z ij ? N (0, 1).</formula><p>The fact that the R V,n function appears as the SNR of the decoupled gaussian channel related to U (and vice-versa) comes from the bipartite nature of the problem. The Gibbs-bracket, simply denoted ? t , is the expectation w.r.t. the posterior distribution, which is proportional to (here ? F and ? are the Frobenius and 2 norms)</p><formula xml:id="formula_53">dP n,t, u, v|W (t),W U (t, ),W V (t, ) ? n i=1 dP U,n (u i ) m j=1 dP V,n (v j ) exp ? 1 2 W (t) ? (1 ? t) ? n n u ? v 2 F ? 1 2 W U (t, ) ? R V,n (t, ) u 2 ? 1 2 W V (t, ) ? R U,n (t, ) v 2 .</formula><p>The mutual information density for this interpolating model is</p><formula xml:id="formula_54">i n (t, ) ? 1 n I (U , V ); (W (t),W U (t, ),W V (t, )) .</formula><p>The proof of the following lemma is similar to the one of Lemma 1.</p><formula xml:id="formula_55">Lemma 2 (Boundary values). Let? n = max(? U,n , ? V,n ), U ? P U,n , V ? P V,n and Z ? N (0, 1). Then i n (0, ) = 1 n I((U , V ); W ) + O(? n s n ) , i n (1, ) = I n (U ; {? n ? n 1 0 q V,n (s, )} 1/2 U +Z)+? n I n (V ; {? n 1 0 q U,n (s, )} 1/2 V +Z)+O(? n s n ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Fundamental sum-rule.</head><p>As before, our proof is bases on an important sum-rule.</p><formula xml:id="formula_56">Proposition 4 (Sum rule). Let? n = max(? U,n , ? V,n ), ? n = m/n, U ? P U,n , V ? P V,n and Z ? N (0, 1). Then 1 n I((U , V ); W ) = I n (U ; {? n ? n 1 0 q V,n (s, )} 1/2 U + Z) + ? n I n (V ; {? n 1 0 q U,n (s, )} 1/2 V + Z) + ? n ? n 2 ? U,n ? V,n + ? n ? n 2 1 0 dt E Q U t E Q V t ? E Q U Q V t + O(? n s n ) + ? n ? n 2 1 0 dt q V,n (t, )(E Q U t ? ? U,n ) + q U,n (t, )(E Q V t ? ? V,n ) ? E Q U t E Q V t</formula><p>where the overlaps are de ned as</p><formula xml:id="formula_57">Q U ? 1 n u ? U , Q V ? 1 m v ? V .</formula><p>Proof. We compare the boundaries values (10) using the fundamental theorem of calculus i n (0, ) = i n (1, ) ? 1 0 dt d dt i n (t, ). Using the I-MMSE formula ( rst equality) and then the Nishimori identity (second equality) we have</p><formula xml:id="formula_58">d dt i n (t, ) = ? ? n ? n 2nm E U ? V ? u ? v t 2 F + ? n ? n q V,n (t, ) 2n E U ? u t 2 + ? n ? n q U,n (t, ) 2m E V ? v t 2 N = ? n ? n 2 E Q U Q V t ? ? U,n ? V,n + q V,n (t, )(? U,n ? E Q U t ) + q U,n (t, )(? V,n ? E Q V t ) .</formula><p>The N stands for "Nishimori", and each time we use the Nishimori identity of Lemma 12 for a simplication we write a N on top of the equality. Replacing this result and the boundary values (10) in the fundamental theorem of calculus yields the sum rule after few lines of algebra.</p><p>We now derive two matching bounds, under the scalings <ref type="bibr" target="#b23">(24)</ref>, which implie Theorem 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Upper bound: partially adaptive interpolation path.</head><p>We start again with the simplest bound:</p><p>Proposition 5 (Upper bound). Under the scalings (24) we have</p><formula xml:id="formula_59">1 n I((U , V ); W ) ? inf q U ?[0,? U,n ] sup q V ?[0,? V,n ] i pot n q U , q V , ? n , ? n , ? U,n , ? V,n + O s n + | ln ? V,n | 11/24 s 2 n n 1/3 ? 9/24 V,n s n + ? V,n | ln ? V,n | 1/3 .<label>(26)</label></formula><p>Proof. For this bound only one of the interpolation function is adapted. Consider the following Cauchy problem for R n (t, ) = (R U,n (t, ), R V,n (t, )):</p><formula xml:id="formula_60">dR n dt (t, ) = ? n q U , G V,n (t, R n (t, )) , R n (0, ) = , where q U ? [0, ? U,n ] and G V,n (t, R n (t, )) ? ? n ? n E Q V t ? [0, ? n ? n ? V,n ], i.e., q U,n (t, ), q V,n (t, ) = q U , E Q V t , R n (0, ) = .</formula><p>By the Cauchy-Lipschitz theorem this ODE admits a unique global solution</p><formula xml:id="formula_61">R * n (t, ) = (R * U,n (t, ) = U + ? n q U,n t, R * V,n (t, ) = V + ? n ? n t 0 ds q * V,n (s, )) .</formula><p>Because the function (q U , E Q V t ) is C 1 the solution R * n is C 1 in all its arguments. By the Liouville formula the Jacobian determinant J n (t, ) of the ow ? R * n (t, ) satis es</p><formula xml:id="formula_62">J n (t, ) ? det ?R n,t (t, ) ? = exp t 0 ?G V,n ?R V (s, R * U,n (s, ), R V = R * V,n (s, ))ds ? 1 .<label>(27)</label></formula><p>We show at the end of the proof that</p><formula xml:id="formula_63">?G V,n ?R V ? 0.</formula><p>The intuition is the same as before: increasing the SNR R V cannot decrease the overlap E Q V t , or equivalently it cannot increase the MMSE ? V,n ? E Q V t . The ow ? R * n (t, ) thus has Jacobian greater or equal to one, and is surjective. It is also injective by unicity of the solution of the di erential equation, and is thus a C 1 -di eomorphism. A C 1 -di eomrophic ow with Jacobian greater or equal to one is called regular.</p><p>By the Cauchy-Schwarz inequality and Fubini's theorem we have</p><formula xml:id="formula_64">? n ? n 2s 2 n d 1 0 dt E Q U t E Q V t ? E Q U Q V t = ? n ? n 2s 2 n 1 0 dt d E (Q U ? E Q U t )(Q V ? E Q V t ) t ? ? n ? n 2s 2 n 1 0 dt d E (Q U ? E Q U t ) 2 t 1/2 d E (Q V ? E Q V t ) 2 t 1/2 .</formula><p>By the regularity of the ow we are allowed to use Propositions 12, 13 of section 7. Together with inequality (55) and a similar one for L U (see section <ref type="bibr" target="#b6">7)</ref> we obtain under the scalings <ref type="formula" target="#formula_2">(24)</ref>,</p><formula xml:id="formula_65">? n ? n 2s 2 n d 1 0 dt E Q U t E Q V t ? E Q U Q V t ? C s 2 n | ln ? V,n | ? V,n 1 n | ln ? V,n | ? V,n s n + ? V,n | ln ? V,n | 2 ? (ln ? V,n ) 2 n? V,n 1/6 = C | ln ? V,n | 11/24 s 2 n n 1/3 ? 9/24 V,n s n + ? V,n | ln ? V,n | 1/3 .<label>(28)</label></formula><p>Therefore, averaging the sum-rule over ? [s n , 2s n ] 2 and using the solution R * n of the above Cauchy problem, we obtain</p><formula xml:id="formula_66">1 n I((U , V ); W ) = 1 s 2 n [sn,2sn] 2 d i pot n q U , 1 0 q * V,n (t, )dt, ? n , ? n , ? U,n , ? V,n + O s n + | ln ? V,n | 11/24 s 2 n n 1/3 ? 9/24 V,n s n + ? V,n | ln ? V,n | 1/3 .</formula><p>Because this inequality is true for any q U ? [0, ? U,n ] we obtain the result. It remains to prove that</p><formula xml:id="formula_67">?G V,n ?R V ? 0, i.e., ?E Q V t ?R V ? 0.</formula><p>We drop un-necessary dependencies. Let ? ? 0. Consider the following modi cation of the model <ref type="formula" target="#formula_2">(25)</ref>:</p><formula xml:id="formula_68">? ? ? ? ? ? ? W = (1 ? t) ?n n U ? V + Z , W U (R V , ?) = U +Z U / ? R V +? U ?/R V , W V = ? R U V +Z V ,</formula><p>where? U ? N (0, I n ) independently of the rest. By stability of the gaussian distribution under addition we have in lawZ U / ?</p><formula xml:id="formula_69">R V +? U ?/R V =Z U (? + 1)/R V , therefore the MMSE for model (25) MMSE(V |W ,W U (R V ),W V ) (we made explicit the dependence ofW U in R V = R V,n (t, )) veri es MMSE(V |W ,? U (R V , ?),W V ) = MMSE(V |W ,W U (R V /(? + 1)),W V ) .</formula><p>We then have</p><formula xml:id="formula_70">MMSE(V |W ,W U (R V ),W V ) = MMSE(V |W ,? U (R V , ?),W V ,? U ) ? MMSE(V |W ,? U (R V , ?),W V ) = MMSE(V |W ,W U (R V /(? + 1)),W V )</formula><p>where the inequality follows from Lemma 16.</p><formula xml:id="formula_71">Because R V ?+1 ? R V , MMSE(V |W ,W U (R V ),W V ) is non-increasing in R V . Recalling MMSE(V |W ,W U (R V ),W V ) N = ? V,n ? E Q V t this proves ?G V,n ?R V ? 0.</formula><p>We provide here an alternative proof. Consider the interpolating model <ref type="bibr" target="#b24">(25)</ref> where a positive quantity ? is added to R V . We denote I ? (U , V ); (W ,W U ,W V ) the mutual information for this new model, so i n (t, ) = 1 n I 0 (U , V ); (W ,W U ,W V ) . By Lemma 17 this model is mutual information-wise equivalent to the following one:</p><formula xml:id="formula_72">? ? ? ? ? ? ? ? ? ? ? ? ? W = (1 ? t) ?n n U ? V + Z , W U = ? R V U +Z U , W U (?) = ? ? U +? U , W V (t, ) = ? R U V +Z V ,<label>(29)</label></formula><p>where? U ? N (0, I n ) independently of the rest. Namely,</p><formula xml:id="formula_73">I ? (U , V ); (W ,W U ,W V ) = I (U , V ); (W ,W U ,? U (?),W V ) .</formula><p>Using the chain rule for mutual information it is re expressed as</p><formula xml:id="formula_74">I ? V ; (W (t),W U (t, ),W V (t, )) + I ? U ; (W (t),W U )|V = I V ; (W ,W U ,? U (?),W V ) + I U ; (W ,W U ,? U (?))|V .</formula><p>The two mutual information conditioned of V are independent of R V . Taking a R V derivative on both sides, by the I-MMSE formula Lemma 13 the associated MMSE's verify</p><formula xml:id="formula_75">MMSE ? (V |W ,W U ,W V ) = MMSE(V |W ,W U ,? U (?),W V ) .</formula><p>Lemma 16 then implies</p><formula xml:id="formula_76">MMSE ? (V |W ,W U ,W V ) ? MMSE(V |W ,W U ,W V ) or equivalently E Q V t,? ? E Q V t</formula><p>where MMSE ? (V | ? ? ? ) and E Q V t,? are the average MMSE and overlap for V corresponding to model <ref type="bibr" target="#b28">(29)</ref> or equivalently model <ref type="bibr" target="#b24">(25)</ref> with R V,n replaced by R V,n + ?. This proves</p><formula xml:id="formula_77">?G V,n ?R V ? 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Lower bound: fully adaptive interpolation path.</head><p>For the converse bound we need to adapt both interpolating functions.</p><p>Proposition 6 (Lower bound). Under the scalings (24) the converse of the bound (26) holds.</p><p>Proof. Consider this time the following Cauchy problem:</p><formula xml:id="formula_78">dR n dt (t, ) = G U,n (t, R n (t, )), G V,n (t, R n (t, )) , R n (0, ) = ,<label>(30)</label></formula><p>with the functions G U,n (t, R n (t, )) ? ? n (? U,n ? MMSE(U | ? n ? n E Q V t U + Z)) ? [0, ? n ? U,n ] and G V,n (t, R n (t, )) ? ? n ? n E Q V t ? [0, ? n ? n ? V,n ], or in other words,</p><formula xml:id="formula_79">q U,n (t, ), q V,n (t, ) = ? U,n ? MMSE(U | ? n ? n E Q V t U + Z), E Q V t , R n (0, ) = .</formula><p>This ODE admits a unique global C 1 solution R * n (t, ) = (R * U,n (t, ), R * V,n (t, )) by the Cauchy-Lipschitz theorem. By the Liouville formula, the Jacobian determinant of the ow ? R * n (t, ) satis es</p><formula xml:id="formula_80">J n (t, ) = exp t 0 ?G U,n ?R U (s, R U = R * U,n (s, ), R * V,n (s, )) + ?G V,n ?R V (s, R * U,n (s, ), R V = R * V,n (s, )) ds .</formula><p>Both partial derivatives are positive by the same proof as in the previous paragraph. Then using teh same arguments as previously we conclude that the ow is regular (a C 1 -di eomorphism with Jacobian greater or equal to one). Using this solution we can thus use Propositions 12, 13 of section 7 to deduce from the sum rule of Proposition 4</p><formula xml:id="formula_81">1 n I((U , V ); W ) = 1 s 2 n d I n (U ; {? n ? n 1 0 q * V,n (t, )dt} 1/2 U + Z) + ? n I n (V ; {? n 1 0 q * U,n (t, )dt} 1/2 V + Z) + ? n ? n 2 1 0 dt(q * U,n (t, ) ? ? U,n )(q * V,n (t, ) ? ? V,n ) + O s n + | ln ? V,n | 11/24 s 2 n n 1/3 ? 9/24 V,n s n + ? V,n | ln ? V,n | 1/3 ? 1 s 2 n d 1 0 dt i pot n q * U,n (t, ), q * V,n (t, ), ? n , ? n , ? U,n , ? V,n + O(? ? ? ) .</formula><p>To get the last inequality we used the concavity in the SNR of the mutual information for gaussian channels, see Lemma 14 of section 11. Now note that i pot n q * U,n (t, ), q * V,n (t, ), ? n , ? n , ? U,n , ? V,n = sup</p><formula xml:id="formula_82">q V ?[0,? V,n ]</formula><p>i pot n q * U,n (t, ), q V , ? n , ? n , ? U,n , ? V,n .</p><p>Indeed, the function g n (q U , ?) : q V ? i pot n (q U , q V ; ? n , ? n , ? U,n , ? V,n ) is concave (by concavity of the mutual information in the SNR, see <ref type="bibr">Lemma 14)</ref> </p><formula xml:id="formula_83">with q V -derivative dg dq V (q U , q V ) = ? n ? n 2 q u ? ? U,n + MMSE(U | ? n ? n q V U + Z)</formula><p>(using the I-MMSE relation). By de nition of the solution R * n of the ODE (30) we have</p><formula xml:id="formula_84">dg dq V (q * U,n (t, ), q V = q * V,n (t, )) = 0 .</formula><p>By concavity this corresponds to a maximum. Therefore</p><formula xml:id="formula_85">1 n I((U , V ); W ) ? 1 s 2 n d 1 0 dt sup q V ?[0,? V,n ] i pot n q * U,n (t, ), q V , ? n , ? n , ? U,n , ? V,n + O(? ? ? ) ? inf q U ?[0,? U,n ] sup q V ?[0,? V,n ] i pot n q U , q V , ? n , ? n , ? U,n , ? V,n + O(? ? ? ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Concentration of free energies</head><p>For this appendix it is convenient to use the language of statistical mechanics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Statistical mechanics notations for the spiked Wigner (interpolating) model.</head><p>We express the posterior of the interpolating model dP n,t, (x|W (t),W (t, )) = 1 Z n,t, (W (t),W (t, ))</p><formula xml:id="formula_86">? n i=1 dP X,n (x i ) exp ? H n,t, (x, W (t),W (t, ))<label>(31)</label></formula><p>with normalization constant (partition function) Z n,t, and "hamiltonian"</p><formula xml:id="formula_87">H n,t, (x, W (t),W (t, )) = H n,t, (x, X, Z,Z) (32) ? n i&lt;j (1 ? t) ? n n x 2 i x 2 j 2 ? (1 ? t) ? n n x i x j W ij (t) + R n (t, ) x 2 2 ? R n (t, )x ?W (t, ) = (1 ? t)? n n i&lt;j x 2 i x 2 j 2n ? x i x j X i X j n ? x i x j Z ij n(1 ? t)? n + R n (t, ) x 2 2 ? x ? X ? x ?Z R n (t,</formula><p>) .</p><p>It will also be convenient to work with "free energies" rather than mutual informations. The free energy F n (t, ) and (its expectation f n (t, )) for the interpolating model is simply minus the (expected) logpartition function:</p><p>F n,t, (W (t),W (t, )) ? ? 1 n ln Z n,t, (W (t),W (t, )) ,</p><p>f n (t, ) ? E F n,t, (W (t),W (t, )) .</p><p>The expectation E carries over the data. The averaged free energy is related to the mutual information i n (t, ) given by (8) through i n (t, ) = f n (t, ) + n ? 1 n</p><formula xml:id="formula_90">? 2 ?(1 ? t) 4 + ?R n (t, ) 2 .<label>(35)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Statistical mechanics notations for the spiked Wishart (interpolating) model.</head><p>Let the set D n,t, = {W (t),W U (t, ),W V (t, )}. In the Wishart case the posterior reads dP n,t, (u, v|D n,t, ) = 1 Z n,t, (D n,t, )</p><formula xml:id="formula_91">n i=1 dP U,n (u i ) m j=1 dP V,n (v j ) ? exp ? H n,t, (u, v, D n,t, )<label>(36)</label></formula><p>with hamiltonian</p><formula xml:id="formula_92">H n,t, (u, v, D n,t, ) ? (1 ? t) ? n n u 2 v 2 2 ? (1 ? t) ? n n u ? (W (t)v) + R V,n (t, ) u 2 2 ? R V,n (t, )u ?W U (t, ) + R U,n (t, ) v 2 2 ? R U,n (t, )v ?W V (t, ) = (1 ? t) ? n n u 2 v 2 2 ? (1 ? t) ? n n (u ? U )(v ? V ) ? (1 ? t) ? n n u ? (Zv) + R V,n (t, ) u 2 2 ? R V,n (t, )u ? U ? R V,n (t, )u ?Z U + R U,n (t, ) v 2 2 ? R U,n (t, )v ? V ? R U,n (t, )v ?Z V .<label>(37)</label></formula><p>The free energy and its expectation (over the data) are</p><formula xml:id="formula_93">F n,t, (D n,t, ) ? ? 1 n ln Z n,t, (D n,t, ) ,<label>(38)</label></formula><formula xml:id="formula_94">f n (t, ) ? E F n,t, (D n,t, ) .<label>(39)</label></formula><p>Similarly to <ref type="bibr" target="#b34">(35)</ref> the averaged free energy is related to the mutual information by an additive constant (linear in R U,n and R V,n ) that does not change its concavity properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Free energy concentration for the Wigner case</head><p>In this section we prove a concentration identity for the free energy (33) onto its average <ref type="bibr" target="#b33">(34)</ref>.</p><p>Proposition 7 (Free energy concentration for the spiked Wigner model). We have E F n,t, (W (t),W (t, )) ? f n (t, ) 2 ? 2? n S 2 n (2s n + ? n ? n ) 2 + S 4 + 3 2</p><p>? n ? 2 n n + 2 s n ? n n .</p><p>Considering sequences ? n and ? n verifying (23) and with s n = (1/2)n ?? ? 0 + the bound simpli es to C(S)? 2 n ? 3 n /n with positive constant C(S) ? 5 2 + 8S 2 + 2S 6 .</p><p>The proof is based on two classical concentration inequalities, Proposition 8 (Gaussian Poincar? inequality). Let U = (U 1 , . . . , U N ) be a vector of N independent standard normal random variables. Let g : R N ? R be a continuously di erentiable function. Then</p><formula xml:id="formula_95">Var(g(U )) ? E ?g(U ) 2 .</formula><p>Proposition 9 (Efron-Stein inequality). Let U ? R, and a function g : U N ? R. Let U = (U 1 , . . . , U N ) be a vector of N independent random variables with law P U that take values in U. Let U (i) a vector which di ers from U only by its i-th component, which is replaced by U i drawn from P U independently of U . Then</p><formula xml:id="formula_96">Var(g(U )) ? 1 2 N i=1 E U E U i (g(U ) ? g(U (i) )) 2 .</formula><p>We start by proving the concentration w.r.t. the gaussian variables. It is convenient to make explicit the dependence of the partition function of the interpolating model in the independent quenched variables instead of the data: Z n,t, (X, Z,Z) = Z n,t, (W (t),W (t, )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 3 (Concentration w.r.t. the gaussian variables). We have</head><formula xml:id="formula_97">E 1 n ln Z n,t, (X, Z,Z) ? 1 n E Z,Z ln Z n,t, (X, Z,Z) 2 ? 3 2 ? n ? 2 n n + 2 s n ? n n .</formula><p>Proof. Fix all variables except Z,Z. Let g(Z,Z) ? ? 1 n ln Z n,t, (X, Z,Z) be the free energy seen as a function of the gaussian variables only. The free energy gradient reads E ?g 2 = E ? Z g 2 + E ?Zg 2 . Let us denote H(t) ? H n,t, the interpolating Hamiltonian <ref type="bibr" target="#b31">(32)</ref>.</p><formula xml:id="formula_98">E ? Z g 2 = 1 n 2 E ? Z H(t) t 2 = (1 ? t)? n n 3 i&lt;j E[ x i x j 2 t ] ? (1 ? t)? n n 3 i&lt;j E (x i x j ) 2 t N = (1 ? t)? n n 3 i&lt;j E[(X i X j ) 2 ] ? ? n ? 2 n 2n .</formula><p>where we used a Nishimori identity for the last equality. Similarly, and using ? n ? n ? 1 and s n &lt; 1/2,</p><formula xml:id="formula_99">E ?Zg 2 = R( ) n 2 E x t 2 ? R( ) n 2 E x 2 t N = R( ) n 2 E X 2 ? (2s n + ? n ? n )? n n .</formula><p>Therefore Proposition 8 directly implies the stated result.</p><p>We now consider the uctuations due to the signal realization:</p><p>Lemma 4 (Concentration w.r.t. the spike). We have</p><formula xml:id="formula_100">E ? 1 n E Z,Z ln Z n,t, (X, Z,Z) ? f n (t, ) 2 ? 2? n S 2 n (2s n + ? n ? n ) 2 + S 4 .</formula><p>Proof. Let g(X) ? ? 1 n E Z,Z ln Z n,t, (X, Z,Z). De ne X (i) as a vector with same entries as X except the i-th one that is replaced by X i drawn independently from P X,n . Let us estimate (g(X)?g(X (i) )) 2 by interpolation. Let H(t, sX + (1 ? s)X (i) ) be the interpolating Hamiltonian (32) with X replaced by sX + (1 ? s)X (i) . Then</p><formula xml:id="formula_101">E (g(X)?g(X (i) )) 2 = E 1 0 ds dg ds (sX + (1 ? s)X (i) ) 2 = 1 n 2 E 1 0 ds dH ds (t, sX + (1 ? s)X (i) ) t 2 = 1 n 2 E (X i ? X i ) R (t)x i + 1 ? t n x i j( =i) X j x j t 2 ? 2 n 2 E (X i ? X i ) 2 x i 2 t (2s n + ? n ? n ) 2 + 1 n 2 j,k( =i) X j X k x i x j t x i x k t ? 2 n 2 E (X i ? X i ) 2 S 2 (2s n + ? n ? n ) 2 + S 6 ? 4? n S 2 n 2 (2s n + ? n ? n ) 2 + S 4 .</formula><p>We used (a + b) 2 ? 2(a 2 + b 2 ) for the second inequality and E[(X i ? X i ) 2 ] = 2Var(X i ) ? 2? n . Therefore Proposition 9 implies the claim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Free energy concentration for the Wishart case</head><p>In this section we prove a concentration identity for the free energy (38) onto its average (39).</p><p>Proposition 10 (Free energy concentration for the spiked Wishart model). We have</p><formula xml:id="formula_102">E F n,t, (W (t),W U (t, ),W V (t, )) ? f n (t, ) 2 ? C F,n n</formula><p>where C F,n ? 2? U,n S 2 (2s n + ? n ? n ? V,n ) 2 + ? 2 n S 4 + 2? n ? V,n S 2 (2s n + ? n ? U,n ) 2 + S 4 + 3? n ? n ? U,n ? V,n + 2s n (1 + ? n )? n .</p><p>In the particular case of the scalings (24) we have C F,n ? C| ln ? V,n | for some positive constant C that may depend on anything but n.</p><p>The proofs are brief as they are similar to those for the spiked Wigner model. The partition function expressed with the independent quenched variables is Z n,t, (U , V , Z,Z U ,Z V ) ? Z n,t, (D n,t, ).</p><p>Lemma 5 (Concentration w.r.t. the gaussian variables). Let? n ? max(? U,n , ? V,n ). We have</p><formula xml:id="formula_103">E 1 n ln Z n,t, (U , V , Z,Z U ,Z V ) ? 1 n E Z,Z U ,Z V ln Z n,t, (U , V , Z,Z U ,Z V ) 2 ? 3 ? n ? n ? U,n ? V,n n + 2 s n (1 + ? n )? n n .</formula><p>Proof. Let g(Z,Z U ,Z V ) be the free energy <ref type="bibr" target="#b37">(38)</ref> seen as a function of only the gaussian variables. Based on the hamiltonian expression (37) we compute the gradient:</p><formula xml:id="formula_104">E ? Z g 2 = (1 ? t)? n n 3 E u ? v t 2 F ? ? n n 3 E U 2 E V 2 ? ? n ? n ? U,n ? V,n n</formula><p>where the bracket is w.r.t. the interpolating model posterior <ref type="bibr" target="#b35">(36)</ref>. We used that u, U ? R n while v, V ? R m , and ? n ? m/n. Similarly</p><formula xml:id="formula_105">E ?Z U g 2 = R V,n (t, ) 2 n 2 E u t 2 ? (2s n + ? V,n ? n ? n )? U,n n , E ?Z V g 2 = R U,n (t, ) 2 n 2 E v t 2 ? (2s n + ? U,n ? n )? n ? V,n n .</formula><p>Proposition 8 implies the result.</p><p>Lemma 6 (Concentration w.r.t. the spikes). We have</p><formula xml:id="formula_106">E ? 1 n E Z,Z U ,Z V ln Z n,t, (U , V , Z,Z U ,Z V ) ? f n (t, ) 2 ? 2? U,n S 2 n (2s n + ? n ? n ? V,n ) 2 + ? 2 n S 4 + 2? n ? V,n S 2 n (2s n + ? n ? U,n ) 2 + S 4 .</formula><p>Proof. Let g(U ) be the free energy <ref type="bibr" target="#b37">(38)</ref> seen as a function of U only. De ne U (i) as a vector with same entries as U except the i-th one that is replaced by U i drawn independently from P U,n . Let H(t, sU + (1 ? s)U (i) ) be the interpolating Hamiltonian <ref type="bibr" target="#b36">(37)</ref> with U replaced by sU + (1 ? s)U (i) .</p><p>We bound</p><formula xml:id="formula_107">E (g(U ) ? g(U (i) )) 2 = 1 n 2 E 1 0 ds dH ds (t, sU + (1 ? s)U (i) ) t 2 = 1 n 2 E (U i ? U i ) R V,n u i + 1 ? t n u i (v ? V ) t 2 ? 2 n 2 E (U i ? U i ) 2 u i 2 t (2s n + ? n ? n ? V,n ) 2 + 1 n 2 u i (v ? V ) 2 t ? 2 n 2 E (U i ? U i ) 2 S 2 (2s n + ? n ? n ? V,n ) 2 + ? 2 n S 6 ? 4? U,n S 2 n 2 (2s n + ? n ? n ? V,n ) 2 + ? 2 n S 4 .</formula><p>Similarly, and with an anlogous notation V (i) , we obtain</p><formula xml:id="formula_108">E (g(V ) ? g(V (i) )) 2 ? 4? V,n S 2 n 2 (2s n + ? n ? U,n ) 2 + S 4 .</formula><p>Proposition 9 then implies the claim.</p><p>7 Concentration for the overlaps 7.1 Overlap concentration for the Wigner case: proof of inequality <ref type="bibr" target="#b18">(19)</ref> The derivations below will apply for any t ? [0, 1] so we drop all un-necessary notations and indices. Only the dependence of the free energies in R( ) ? R n (t, ) matters, so we denote F (R( )) ? F n,t, (W (t),W (t, )) and f (R( )) ? f n (t, ).</p><p>Let L be the R( )-derivative of the Hamiltonian (32) divided by n:</p><p>L(x, X,Z) = L ? 1 n dH n,t, dR( ) = 1 n</p><formula xml:id="formula_109">x 2 2 ? x ? X ? x ?Z 2 R( ) .<label>(40)</label></formula><p>The overlap uctuations are upper bounded by those of L, which are easier to control, as</p><formula xml:id="formula_110">E (Q ? E Q t ) 2 t ? 4 E (L ? E L t ) 2 t .<label>(41)</label></formula><p>The bracket is again the expectation w.r.t. the posterior of the interpolating model <ref type="bibr" target="#b8">(9)</ref>. A detailed derivation of this inequality can be found in appendix 8 and involves only elementary algebra using the Nishimori identity and integrations by parts w.r.t. the gaussian noiseZ. We have the following identities: for any given realisation of the quenched disorder</p><formula xml:id="formula_111">dF dR( ) = L t ,<label>(42)</label></formula><p>1 n</p><formula xml:id="formula_112">d 2 F dR( ) 2 = ? (L ? L t ) 2 t + 1 4n 2 R( ) 3/2 x t ?Z .<label>(43)</label></formula><p>The gaussian integration by part formula (75) with hamiltonian (32) yields</p><formula xml:id="formula_113">E Z ? x t R( ) = E x 2 t ? E x t 2 N = E x 2 t ? E X ? x t = E x 2 t ? n E Q t .<label>(44)</label></formula><p>Therefore averaging (42) and (43) we nd</p><formula xml:id="formula_114">df dR( ) = E L t N = ? 1 2 E Q t ,<label>(45)</label></formula><p>1 n</p><formula xml:id="formula_115">d 2 f dR( ) 2 = ?E (L ? L t ) 2 t + 1 4n 2 R( ) E x ? x t 2 t .<label>(46)</label></formula><p>We always work under the assumption that the map ? [s n , 2s n ] ? R( ) ? [R(s n ), R(2s n )] is regular, and do not repeat this assumption in the statements below. The concentration inequality <ref type="formula" target="#formula_1">(19)</ref> is a direct consequence of the following result (combined with Fubini's theorem):</p><p>Proposition 11 (Total uctuations of L). Let the sequences ? n and ? n verify (23). Then</p><formula xml:id="formula_116">2sn sn d E (L ? E L t ) 2 t ? C ? n ? n ns n 1 + ? n ? 2 n 1/3</formula><p>for a constant C &gt; 0 that is independent of n, as long as the r.h.s. is ?(1/n).</p><p>The proof of this proposition is broken in two parts, using the decomposition</p><formula xml:id="formula_117">E (L ? E L t ) 2 t = E (L ? L t ) 2 t + E ( L t ? E L t ) 2 .</formula><p>Thus it su ces to prove the two following lemmas. The rst lemma expresses concentration w.r.t. the posterior distribution (or "thermal uctuations") and is a direct consequence of concavity properties of the average free energy and the Nishimori identity.</p><p>Lemma 7 (Thermal uctuations of L). We have</p><formula xml:id="formula_118">2sn sn d E (L ? L t ) 2 t ? ? n n 1 + ln 2 4 .</formula><p>Proof. We emphasize again that the interpolating free energy <ref type="formula" target="#formula_18">(8)</ref> is here viewed as a function of R( ).</p><p>In the argument that follows we consider derivatives of this function w.r.t. R( ). By (46)</p><formula xml:id="formula_119">E (L ? L t ) 2 t = ? 1 n d 2 f dR( ) 2 + 1 4n 2 R( ) E x 2 t ? E x t 2 ? ? 1 n d 2 f dR( ) 2 + ? n 4n ,<label>(47)</label></formula><p>where we used R( ) ? and 1 n E x 2 t N = E[X 2 1 ] = ? n . We integrate this inequality over ? [s n , 2s n ]. Recall the map ? R( ) has a Jacobian ? 1, is C 1 and has a well de ned C 1 inverse since we have assumed that it is regular. Thus integrating (47) and performing a change of variable (to get the second inequality) we obtain</p><formula xml:id="formula_120">2sn sn d E (L ? L t ) 2 t ? ? 1 n 2sn sn d d 2 f dR( ) 2 + ? n 4n 2sn sn d ? ? 1 n R(2sn) R(sn) dR( ) d 2 f dR( ) 2 + ? n 4n 2sn sn d = 1 n df dR( ) (R(s n )) ? df dR( ) (R(2s n )) + ? n 4n ln 2 .</formula><p>We have |f (R( ))| = |E Q t /2| ? ? n /2 so the rst term is certainly smaller in absolute value than ? n /n. This concludes the proof of Lemma 7.</p><p>The second lemma expresses the concentration w.r.t. the quenched disorder variables and is a consequence of the concentration of the free energy onto its average (w.r.t. the quenched variables).</p><p>Lemma 8 (Quenched uctuations of L). Let the sequences ? n and ? n verify (23). Then</p><formula xml:id="formula_121">2sn sn d E ( L t ? E L t ) 2 ? C ? n ? n ns n 1 + ? n ? 2 n 1/3</formula><p>for a constant C &gt; 0 that is independent of n, as long as the r.h.s. is ?(1/n).</p><p>Proof. Consider the following functions of R( ):</p><formula xml:id="formula_122">F (R( )) ? F (R( )) + S R( ) n n i=1 |Z i | , f (R( )) ? EF (R( )) = f (R( )) + S R( )E |Z 1 | .<label>(48)</label></formula><p>Because of (43) we see that the second derivative ofF (R( )) w.r.t. R( ) is negative so that it is concave.</p><formula xml:id="formula_123">Note F (R( )) itself is not necessarily concave in R( ), although f (R( )) is. Concavity of f (R( )) is not obvious from (46) (obtained from di erentiating E L t w.r.t. R( )) but can be seen from (77) (ob- tained instead by di erentiating ? 1 2 E Q t ) which reads d dR( ) E Q t = ?2 d 2 dR( ) 2 f ? 0.</formula><p>Equivalently it follows from the relation (35) between mutual information and free energy and the concavity of the mutual information Lemma 14. Evidentlyf (R( )) is concave too. Concavity then allows to use the following standard lemma: Lemma 9 (A bound for concave functions). Let G(x) and g(x) be concave functions. Let ? &gt; 0 and</p><formula xml:id="formula_124">de ne C ? ? (x) ? g (x ? ?) ? g (x) ? 0 and C + ? (x) ? g (x) ? g (x + ?) ? 0. Then |G (x) ? g (x)| ? ? ?1 u?{x??, x, x+?} |G(u) ? g(u)| + C + ? (x) + C ? ? (x) .</formula><p>First, from (48) we hav?</p><formula xml:id="formula_125">F (R( )) ?f (R( )) = F (R( )) ? f (R( )) + S R( )A n (49) with A n ? 1 n n i=1 |Z i | ? E |Z 1 |.</formula><p>Second, from (42), (45) we obtain for the R( )-derivatives</p><formula xml:id="formula_126">F (R( )) ?f (R( )) = L t ? E L t + SA n 2 R( ) .<label>(50)</label></formula><p>From (49) and (50) it is then easy to show that Lemma 9 implies</p><formula xml:id="formula_127">| L t ? E L t | ? ? ?1 u?{R( )??, R( ), R( )+?} |F (u) ? f (u)| + S|A n | ? u + C + ? (R( )) + C ? ? (R( )) + S|A n | 2 ? (51)</formula><p>where C ? ? (R( )) ?f (R( ) ? ?) ?f (R( )) ? 0 and C + ? (R( )) ?f (R( )) ?f (R( ) + ?) ? 0. We used R( ) ? for the term S|A n |/(2 ? ). Note that ? will be chosen later on strictly smaller than s n so that R( ) ? ? ? ? ? ? s n ? ? remains positive. Remark that by independence of the noise variables E[A 2 n ] = (1 ? 2/?)/n ? 1/n. We square the identity (51) and take its expectation. Then</p><formula xml:id="formula_128">using ( p i=1 v i ) 2 ? p p i=1 v 2 i</formula><p>, and that R( ) ? 2s n + ? n ? n , as well as the free energy concentration Proposition 7 (under the assumption that ? n and ? n verify (23)),</p><formula xml:id="formula_129">1 9 E ( L t ? E L t ) 2 ? 3 n? 2 C? 2</formula><p>n ? 3 n + S(2s n + ? n ? n + ?)</p><formula xml:id="formula_130">+ C + ? (R( )) 2 + C ? ? (R( )) 2 + S 4n .<label>(52)</label></formula><p>Recall |C ? ? (R( ))| = |f (R( ) ? ?) ?f (R( ))|. By (45), (48) and R( ) ? we have</p><formula xml:id="formula_131">|f (R( ))| ? 1 2 ? n + S R( ) ? 1 2 ? n + S ?<label>(53)</label></formula><p>Thus, as ? s n ,</p><formula xml:id="formula_132">|C ? ? (R( ))| ? ? n + S ? ? ? ? ? n + S ? s n ? ? .</formula><p>We reach</p><formula xml:id="formula_133">2sn sn d C + ? (R( )) 2 + C ? ? (R( )) 2 ? ? n + S ? s n ? ? 2sn sn d C + ? (R( )) + C ? ? (R( )) ? ? n + S ? s n ? ? R(2sn) R(sn) dR( ) C + ? (R( )) + C ? ? (R( )) = ? n + S ? s n ? ? f (R(s n ) + ?) ?f (R(s n ) ? ?) + f (R(2s n ) ? ?) ?f (R(2s n ) + ?)</formula><p>where we used that the Jacobian of the C 1 -di eomorphism ? R( ) is ? 1 (by regularity) for the second inequality. The mean value theorem and (53) imply |f (R( )??)?f (R( )+?)| ? ?(? n + S ? sn?? ). Therefore</p><formula xml:id="formula_134">2sn sn d C + ? (R( )) 2 + C ? ? (R( )) 2 ? 2? ? n + S ? s n ? ? 2 .</formula><p>Set ? = ? n = o(s n ). Thus, integrating (52) over ? [s n , 2s n ] yields</p><formula xml:id="formula_135">2sn sn d E ( L t ? E L t ) 2 ? 27s n n? 2 n C? 2 n ? 3 n + S(2s n + ? n ? n + ? n ) + 18? n ? n + S ? s n ? ? n 2 + 9S ln 2 4n ? Cs n ? n ? n n? 2 n (1 + ? n ? 2 n ) + C? n s n + C n</formula><p>where the constant C is generic, and may change from place to place. Finally we optimize the bound choosing ? 3 n = s 2 n ? n ? n (1 + ? n ? 2 n )/n. We verify the condition ? n = o(s n ): we have (? n /s n ) 3 = O(? n ? n (1 + ? n ? 2 n )/(ns n )) which, by <ref type="bibr" target="#b22">(23)</ref>, indeed tends to 0 + for an appropriately chosen sequence s n . So the dominating term ? n /s n gives the result. Again we drop all un-necessary notations and indices and keep only the dependence of the free energies on R( ) = (R U ( ), R V ( )) ? (R U,n (t, ), R V,n (t, )). We denote F (R( )) and f (R( )), respectively, the free energies <ref type="bibr" target="#b37">(38)</ref> and (39). We start proving the ovelap concentration for Q V ? v ? V /m. As the computations are similar as for the spiked Wigner model we are more brief.</p><p>Let L V be the R U ( )-derivative of the hamiltonian (37) divided by m = ? n n:</p><formula xml:id="formula_136">L V ? 1 m v 2 2 ? v ? V ? v ?Z V 2 R U ( ) .<label>(54)</label></formula><p>We have as before</p><formula xml:id="formula_137">E (Q V ? E Q V t ) 2 t ? 4 E (L V ? E L V t ) 2 t .<label>(55)</label></formula><p>We relate L V 's uctuations to the free energy through</p><formula xml:id="formula_138">dF dR U ( ) = ? n L V t ,<label>(56)</label></formula><p>1 n</p><formula xml:id="formula_139">d 2 F dR U ( ) 2 = ?? 2 n (L V ? L V t ) 2 t + 1 4n 2 R U ( ) 3/2 v t ?Z V ,<label>(57)</label></formula><formula xml:id="formula_140">df dR U ( ) = ? n E L V t N = ? ? n 2 E Q V t ,<label>(58)</label></formula><p>1 n</p><formula xml:id="formula_141">d 2 f dR U ( ) 2 = ?? 2 n E (L V ? L V t ) 2 t + 1 4n 2 R U ( ) E v ? v t 2 t .<label>(59)</label></formula><p>We work under the assumption that the map ? [s n , 2s n ] 2 ? (R U ( ), R V ( )) is regular (that is C 1 with a C 1 inverse and a Jacobian determinant ? 1). The concentration inequality (28) follows from:</p><p>Proposition 12 (Total uctuations of L V ). For any sequences (? n ), (s n ) verifying ? n &lt; s n the uctu-</p><formula xml:id="formula_142">ations [sn,2sn] 2 d E (L V ? E L V t ) 2</formula><p>t are bounded by the sum of the r.h.s. of inequalities (60) and (63) below. In the special case of the scalings (24) there exists C &gt; 0 independent of n such that</p><formula xml:id="formula_143">[sn,2sn] 2 d E (L V ? E L V t ) 2 t ? C 1 n | ln ? V,n | ? V,n s n + ? V,n | ln ? V,n | 2 1/3</formula><p>as long as the right hand side is ?(s n /n).</p><p>We start with the proof of the thermal uctuations:</p><p>Lemma 10 (Thermal uctuations of L V ). We have</p><formula xml:id="formula_144">[sn,2sn] 2 d E (L V ? L V t ) 2 t ? ? n ? 2 V,n n + s n ? V,n n? n 1 + ln 2 4 .<label>(60)</label></formula><p>Proof. Integrating (59), using R U ? U and the regularity assumption for ? R( ) we obtain</p><formula xml:id="formula_145">? 2 n [sn,2sn] 2 d E (L V ? L V t ) 2 t ? ? 1 n R([sn,2sn] 2 ) dR( ) d 2 f dR U ( ) 2 + ? V,n ? n 4n [sn,2sn] 2 d U .</formula><p>We have R([s n , 2s n ] 2 ) ? R ? [s n , 2s n + ? n ? U,n ] ? [s n , 2s n + ? n ? n ? V,n ]. Moreover the second R Uderivative of f is negative. This is not immediately obvious from (59) but can be easily shown similarly to the Wigner case, and is equivalent to say that the averaged overlap cannot decrease when the SNR R U increases. Therefore we can integrate over the larger set R to get a bound:</p><formula xml:id="formula_146">? 2 n [sn,2sn] 2 d E (L V ? L V t ) 2 t ? ? 1 n R dR V dR U d 2 f dR 2 U + ? V,n ? n s n 4n ln 2 ? 1 n 2sn+?n?n? V,n sn dR V df dR U (R U = s n , R V )? df dR U (R U = 2s n + ? n ? U,n , R V ) + ? V,n ? n s n 4n ln 2 ? (s n + ? n ? n ? V,n ) n ? V,n ? n + ? V,n ? n s n 4n ln 2 .</formula><p>In the last line we used |f (R U )| ? ? V,n ? n /2 which follows from (58). This concludes the proof of Lemma 10.</p><p>We now consider the randomness due to the quenched variables.</p><p>Lemma 11 (Quenched uctuations of L V ). For any sequences (? n ), (s n ) verifying ? n &lt; s n we have the generic bound (63) below. In the special case of the scalings (24) there exists C &gt; 0 independent of n s.t.</p><formula xml:id="formula_147">[sn,2sn] 2 d E ( L V t ? E L V t ) 2 ? C 1 n | ln ? V,n | ? V,n s n + ? V,n | ln ? V,n | 2 1/3</formula><p>as long as the right hand side is ?(s n /n).</p><p>Proof. Consider the following functions of R U ( ):</p><formula xml:id="formula_148">F (R U ( )) ? F (R( )) + S R U ( ) n m i=1 |Z V,i | , f (R U ( )) ? EF (R( )) = f (R( )) + S? n R U ( )E |Z V,1 | . Both functions are concave in R U ( ). Letting A n ? 1 n m i=1 |Z V,i | ? ? n E |Z V,1 |, Lemma 9 implies ? n | L V t ? E L V t | ?? ?1 u?{R U ??, R U , R U +?} |F (R U = u) ? f (R U = u)| + S|A n | ? u + C + ? (R U ) + C ? ? (R U ) + S|A n | 2 ? U (61) where C ? ? (R U ) ?f (R U ? ?) ?f (R U ) ? 0 and C + ? (R U ) ?f (R U ) ?f (R U + ?) ? 0. We used R U ? U . We have E[A 2</formula><p>n ] ? ? n /n. ? will be chosen strictly smaller than s n so that R U ? ? ? U ? ? ? s n ? ? remains positive. We square the identity (61) and take its expectation. Then using</p><formula xml:id="formula_149">( p i=1 v i ) 2 ? p p i=1 v 2 i</formula><p>, and that R U ? 2s n + ? n ? U,n , as well as the free energy concentration Proposition 10</p><formula xml:id="formula_150">? 2 n 9 E ( L V t ? E L V t ) 2 ?</formula><p>3 n? 2 C F,n + S? n (2s n + ? n ? U,n + ?)</p><formula xml:id="formula_151">+ C + ? (R U ) 2 + C ? ? (R U ) 2 + S? n 4n U .<label>(62)</label></formula><p>We have |f (R U )| ? ? n (? V,n +S/ ? U )/2. Thus |C ? ? (R U )| ? ? n (? V,n +S/ ? s n ? ?). We reach, using the regularity of the map ? R( ) and that R([s n , 2s n ] 2 ) ? [s n , 2s n +? n ? U,n ]?[s n , 2s n +? n ? n ? V,n ],</p><formula xml:id="formula_152">[sn,2sn] 2 d C + ? (R U ( )) 2 + C ? ? (R U ( )) 2 ? ? n ? V,n + S ? s n ? ? [sn,2sn] 2 d C + ? (R U ) + C ? ? (R U ) ? ? n ? V,n + S ? s n ? ? R([sn,2sn] 2 ) dR( ) C + ? (R U ( )) + C ? ? (R U ( )) ? ? n ? V,n + S ? s n ? ? (s n + ? n ? n ? V,n ) 2sn+?n? U,n sn dR U C + ? (R U ) + C ? ? (R U ) = ? n ? V,n + S ? s n ? ? (s n + ? n ? n ? V,n ) f (s n + ?) ?f (s n ? ?) + f (2s n + ? n ? U,n ? ?) ?f (2s n + ? n ? U,n + ?) ? 2?? 2 n ? V,n + S ? s n ? ? 2 (s n + ? n ? n ? V,n ) .</formula><p>For the last inequality we employed the mean value theorem to assert |f</p><formula xml:id="formula_153">(R U ? ?) ?f (R U + ?)| ? ?? n (? V,n + S/ ? s n ? ?)</formula><p>. Thus, integrating (62) over ? [s n , 2s n ] 2 yields that for any ? n &lt; s n we have (the sequence C F,n comes from Proposition 10)</p><formula xml:id="formula_154">[sn,2sn] d E ( L V t ? E L V t ) 2 ? 27s 2 n ? 2 n n? 2 n C F,n + S? n (2s n + ? n ? U,n + ? n ) + 18? n ? V,n + S ? s n ? ? n 2 (s n + ? n ? n ? V,n ) + 9Ss n ln 2 4? n n .<label>(63)</label></formula><p>Under the scalings <ref type="bibr" target="#b23">(24)</ref> and choosing ? n = o(s n ) this simpli es to <ref type="bibr">[sn,2sn]</ref> d E (</p><formula xml:id="formula_155">L V t ? E L V t ) 2 ? Cs 2 n ? 2 n n | ln ? V,n | ? V,n + C? n s n s n + ? V,n | ln ? V,n | + Cs n n<label>(64)</label></formula><p>where the constant C is generic, and may change from place to place. Optimizing ? n yields</p><formula xml:id="formula_156">? 3 n = ? s 3 n | ln ? V,n | n(s n ? ? V,n + ? V,n | ln ? V,n |) .</formula><p>It is easy to see that if ? V,n = ?(1/n), i.e., n? V,n ? +? then ? n = o(s n ). This proves the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Controlling Q U</head><p>For the control of Q U ? u ? U /n we follow the same derivation, except for working with</p><formula xml:id="formula_157">L U ? 1 n u 2 2 ? u ? U ? u ?Z U 2 R V ( ) .</formula><p>The overlap uctuations are bounded as</p><formula xml:id="formula_158">E (Q U ? E Q U t ) 2 t ? 4 E (L U ? E L U t ) 2 t .</formula><p>The free energy R V -derivatives and L U are then related by similar identities as (56)-(59) but with V replaced by U , v by u and ? n replaced by 1. Working out the thermal uctuations then gives</p><formula xml:id="formula_159">[sn,2sn] 2 d E (L U ? L U t ) 2 t ? ? n ? 2 U,n n + s n ? U,n n 1 + ln 2 4 .</formula><p>Considering now the quenched uctuations, a careful derivation of the equivalent identity to (64) under the scalings <ref type="bibr" target="#b23">(24)</ref> yields (under the assumption ? n = o(s n ))</p><formula xml:id="formula_160">[sn,2sn] d E ( L U t ? E L U t ) 2 ? Cs 2 n ? 2 n n | ln ? V,n | + C ? n s n | ln ? V,n | ? V,n + Cs n n .</formula><p>Optimizing over ? n yields ? n = ?(s n n ?1/3 (? V,n | ln ? V,n |) 1/6 ), so ? n = o(s n ). This nally gives, once combined with the thermal uctuations bound:</p><p>Proposition 13 (Total uctuations of L U ). Under the scalings (24) there exists a constant C &gt; 0 independent of n such that</p><formula xml:id="formula_161">[sn,2sn] 2 d E (L U ? E L U t ) 2 t ? C (ln ? V,n ) 2 n? V,n 1/3</formula><p>as long as the right hand side is ?(s n /n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Proof of inequality (41)</head><p>Let us drop the index in the bracket ? t and simply denote R ? R n (t, ). We start by proving the identity</p><formula xml:id="formula_162">?2 E Q(L ? E L ) = E (Q ? E Q ) 2 + E (Q ? Q ) 2 .<label>(65)</label></formula><p>Using the de nitions Q ? 1 n x ? X and (40) gives</p><formula xml:id="formula_163">2 E Q(L ? E L ) = E 1 n Q x 2 ? 2 Q 2 ? 1 n ? R Q(Z ? x) ? E Q E 1 n x 2 ? 2 Q ? 1 n ? RZ ? x .<label>(66)</label></formula><p>The gaussian integration by part formula (75) with Hamiltonian (32) yields</p><formula xml:id="formula_164">1 n ? R E Q(Z ? x) = 1 n E Q x 2 ? 1 n E Q(x ? x ) N = 1 n E Q x 2 ? E[ Q 2 ] .</formula><p>Fort the last equality we used the Nishimori identity as follows</p><formula xml:id="formula_165">1 n E Q(x ? x ) = 1 n 2 E (x ? X)(x ? x ) N = 1 n 2 E (X ? x)(X ? x ) = E[ Q 2 ] .</formula><p>Note that we already proved (44), namely</p><formula xml:id="formula_166">1 n ? R E Z ? x = 1 n E x 2 ? E Q .</formula><p>Therefore (66) nally simpli es to</p><formula xml:id="formula_167">2 E Q(L ? E L ) = E[ Q 2 ] ? 2 E Q 2 + E[ Q ] 2 = ? E Q 2 ? E[ Q ] 2 ? E Q 2 ? E[ Q 2 ] .</formula><p>which is identity (65). This identity implies the inequality</p><formula xml:id="formula_168">2 E Q(L ? E L ) = 2 E (Q ? E Q )(L ? E L ) ? E (Q ? E Q ) 2</formula><p>and an application of the Cauchy-Schwarz inequality gives</p><formula xml:id="formula_169">2 E (Q ? E Q ) 2 E (L ? E L ) 2 1/2 ? E (Q ? E Q ) 2 .</formula><p>This ends the proof of (41).</p><p>9 Heurisitic derivation of the phase transition</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">The Wigner case</head><p>In this section we analyze the potential function in order to heuristically locate the information theoretic transition in the special case of the spiked Wigner model with Bernoulli prior P X = Ber(?). The main hypotheses behind this computation are i) that the SNR ? = ?(?) varies with ? as ? = 4?| ln ?|/? with ? &gt; 0 and independent of ?; that ii) in this SNR regime the potential possesses only two minima {q + , q ? } that approach, as ? ? 0 + , the boundary values q ? = o(?/| ln ?|) and q + ? ?. For the Bernoulli prior the potential explicitly reads</p><formula xml:id="formula_170">i pot n (q, ?, ?) ? ?(q 2 + ? 2 ) 4 ? (1 ? ?)E ln 1 ? ? + ?e ? 1 2 ?q+ ? ?qZ ? ? E ln 1 ? ? + ?e 1 2 ?q+ ? ?qZ .</formula><p>We used that</p><formula xml:id="formula_171">I(X; ? ?X + Z) = ?E ln dP X (x)e ? 1 2 ?x 2 +?Xx+ ? ?Zx + 1 2 E[X 2 ]? .<label>(67)</label></formula><p>Let us compute this function around its assumed minima. Starting with q ? = o(?/| ln ?|) (this means that this quantity goes to 0 + faster than ?/| ln ?| as ? vanishes) we obtain at leading order after a careful Taylor expansion in ?q ? ? 0 + (the symbol ? means equality up to lower order terms as ? ? 0 + )</p><formula xml:id="formula_172">i pot n (q ? , ?, ?) ? ?(q ? ) 2 4 + ?? 2 4 ? ?(?q ? ) 2 8 ? ?? 2 4 = ??| ln ?| .<label>(68)</label></formula><p>For the other minimum q + ? ?, because ?q + ? +? the Z contribution in the exponentials appearing in the potential can be dropped due to the precense of the square root. We obtain at leading order</p><formula xml:id="formula_173">i pot n (q + , ?, ?) ? 2??| ln ?| ? ln{1 + ? 1+2? } ? ? ln{1 + ? 1?2? } .</formula><p>Here there are two cases to consider: ? &gt; 1/2 and 0 &lt; ? ? 1/2. We start with ? &gt; 1/2. In this case the potential simpli es to i pot n (q + , ?, ?) ? ?| ln ?| . Now for 0 &lt; ? ? 1/2 we have i pot n (q + , ?, ?) ? 2??| ln ?| .</p><p>The information theoretic threshold ? c = ? c (?) is de ned as the rst non-analiticy in the mutual information. In the present setting this corresponds to a discontinuity of the rst derivative w.r.t. the SNR of the mutual information (and we therefore speak about a" rst-order phase transition"). By the I-MMSE formula this threshold manifests itself as a discontinuity in the MMSE. In the high sparsity regime ? ? 0 + the transition is actually as sharp as it can be with a 0-1 behavior. This translates, at the level of the potential, as the SNR threshold where its minimum is attained at q ? just below and instead at q + just above. So we equate lim ??0 + i pot n (q ? , ? c , ?) = lim ??0 + i pot n (q + , ? c , ?) and solve for ? c . This is only possible, under the constraint ? &gt; 0 independent of ?, in the case ? &gt; 1/2 and gives ? = 1 which is the claimed information theoretic threshold ? c (?) = 4| ln ?|/?. Repeating this analysis for the Bernoulli-Rademacher prior P X = (1 ? ?)? 0 + 1 2 ?(? ?1 + ? 1 ) leads the same threshold. Another piece of information gained from this analysis is that around the transition the mutual information divided by n is ?(?| ln ?|). Therefore the proper normalization for the mutual information is (n?| ln ?|) ?1 I(X; W ) for it to have a well de ned non trivial limit in the regime ? ? 0 + .</p><p>Finally for ? ? 1 the minimum of the potential is attained at q ? and the rescaled mutual information (n?| ln ?|) ?1 I(X; W ) equals ? as seen from (68). If instead ? ? 1 the minimum is attained at q + and the mutual information instead saturates to 1, so we get formula (5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">The Wishart case</head><p>We do the same analysis but for the spiked covariance model with Bernoulli-Rademacher distributed V , namely P U = N (0, 1) (so ? U = 1) and P V = (1 ? ?)? 0 + 1 2 ?(? ?1 + ? 1 ). But again, the analysis is similar for Bernoulli prior P V = Ber(?) and leads to the same threshold. In the Bernoulli-Rademacher case the potential simpli es to</p><formula xml:id="formula_174">i pot n (q U , q V , ?, ?, 1, ?) = ?? 2 (q U ? 1)(q V ? ?) + 1 2 ln(1 + ??q V ) + ?I n (V ; ?q U V + Z) .</formula><p>This potential is concave in q V . Equating the q V -derivative of this potential to zero yields the stationary condition</p><formula xml:id="formula_175">q U = q U (q V ) = ??q V 1 + ??q V .<label>(69)</label></formula><p>So plugging back this supremum in the two-letters potential and using again (67) gives</p><formula xml:id="formula_176">i pot n (q U , q V , ?, ?, 1, ?) = ?? 2 ? ? q V 1 + ??q V + 1 2 ln(1 + ??q V ) + ???q U 2 ? ?E ln dP V (v)e ? 1 2 ?q U v 2 +?q U V v+ ? ?q U Zv</formula><p>where q U = q U (q V ) veri es (69). It nally becomes, using the Bernoulli-Rademacher prior for P V as well as Z = ?Z in law (because Z ? N (0, 1)),</p><formula xml:id="formula_177">i pot n (q U , q V , ?, ?, 1, ?) = ?? 2 ? ? q V 1 + ??q V + 1 2 ln(1 + ??q V ) + ???q U 2 ? ?E (1 ? ?) ln 1 ? ? + ? 2 e ? 1 2 ?q U + ? ?q U Z + ? 2 e ? 1 2 ?q U ? ? ?q U Z + ? ln 1 ? ? + ? 2 e 1 2 ?q U + ? ?q U Z + ? 2 e ? 3 2 ?q U ? ? ?q U Z .<label>(70)</label></formula><p>Similarly as for the Wigner case the hypotheses behind this computation are i) that the SNR ? = ?(?) varies with ? as ? = 4?| ln ?|/(??) with ? &gt; 0 and independent of ?; that ii) in this SNR regime the potential possesses only two minima {q + V , q ? V } that approach, as ? ? 0 + , the boundary values</p><formula xml:id="formula_178">q ? V = o(?/| ln ?|) and q + V ? ?. This implies that as ? ? 0 + ?q U (q + V ) ? 4?| ln ?| 1 + 4???| ln ?| ? +? and ?q U (q ? V ) = o(1) ? 0 + . Because both ?q + V = ?( ?| ln ?|) ? 0 + and ?q ? V = o( ?/| ln ?|) ? 0 + we have ? ?? 2 q ? V 1 + ??q ? V + 1 2 ln(1 + ??q ? V ) ? ? ??q ? V 2 + (??q ? V ) 2 2 + ??q ? V 2 ? (??q ? V ) 2 4 = (??q ? V ) 2 4 .</formula><p>We start by considering the case q ? V . In this case a Taylor expansion gives at leading order</p><formula xml:id="formula_179">i pot n (q U (q ? V ), q ? V , ?, ?, 1, ?) ? ??? 2 ? ?(??) 2 q ? V 2 + (??q ? V ) 2 4 + ???q U (q ? V ) 2 ? ??(?q U (q ? V )) 2 8 ? ??? 2 + o(?) = ???| ln ?| + o(?) .<label>(71)</label></formula><p>We now consider the other minimum q + V ? ?. In this case we have (??q + V ) 2 /4 ? ???? ln ?. As ?q U (q + V ) ? +? the Z contributions in the exponentials appearing in (70) are sub-dominant and therefore dropped. We obtain at leading order</p><formula xml:id="formula_180">i pot n (q U (q + V ), q + V , ?, ?, 1, ?) ? ??? 2 ? ?(??) 2 q + V 2 + (??q + V ) 2 4 + ???q U (q + V ) 2 ? ?? ln 1 + ? 2 e 1 2 ?q U (q + V ) ? ??? 2 + 2??? ln ? ? ??? ln ? ? 2??? ln ? ? ?? ln 1 + ? e 1 2 ?q U (q + V ) = ???| ln ?| ? ??? ln ? ? ?? ln 1 + ? 1?2? .</formula><p>We need again to distinguish cases. Starting with ? &gt; 1/2 this becomes</p><formula xml:id="formula_181">i pot n (q U (q + V ), q + V , ?, ?, 1, ?) ? ???| ln ?| ? ?(1 ? ?)? ln ? .</formula><p>We recall that here ? = ?(1) so in the regime ? ? 0 + the right hand side remains positive. If instead ? ? 1/2 then</p><formula xml:id="formula_182">i pot n (q U (q + V ), q + V , ?, ?, 1, ?) ? ???| ln ?| ? ??? ln ? .</formula><p>Comparing these two last expressions with (71), we see that equating the potential at its two minima in order to locate the phase transition is possible only when ? &gt; 1/2 (because ? &gt; 0 and independent of ?). This gives ? = 1 and therefore identi es the transition at ? c = 4| ln ?|/(??).</p><p>From this analysis we also obtain that the mutual information divided by n is ?( ?| ln ?|) which justi es the normalization (n ?| ln ?|) ?1 I((U , V ); W ) for it to have a non-trivial limit as ? ? 0 + .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">The Nishimori identity</head><p>Lemma 12 (Nishimori identity). Let (X, Y ) be a couple of random variables with joint distribution P (X, Y ) and conditional distribution P (X|Y ). Let k ? 1 and let x (1) , . . . , x (k) be i.i.d. samples from the conditional distribution. We use the bracket ? for the expectation w.r.t. the product measure P (x (1) |Y )P (x (2) |Y ) . . . P (x (k) |Y ) and E for the expectation w.r.t. the joint distribution. Then, for all continuous bounded function g we have</p><formula xml:id="formula_183">E g(Y , x (1) , . . . , x (k) ) = E g(Y , X, x (2) , . . . , x (k) ) .</formula><p>Proof. This is a simple consequence of Bayes formula. It is equivalent to sample the couple (X, Y ) according to its joint distribution or to sample rst Y according to its marginal distribution and then to sample X conditionally on Y from the conditional distribution. Thus the two (k + 1)-tuples (Y , x (1) , . . . , x (k) ) and (Y , X, x <ref type="bibr" target="#b1">(2)</ref> , . . . , x (k) ) have the same law.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Information theoretic properties of gaussian channels</head><p>In this appendix we prove important information theoretic properties of gaussian channels. These are mostly known <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, but we adapt them to our setting and provide detailed proofs for the convenience of the reader.</p><p>Let us start with a key relation between the mutual information and the MMSE for gaussian channels. Equation (72) below is called the I-MMSE formula.</p><p>Lemma 13 (I-MMSE formula). Consider a signal X ? R n with X ? P X that has nite support, and gaussian corrupted data Y ? N ( ? R X, I n ) and possibly additional generic data W ? P W |X (? |X) with H(W ) bounded. The I-MMSE formula linking the mutual information and the MMSE then reads</p><formula xml:id="formula_184">d dR I X; (Y , W ) = d dR I(X; Y |W ) = 1 2 MMSE(X|Y , W ) = 1 2 E X ? x 2 ,<label>(72)</label></formula><p>where the Gibbs-bracket ? is the expectation acting on x ? P (? |Y , W ).</p><p>Proof. First note that by the chain rule for mutual information I(X; (Y , W )) = I(X; Y |W ) + I(X; W ), so the derivatives in (72) are equal. We will now look at d dR I(X; (Y , W )). Since, conditionally on X, Y and W are independent, we have</p><formula xml:id="formula_185">I X; (Y , W ) = H(Y , W ) ? H(Y , W |X) = H(Y , W ) ? H(Y |X) ? H(W |X) .</formula><p>With gaussian noise contribution H(Y |X) = n 2 ln(2?e). Therefore only H(Y , W ) depends on R.</p><p>Let us then compute, using the change of variable Y = ? R X + Z,</p><formula xml:id="formula_186">d dR I X; (Y , W ) = d dR H(Y , W ) = ? d dR dP X (X)dY dW P W |X (W |X) e ? 1 2 Y ? ? RX 2 (2?) n/2 ln dP X (x)P W |X (W |x) e ? 1 2 Y ? ? Rx 2 (2?) n/2 = ? dP X (X)dZdW P W |X (W |X) e ? 1 2 Z 2 (2?) n/2 d dR ln dP X (x)P W |X (W |x) e ? 1 2 Z? ? R(x?X) 2 (2?) n/2 = 1 2 ? R E X,Z,W |X (Z + ? R(X ? x)) ? (X ? x)<label>(73)</label></formula><p>where Z ? N (0, I n ) and the bracket notation is the expectation w.r.t. the posterior proportional to</p><formula xml:id="formula_187">dP X (x)dP W |X (W |x)dZ exp ? 1 2 Z ? ? R(x ? X) 2 .</formula><p>In (73) the interchange of derivative and integrals is permitted by a standard application of Lebesgue's dominated convergence theorem in the case where the support of P X is bounded. Now we use the following gaussian integration by part formula: for any bounded function g : R n ? R n of a standard gaussian random vector Z ? N (0, I n ) we obviously have</p><formula xml:id="formula_188">E[Z ? g(Z)] = E[? Z ? g(Z)] .<label>(74)</label></formula><p>This formula applied to a Gibbs-bracket associated to a general Gibbs distribution with hamiltonian H( </p><p>Applied to (73), where the "hamiltonian" is H(x, Z) = ? ln P W |X (W |x) + 1 2 Z ? ? R(x ? X) 2 , this identity gives</p><formula xml:id="formula_190">d dR I X; (Y , W ) = 1 2 E X ? x 2 + 1 ? R ? Z ? X ? x = 1 2 E X ? x 2 ? 1 ? R (X ? x) ? (Z + ? R(X ? x)) + 1 ? R (X ? x) ? Z + ? R(X ? x) = 1 2 E X ? x 2 .</formula><p>The MMSE cannot increase when the SNR increases. This translates into the concavity of the mutual information of gaussian channels as a function of the SNR.</p><p>Lemma 14 (Concavity of the mutual information in the SNR). Consider the same setting as Lemma 13. Then the mutual informations I(X; (Y , W )) and I(X; Y |W ) are concave in the SNR of the gaussian channel:</p><formula xml:id="formula_191">d 2 dR 2 I X; (Y , W ) = d 2 dR 2 I(X; Y |W ) = 1 2 d dR MMSE(X|Y , W ) = ? 1 2n n i,j=1 E ( x i x j ? x i x j ) 2 ? 0</formula><p>where the Gibbs-bracket ? is the expectation acting on x ? P (? |Y , W ).</p><p>Proof. Set Q ? x ? X/n where x ? P (? |Y , W ). From a Nishimori identity MMSE(X|Y , W ) = E P X [X 2 ] ? E Q . Thus by the I-MMSE formula we have, by a calculation similar to (75),</p><formula xml:id="formula_192">?2 d 2 dR 2 I X; (Y , W ) = d E Q dR = nE[ Q L ? QL ]<label>(76)</label></formula><p>where we have set L ? 1 n</p><formula xml:id="formula_193">1 2 x 2 ? x ? X ? 1 2 ? R x ? Z .</formula><p>Now we look at each term on the right hand side of this equality. The calculation of appendix 8 shows that</p><formula xml:id="formula_194">?E QL = E Q 2 ? 1 2 E[ Q 2 ]</formula><p>so it remains to compute</p><formula xml:id="formula_195">E[ Q L ] = E Q x 2 2n ? Q 2 ? Q Z ? x 2n ? R .</formula><p>By formulas (74) and (75) in which the Hamiltonian is (32) we have</p><formula xml:id="formula_196">? 1 2n ? R E Z ? x Q = ? 1 2n ? R E Q ? Z ? x + x ? ? Q = ? 1 2n E Q x 2 ? x 2 + x ? Qx ? Q x N = ? 1 2n E Q x 2 + 1 n E Q x 2 ? 1 2 E[ Q 2 ] .</formula><p>In the last equality we used the following consequence of the Nishimori identity. Let x, x (2) be two replicas, i.e., conditionally (on the data) independent samples from the posterior <ref type="bibr" target="#b30">(31)</ref>. Then</p><formula xml:id="formula_197">1 n E x ? Qx = 1 n 2 E (x (2) ? x)(x ? X) N = 1 n 2 E (x (2) ? X)(X ? x) = E[ Q 2 ] .</formula><p>Thus we obtain</p><formula xml:id="formula_198">E[ Q L ? QL ] = E Q 2 ? 2E[ Q 2 ] + 1 n E Q x 2 = 1</formula><p>n 2 E (x ? X) 2 ? 2(x ? X)(x (1) ? X) + (x ? X)(x (2) ? x (3) ) N = 1 n 2 E (x ? x (0) ) 2 ? 2(x ? x (0) )(x (1) ? x (0) ) + (x ? x (0) )(x (2) ? x <ref type="bibr" target="#b2">(3)</ref> ) where x (0) , x, x <ref type="bibr" target="#b1">(2)</ref> , x <ref type="bibr" target="#b2">(3)</ref> are replicas and the last equality again follows from a Nishimori identity.</p><p>Multiplying this identity by n and rewriting the inner products component-wise we get</p><formula xml:id="formula_199">d E Q dR = 1 n n i,j=1 E x i x (0) i x j x (0) j ? 2x i x (0) i x (1) j x (0) j + x i x (0) i x (2) j x (3) j = 1 n n i,j=1 E x i x j 2 ? 2 x i x j x i x j + x i 2 x j 2<label>(77)</label></formula><p>Using (76) this ends the proof of the lemma. Note that we have also shown the positivity claimed in <ref type="bibr" target="#b21">(22)</ref> of section 3.</p><p>Lemma 15 (Concavity of the average MMSE in P X ). Consider the same setting as Lemma 13. The functionnal MMSE(X|Y , W ) is concave in P X .</p><p>Proof. Let B ? Ber(?) be a Bernoulli variable. Consider any random variables X 0 ? P X 0 , X 1 ? P X 1 independent of B. Let X B ? P X = (1 ? ?)P X 0 + ?P X 1 . Consider the problem of estimating X B given Y B = ? ? X B + Z with Z ? N (0, I n ) (and possibly other data W B ? P W |X (?|X B )). If B is given one can then choose the MMSE estimator based on P X 0 if B = 0, or P X 1 else. Therefore knowing B can only lower the MMSE in average. In equations,</p><formula xml:id="formula_200">MMSE(X B |Y B , W ) = E B E X B ? x B,P X 2 = (1 ? ?)E X 0 ? x 0,P X 2 + ?E X 1 ? x 1,P X 2 .</formula><p>Here the bracket notation x b,P X , b ? {0, 1}, means the expectation of x distributed according to the probability distribution proportional to dP X (x)P W |X (W b |x) exp{? 1 2 Y b ? ? ?x 2 }. By de nition of the MMSE</p><formula xml:id="formula_201">MMSE(X b |Y b , W , B = b) = E X b ? x b,P X b 2 ? E X b ? x b,P X 2 .</formula><p>Therefore we have MMSE(X B |Y B , W B ) ? (1 ? ?)MMSE(X 0 |Y 0 , W 0 , B = 0) + ?MMSE(X 1 |Y 1 , W 1 , B = 1)</p><formula xml:id="formula_202">? MMSE(X B |Y B , W B , B)</formula><p>which proves the desired concavity.</p><p>As a fundamental measure of uncertainty, the MMSE decreases with additional side information available to the estimator. This is because that an informed optimal estimator performs no worse (in average) than any uninformed estimator by simply discarding the side information. Lemma 17 (Stability of mutual information for gaussian channels). Consider a random variable R n ? R m (U , V ) ? P U V with conditionally (on (U , V )) independent data Y R 1 ? N (</p><formula xml:id="formula_203">? R 1 U , I n ), Y R 2 ? N ( ? R 2 U , I n ), and W ? P W |U V (? |U , V ). If Y R 1 +R 2 ? N ( ? R 1 + R 2 U , I n ) independently of the rest then I (U , V ); (Y R 1 , Y R 2 , W ) = I (U , V ); (Y R 1 +R 2 , W ) .</formula><p>Proof. The proof is a simple consequence of the stability of the normal law under addition. By conditional independence of the data on (U , V ) we have</p><formula xml:id="formula_204">I (U , V ); (Y R 1 , Y R 2 , W ) = H(Y R 1 , Y R 2 , W ) ? H(Y R 1 |U ) ? H(Y R 2 |U ) ? H(W |U , V ) (78)</formula><p>where H(Y R 1 |U ) + H(Y R 2 |U ) = n ln(2?e) because the noise is i.i.d. gaussian. Then</p><formula xml:id="formula_205">H(Y R 1 , Y R 2 , W ) ? H(Y R 1 |U ) ? H(Y R 2 |U ) = ? dP U V (U , V )dY R 1 dY R 2 dP W |U V (W |U , V ) 1 (2?) n e ? 1 2 Y R 1 ? ? R 1 U 2 ? 1 2 Y R 2 ? ? R 2 U 2 ? ln dP U V (u, v)P W |U V (W |u, v) 1 (2?) n e ? 1 2 Y R 1 ? ? R 1 u 2 ? 1 2 Y R 2 ? ? R 2 u 2 ? n ln(2?e) = ?n ln(2?e) ? dP U V (U , V )dZ 1 dZ 2 dP W |U V (W |U , V ) 1 (2?) n e ? 1 2 Z 1 2 ? 1 2 Z 2 2 ? ln dP U V (u, v)P W |U V (W |u, v) 1 (2?) n e ? 1 2 Z 1 ? ? R 1 (u?U ) 2 ? 1 2 Z 2 ? ? R 2 (u?U ) 2 = ?E ln dP U V (u, v)P W |U V (W |u, v)e ? 1 2 (R 1 +R 2 ) U ?u 2 +( ? R 1 Z 1 + ? R 2 Z 2 )?(u?U )</formula><p>where E = E (U ,V ),Z 1 ,Z 2 ,W |(U ,V ) with Z 1 and Z 2 being i.i.d. N (0, I n ) random variables. Because in law</p><formula xml:id="formula_206">? R 1 Z 1 + ? R 2 Z 2 = ? R 1 + R 2 Z with Z ? N (0, I n ) we have H(Y R 1 , Y R 2 , W ) ? H(Y R 1 |U ) ? H(Y R 2 |U ) = ?E ln dP U V (u, v)P W |U V (W |u, v)e ? 1 2 (R 1 +R 2 ) U ?u 2 + ? R 1 +R 2 Z?(u?U ) .</formula><p>Similarly we obtain that H(Y R 1 +R 2 , W ) ? H(Y R 1 +R 2 |U ), with H(Y R 1 +R 2 |U ) = n 2 ln(2?e), also equals the right hand side of the above equality. Then I((U , V ); (Y R 1 +R 2 , W )) = H(Y R 1 +R 2 , W ) ? H(Y R 1 +R 2 |U ) ? H(W |U , V ) combined with (78) implies the result.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>) 2 Figure 2 :</head><label>22</label><figDesc>Sequence of MMSE curves as a function of ? for the spiked covariance model. Left: In the sparse limit ? ? 0+ the suitably rescaled of the sparse signal V approaches a 0-1 transition with a jump discontinuity at ?c(?) = {4?| ln ?|/(??)} 1/2 . Right: In the asymptotic limit the MMSE for the gaussian signal U approaches 1 as ? ? 0+. The phase transition is seen only as a higher order e ect.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>7. 2</head><label>2</label><figDesc>Overlap concentration for the Wishart case: proof of inequality (28) 7.2.1 Controlling Q V</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>x, Z) (depending on the Gaussian noise and possibly other variables) yieldsE[Z ? h(x) ] = E ? Z ? dP (x)e ?H(x,Z) h(x) dP (x )e ?H(x ,Z) = ?E dP X (x)e ?H(x,Z) h(x) ? ? Z H(x, Z) dP X (x )e ?H(x ,Z) + E dP X (x)e ?H(x,Z) h(x) dP X (x )e ?H(x ,Z) ? dP X (x)e ?H(x,Z) ? Z H(x, Z) dP X (x )e ?H(x ,Z) = ?E h(x) ? ? Z H(x, Z) + E h(x) ? ? Z H(x, Z) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Lemma 16 (</head><label>16</label><figDesc>Conditionning reduces the MMSE). Consider the same setting as Lemma 13. For any W jointly distributed with X we haveMMSE(X|Y , W ) ? MMSE(X|Y , W , W ) .Proof. This follows directly from Lemma 15 using P X (X) = dP W (w )P X|W (X|w ):MMSE(X|Y , W ) ? dP W (w )MMSE(X|Y , W , W = w ) = MMSE(X|Y , W , W ) .</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">One may also be interested in reconstructing the vectors U and/or V rather than the spike, but in general this is only possible up to a global sign.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This is the generic singularity and one speaks of a rst order transition. In special cases the MMSE may be continuous with a higher discontinuous derivative of the mutual information.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">High dimensional regression with binary coe cients. estimating squared error and a phase transtition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gamarnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zadik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="948" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The all-or-nothing phenomenon in sparse linear regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Reeves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zadik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.05046</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">On the distribution of the largest eigenvalue in principal components analysis. The Annals of statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Johnstone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="295" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0901.4392</idno>
		<title level="m">Sparse principal components analysis</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note>math.ST</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Constrained low-rank matrix estimation: phase transitions, approximate message passing and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lesieur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Krzakala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zdeborov?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Mechanics: Theory and Experiment</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">73403</biblScope>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The dynamics of message passing on dense graphs, with applications to compressed sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bayati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="764" to="785" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Message-passing algorithms for compressed sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maleki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="18914" to="18919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exact solution of the gauge symmetric p-spin glass model on a complete graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Korada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Macris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Physics</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="205" to="230" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mutual information in rank-one matrix estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Krzakala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zdeborov?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Information Theory Workshop (ITW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="71" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mutual information for symmetric rank-one matrix estimation: A proof of the replica formula</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barbier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Macris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Krzakala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lesieur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zdeborov?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fundamental limits of symmetric low-rank matrix estimation. Probability Theory and Related Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lelarge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Miolane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="page" from="859" to="929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fundamental limits of low-rank matrix estimation: The non-symmetric case</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Miolane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-02" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The adaptive interpolation method: a simple scheme to prove replica formulas in bayesian inference. Probability Theory and Related Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barbier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Macris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The adaptive interpolation method for proving replica formulas. applications to the curie-weiss and wigner spike models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barbier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Macris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physics A: Mathematical and Theoretical</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">29</biblScope>
			<biblScope unit="page">294002</biblScope>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Layered Structure of Tensor Estimation and its Mutual Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barbier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Macris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Miolane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">55th Annual Allerton Conference on Communication, Control, and Computing (Allerton)</title>
		<imprint>
			<date type="published" when="2017-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Estimation in the spiked wigner model: a short proof of the replica formula</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">El</forename><surname>Alaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Krzakala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Symposium on Information Theory (ISIT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1874" to="1878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Mutual information for low-rank even-order symmetric tensor factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barbier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Macris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04565</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Hamilton-jacobi equations for nite-rank matrix inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Mourrat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05294</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mutual information and minimum mean-square error in gaussian channels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shamai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verdu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Information Theory</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1261" to="1282" />
			<date type="published" when="2005-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Estimation in gaussian noise: Properties of the minimum meansquare error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Shitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verd?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2371" to="2385" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optimality and sub-optimality of pca i: Spiked random matrix models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Wein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Bandeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2416" to="2451" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Alaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Krzakala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.02903</idno>
		<title level="m">Finite size corrections and likelihood ratio uctuations in the spiked wigner model</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Detection limits in the high-dimensional spiked rectangular model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Alaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference On Learning Theory</title>
		<meeting><address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="410" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Arous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>P?ch?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Probability</title>
		<imprint>
			<biblScope unit="page">1643</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The largest eigenvalue of small rank perturbations of hermitian random matrices. Probability Theory and Related Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>P?ch?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="127" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The largest eigenvalue of rank one deformation of large wigner matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>F?ral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>P?ch?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications in mathematical physics</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">272</biblScope>
			<biblScope unit="page" from="185" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">High-dimensional analysis of semide nite relaxations for sparse principal components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5B</biblScope>
			<biblScope unit="page" from="2877" to="2921" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reducibility and computational lower bounds for problems with planted sparse structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bresler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huleihel</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Conference On Learning Theory</title>
		<editor>S. Bubeck, V. Perchet, and P. Rigollet</editor>
		<meeting>the 31st Conference On Learning Theory</meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="6" to="09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gamarnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jagannath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09959</idno>
		<title level="m">The overlap gap property in principal submatrix recovery</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sparse pca via covariance thresholding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">141</biblScope>
			<biblScope unit="page" from="1" to="41" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Information-theoretically optimal sparse pca</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Symposium on Information Theory</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2197" to="2201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Optimal estimation and rank detection for sparse spiked covariance matrices. Probability Theory and Related Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-04" />
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page" from="781" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Do semide nite relaxations solve sparse pca up to the information limit?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krauthgamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vilenchik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1300" to="1322" />
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Statistical and computational trade-o s in estimation of sparse principal components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Berthet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Samworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1896" to="1930" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Complexity theoretic lower bounds for sparse principal component detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Berthet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rigollet</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual Conference on Learning Theory</title>
		<editor>S. Shalev-Shwartz and I. Steinwart</editor>
		<meeting>the 26th Annual Conference on Learning Theory<address><addrLine>Princeton, NJ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="12" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sum-of-squares lower bounds for sparse pca</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wigderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1612" to="1620" />
		</imprint>
	</monogr>
	<note>NIPS&apos;15</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Finite sample analysis of approximate message passing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkataramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Theory</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="7264" to="7286" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Phase transitions in spiked matrix estimation: information-theoretic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Miolane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.04343</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
