<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantically-Aligned Universal Tree-Structured Solver for Math Word Problems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghui</forename><surname>Qin</surname></persName>
							<email>qinjingh@mail2.sysu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihui</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dark Matter</orgName>
								<orgName type="institution">AI Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
							<email>xdliang328@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dark Matter</orgName>
								<orgName type="institution">AI Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rumin</forename><surname>Zhang</surname></persName>
							<email>rmzhang@foxmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Dark Matter</orgName>
								<orgName type="institution">AI Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<email>linliang@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dark Matter</orgName>
								<orgName type="institution">AI Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semantically-Aligned Universal Tree-Structured Solver for Math Word Problems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A practical automatic textual math word problems (MWPs) solver should be able to solve various textual MWPs while most existing works only focused on one-unknown linear MWPs. Herein, we propose a simple but efficient method called Universal Expression Tree (UET) to make the first attempt to represent the equations of various MWPs uniformly. Then a semantically-aligned universal tree-structured solver (SAU-Solver) based on an encoder-decoder framework is proposed to resolve multiple types of MWPs in a unified model, benefiting from our UET representation. Our SAU-Solver generates a universal expression tree explicitly by deciding which symbol to generate according to the generated symbols' semantic meanings like human solving MWPs. Besides, our SAU-Solver also includes a novel subtree-level semanticallyaligned regularization to further enforce the semantic constraints and rationality of the generated expression tree by aligning with the contextual information. Finally, to validate the universality of our solver and extend the research boundary of MWPs, we introduce a new challenging Hybrid Math Word Problems dataset (HMWP), consisting of three types of MWPs. Experimental results on several MWPs datasets show that our model can solve universal types of MWPs and outperforms several state-of-the-art models 1 . * Corresponding Author 1 The code and the new HMWP dataset are available at https://github.com/QinJinghui/SAU-Solver.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Math word problems (MWPs) solving aims to automatically answer a math word problem by understanding the textual description of the problem and reasoning out the underlying answer. A typical MWP is a short story that describes a partial state of the world and poses a question about an unknown quantity or multiple unknown quantities. Thus, a machine should have the ability of natural language understanding and reasoning. To solve an MWP, the relevant quantities need to be identified from the text, and the correct operators and their computation order among these quantities need to be determined. Many traditional methods <ref type="bibr" target="#b28">(Yuhui et al., 2010;</ref><ref type="bibr" target="#b17">Kushman et al., 2014;</ref><ref type="bibr" target="#b21">Shi et al., 2015)</ref> have been proposed to address this problem, but they relied on tedious hand-crafted features and template annotation, which required extensive human efforts and knowledge. Recently, deep learning has opened a new direction towards automatic MWPs solving <ref type="bibr" target="#b11">Huang et al., 2018;</ref><ref type="bibr" target="#b24">Wang et al., 2018b</ref><ref type="bibr" target="#b25">Wang et al., , 2019</ref><ref type="bibr" target="#b27">Xie and Sun, 2019;</ref><ref type="bibr" target="#b9">Chiang and Chen, 2019)</ref>. Most of deep learning-based methods try to train an end-to-end neural network to automatically learn the mapping function between problems and their corresponding equations. However, there are some limitations hindering them from arXiv:2010.06823v1 [cs.CL] 14 Oct 2020 being applied in real-world applications. First, although seq2seq model  can be applied to solve various MWPs, it suffers from fake numbers generation and mispositioned numbers generation due to all data share the same target vocabulary without problem-specific constraints. Second, some advanced methods <ref type="bibr" target="#b24">(Wang et al., 2018b</ref><ref type="bibr" target="#b25">(Wang et al., , 2019</ref><ref type="bibr" target="#b27">Xie and Sun, 2019)</ref> only target at arithmetic word problems without any unknown or with one unknown that do not need to model the unknowns underlying in MWPs, which prevent them from generalizing to various MWPs, such as equation set problems. Thus, their methods can only handle arithmetic problems with no more than one unknown. Besides, they also lack an efficient equation representation mechanism to handle those MWPs with multiple unknowns and multiple equations, such as equation set problems. Finally, though some methods <ref type="bibr" target="#b11">Huang et al., 2018;</ref><ref type="bibr" target="#b9">Chiang and Chen, 2019)</ref> can handle multiple types of MWPs, they neither generate next symbol by taking full advantage of the generated symbols like a human nor consider the semantic transformation between equations in a problem, resulting in poor performance on the multiple-unknown MWPs, such as the MWPs involving equation set.</p><p>To address the above issues, we propose a simple yet efficient method called Universal Expression Tree (UET) to make the first attempt to represent the equations of various MWPs uniformly like the expression tree of one-unknown linear word problems with considering unknowns. Specifically, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, UET integrates all expression trees underlying in an MWP into an ensemble expression tree via math operator symbol extension so that the grounded equations of various MWPs can be handled in a unified manner as handling one-unknown linear MWPs. Thus, it can significantly reduce the difficulty of modeling equations of various MWPs.</p><p>Then, we propose a semantically-aligned universal tree-structured solver (SAU-Solver), which is based on our UET representation and an Encoder-Decoder framework, to solve multiple types of MWPs in a unified manner with a single model. In our SAU-Solver, the encoder is designed to understand the semantics of MWPs and extract number semantic representation while the tree-structured decoder is designed to generate the next symbol based on the problem-specific target vocabulary in a semantically-aligned manner by taking full advantage of the semantic meanings of the gener-ated expression tree like a human uses problem's contextual information and all tokens written to reason next token for solving MWPs. The problemspecific target vocabulary can help our solver to mitigate the problem of fake numbers generation as much as possible.</p><p>Besides, to further enforce the semantic constraints and rationality of the generated expression tree, we also propose a subtree-level semanticallyaligned regularization to further improve subtreelevel semantic representation by aligning with the contextual information of a problem, which can improve answer accuracy effectively.</p><p>Finally, to validate the universality of our solver and push the research boundary of MWPs to math real-word applications better, we introduce a new challenging Hybrid Math Word Problems dataset (HMWP), consisting of one-unknown linear word problems, one-unknown non-linear word problems, and equation set problems with two unknowns. Experimental results on HWMP, ALG514, Math23K, and Dolphin18K-Manual show the universality and superiority of our approach compared with several state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Numerous methods have been proposed to attack the MWPs task, ranging from rule-based methods <ref type="bibr">(Bakman, 2007;</ref><ref type="bibr" target="#b28">Yuhui et al., 2010)</ref>, statistical machine learning methods <ref type="bibr" target="#b17">(Kushman et al., 2014;</ref><ref type="bibr" target="#b29">Zhou et al., 2015;</ref><ref type="bibr" target="#b19">Mitra and Baral, 2016;</ref><ref type="bibr" target="#b13">Huang et al., 2016;</ref><ref type="bibr" target="#b20">Roy and Roth, 2018)</ref>,semantic parsing methods <ref type="bibr" target="#b21">(Shi et al., 2015;</ref><ref type="bibr" target="#b16">Koncelkedziorski et al., 2015;</ref><ref type="bibr" target="#b12">Huang et al., 2017)</ref>, and deep learning methods <ref type="bibr" target="#b18">(Ling et al., 2017;</ref><ref type="bibr" target="#b18">Wang et al., 2017</ref><ref type="bibr" target="#b24">Wang et al., , 2018b</ref><ref type="bibr" target="#b11">Huang et al., 2018;</ref><ref type="bibr" target="#b23">Wang et al., 2018a;</ref><ref type="bibr" target="#b27">Xie and Sun, 2019;</ref><ref type="bibr" target="#b25">Wang et al., 2019)</ref>. Due to space limitations, we only review some recent advances on deep leaning-based methods.  made the first attempt to generate expression templates using Seq2Seq model. Seq2seq method has achieved promising results, but it suffers from generating spurious numbers, predicting numbers at wrong positions, or equation duplication problem <ref type="bibr" target="#b11">(Huang et al., 2018;</ref><ref type="bibr" target="#b23">Wang et al., 2018a)</ref>. To address them, <ref type="bibr" target="#b11">(Huang et al., 2018)</ref> proposed to add a copy-and-alignment mechanism to the standard Seq2Seq model. <ref type="bibr" target="#b23">(Wang et al., 2018a)</ref> proposed equation normalization to normalize the duplicated equations by considering the uniqueness of an expression tree.</p><p>Different from seq2seq-based works, <ref type="bibr" target="#b27">(Xie and Sun, 2019)</ref> proposed a tree-structured decoder to generate an expression tree inspired by the goaldriven problem-solving mechanism. <ref type="bibr" target="#b25">(Wang et al., 2019)</ref> proposed a two-stage template-based solution based on a recursive neural network for math expression construction. However, they do not model the unknowns underlying in MWPs, resulting in only handling one-unknown linear word problems. Besides, they also lack an efficient mechanism to handle those MWPs with multiple unknowns and multiple equations, such as equation set problems. Therefore, their solution can not solve other types of MWPs that are more challenging due to larger search space, such as equation set problems, non-linear equation problems, etc. (Chiang and Chen, 2019) is a general equation generator that generates expression via the stack, but they did not consider the semantic transformation between equations in a problem, resulting in poor performance on the multiple-unknown MWPs, such as equation set problems.</p><p>3 The design of SAU-Solver</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Universal Expression Tree (UET)</head><p>The primary type of textual MWPs can be divided into two groups: arithmetic word problems and equation set problems. For a universal MWPs solver, it is highly demanded to represent various equations of various MWPs in a unified manner so that the solver can generate equations efficiently. Although most of the existing works can handle one-unknown linear word problems well, it is more challenging and harder for current methods to handle the equation set MWPs with multiple unknowns well since they not only do not model the unknowns in the MWPs but also lack of an efficient equations representation mechanism to make their decoder generate required equations efficiently. To handle the above issue, an intuitive way is treating the equation set as a forest of expression trees and all trees are processed iteratively in a certain order. Although this is an effective way to handle equations set problems, it increases the difficulty of equation generation since the model needs to reason out the number of equations before starting equation generation and the prediction error will influence equation generation greatly. Besides, it is also challenging to take full advantage of the context information from the problem and the generated trees. Another way is that we can deploy Seq2Seq-based architecture to handle various equations in infix order like in previous works <ref type="bibr" target="#b11">Huang et al., 2018)</ref>, but there are some limitations, such as generating invalid expression, generating spurious numbers, and generating numbers at wrong positions.</p><p>To overcome the above issues and maintain simplicity, we propose a new equation representation called Universal Expression Tree (UET) to make the first attempt to represent the equations of various MWPs uniformly. Specially, we extend the math operator symbol table by introducing a new operator ; as the lowest priority operator to integrate one or more expression trees into a universal expression tree, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. With UET, a solver can handle the underlying equations of various textual MWPs easier in a unified manner like the way on arithmetic word problems. Although our UET is simple, it provides an efficient, concise, and uniform way to utilize the context information from the problem and treat the semantic transformation between equations as simple as treating the semantic transformation between subtrees in an equation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SAU-Solver</head><p>Based on our proposed UET representation, we design a universal tree-structured solver to generate a universal expression tree explicitly according to the problem context and explicitly model the relationships among unknown variables, quantities, math operations, and constants in a tree-structured way, as shown in <ref type="figure">Fig. 2</ref>. Our solver consists of a Bi-GRU-based problem encoder and an explicit tree-structured equation decoder. When a problem is entered, our model first encodes each word of the problem to generate the problem's contextual representation g 0 by our problem encoder. Then, the g 0 will be used as the initial hidden state by our tree-structured equation decoder to guide the equation generation in prefix order with two intertwined processes: top-down tree-structured decoding and bottom-up subtree semantic transformation. With the help of top-down tree-structured decoding and bottom-up subtree semantic transformation, SAU-Solver can generate the next symbol by taking full advantage of generated symbols in a semanticallyaligned manner like human solving MWPs. Finally, we apply infix traversal and inverse number mapping to generate the corresponding human-readable Semantically-Aligned Regularization <ref type="figure">Figure 2</ref>: An overview of our SAU-Solver. When a problem preprocessed by number mapping and replacement is entered, our problem encoder encodes the problem text as context representation. Then our equation decoder generates an expression tree explicitly in pre-order traversal for the problem according to the context representation. Finally, infix traversal and inverse number mapping are applied to generate the corresponding equation. equation that can be computed by SymPy 2 , which is a python library for symbolic mathematics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Problem Encoder</head><p>Bidirectional Gated Recurrent Unit (BiGRU) <ref type="bibr" target="#b10">(Cho et al., 2014)</ref> is an efficient method to encode sequential information. Formally, given an input math word problem sentence P = {x t } n t=1 , we first embed each word into a vector x t . Then these embeddings are fed into a two-layer BiGRU from beginning to end and from end to beginning to model the problem sequence:</p><formula xml:id="formula_0">? ? h p t = GRU ( ??? h p t?1 , x t ) ? ? h p t = GRU ( ??? h p t+1 , x t ) h p t = ? ? h p t + ? ? h p t<label>(1)</label></formula><p>where GRU (?, ?) represents the function of a twolayer GRU. h p t is the sum of the hidden states ? ? h p t and ? ? h p t , which are from both forward and backward GRUs. These representation vectors are then fed into our tree-structured equation decoder for ensemble expression tree generation. Besides, we also construct the hidden state g 0 as the initial hidden state of our equation decoder: </p><formula xml:id="formula_1">g p 0 = ? ? h p n + ? ? h p 0<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Equation Decoder</head><p>For decoding, inspired by previous works <ref type="bibr" target="#b27">(Xie and Sun, 2019;</ref><ref type="bibr" target="#b9">Chiang and Chen, 2019)</ref>, we build a semantically-aligned tree decoder to decide which symbol to generate by taking full advantage of the semantic meanings of the generated symbols with two intertwined processes: top-down treestructured decoding and bottom-up subtree semantic transformation. Our decoder takes tree-based information g parent (left node) or (g parent , t l ) (right node) as the input and maintains two auxiliary stacks G and T to enforce semantically-aligned decoding procedure. The stack G maintains the hidden states generated from the parent node while the stack T helps the model decide which symbol to generate by maintaining subtree semantic information of generated symbols. Benefiting from UET, our decoder can automatically end the decoding procedure without any special token. If the predicted token y t is an operator, then we generate two children hidden states g l and g r according to the current node embedding n of y t , and push them into the stack G to maintain the state transition among nodes and be used to predict token and its node embedding. Besides, we also push the token embedding e(y t |P ) of y t into the stack T so that we can maintain subtree semantic information of generated symbols after right child node generation. If the predicted token y t is not an operator, we check the size of the stack T to judge whether the current node is a right node. If the current node is a right node, we transform the embedding of parent node op, left sibling node l and current node e(y t |P ) to a subtree semantic representation t, which represents the semantic meanings of generated symbols for current subtree and is used to help the right node generation of the upper subtree. In this way, our equation decoder can decode out an equation as a human writes out an equation according to the problem description. Token Embedding. For a problem P , its target vocabulary V tar consists of 4 parts: math operators V op , unknowns V u , constants V con that are those common-sense numerical values occurred in the target expression but not in the problem text (e.g. a chick has 2 legs.), and the numbers n p occurred in P . For each token y in V tar , its token embedding e(y|P ) is defined as:</p><formula xml:id="formula_2">e(y|P ) = ? ? ? ? ? ? ? M op (y) if y ? V op M u (y) if y ? V u M con (y) if y ? V con h p loc (y, P ) if y ? n P<label>(3)</label></formula><p>where M op , M u , and M con are three trainable word embedding matrices independent of the specific problem. However, for a numeric value in n P , we take the corresponding hidden state h p loc from encoder as its token embedding, where loc(y, P ) is the index position of numeric value y in P . Gating Mechanism and Attention Mechanism. To better flow important information and ignore useless information, we apply a gating mechanism to generate node state n which will be used for predicting the output and generating child hidden states g l and g r for descendant nodes if the output of the current node is a math operator:</p><formula xml:id="formula_3">q = ? (W q I) Q = tanh (W Q I) O = q Q (4)</formula><p>where O can be a left node state n l , a right node state, a left child hidden state g l , or a right child hidden state g r . For n l , I is g l generated by the parent node. For n r , I is [g r , t l ] which is the concatenation of the hidden state g r generated by the parent node and the subtree semantic embedding t l of left sibling. For g l and g r , I is [n, c, e(y t |P )] which is the concatenation of the current node state n, the contextual vector c aggregating relevant information of the problem as a weighted representation of the input tokens by attention mechanism, and the token embedding e(y t |P ) of the predicted token y t .</p><p>For better predicting a token y t by utilizing contextual information, we deploy an attention mechanism to aggregate relevant information from the input vectors. Formally, given current node state n and the encoder outputs {h p t } n t=1 , we calculate the contextual vector c as follows:</p><formula xml:id="formula_4">c = s exp (V a tanh (W a [n, h p s ])) i exp (V a tanh (W a [n, h p i ])) h p s (5)</formula><p>Based on the contextual vector c and current node state n, we can predict the token y t as follows:</p><formula xml:id="formula_5">y = arg max exp(s(y|n, c, P )) i exp (s (y i |n, c, P ))<label>(6)</label></formula><p>where s(y|n, c, P ) = V n tanh (W s [n, c, e(y|P )]) (7)</p><p>Subtree Semantic Transformation. Although our decoder decodes a universal expression tree in the prefix, to help our model to generate the next symbol in a semantically-aligned manner by taking full advantage of the semantic meanings of the generated expression tree, we design a recursive neural network to transform the semantic representations of the current node and its two child subtrees t l and t r into a high-level embedding t in a bottom-up manner. Formally, let t be a subtree, and y denotes the predicted token of the root node of the subtree. If y is a math operator, which means that the current subtree t must have two child subtrees t l and t r , the high-level embedding t should fuse the semantic information from the operator token y, the left child subtree t l and the right child subtree t r as follows:</p><formula xml:id="formula_6">g t = ? (W gt [t l , t r , e(?|P )]) C t = tanh (W ct |t l , t r , e(?|P )]) t = g t C t<label>(8)</label></formula><p>Otherwise, t is the embedding e(y|P ) of the predicted token y because y is a numeric value, an unknown variable, or a constant quantity and the recursion stops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Semantically-Aligned Regularization</head><p>When a subtree t is produced by our model, this means that we have a computable unit. The semantics of this computable unit should be consistent with the problem text P . To achieve this goal, we propose a subtree-level semantically-aligned regularization to help train a better model with higher performance. For each subtree embedding t and encoder outputs h P 1 , h P 1 , ? ? ? , h P n , we first apply an attention function to compute a semanticallyaligned vector a as Equation <ref type="formula">(5)</ref>, then we use a two-layer feed-forward neural network with tanh activation to transform t and a into same semantic space respectively. The procedure can be formulated as:</p><formula xml:id="formula_7">e sa = W e2 tanh (W e1 a) d sa = W d2 tanh (W d1 t)<label>(9)</label></formula><p>where W e1 , W e2 , W d1 , and W d2 are trainable parameter matrices. With the vectors e sa and d sa Let m be the number of subtrees in a universal expression tree, we can regularize our model by minimizing the following loss:</p><formula xml:id="formula_8">L sa (T |P ) = 1 m m i=1 d sa ? e sa 2<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Training Objective</head><p>Given the training dataset D={(P i , T 1 ), (P 2 , T 2 ), ? ? ? ,(P N , T N ) }, where T i is the universal expression tree of problem P i , we minimize the following loss function:</p><formula xml:id="formula_9">L(T |P ) = (P,T )?D [? log p(T |P )+? * L sa (T |P )] (11) where p(T |P ) = m t=1 prob(y t |g t , c t , P )<label>(12)</label></formula><p>where m denotes the size of T, and g t and c t are the hidden state vector and its contextual vector at the t-th node. We set ? as 0.01 empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion</head><p>The methods most relevant to our method are GTS <ref type="bibr" target="#b27">(Xie and Sun, 2019)</ref> and StackDecoder (Chiang and Chen, 2019). However, compared with them, our method is different from them as follows. First, our method applies a universal expression tree to represent the diverse equations underlying different MWPs uniformly, which match real-word MWPs better than GTS and StackDecoder which either can only handle single-var linear MWPs without considering unknowns or can handle equations set problem iteratively. Second, we introduce subtree-level semantically-aligned regularization for better enforcing the semantic constraints and rationality of generated expression tree during training, leading to higher answer accuracy, as illustrated in It should be noticed that our dataset is sufficient for validating the universality of math word problem solvers since these problems can cover most cases about MWPs. We labeled our data with structured equations and answers as Math23K . The data statistics of our dataset and several publicly available datasets are shown in <ref type="table">Table 1</ref>. From the statistics, we can see that the #AVG EL (average equation length), #Avg PN (average number of quantities occurred in problems and their corresponding equations), and #Avg Ops (average numbers of operators in equations) are the largest among the serval publicly available datasets. <ref type="bibr" target="#b27">(Xie and Sun, 2019)</ref> showed the higher these values, the more difficult it is. Therefore, our dataset is more challenging for MWPs solvers.  <ref type="table">Table 1</ref>: Statistics of our dataset and several publicly available datasets. Avg EL, Avg SNI, Avg Constants, and Avg Ops represent average equation length, average number of quantities occurred in problems and their corresponding equations, average numbers of constants only occurred in equations, and average numbers of operators in equations, respectively. The higher these values, the more difficult it is. This has been shown in <ref type="bibr" target="#b27">(Xie and Sun, 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup and Training Details</head><p>Datasets, Baselines, and Evaluation metric.</p><p>We conduct experiments on four datasets, such as HMWP, Alg514 <ref type="bibr" target="#b17">(Kushman et al., 2014)</ref>, Math23K  and Dolphin18K-Manual <ref type="bibr" target="#b13">(Huang et al., 2016)</ref>. The data statistics of four datasets are shown in <ref type="table">Table 1</ref>. The main state-of-the-art learning-based methods to be compared are as follows: Seq2Seq-attn w/ SNI  is a universal solver based on the seq2seq model with significant number identification(SNI). GTS <ref type="bibr" target="#b27">(Xie and Sun, 2019</ref>) is a goaldriven tree-structured MWP solver only for oneunknown-variable non-linear MWPs. StackDecoder (Chiang and Chen, 2019) is a semanticallyaligned MWPs solver. SAU-Solver w/o SSAR and SAU-Solver are two universal tree-structured solvers proposed in this paper without and with subtree semantically-aligned regularization. Following our baselines, we use answer accuracy as the evaluation metric: if the calculated value of the predicted expression tree equals to the true answer, it is thought of correct since the predicted expression is equivalent to the target expression.   <ref type="bibr">, 2015)</ref> with ? 1 = 0.9, ? 2 =0.999, and = 10 ?8 . The mini-batch size is set to 32. The initial learning rate is set to 10 ?3 and then decreases to half every 20 epochs. To prevent overfitting, we set the dropout probability as 0.5 and weight decay as 1e ?5 . Finally, we set beam size as 5 in beam search to generate expression trees.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and Analyses</head><p>Answer Accuracy. We conduct 5-fold crossvalidation to evaluate the performances of baselines and our models on all four datasets. The results are shown in <ref type="table" target="#tab_2">Table 2</ref>. Several observations can be made from the results in <ref type="table" target="#tab_2">Table 2</ref> as follows: First, our SAU-Solver has achieved significantly better than the baselines on four datasets. It proves that our model is feasible for solving multiple types of MWPs. It also proves that our model is more general and more effective than other state-of-theart models on the real-word scenario that need to solve multiple types of MWPs with a unified solver.</p><p>Second, with our subtree-level semanticallyaligned regularization on training procedure, our SAU-Solver has gained additional absolute 0.43% accuracy on HMWP, absolute 1.95% accuracy on ALG514, absolute 0.31% accuracy on Math23k, and absolute 0.39% accuracy on Dolphin18k-Manual. This shows that subtree-level semantically-aligned regularization is helpful for improving subtree semantic embedding, resulting in improving expression tree generation, especially for the generation of the right child node. Although StackDecoder can be a universal math word problem solver via simple operator extension, the performances on HMWP, ALG514, and Dolphin18k-Manual are very poor, since it generates expression trees independently and only considers the semantic-aligned transformation in an expression tree. Different from it, our SAU-Solver generates multiple expression trees as a universal expression tree and conducts subtree-level semantic-aligned transformation for subsequent tree node generation in our universal expression tree. In this way, we can deliver the semantic information of the previous expression tree to help the generation of the current expression tree. Therefore we can achieve better performance than StackDecoder.</p><p>Overall, our model is more general and effective than other state-of-the-art models on multiple MWPs and outperforms the compared state-of-theart models by a large margin on answer accuracy.</p><p>Performance on different types of MWPs. We drill down to analyse the performance of Retrieval-Jaccard, Seq2seq-attn w/SNI, and SAU-Solver on different types of MWPs in HMWP. The data statistics and performance results are shown in Table 3. We can observe that our model outperforms the other two models by a large margin on all subsets. Intuitively, the longer the expression length is, the more complex the mathematical relationship of the problem is, and the more difficult it is. And the average expression length of our dataset is much longer than Math23K according to the data statistics of <ref type="table" target="#tab_7">Table 3 and Table 1</ref>. Therefore, we can observe that the accuracy of our model on linear (One-VAR) is lower than Math23K in <ref type="table" target="#tab_2">Table 2</ref>  <ref type="table">Table 5</ref>: Accuracy of different expression tree size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Error Analysis</head><p>In <ref type="table">Table 5</ref>, we show the results of how the accuracy changes as the expression tree size becomes larger. We can observe that as the expression tree size becomes larger, our model's performance becomes lower. This shows that although our model can handle various equations in a unified manner, it still has limitations at predicting long equations since longer equations often match with more complex MWPs which are more difficult to solve. Thus, our model still has room for improvement in reasoning, inference, and semantic understanding. Besides, compared with performances on Math23K which has only a few examples with complex templates, our model achieves significant improvement on the subset of HMWP with expression tree size 13+. This shows that constructing datasets with abundant complex examples can improve the model's ability to handle complex problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Case Study</head><p>Further, we conduct a case analysis and provide four cases in <ref type="table" target="#tab_8">Table 4</ref>, which shows the effectiveness of our approach. Our analyses are summarized as follows. From Case 1, Seq2Seq generates a spurious number n 2 not in problem text while both SAU-Solver w/o SSAR and SAU-Solver predict correctly owning to the problem-specific target vocabulary. Besides, although both SAU-Solver w/o SSAR and SAU-Solver can generate correct an equation, the equation generated by our SAU-Solver is more semantically-aligned with a human than the equation generated by SAU-Solver. From Case 2, we can see that Seq2Seq generates an invalid expression containing consecutive operators while our models can guarantee the validity of expressions since they generate expression trees directly. From Case 3, we find it interesting that tree-based models can avoid generating redundant operations, such as "n 1 *". From Case 4, we can see that SAU-Solver can prevent generating the similar subtree as its left sibling when the parent node is "*".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose an SAU-Solver, which is able to solve multiple types of MWPs, to generate the universal express tree explicitly in a semantically-aligned manner. Besides, we also propose a subtree-level semantically-aligned regularization to improve subtree semantic representation. Finally, we introduce a new MWPs datasets, called HMWP, to validate our solver's universality and push the research boundary of MWPs to math real-world applications better. Experimental results show the superiority of our approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Universal Expression Trees (UET). In our UET representation, multiple expression trees underlying a MWP will be integrated as an universal expression tree (UET) via symbol extension. UET can enable a solver to handle multiple types of MWPs in an unified manner like a single expression tree of an equation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell>4 Hybrid Math Word Problem Dataset</cell></row><row><cell>Most public datasets for automatic MWPs solv-</cell></row><row><cell>ing either are quite small such as Alg514 (Kush-</cell></row><row><cell>man et al., 2014), DRAW-1K (Upadhyay and</cell></row><row><cell>Chang, 2017), MaWPS (Koncel-Kedziorski et al.,</cell></row><row><cell>2016) or exist some incorrect labels such as Dol-</cell></row><row><cell>phin18K (Huang et al., 2016). An exception is</cell></row><row><cell>the Math23K dataset which contains 23161 prob-</cell></row><row><cell>lems labeled well with structured equations and</cell></row><row><cell>answers. However, it only contains one-unknown</cell></row><row><cell>linear MWPs, which is not sufficient to validate</cell></row><row><cell>the ability of a math solver about solving multi-</cell></row><row><cell>ple types of MWPs. Therefore, we introduce a</cell></row><row><cell>new high-quality MWPs dataset, called HMWP, in</cell></row><row><cell>which each sample is extracted from a Chinese K12</cell></row><row><cell>math word problem bank, to validate the univer-</cell></row><row><cell>sality of math word problem solvers and push the</cell></row><row><cell>research boundary of MWPs to match real-world</cell></row><row><cell>scenes better. Our dataset contains three types of</cell></row><row><cell>MWPs: arithmetic word problems, equations set</cell></row><row><cell>problems, and non-linear equation problems. There</cell></row><row><cell>are 5491 MWPs, including 2955 one-unknown-</cell></row><row><cell>variable linear MWPs, 1636 two-unknown-variable</cell></row><row><cell>linear MWPs, and 900 one-unknown-variable non-</cell></row><row><cell>linear MWPs.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Model comparison on answer accuracy via 5fold cross-validation. "-" means either the code is not released or the model is not suitable on those datasets.</figDesc><table><row><cell>Implementation Details. We use PyTorch 3 to im-</cell></row><row><cell>plement our model on Linux with NVIDIA RTX</cell></row><row><cell>2080Ti. All the words with less than five occur-</cell></row><row><cell>rences are converted into a special token UNK. We</cell></row><row><cell>3 http://pytorch.org</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>The data statistics and performance on different subset of HMWP.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Typical cases. Note that the results are represented as infix traversal of expression trees which is more readable than prefix traversal.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>.</figDesc><table><row><cell>Expression</cell><cell></cell><cell>Math23K</cell><cell></cell><cell>HMWP</cell><cell></cell></row><row><cell>Tree Sizes</cell><cell cols="5">Correct Error Acc(%) Correct Error Acc(%)</cell></row><row><cell>3-</cell><cell>729</cell><cell>168 81.27%</cell><cell>0</cell><cell>0</cell><cell>0%</cell></row><row><cell>5</cell><cell cols="2">1872 435 81.14%</cell><cell>3</cell><cell cols="2">1 75.00%</cell></row><row><cell>7</cell><cell>620</cell><cell cols="2">291 68.06% 32</cell><cell cols="2">25 56.14%</cell></row><row><cell>9</cell><cell>147</cell><cell cols="2">143 50.69% 159</cell><cell cols="2">69 69.74%</cell></row><row><cell>11</cell><cell>66</cell><cell cols="2">74 47.14% 102</cell><cell cols="2">111 47.89%</cell></row><row><cell>13+</cell><cell>20</cell><cell cols="2">66 23.26% 197</cell><cell cols="2">395 33.28%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank all anonymous reviewers for their constructive comments. This work was supported in part by National <ref type="figure">Key</ref>  </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An unknown number of rabbits and chickens were locked in a cage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>??? ?? ? ?? ? ?? ?</surname></persName>
		</author>
		<idno>Case 1: ???? ? ?? ? NUM(n 0 [20]) ?? ? ?? ? NUM</idno>
		<imprint/>
	</monogr>
	<note>counting from the top, there were NUM(n 0 [20]) heads, counting from the bottom, there were NUM(n 1 [50]) feet. How many chickens were locked in this cage? ) Seq2Seq</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sau-Solver W/O Ssar</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n 0 +x+4.0*x=n 1 ; (correct) SAU-Solver: 2.0*x+4.0*(n 0 -x)=n 1 ; (correct</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? ?? ?? ? A ? B Num ; ? ?? ?? ? ?? ? Num ; ?? ??? ? Num ; ?? ? ?? ??</forename><surname>?? ? ? ?? Num ; ? ? ? ? A ? B ?? ?? ? ?? ? ? ? ( Num</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n 4 [5. n 0 [1]) boat sailing between NUM(n 1 [2]) docks, it takes NUM(n 2 [5]) hours to sail from A to B downstream, while NUM(n 3 [7]) hours sailing upstream. Knowing the velocity of the water flow is 5 km/h, what is the distance between A and B? ) Seq2Seq: x/(n 2 +n 1 )+n 1 =x-/n 2 ; (error) SAU-Solver w/o SSAR: x/n 2 -n 4 =x/n 3 +n 4 ; (correct) SAU-Solver: x/n 2 -n 4 =x/n 3 +n 4 ; (correct</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>?? ? ?? ? Num ; ? ? ?? ?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? ? Num ; ?? ??? ? ??? ? ? Num ; ??</forename><surname>??</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">??</forename><surname>?? Num ; ? ? ?? ?? ? ? ? Num ; ??</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?? ?? ?? ?? ??? ?? ? ? ???? ?? ??? ? ?? ?? ?</forename><surname>?? ?</surname></persName>
		</author>
		<idno>Case 3: ?? NUM(n 0 [1</idno>
		<imprint/>
	</monogr>
	<note>n 4 [15</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">1]) hours, there were several students sorting books, later, NUM(n 4 [15]) more students joined them, and they finished the job in another NUM(n 5 [2]) hours together. If each student is as efficient as the others, how many students were working at the beginning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>?? ? ?</surname></persName>
		</author>
		<idno>4: ? ?? ?? ?? ?? NUM(n 0</idno>
		<imprint/>
	</monogr>
	<note>Given NUM(n 0 [1]) stack of books, NUM(n 1 [1]) student can sort them in NUM(n 2 [60]) hours. In the first NUM(N 3 [. Seq2Seq: n 1 *(x/n 2 )+n 5 *(x+n 4 )/n 2 =1.0; (error) SAU-Solver w/o SSAR: x/n 2 +n 5 *(x+n 4 )/n 2 =1.0; (correct) SAU-Solver: x/n 2 +n 5 *(x+n 4 )/n 2 =1.0; (correct</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? ?? ?? ?? ?? ? ?? ?? ?</forename><surname>Num</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>?? ? ? ?? ? ?? ? ?? ? Num</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>?? Num</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A farm owner plans to build a rectangle sheepfold, with NUM(n 1 [1]) side against the wall. The wall is 25 meters long, and he used NUM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? ? ?? ? Num ; M ? ?? ?? ( ?? ?? ? ?? ? ? ? ? ? ?? ?? ? ?? ) ??? ? ?? ?? ? ?? ? Num ; ? ? ??? ? ? ?</forename><surname>??</surname></persName>
		</author>
		<idno type="arXiv">arXiv:math/0701393</idno>
	</analytic>
	<monogr>
		<title level="m">If the area of the sheepfold is NUM</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>) References Yefim Bakman. Robust understanding of word problems with extraneous information. Computing Research Repository</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semantically-aligned equation generation for solving and reasoning math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2656" to="2668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural math word problem solver with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="213" to="223" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning fine-grained expressions to solve math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="805" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">How well do computers solve math word problems? large-scale dataset construction and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="887" to="896" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on learning representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mawps: A math word problem repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aida</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1152" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Parsing algebraic word problems into equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncelkedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siena</forename><surname>Dumas Ang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="585" to="597" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to automatically solve algebra word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="271" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Program induction by rationale generation: Learning to solve and explain algebraic word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="158" to="167" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to use formulas to solve simple arithmetic problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2144" to="2153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mapping to declarative knowledge for word problem solving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="159" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatically solving number word problems by semantic parsing and reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuehui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1132" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Annotating derivations: A new evaluation strategy and dataset for algebra word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingwei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="494" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Translating a math word problem to a expression tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1064" to="1069" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mathdqn: Solving arithmetic word problems via deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianli</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5545" to="5552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Template-based math word problem solvers with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Jipeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianli</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><forename type="middle">Tian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Third AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7144" to="7151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep neural solver for math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="845" to="854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A goal-driven tree-structured neural model for math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5299" to="5305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Frame-based calculus of solving arithmetic multi-step addition and subtraction word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ma</forename><surname>Yuhui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cui</forename><surname>Guangzuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename><surname>Ronghuai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Workshop on Education Technology and Computer Science</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="476" to="479" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learn to solve algebra word problems using quadratic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lipu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaixiang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="817" to="822" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
