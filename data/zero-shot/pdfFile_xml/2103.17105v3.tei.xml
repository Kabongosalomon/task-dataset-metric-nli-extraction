<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The GIST and RIST of Iterative Self-Training for Semi-Supervised Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eu</forename><surname>Wern Teh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Guelph</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Vector Institute</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Modiface, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Guelph</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Duke</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Modiface, Inc</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruowei</forename><surname>Jiang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Modiface, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parham</forename><surname>Aarabi</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Modiface, Inc</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Guelph</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Vector Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The GIST and RIST of Iterative Self-Training for Semi-Supervised Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>semi-supervised learning</term>
					<term>semantic segmentation</term>
					<term>self-training</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the task of semi-supervised semantic segmentation, where we aim to produce pixel-wise semantic object masks given only a small number of human-labeled training examples. We focus on iterative self-training methods in which we explore the behavior of self-training over multiple refinement stages. We show that iterative self-training leads to performance degradation if done na?vely with a fixed ratio of human-labeled to pseudo-labeled training examples. We propose Greedy Iterative Self-Training (GIST) and Random Iterative Self-Training (RIST) strategies that alternate between training on either human-labeled data or pseudo-labeled data at each refinement stage, resulting in a performance boost rather than degradation. We further show that GIST and RIST can be combined with existing semi-supervised learning methods to boost performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Semantic segmentation is the task of producing pixel-wise semantic labels over a given image. This is an important problem that has many useful applications such as medical imaging, robotics, scene-understanding, and autonomous driving. Supervised semantic segmentation models are effective, but they require tremendous amounts of pixel-wise labels, typically provided by a time consuming human annotation process. To overcome the need of collecting more pixel-wise labeled data, there has been an increase in interest in semi-supervised semantic segmentation in recent years <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr">[6]</ref>, <ref type="bibr" target="#b5">[7]</ref>, <ref type="bibr" target="#b6">[8]</ref>.</p><p>Self-training is a classic semi-supervised learning method that uses pseudo-labels to guide its learning process. We define pseudo-labels as predictions generated by a given model in contrast to human-provided annotations. Selftraining means using a model's own predictions as pseudolabels in its loss during training. Recently, there has been a resurgence of self-training methods in semi-supervised learning <ref type="bibr" target="#b7">[9]</ref>, <ref type="bibr" target="#b8">[10]</ref>, <ref type="bibr" target="#b9">[11]</ref>, <ref type="bibr" target="#b10">[12]</ref>, <ref type="bibr" target="#b11">[13]</ref>. Despite the recent comeback of self-training methods, most recent self-training works are confined to only one refinement stage.</p><p>Iterative self-training consists of multiple refinement stages, each consisting of K training iterations. At the beginning of each stage the model is initialized with weights from the previous stage, and pseudo-labels are regenerated (see Sec. III-B). We aim to investigate the behaviour of selftraining after many (i.e. &gt; 3) refinement stages for semisupervised semantic segmentation.  The performance of iterative self-training with various ratios of human-labels to pseudo-labels ? on the PASCAL VOC 2012 validation datasets. Models are refined iteratively by bootstrapping on weights trained on a previous refinement stage, with only 2% of humanlabels. A development set is used to select the best refinement stage. <ref type="bibr" target="#b1">2</ref> Iterative refinement on a small number of human-labels 1 may cause over-fitting on the training set, as no new information is introduced. Iterative refinement on pseudo-labels does introduce new information which can improve performance. However, it also results in a feedback loop that repeatedly reinforces and compounds incorrect predictions from previous iterations, ultimately resulting in "pseudo-label bloat", where a single dominant class prediction spreads to cover an entire image eventually (see <ref type="figure" target="#fig_2">Figure 2</ref>).</p><p>The na?ve solution of combining both human-labels and pseudo-labels in each batch slows the rate at which pseudolabel bloat occurs but does not combat it entirely. Instead, we find that alternating training on only human-labels or only pseudo-labels results in a more controlled training dynamic where pseudo-labels help expand predictions to regions that may have been missed, while human-labels prevent pseudo-labels from drifting too far away from the expected annotations. By switching between these two extremes in a greedy fashion (GIST) or random fashion (RIST), our model enjoys the benefits of both label types, ultimately yielding better performance (see <ref type="figure" target="#fig_1">Figure 1</ref>). Our contributions are the following:</p><p>? We show that na?ve application of iterative self-training to the problem of semi-supervised segmentation via a fixed human-labels to pseudo-labels ratio results in significant performance degradation when ? &lt; 1. ? We introduce Greedy Iterative Self-Training <ref type="bibr">(GIST)</ref> and Random Iterative Self-Training (RIST) to overcome performance degradation through iteratively training on either human-or pseudo-labels. <ref type="bibr">?</ref> We demonstrate that both RIST and GIST can improve existing semi-supervised learning methods, yielding performance boost in both the PASCAL VOC 2012 and City-scapes datasets across all eight subsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Semi-supervised semantic segmentation has gained ground in recent years. Souly et al. <ref type="bibr" target="#b0">[1]</ref> extend a typical Generative Adversarial Network (GAN) network by designing a discriminator that accepts images as input and produces pixel-wise predictions, consisting of confidence maps for each class and a fake label. Hung et al. <ref type="bibr" target="#b1">[2]</ref> improve GAN-based semantic segmentation by using a segmentation network as a conditional generator. They also redesign the discriminator to accept the segmentation mask as input and restrict it to produce pixel-wise binary predictions. Mittal et al. <ref type="bibr" target="#b2">[3]</ref> extend Hung et al.'s network by making the discriminator produce an image-level binary prediction. They also add a feature matching loss and self-training loss in their training pipeline. They further propose a separate semi-supervised classification network <ref type="bibr" target="#b12">[14]</ref> to clean up segmentation masks' prediction.</p><p>Recently, a few works propose non-GAN based solutions for semi-supervised semantic segmentation. Mendel et al. <ref type="bibr" target="#b3">[4]</ref> propose an error-correcting network that aims to fix the predictions of the main segmentation network. This error-correcting network is also applied to unlabeled images to correct the generated mask. French et al. <ref type="bibr" target="#b4">[5]</ref> adapt CutMix <ref type="bibr" target="#b13">[15]</ref> to augment and regularize a segmentation network by creating composite images via mixing two different images and their corresponding segmentation masks. Similarly, Olsson et al.</p><p>[6] also aim to regularize the segmentation network by mixing segmentation masks from two images by selecting half of the classes from one image and the other half from another image. Ouali et al. <ref type="bibr" target="#b5">[7]</ref> minimize consistency loss between features of multiple auxiliary decoders, in which the perturbed encoder outputs are fed. Alonso et al. <ref type="bibr" target="#b6">[8]</ref> improve the supervision signals from their teacher model by storing more samples in a separate memory banks.</p><p>We explore the iterative self-training method to tackle semi-supervised semantic segmentation. Self-training or pseudo-labeling is a classic semi-supervised learning recipe that can be traced back to 1996, where it was used in an NLP application <ref type="bibr" target="#b14">[16]</ref>. A major benefit of self-training is that it allows easy extension from an existing supervised model without discarding any information. In the deep learning literature, Lee et al. <ref type="bibr" target="#b7">[9]</ref> popularized self-training in semi-supervised classification. After its reappearance, it has gained traction in recent years. Zhai et al. <ref type="bibr" target="#b9">[11]</ref> show the effectiveness of mixing self-supervised and semi-supervised learning along with pseudo-labeling in their training regime. Zoph et al. <ref type="bibr" target="#b10">[12]</ref> show that a randomly initialized model with self-training via a joint-loss can yield better performance than a model initialized with a pre-trained model without self-training. Xie et al. <ref type="bibr" target="#b11">[13]</ref> show that iterative selftraining with noisy labels improves a classification model's accuracy and robustness. Radosavovic et al. <ref type="bibr" target="#b8">[10]</ref> use selftraining in Omni-supervised learning, where they generate pseudo-labels by taking the average prediction of multiple perturbations of a single unlabeled image. Most of the recent self-training works <ref type="bibr" target="#b7">[9]</ref>, <ref type="bibr" target="#b8">[10]</ref>, <ref type="bibr" target="#b9">[11]</ref>, <ref type="bibr" target="#b10">[12]</ref> are confined to only one refinement stage, with the exception of Xie et al.'s work <ref type="bibr" target="#b11">[13]</ref>, where they benefit from iterative self-training by repeating self-training for three stages of refinement. In this work, we aim to extend self-training for semi-supervised semantic segmentation and investigate self-training behavior under many (i.e. &gt; 3) refinement stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODS</head><p>Self-training is a semi-supervised learning method that uses pseudo-labels to guide its learning process. As we improve the model, we also improve the quality of pseudolabels. Self-training typically consists of the following steps:</p><p>(1) training a model using human-labeled data; (2) generating pseudo-labels using the trained model; and (3) finetuning the trained model with the combination of human-labeled data and pseudo-labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Combining both human-labels and pseudo-labels in training</head><p>Given a set of images {(x i , y i )}, where x i represents the image and y i represents the corresponding human-label, we begin training a segmentation model, SEG for K iterations using a 2D Cross-Entropy Loss, ENT on available humanlabeled data. The loss for iteration j is computed as:</p><formula xml:id="formula_0">L j = 1 B B i ENT(o i , y i ),<label>(1)</label></formula><formula xml:id="formula_1">o i = SEG(x i ).</formula><p>where B represents the batch size, i indexes examples in a batch, and o i represents a model's output. After a model is trained on available human-labeled data, we can now use this pre-trained model to generate pseudo-labels on unlabeled data. Given another set of images {(x p i , y p i )}, where x p i represents the unlabeled image and y p i represents the corresponding pseudo-label, we can now combine human-labels and pseudo-labels by a linear combination of respective losses computed by Eq. 1.</p><formula xml:id="formula_2">L ? j = 1 B B i (ENT(o i , y i ) * ?+ENT(o p i , y p i ) * (1??)). (2)</formula><p>We define ? as the ratio of human-labels to pseudo-labels. Eq. 1 is a small modification to the classic self-training loss by Lee et al. <ref type="bibr" target="#b7">[9]</ref>, who apply a coefficient only to the unlabeled loss term, where that coefficient is iterationdependent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Iterative Self-training</head><p>Self-training can be repeated through multiple refinement stages, where each refinement stage consists of a pseudolabel generation step and a finetuning step. A na?ve solution is to fix ? throughout all refinement stages, which we call Fixed Iterative Self-Training (FIST). Given that ? can take a floating-point number between zero and one, there are infinitely many possible ? values at each stage of refinement. By making ? binary, we turn the problem into discrete path selection. In this setting, our goal is to find the sequence of stages which yields the best solution. We explore two different selection strategies: Greedy Iterative Self-Training (GIST) and Random Iterative Self-Training (RIST).</p><p>We define S as the maximum number of refinement stages and K as the maximum number of training iterations at a given stage. The number of possible paths in the search space (2 S ) is exponential in the number of refinement stages.</p><formula xml:id="formula_3">Algorithm 1 GIST 1: ? list ? ? [?0]; ? list ? ? [0, 1] 2: for s ? ? 1 . . . S do 3: ? * list ? ? [ ]; R list ? ? [ ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>for ?c in ? list do 5:</p><p>for ? in ? list do 6:</p><formula xml:id="formula_4">for m ? ? 1 . . . M do 7: y p ? ? arg max(SEG(x p m ))</formula><p>8:</p><p>end for 9:</p><p>for j ? ? 1 . . . K do 10: </p><formula xml:id="formula_5">L ? j ? ? 1 B B i (ENT (oi, yi) * ? + ENT o p i , y p i * (1 ? ?)) 11: ?c ? ? ?c ? ? ?L ? j</formula><formula xml:id="formula_6">Algorithm 2 RIST 1: for s ? ? 1 . . . S do 2: ? ? ? (Uniform(0, 1) &gt; 0.5) 3: ?s ? ? ?s?1 4: for m ? ? 1 . . . M do 5: y p ? ? arg max(SEG(x p m )) 6: end for 7: for j ? ? 1 . . . K do 8: L ? j ? ? 1 B B i (ENT (oi, yi) * ? + ENT o p i , y p i * (1 ? ?))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>?s ? ? ?s ? ? ?L ? j ??s 10: end for 11: end for GIST works by finding the best stage of refinement by relying on the development set. Algorithm 1 describes the GIST algorithm, which is a beam search strategy. In line 3, we keep a list of candidates ?, and the corresponding evaluation results, R list , using the EVAL function on the development set. These lists are updated in lines 13 and 14.</p><p>In line 17, we sort ? * list in descending order based on R list . In line 18, we keep the top G ? in the ? * list . and assign it to ? list , where it will be used as the initial model for the next stage of refinement. In Line 6 to 8, we generate our pseudolabels for M unlabeled images. In Line 9 to 12, we finetune a segmentation model for K iterations with learning rate ?. <ref type="figure" target="#fig_3">Figure 3</ref> shows a hypothetical ? path selection scenario using GIST.</p><p>One weakness of GIST is its smaller search space when compared to a random search (RIST). Bergstra et al. <ref type="bibr" target="#b15">[17]</ref> show that in low dimensions, random search is an effective search strategy compared to a grid search. One can see that a greedy search is a subset of a grid search where we only explore the top performing branches. With a beam size of 1, the search space for a greedy search is log(S), and the search space for a random search is 2 S . At first glance, RIST may seem counter-intuitive, but RIST's performance spread is relatively small (?0.93), and this spread can be reduced by eliminating obvious degenerate solutions and increasing search paths (see Sec. IV-A for details). Algorithm 2 describes the RIST algorithm. In line 2, we randomly set ? to either zero or one. Line 7 to 10 is similar to the GIST algorithm (line 9 to line 12).</p><p>In GIST, the time complexity of a beam search is O(S * G). A beam search is only as efficient as a random search if we can parallelize the training for each ? at each stage, and this condition requires a beam search to be trained on consecutive numbers of GPU resources. Unlike a beam search, each random search can be trained independently on a single GPU resource. It is easier and cheaper to obtain N independent GPUs rather than N consecutive GPU resources, making RIST computationally cheaper and faster in practical settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Add-ons</head><p>In addition to cross entropy loss in Eq 2, we include three additional add-ons to boost the performance of FIST, GIST and RIST: Consistency Loss (CL), Label Erase (LE), and Temperature Scaling (TS). <ref type="table">Table V</ref> shows the ablation study of the add-ons for both RIST and GIST for models trained on the Pascal VOC 1/50 subset.</p><p>Consistency Loss (CL): Consistency Loss (CL) is a common loss in semi-supervised learning where the goal is to minimize features between two perturbations of the same input. CL is a common loss in semi-supervised learning <ref type="bibr" target="#b2">[3]</ref>, [6]. Mittal et al. <ref type="bibr" target="#b2">[3]</ref> use CL via a feature matching loss to minimize the discrepancy between predicted features and ground truth features. Olsson et al. <ref type="bibr" target="#b2">[3]</ref> use CL via meanteacher <ref type="bibr" target="#b12">[14]</ref> method. We also use the mean-teacher method as our CL loss. We use the following equations (Equations 3 to 7) to calculate CL for both x i and x p i .</p><formula xml:id="formula_7">? * = ? * ? + ? * (1 ? ?)) (3) o i , f i = SEG(x i ) (4) o * i , f * i = SEG * (x i ) (5) f i = drop(pool(f i )) (6) f * i = drop(pool(f * i )) (7) L feature = |f i ? f * i | (8)</formula><p>Following <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b1">[2]</ref>, we use DeepLabV2 <ref type="bibr" target="#b16">[18]</ref> as our segmentation model; therefore, f i represents features before an Atrous Spatial Pyramid Pooling (ASPP) layer. Equation 3 describes the weight update rule for the teacher model, SEG * , where an exponential moving average rule is applied to its weights, ? * , which is controlled by ?, (0 ? ? ? 1). Equations 4 and 5 illustrate the extraction of features before the ASPP layer in both the student model, SEG, and the teacher model, SEG*. In Equations 6 and 7, we apply global average pooling followed by a dropout perturbation to these features. After that, we compute the absolute differences between these two features. <ref type="figure">Figure 4</ref> summarizes the feature consistent loss in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Student</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet Model ASPP Teacher ResNet Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Average</head><p>Pooling Dropout <ref type="figure">Figure 4</ref>. Depiction of the Consistency Loss (CL). At each batch of training, there are two copies of Resnet Models. The student model is the main model that is used to predict the segmentation mask, and the teacher model is a copy of the student model with an exponential moving weight update. The goal of CL is to minimize the differences between the features of the student and teacher models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Absolute Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Entropy Loss</head><p>Label Erase (LE): Pseudo-labels are often very noisy, especially when we have a small amount of human-labeled data. To remove noise from pseudo-labels, we only keep predictions that pass a certain confidence threshold, ?. Label Erase (LE) is also used in <ref type="bibr" target="#b2">[3]</ref>, [6], <ref type="bibr" target="#b8">[10]</ref>, <ref type="bibr" target="#b10">[12]</ref>. <ref type="bibr">Equation 9</ref> illustrates the process of flagging low confidence prediction regions so that they are ignored by the loss function in Equation 1. We define pixel-wise confidence, c p i,j as the pixel softmax output of o p i,j , where i represents the image index and j represents the pixel index.</p><formula xml:id="formula_8">y p i,j = arg max(c p i,j ), if max(c p i,j ) &gt;= ?, ignore label, if max(c p i,j ) &lt; ?,<label>(9)</label></formula><p>Temperature Scaling (TS): Temperature scaling (TS) is introduced by Hinton et al. <ref type="bibr" target="#b17">[19]</ref>, where it is used to create a softer probability distribution for knowledge distillation.</p><p>The formula for TS is defined as q i = exp(yi * T ) j exp(yj * T ) . As T becomes smaller, the output of the softmax function will tend towards uniform distribution. We employ TS <ref type="bibr" target="#b17">[19]</ref> to overcome over-confident prediction in our model, where the activation values after softmax are highly skewed towards 100% (see <ref type="figure" target="#fig_4">Figure 5</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We perform experiments on two semantic segmentation datasets: PASCAL VOC 2012 <ref type="bibr" target="#b18">[20]</ref> and Cityscapes <ref type="bibr" target="#b19">[21]</ref>. In each dataset, there are three subsets, where pre-defined ratios (1/50, 1/20, and 1/8) of training images are selected as images with human-labels. We also experiment on two additional subsets (1/30 and 1/4) for the Cityscapes dataset. These subsets of labeled images are selected using the same split as <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr">[6]</ref>. We treat the remaining images as unlabeled examples. We use the mean intersection-over-union (mIoU) as a performance metric. The validation images for both datasets are set aside and used for evaluation, which is consistent with <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr">[6]</ref>. We select 50 additional images from the training set as a modest "development set" for meta-parameter tuning.</p><p>For all of our experiments, we follow the same experimental setup as <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr">[6]</ref>. We use a DeepLabV2 <ref type="bibr" target="#b16">[18]</ref> segmentation model that is initialized with MS-COCO pretrained weights <ref type="bibr" target="#b20">[22]</ref>  <ref type="bibr" target="#b4">5</ref> . We also freeze all the BatchNorm Layers in DeepLabV2. We optimize our model using the Stochastic Gradient Descent (SGD) optimizer with a base learning rate of 2.5e-4, a momentum of 0.9, and a weight decay of 5e-4. We use a polynomial learning rate decay policy, where we adjust the learning rate with the following equation: ? iter = ? 0 (1 ? iter max iter ) 0.9 where ? 0 is a base learning rate. To augment the dataset, we use randomcropping (321 ? 321 for PASCAL VOC 2012 and 256 ? 512 for Cityscapes), horizontal-flipping (with a probability of 0.5), and random-resizing (with a range of 0.5 to 1.5) in all of our experiments.</p><p>For all subsets, we train our supervised models and stage-0 models for 25,000 iterations. Additionally, for the Pascal VOC 2012 dataset, we use a batch-size of 8, and we refine our model for 3,000 iterations at each refinement stage (Stage 1 to 9). For the Cityscapes dataset, we use a batchsize of 6, and we refine our models for 4,000 iterations at each refinement stage (Stage 1 to 9). We use a search cost of two for both GIST and RIST and use the development set to select the best path for all experiments. <ref type="table" target="#tab_1">Table I</ref> shows the results of our experiments as well as relevant results that others have reported <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr">[6]</ref>. We use the code provided by the respective authors for our experiments with S4GAN <ref type="bibr" target="#b2">[3]</ref> and ClassMix <ref type="bibr">[6]</ref>. For a fair comparison, we set the batch size for S4GAN and ClassMix to match with our experiments, and we also select the best iterations using our development set. We notice performance differences in S4GAN and ClassMix when compared to performances reported in the original papers. We speculate that the differences are caused by best iteration selection and batch sizes. For experiment with S4GAN+GIST/RIST and ClassMix+GIST/RIST, we first train the segmentation model with S4GAN and ClassMix algorithm (stage-0 models). After that, we further refine the segmentation model using the GIST/RIST algorithm by bootstrapping on the DeepLabV2 model trained with S4GAN/ClassMix. <ref type="figure" target="#fig_1">Figure 1</ref> shows that a na?ve application of iterative selftraining leads to significant performance degradation in both datasets. <ref type="figure" target="#fig_2">Figure 2</ref> shows the qualitative evidence of performance degradation in FIST at ? = 0.75. FIST suffers from over-confident pseudo-label predictions which spread to the surrounding pixel. Over multiple stages of refinement, the pseudo-labels expand and eventually engulf most of the image. We speculate that this may be why most of the recent self-training works are confined to one refinement stage. <ref type="figure" target="#fig_1">Figure 1</ref> also shows that both RIST and GIST overcome performance degradation. Additionally, both RIST and GIST generalize better than FIST in each successive refinement.   <ref type="figure" target="#fig_5">Figure 6</ref> demonstrates the importance of training on a single label type (i.e., human-label or pseudo-label) for an extended number of iterations. We find that randomly selecting the label type for each batch (batch-wise random) performs just as poorly as training with both label types in each batch (FIST). We speculate that the clean human-labels and the noisy pseudo-labels represent competing objectives, which are difficult for the model to satisfy simultaneously, resulting in it getting stuck at poor solutions. By applying stage-wise training, we allow the model to focus on a single objective at a time, potentially escaping sub-optimal solutions from a previous stage.  <ref type="table" target="#tab_1">Table II</ref> examines the stability of performance for RIST on the PASCAL VOC 2012 1/50 subset. We use the same subset for training and generate fifteen different permutations of binary ? values uniformly at random. The degenerate solutions occur in row 5 and 6, where we have more than four consecutive numbers of the same ? choice during training. If we were to perform a random search once, the standard deviation of mIoU is 0.93. If we were to remove the obvious degenerate solutions (row 5 and 6) using some heuristics, the standard deviation of mIoU is reduced to 0.52. Nevertheless, we can reduce this standard deviation further by selecting the best solution out of five different random solutions yielding a standard deviation of 0.23. <ref type="figure" target="#fig_6">Figure 7</ref> shows that as we increase the number of random solutions, the spread decreases.   <ref type="table" target="#tab_1">Table III</ref> explores the sensitivity of RIST and GIST's performance on the PASCAL VOC 2012 1/50 subset with respect to the size of the development set. In general, RIST and GIST are relatively stable. RIST performance remains the same for 50 to 500 sample sizes. GIST performance improves slightly as we increase the sample size from 50 to 500. On the Pascal VOC 2012 dataset, we show that our supervised+GIST and supervised+RIST trained with 211 human-labels (1/50 subset) outperform the supervised model that is trained with 529 human-labels (1/20 subset). This result shows that GIST and RIST improve the model, not just because they got more supervised signals from the development set (see <ref type="table" target="#tab_1">Table I</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Discussion</head><p>Table IV explores GIST at various beam sizes. GIST can find a better solution for the development set; however, since there is a mismatch between the distribution of the development set and the original validation set due to the small sample size, the best solution of the development set is not the best solution for the original validation set. This study shows that GIST has a higher chance to overfit the development set when compared to RIST. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We show that iterative self-training with a fixed humanlabels to pseudo-labels ratio (FIST) leads to performance degradation. This degradation can be overcome by alternating training on only human-labels or only pseudo-labels in a greedy (GIST) or random (RIST) fashion. A clear benefit of self-training is that it can easily extend existing architectures. We show that both GIST and RIST can further refine models trained with other semi-supervised techniques resulting in a performance boost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Supervised GIST RIST <ref type="figure">Figure 8</ref>. The qualitative results of our model train on 2% human-labels from the PASCAL VOC 2012 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image</head><p>Ground Truth Supervised GIST <ref type="figure">Figure 9</ref>. The qualitative results of our model trained on a 2% human-labels from the Cityscapes dataset.</p><p>[6] V. Olsson, W. Tranheden, J. Pinto, and L. Svensson, "Classmix: Segmentation-based data augmentation for semi-supervised learning," 2020.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The performance of iterative self-training with various ratios of human-labels to pseudo-labels ? on the PASCAL VOC 2012 validation datasets. Models are refined iteratively by bootstrapping on weights trained on a previous refinement stage, with only 2% of humanlabels. A development set is used to select the best refinement stage. 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Pseudo-label degradation when a model is trained iteratively with a fixed human-labels to pseudo-labels ratio (? = 0.75). The first column consists of input images. The second column consists of ground truth labels. The third to fifth columns show pseudo-labels generated after at refinement stage 1, 5 and 9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>A hypothetical ? path selection scenario with six refinement stages in self-training using the greedy approach (GIST). ? indicates the ratio of human-labels to pseudo-labels. The open nodes indicate that only human-labels (? = 1) are being used for training and the shaded nodes indicate only pseudo-labels (? = 0) are being used. The number of possible paths is exponential in the number of refinement stages (2 6 ). At each stage of refinement, we evaluate the mean intersection over union (mIOU) of a model using a development set. Here, the optimum value is found at stage four of the refinement process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>The before and after effects of applying temperature scaling to the output activation of a single image. A temperature scale of 0.2 is applied to the output activation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>1/50 subset stage-wise random (RIST) batch-wise random =0.75 (best FIST) Self-training performance at various stages between batch-wise and stage-wise random selection strategies on the PASCAL VOC 2012 validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>The effect of increasing stability of RIST when we increase the number of random solution. The best results of the random solutions are selected based on best development mIoU. The shaded area represent one standard deviation of uncertainty.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table I</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">VOC 2012 (?10k images)</cell><cell></cell><cell cols="3">Cityscapes (?3k images)</cell><cell></cell></row><row><cell># of labeled images</cell><cell>211</cell><cell>529</cell><cell>1,322</cell><cell>59</cell><cell>100</cell><cell>148</cell><cell>371</cell><cell>743</cell></row><row><cell>Subset</cell><cell>1/50</cell><cell>1/20</cell><cell>1/8</cell><cell>1/50</cell><cell>1/30</cell><cell>1/20</cell><cell>1/8</cell><cell>1/4</cell></row><row><cell>Supervised</cell><cell>54.15</cell><cell>62.94</cell><cell>67.44</cell><cell>49.68</cell><cell>53.96</cell><cell>54.71</cell><cell>59.90</cell><cell>62.21</cell></row><row><cell>Supervised + GIST</cell><cell>66.33</cell><cell>66.95</cell><cell>70.27</cell><cell>53.51</cell><cell>56.38</cell><cell>58.11</cell><cell>60.94</cell><cell>63.04</cell></row><row><cell>Supervised + RIST</cell><cell>66.71</cell><cell>68.28</cell><cell>69.90</cell><cell>53.33</cell><cell>56.28</cell><cell>57.81</cell><cell>61.38</cell><cell>63.92</cell></row><row><cell>S4GAN [3]</cell><cell>62.87</cell><cell>62.35</cell><cell>68.56</cell><cell>50.48</cell><cell>54.58</cell><cell>55.61</cell><cell>60.95</cell><cell>61.30</cell></row><row><cell>S4GAN [3] + GIST</cell><cell>67.21</cell><cell>68.50</cell><cell>70.61</cell><cell>52.36</cell><cell>57.18</cell><cell>57.40</cell><cell>61.27</cell><cell>64.24</cell></row><row><cell>S4GAN [3] + RIST</cell><cell>66.51</cell><cell>68.50</cell><cell>70.31</cell><cell>53.47</cell><cell>57.12</cell><cell>57.48</cell><cell>62.50</cell><cell>64.64</cell></row><row><cell>ClassMix [6]</cell><cell>63.63</cell><cell>66.74</cell><cell>66.14</cell><cell>52.14</cell><cell>57.02</cell><cell>58.77</cell><cell>61.56</cell><cell>63.90</cell></row><row><cell>ClassMix [6] + GIST</cell><cell>65.60</cell><cell>69.05</cell><cell>70.65</cell><cell>52.43</cell><cell>58.70</cell><cell>59.98</cell><cell>62.44</cell><cell>64.53</cell></row><row><cell>ClassMix [6] + RIST</cell><cell>66.30</cell><cell>69.40</cell><cell>70.76</cell><cell>53.05</cell><cell>58.55</cell><cell>59.54</cell><cell>62.57</cell><cell>65.14</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Table I</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">SEMANTIC SEGMENTATION RESULTS (MIOU) ON THE PASCAL VOC 2012 AND CITYSCAPES VALIDATION DATASETS.</cell><cell></cell></row><row><cell cols="4">boosts across all subsets in the Pascal VOC 2012 and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Cityscapes datasets. Figures 8 and 9 show RIST and GIST's</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">qualitative results that are trained with 2% of human-labels</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>in both datasets.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">shows that both RIST and GIST can improve other</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">semi-supervised segmentation techniques such as S4GAN</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">and ClassMix. We show that both RIST and GIST can</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">further refine S4GAN and ClassMix yielding performance</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table II RIST SEMANTIC SEGMENTATION RESULTS ON THE PASCAL VOC 2012 VALIDATION SET. GROUP EXPERIMENT RESULTS ARE SELECTED BASED ON THE BEST DEVELOPMENT MIOU. RANDOM SELECTION CHOICES ARE P (PSEUDO-LABEL ONLY) OR L (HUMAN-LABEL ONLY). THERE ARE A TOTAL OF NINE REFINEMENT STAGES ORDERED SEQUENTIALLY FROM LEFT TO RIGHT.</figDesc><table><row><cell>Selection</cell><cell cols="3">mIoU (devel) mIoU (val) Group mIoU (val)</cell></row><row><cell>PPLLPLPLL</cell><cell>54.35</cell><cell>66.14</cell><cell>67.03</cell></row><row><cell>LPPLLLPPL</cell><cell>55.05</cell><cell>66.71</cell><cell></cell></row><row><cell>PPLLPPPLP</cell><cell>54.21</cell><cell>66.05</cell><cell></cell></row><row><cell>LPLPPLLPL</cell><cell>55.12</cell><cell>67.03</cell><cell></cell></row><row><cell>LLLLLLPPP</cell><cell>54.43</cell><cell>63.75</cell><cell></cell></row><row><cell>PPPPPPLLP</cell><cell>50.43</cell><cell>64.37</cell><cell>66.63</cell></row><row><cell>PPLLLPLPL</cell><cell>54.00</cell><cell>66.02</cell><cell></cell></row><row><cell>LLPLPLLPL</cell><cell>55.19</cell><cell>66.24</cell><cell></cell></row><row><cell>LPLPLLPLP</cell><cell>55.29</cell><cell>66.63</cell><cell></cell></row><row><cell>LLLPLPPLP</cell><cell>54.88</cell><cell>65.17</cell><cell></cell></row><row><cell>PLPLPPPLP</cell><cell>54.64</cell><cell>66.40</cell><cell>66.62</cell></row><row><cell>LPPLPPLPP</cell><cell>54.46</cell><cell>66.83</cell><cell></cell></row><row><cell>LPPLLLLPP</cell><cell>54.67</cell><cell>65.61</cell><cell></cell></row><row><cell>LPPPLLLLP</cell><cell>54.82</cell><cell>66.62</cell><cell></cell></row><row><cell>LPPPLLPPL</cell><cell>54.64</cell><cell>66.61</cell><cell></cell></row><row><cell>Mean?1 std. dev.</cell><cell></cell><cell>66.01?0.93</cell><cell>66.76?0.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>RESULTS ON THE PASCAL VOC 2012 VALIDATION SET FOR GIST ON DIFFERENT BEAM SIZE. SELECTION CHOICES AT EACH STAGE ARE P (PSEUDO-LABEL ONLY) OR L (HUMAN-LABEL ONLY).</figDesc><table><row><cell cols="2">Sample Size</cell><cell cols="2">mIoU (RIST)</cell><cell>mIoU (GIST)</cell></row><row><cell>10</cell><cell></cell><cell>66.04</cell><cell></cell><cell>66.23</cell></row><row><cell>25</cell><cell></cell><cell>65.63</cell><cell></cell><cell>62.76</cell></row><row><cell>50</cell><cell></cell><cell>66.71</cell><cell></cell><cell>66.33</cell></row><row><cell>100</cell><cell></cell><cell>66.71</cell><cell></cell><cell>66.67</cell></row><row><cell>200</cell><cell></cell><cell>66.71</cell><cell></cell><cell>66.69</cell></row><row><cell>500</cell><cell></cell><cell>66.71</cell><cell></cell><cell>67.00</cell></row><row><cell></cell><cell></cell><cell>Table III</cell><cell></cell><cell></cell></row><row><cell cols="5">SEMANTIC SEGMENTATION RESULTS ON THE PASCAL VOC 2012</cell></row><row><cell cols="5">VALIDATION SET FOR RIST AND GIST BASED ON BEST EPOCH</cell></row><row><cell cols="5">SELECTED WHILE VARYING THE NUMBER OF EXAMPLES IN THE</cell></row><row><cell></cell><cell></cell><cell cols="2">DEVELOPMENT SET.</cell><cell></cell></row><row><cell>Beam</cell><cell>Search</cell><cell>mIoU</cell><cell>mIoU</cell><cell>Solution</cell></row><row><cell>Size</cell><cell>Cost</cell><cell>(devel)</cell><cell>(val)</cell><cell></cell></row><row><cell>1</cell><cell>2</cell><cell>55.17</cell><cell>66.33</cell><cell>LPLLPLLPL</cell></row><row><cell>2</cell><cell>4</cell><cell>55.46</cell><cell>64.95</cell><cell>LLPLLPLLP</cell></row><row><cell>3</cell><cell>6</cell><cell>55.46</cell><cell>64.95</cell><cell>LLPLLPLLP</cell></row><row><cell></cell><cell></cell><cell>Table IV</cell><cell></cell><cell></cell></row><row><cell cols="3">SEMANTIC SEGMENTATION</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>ABLATION STUDY OF THE ADD-ONS FOR BOTH RIST AND GIST FOR MODELS TRAINED ON THE PASCAL VOC 1/50 SUBSET. RESULTS ARE REPORTED IN MIOU ON THE VALIDATION SET.</figDesc><table><row><cell>Method</cell><cell>RIST</cell><cell>GIST</cell></row><row><cell>no add-on</cell><cell>60.80</cell><cell>58.23</cell></row><row><cell>+CL</cell><cell>61.72</cell><cell>62.37</cell></row><row><cell>+CL +LE</cell><cell>64.69</cell><cell>64.56</cell></row><row><cell>+CL +LE +TS</cell><cell>66.71</cell><cell>66.33</cell></row><row><cell></cell><cell>Table V</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For convenience, we refer to human-labeled training examples as "human-labels" and pseudo-labeled training examples as "pseudo-labels".</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">A development set is the set of additional images used for metaparameter selection (Sec 4 paragraph 1).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">DeepLabV2 backbone is used in our experiments so that our method is comparable to S4GAN and ClassMix.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semi supervised semantic segmentation using generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Souly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5688" to="5696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adversarial learning for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Liou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with high-and low-level consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semisupervised segmentation based on error-correcting supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>De Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paulo</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Semi-supervised semantic segmentation needs strong, high-dimensional perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Finlayson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01916</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with cross-consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ouali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with pixellevel contrastive learning from a class-wise memory bank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sabater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Montesano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8219" to="8228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semisupervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Data distillation: Towards omni-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4119" to="4128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">S4l: Selfsupervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1476" to="1485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Rethinking pre-training and self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06882</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semisupervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised word sense disambiguation rivaling supervised methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd annual meeting of the association for computational linguistics</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Random search for hyperparameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
