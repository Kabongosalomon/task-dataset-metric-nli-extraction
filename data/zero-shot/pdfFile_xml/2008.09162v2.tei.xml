<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-scale Interaction for Real-time LiDAR Data Segmentation on an Embedded Platform</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xieyuanli</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrill</forename><surname>Stachniss</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Juergen</forename><surname>Gall</surname></persName>
						</author>
						<title level="a" type="main">Multi-scale Interaction for Real-time LiDAR Data Segmentation on an Embedded Platform</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED NOVEMBER, 2021 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real-time semantic segmentation of LiDAR data is crucial for autonomously driving vehicles and robots, which are usually equipped with an embedded platform and have limited computational resources. Approaches that operate directly on the point cloud use complex spatial aggregation operations, which are very expensive and difficult to deploy on embedded platforms. As an alternative, projection-based methods are more efficient and can run on embedded hardware. However, current projectionbased methods either have a low accuracy or require millions of parameters. In this paper, we therefore propose a projectionbased method, called Multi-scale Interaction Network (MINet), which is very efficient and accurate. The network uses multiple paths with different scales and balances the computational resources between the scales. Additional dense interactions between the scales avoid redundant computations and make the network highly efficient. The proposed network outperforms point-based, image-based, and projection-based methods in terms of accuracy, number of parameters, and runtime. Moreover, the network processes more than 24 scans, captured by a high-resolution LiDAR sensor with 64 beams, per second on an embedded platform, which is higher than the framerate of the sensor. The network is therefore suitable for robotics applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Environment perception and understanding are key to realize self-driving vehicles and robots. For the full-view perception of the environment, autonomously driving vehicles are usually equipped with multi-sensor systems, among which light detection and ranging (LiDAR) sensors play a key role due to their precise distance measurements. The large point clouds that are generated by the LiDAR sensors, however, need to be interpreted in order to understand the environment.</p><p>Although convolution neural networks (CNNs) perform well for semantic image segmentation <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, they cannot be applied directly to 3D point clouds. This is because standard convolutions require a regular grid structure, whereas a raw point cloud is an unordered structure. To address this problem, some methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> directly process point clouds using some spatial aggregation operations like grouping and gathering. Although these methods work well in indoor scenarios, it is difficult to apply them to large outdoor scenarios since the computational cost of the aggregation operation increases with the number of points. Another issue is that these methods are inefficient on embedded platforms since they use operations Manuscript received: June 9, 2021; Revised October 6, 2021; Accepted November 18, 2021. This paper was recommended for publication by Editor Markus Vincze upon evaluation of the Associate Editor and Reviewers' comments.</p><p>S. Li, X. Chen, C. Stachniss, and J. Gall are with the University of Bonn, Germany. D. Dai is with MPI for Informatics, Germany. Y. Liu is with ETH Zurich, Switzerland. This work was supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy -EXC 2070 -390732324 and GA1927/5-2 (FOR 2535 Anticipating Human Behavior).</p><p>that cannot be efficiently mapped on embedded hardware like Jetson AGX using TensorRT. However, runtime efficiency is of vital importance for real-world applications, especially for autonomously driving vehicles and robots.</p><p>Wu et al. <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> thus proposed to represent point clouds produced by a LiDAR sensor as an ordered projection map, such that CNNs can then be applied. However, projected LiDAR data and RGB images are different modalities and applying directly 2D image-based methods does not yield a high segmentation accuracy. For this reason, some specific CNNs have been designed for LiDAR-based depth images, named as projection-based methods. Recent projection-based methods like <ref type="bibr" target="#b9">[10]</ref>, however, are very large with more than 50M parameters, making them not suitable for embedded platforms.</p><p>In this work, we therefore propose a lightweight projectionbased model for semantic segmentation of LiDAR data that runs in real-time on an embedded platform. To this end, we revisit common multi-scale approaches like U-Net <ref type="bibr" target="#b10">[11]</ref> that have one path for each scale, i.e., each scale is processed independently and then fused at the end of the network. These networks, however, use the same operations for each path, which makes them either too expensive for embedded platforms or the accuracy is very low depending how complex the used operations are. In order to achieve a good balance between effectiveness and efficiency, we therefore adapt the computational operations for each path. While the top path extracts low-level clues, which can be easily detected with shallow layers operating on high-resolution feature maps, the bottom path extracts high-level semantic information, which requires more complex operations but on low-resolution feature maps. In order to avoid redundant computations across the paths, we furthermore propose a dense top-to-bottom interaction strategy where feature maps from a path are passed to all lower paths. We term the network, which is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, Multi-scale Interaction Network (MINet).</p><p>In addition, we show that the accuracy can be increased if additional supervision is added. While this is consistent with <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, we demonstrate that it is important to use the right type of supervision for the right part of the network. In fact, we use semantic supervision only for the two top paths but not for the bottom path and edge supervision for the fusion of the multiple paths. The latter is important to obtain accurate segment boundaries after upsampling the paths with lower resolution. Finally, we process the multi-modal data consisting of 3D coordinates, remission, and depth information first independently and then fuse them in the feature space. This is in contrast to previous works for LiDAR data that just concatenate the modalities and therefore ignore that the characteristics of each modality are different.</p><p>In summary, our contributions include:  ? We propose a multi-scale approach where the computational operations are balanced across the different scales and a top-to-bottom interaction strategy avoids redundant computations. ? We exploit different types of additional supervision to improve the accuracy without increasing the inference time. ? Different from previous methods, we process each modality independently and fuse them in the feature space, which improves the overall segmentation performance. ? By incorporating the above design decisions, we propose a lightweight projection-based model for semantic segmentation of LiDAR data that runs in real-time on an embedded platform. The experimental results demonstrate that our method reduces the number of parameters by about 98% and is about 4? faster than the state-of-the-art projection-based method <ref type="bibr" target="#b9">[10]</ref>, while achieving a higher accuracy. We also evaluate our model on an embedded platform and demonstrate that our method can be deployed for autonomous driving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Point-based Semantic Segmentation</head><p>Although CNNs <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> are successful for 2D image-based semantic segmentation, they cannot handle unstructured data like point clouds. To address this problem, tangent convolutions <ref type="bibr" target="#b14">[15]</ref> project local points to a tangent plane and vanilla convolutions are then applied to it. Point-Net <ref type="bibr" target="#b4">[5]</ref> is the first method that directly processes the point cloud. It applies a convolution operation for each point and uses a permutation invariant operation to aggregate information. However, PointNet does not take local information into consideration, which is realized by PointNet++ <ref type="bibr" target="#b5">[6]</ref>. SPGraph <ref type="bibr" target="#b15">[16]</ref> tackles semantic segmentation of large-scale point clouds by defining a super point graph (SPG). Because point-based methods are inefficient for large point clouds, RandLA <ref type="bibr" target="#b16">[17]</ref> addresses this problem by adopting random sampling and designing a better grouping strategy to maintain a better performance. P 2 Net [18] applies point-based methods on projected LiDAR data. However, these methods are too expensive for many applications, especially for embedded platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Projection-based Semantic Segmentation</head><p>Projection-based segmentation methods project LiDAR point clouds onto 2D multi-modal images and use 2D CNNs for semantic segmentation. SqueezeSeg <ref type="bibr" target="#b7">[8]</ref> and Squeeze-SegV2 <ref type="bibr" target="#b8">[9]</ref> use a lightweight network called SqueezeNet <ref type="bibr" target="#b18">[19]</ref> for semantic segmentation and a CRF for post-processing. Based on SqueezeSeg, RangeNet++ <ref type="bibr" target="#b9">[10]</ref> adopts Darknet <ref type="bibr" target="#b19">[20]</ref> and replaces the CRF with a k-NN for post-processing. It has also been successfully used to improve LiDAR-based odometry <ref type="bibr" target="#b20">[21]</ref> and loop closure detection <ref type="bibr" target="#b21">[22]</ref>. Current projectionbased methods, however, do not achieve the same segmentation accuracy as point-based methods and the best performing approaches use very large networks. In this paper, we propose a novel lightweight model that can run in real-time on an embedded platform while achieving state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MULTI-SCALE INTERACTION NETWORK</head><p>The proposed Multi-scale Interaction Network (MINet) operates on projection maps generated from LiDAR point clouds. To associate a LiDAR point a = (x, y, z) to a pixel (u, v) in the projection map of size (h, w), we compute yaw (1) and pitch <ref type="bibr" target="#b1">(2)</ref> and map it to pixel coordinates by translation and scaling <ref type="bibr" target="#b9">[10]</ref>:  <ref type="figure" target="#fig_1">Fig. 2</ref>. Each operation has a kernel size k, stride s, and c output channels, repeated t times. The three sections of MIM denote the three paths.</p><formula xml:id="formula_0">u = 1 2 [1 ? arctan2(y, x)? ?1 ]w,<label>(1)</label></formula><formula xml:id="formula_1">v = [1 ? (arcsin(zd ?1 ) + o up )o ?1 ]h.<label>(2)</label></formula><p>The vertical field-of-view of the LiDAR sensor is o = o up + o down , where o up and o down represent the above and below horizon of the field-of-view, respectively. d = ||a|| denotes the depth of a point. After this transformation, we obtain a projection map of size (h, w, 5) where the 5 channels correspond to the coordinates (x, y, z), the depth, and the remission of the corresponding 3D point. The remission value indicates the proportion of the light that is diffusely reflected. It provides therefore information of the surface, which is helpful for distinguishing different classes. While depth can be computed from the coordinates, the network operations do not compute the depth explicitly. Adding depth in addition to the coordinates thus improves the accuracy as we show in our experiments. Each channel is normalized by the mean and standard deviation computed over the training and validation set. The architecture of MINet is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. The projection map is first processed by the Mini Fusion Module (MFM) (Sec. III-A) to fuse the multi-modal information in the feature space. In the Multi-scale Interaction Module (MIM) (Sec. III-B), the data is processed at three different scales where the resolution is reduced by factor two for each path. As it is shown in Tab. I, the computation differs for each path where we use two basic components, namely MobileBlock and BasicBlock. The MobileBlock <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> utilizes depthwise convolutions and has thus fewer parameters, but its learning capacity is also limited. The BasicBlock <ref type="bibr" target="#b24">[25]</ref> is stronger, but also more expensive. MobileBlock and BasicBlock are shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. We therefore balance the computational resources across the three paths as it is shown in Tab. I. While we use the expensive BasicBlock for the bottom path with lowest resolution, we decrease the computational cost as the resolution increases using five MobileBlocks for the middle path and three for the top path. The connections from each path to lower paths avoid redundant computations at lower paths and make the network more efficient. Finally, the 2D predictions for the original resolution are produced by the Up Fusion Module (UFM) (Sec. III-C), which are then mapped back to the 3D space. In the remainder of this section, we describe each module of MINet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Mini Fusion Module (MFM)</head><p>Different from an RGB image, the projection map contains channels of different modalities. Previous projectionbased segmentation methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> treat such different modalities equally, but we show that processing each channel independently using MFM is more efficient. Specifically, each channel of the multi-modal image is mapped to an independent feature space using five convolution blocks, including normalization and activation. This corresponds to the first row of Tab. I. This step can be considered as a feature calibration step for each modality before fusing them. It needs to be noted that we also treat the x, y, and z coordinates separately. After the first five convolutional layers, these features are concatenated and fed into several MobileBlocks for fusing them. Since a small resolution leads to less computation, the information of the feature maps are gradually aggregated by average pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-scale Interaction Module (MIM)</head><p>After the fusion module, the data is processed by three paths where each path corresponds to a different scale as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. From top to bottom, the resolution of the feature maps is decreased by factor two using average pooling and the receptive field is accordingly increased. For the top path, we use the highest resolution. Since processing high resolution feature maps is very expensive, we use only three MobileBlocks as shown in Tab. I. The bottom path, which has the largest receptive field and lowest resolution, can offer more abstract semantic clues if we use more expensive operations. Hence, it uses three BasicBlocks. The middle path is a compromise between the top and bottom path and consists of five  A second important design choice is to allow interactions among the paths. Since the computational complexity of the paths increases for lower paths, we use a dense top-to-bottom fusion design for efficient multi-scale feature interaction. Especially, feature maps of the first and second path will be resized by average pooling and passed to all lower paths. To avoid a mismatch of the number of channels, the number of channels is increased gradually for each path and kept the same at each interaction position. Hence, no other operations are used to adjust the number of channels as shown in Tab. I. Due to the interaction, the lower paths benefit from the features computed from higher paths. The lower paths can therefore focus on information that has not been extracted by higher paths due to limited computational resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Up Fusion Module (UFM)</head><p>To obtain the semantic labels of each pixel in the projection map, UFM shown in <ref type="figure" target="#fig_2">Fig. 3</ref> combines the features from different scales and upsamples them to the input resolution. In addition, features after the first MobileBlock of the Mini Fusion Module are used to recover detailed spatial information as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. The lower part of <ref type="figure" target="#fig_2">Fig. 3</ref> shows the feature maps of the three different paths that are first resized to the same size, concatenated together, and fused by a 1?1 convolution. The fused feature maps are then upsampled to the original resolution and processed by a convolution block including a 3?3 convolution, batch normalization, and ReLU activation. The upper part shows the feature maps from the Mini Fusion Module that have already the original resolution. They are processed by a MobileBlock and a convolution S S E(FL) 49.4 * "S" denotes semantic supervision. "E" denotes edge supervision.</p><p>"(FL)" indicates that the focal loss is used for edge supervision. block. Finally, the processed features from both modules are added together. Although the spatial information of the original feature maps already helps to sharpen segment boundaries, which can be fuzzy due to the upsampling, adding additional supervision for the segment boundaries emphasizes this effect as we will explain in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Booster Training Strategy</head><p>Adding supervision to intermediate parts of a network <ref type="bibr" target="#b25">[26]</ref> has been shown to be useful for network optimization <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. In this work, we also use intermediate supervision, however, we propose two different types of supervision. Similar to balancing the computational resources across scales, it is very important to use the right supervision for the right part of the network.</p><p>We use the standard weighted cross-entropy loss as semantic supervision</p><formula xml:id="formula_2">L s = ? 1 |I| i?I N n=1 w n p n i log(p n i ),<label>(3)</label></formula><p>where N is the number of classes, |I| is the total number of image pixels, p n i is the ground-truth semantic label for pixel i and class n (p n i ? {0, 1}), andp n i is the predicted class probability. The weight w n for class n is inversely proportional to its occurrence frequency as in <ref type="bibr" target="#b9">[10]</ref>.</p><p>Besides at the end of the network, we use the weighted cross-entropy loss L s for the top and middle path as indicated by the dashed arrows in <ref type="figure" target="#fig_0">Fig. 1</ref>. As we will show in the experiments, this intermediate supervision improves the training and boosts the accuracy. Adding this semantic supervision to the bottom path, however, does not help since the resolution of the lower path is too low and downsampling of the ground-truth introduces too many artifacts.</p><p>As discussed in Sec. III-C, obtaining accurate segment boundaries after upsampling is an issue. Inspired by <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, we extract the semantic boundaries from the ground-  truth labels and compare it to the semantic boundaries after upsampling. The semantic edge loss is then obtained by</p><formula xml:id="formula_3">L e = ? 1 |I| i?I (e i log(? i ) + (1 ? e i ) log(1 ?? i )) ,<label>(4)</label></formula><p>where e i is the ground-truth edge label at pixel i (e i ? {0, 1}) and? i is the predicted edge probability at pixel i. Besides of the weighted cross entropy loss, which we denote by L fs and which is computed in the same way as L s , we use the Lov?sz-Softmax loss L ls <ref type="bibr" target="#b28">[29]</ref> at the end of the network, which maximizes the intersection-over-union (IoU) score:</p><formula xml:id="formula_4">L ls = 1 N N n=1 ? Jn (m(n)),<label>(5)</label></formula><formula xml:id="formula_5">m i (n) = 1 ?p n i if p n i = 1 p n i otherwise,<label>(6)</label></formula><p>where ? Jn defines the Lov?sz extension of the Jaccard index, p n i ? [0, 1] and p n i ? {0, 1} denote for class n at pixel i the predicted probability and ground-truth label, respectively. In summary, the combined loss is given by</p><formula xml:id="formula_6">L = L fs + L ls + L e + ? s L s<label>(7)</label></formula><p>where ? = 0.1 and L s are the loss functions for the top and middle path. The arrows in <ref type="figure" target="#fig_0">Fig. 1</ref> show where each type of supervision is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS A. Experimental Settings</head><p>We use two challenging datasets to evaluate our method, namely SemanticKITTI <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> and SemanticPOSS <ref type="bibr" target="#b31">[32]</ref>.  <ref type="figure">Fig. 5</ref>. Visualization of Tab. VI. We omit point-based methods that need more than 140ms per scan. The proposed MINet is not only the most accurate method, it also achieves the best trade-off between accuracy and runtime.</p><p>Based on the KITTI Odometry Benchmark <ref type="bibr" target="#b32">[33]</ref>, Se-manticKITTI provides a semantic label for each point in all scans. It includes over 43,000 scans from 21 sequences, among which sequences 00 to 10 with over 21,000 scans are available for training and the remaining scans from sequences 11 to 21 are used for testing. Sequence 08 is used as the validation set, and we train our approach on the remaining training set. We report the results on the validation set for the ablation study. For the test set, which we use to compare to the state-of-theart, the ground-truth is withheld and the results are evaluated by an on-line server. SemanticPOSS is a smaller dataset with 2988 LiDAR scans, captured at the Peking University. The point clouds of Seman-ticPOSS are more sparse compared to SemanticKITTI due to the lower resolution of the LiDAR sensor. SemanticPOSS is split into six subsets equally, among which we use the 3 rd subset for validation and the others for training. We report the results on the validation set. Since these two datasets differ in size, LiDAR sensor, and environment, they provide an ideal testbed for evaluating the proposed approach. As for the evaluation metric, we calculate the standard mean intersection over union (mIoU) <ref type="bibr" target="#b39">[40]</ref> over all classes:</p><formula xml:id="formula_7">mIoU = 1 N N n=1 TP n TP n + FP n + FN n<label>(8)</label></formula><p>where TP n , FP n , and FN n denote the numbers of true positive, false positive, and false negative predictions for class n, respectively. N is the number of classes.   B. Ablation Study 1) Modules: As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, our network consists of three modules. In Tab. II we evaluate some design choices for the Mini Fusion Module (MFM) (Sec. III-A), the Multi-scale Interaction Module (MIM) (Sec. III-B), and the Up Fusion Module (UFM) (Sec. III-C). While the proposed approach achieves 51.8% mIoU (Row 4), we evaluate the impact of processing each modality separately before fusing them in the features space in the first row. To keep the number of parameters the same, we use 3?3 convolutions to process the input multi-modal image instead of processing each modality separately in MFM. In this case, the accuracy drops by 0.9% (Row 1). This shows the benefit of processing each modality separately at the beginning. In the last row, we also report the result when we omit the depth from the input. In this case, the accuracy drops to 2.2%. In <ref type="figure" target="#fig_0">Fig. 1</ref>, we have connections between the three paths. If we remove the top-todown interactions, the accuracy is reduced by 1.1% (Row 2). This demonstrates the benefit of allowing interactions between the multi-scale features. If the multi-resolution features are just resized, concatenated, and processed by convolutional layers, instead of using UFM, the accuracy is reduced by 1.2% (Row 3).</p><p>2) Supervision Setting: As discussed in Sec. III-D, we use additional supervision for MIM and UFM. For MIM, we use the semantic labels (S) to add the loss (3) to the top and middle path. For UFM, we add the loss (4) for the segment boundaries (E). In Tab. III, we report the mIoU for different settings. The best setting is achieved by adding semantic supervision to the top and middle path in MIM and applying edge supervision to UFM (Row 1). Removing any of this additional supervision leads to an accuracy loss by more than 1.3% <ref type="figure" target="#fig_1">(Row 2-4</ref>). If the additional supervision is only used for UFM, the accuracy even decreases further (Row 5). If we do not use any additional supervision, the accuracy is lowest and 3.4% below the proposed setting (Row 6). While we removed so far additional supervision, the last two rows in the table show results when we add or change the type of supervision. If we add additional supervision to the bottom path, the accuracy decreases by 0.9%. This is due to the large difference between the ground-truth resolution and the resolution of the bottom path, which results in sampling artifacts that have a negative impact. Instead of using the edge loss (4) for UFM, we also replaced it by the semantic loss that is used for MIM. While adding the semantic loss (Row 8) is better than using no additional loss for UFM (Row 4), the edge loss (Row 1) achieves a 1% higher accuracy than the semantic loss. This is expected since the purpose of the edge loss is to improve the segment boundaries after the upscaling, which is done by the Up Fusion Module. We also investigated what happens if the focal loss <ref type="bibr" target="#b40">[41]</ref> is used for the edge loss instead of (4) (Row 9). In this case, the accuracy decreases.</p><p>3) Impact of ?: Our loss function <ref type="bibr" target="#b6">(7)</ref> contains only one hyper-parameter, namely ?. We evaluate the impact of ? in Tab. IV. The setting ? = 0 corresponds to row 5 of Tab. III where no additional supervision is added to the paths. Setting ? between 0.1 and 1.0 performs well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Path Settings:</head><p>As shown in Tab. I, we balance the computational resources across the three paths where we increase the complexity as the resolution decreases. In rows 1-10 of Tab. V, we report the results when we vary the number of blocks for the top, middle, and bottom path. The results show that increasing the parameters only for one path does not result in an improvement. For instance, using 9 instead of 3 MobileBlocks for the top path (Row 4) decreases the accuracy by 2.9%. This shows that a good computational balance between the paths is required. In rows 11-13, we report the results when we use the same operations for all paths, i.e., either 3 or 5 MobileBlocks or 3 BasicBlocks. Using only MobileBlocks reduces the number of parameters, but it improves the runtime only slightly and this is only the case for 3 MobileBlocks. This, however, comes at a substantially lower accuracy. In terms of runtime and accuracy, the proposed setting provides a much better trade-off. If the computational expensive BasicBlocks are used for all paths, the number of parameters and runtime nearly doubles while the accuracy is nearly the same. This shows that using the same operations for all resolutions is highly inefficient and that the proposed approach achieves a good balance between efficiency and effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with other Methods</head><p>We first compare the proposed approach (MINet) with other methods on the SemanticKITTI test set in terms of both accuracy and efficiency. For a fair comparison, all methods including image-based methods are trained from scratch. The results are shown in Tab. VI. In the first rows, we show the results for point-based methods. Most of the approaches are very slow and cannot process more than 2 scans per second since spatial aggregation operations are usually very timeconsuming for large point clouds. The very recent works <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b34">[35]</ref> are faster and process up to 16 scans per second, but it is difficult to deploy these networks on embedded systems due to their complex operations. The highest accuracy is achieved by <ref type="bibr" target="#b34">[35]</ref>, but the network is very large with over 16M parameters. Our proposed approach outperforms all pointbased methods in terms of runtime, number of parameters, and accuracy.</p><p>As for image-based methods, we use four widely used methods, namely PSPNet <ref type="bibr" target="#b1">[2]</ref>, DeepLabV3+ <ref type="bibr" target="#b35">[36]</ref>, DenseASPP <ref type="bibr" target="#b38">[39]</ref>, and the lightweight model BiseNet <ref type="bibr" target="#b37">[38]</ref>. We adjust the input channels so that these methods can be applied to the projection map. While these methods are faster than pointbased methods, they have by far more parameters and the accuracy is significantly lower. This shows that projected LiDAR data cannot be directly processed by image-based segmentation methods since the modality differs from RGB images.</p><p>We also compare our approach to other projection-based methods. While SqueezeSeg <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> has the same amount of parameters and depending on the setting runs slightly faster, the accuracy is very low. The state-of-the-art projection-based method RangeNet <ref type="bibr" target="#b9">[10]</ref> outperforms SqueezeSeg in terms of accuracy, but this is achieved by increasing the number of parameters to over 50M and decreasing the runtime. Our approach is much more efficient. It uses only 2% of the number of parameters compared to RangeNet53 and it is about 4? faster while achieving a higher accuracy. <ref type="figure">Fig. 5</ref> visualizes the accuracy and runtime of the methods of Tab. VI and shows the effectiveness and efficiency of the proposed approach.</p><p>We also evaluate our method on SemanticPOSS <ref type="bibr" target="#b31">[32]</ref> and compare our approach to other methods in Tab. VII. Since the dataset is smaller and the point clouds are more sparse compared to SemanticKitti, the mIoU is lower for all methods. However, our approach still outperforms other methods with a large margin. This proves the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Performance on an Embedded Platform</head><p>We finally compare our method with RangeNet on an embedded platform. Here, we use a Jetson AGX that is an AI module for embedded systems, as it is usually used for autonomous driving. We optimize our method and RangeNet using TensorRT. The results are summarized in Tab. VIII. Since the input resolution can be decreased to reduce the runtime, we report the results for three different input resolutions. We can see that at each resolution, the proposed MINet outperforms RangeNet53 <ref type="bibr" target="#b9">[10]</ref> with or without postprocessing. Furthermore, MINet is also much faster than RangeNet53. Specifically, MINet is about 4? faster than RangeNet53 without post-processing and 2? faster with postprocessing. Such high efficiency makes MINet quite suitable for robotics applications. Even with full resolution and postprocessing, MINet runs at real-time since the LiDAR scan frequency is 10Hz. Compared to Tab. VI where the runtime is measured on a workstation with a single Quadro P6000, the post-processing has a higher impact on the runtime for the embedded platform since the post-processing is not optimized by TensorRT. Moreover, the post-processing is applied to the point cloud, so it cannot benefit from reducing the resolution of the projection map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work, we proposed a novel lightweight projectionbased method, called Multi-scale Interaction Network (MINet), for semantic segmentation of LiDAR data. The network is highly efficient and runs in real-time on GPUs and embedded platforms. It outperforms point-based, imagebased, and projection-based methods in terms of accuracy, number of parameters, and runtime. This is achieved by using a multi-scale approach where the computational resources are balanced between the scales and by introducing interactions between the scales. By processing the modalities separately before fusing them and adding additional different types of supervision, we could further improve the accuracy without decreasing the runtime. Compared to the state-ofthe-art projection-based method RangeNet, MINet reduces the number of parameters by 98% and is 4? faster while achieving a higher accuracy. Since MINet processes more than 24 scans per second on an embedded platform, it can be used for autonomous vehicles and robots. Our method also achieves good performance on other tasks, like moving object segmentation <ref type="bibr" target="#b41">[42]</ref>. Projection-based methods, however, have some limitations. For instance, it is not straightforward to integrate temporal information from multiple views. Finally, we expect that the design principles of the network are also valuable for other tasks like 3D car detection. The source code is available at https://github.com/sj-li/MINet.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of the MINet architecture with three paths in the Multi-scale Interaction Module. The numbers 2 and 4 for interpolation (U) and average pooling (D) indicate the upsampling and downsampling factor. The dashed arrows indicate the supervision type. The detailed description of the architecture is given in Tab. I where the different blocks are illustrated in Fig. 2 and the Up Fusion Module is illustrated in Fig. 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of the MobileBlock (top) and the BasicBlock (bottom). DWConv means depth-wise convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of the Up Fusion Module (UFM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Qualitative results. RangeNet segments most points of the truck as a car, while MINet segments the truck correctly (red circle). In some cases, both approaches fail (yellow circle). Although MINet segments the object correctly, it misclassifies it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>wise Add Mini Fusion Module Edge Supervision Le</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Up Fusion Module</cell></row><row><cell>2</cell><cell>2/4</cell><cell>2/4</cell><cell>2</cell><cell></cell></row><row><cell>2</cell><cell>2/4</cell><cell>2/4</cell><cell>4</cell><cell></cell></row><row><cell></cell><cell cols="3">Multi-scale Interaction Module</cell><cell></cell></row><row><cell>MobileBlock</cell><cell>BasicBlock</cell><cell>U</cell><cell>Interpolation</cell><cell>AvgPooling</cell></row><row><cell>Semantic Supervision Ls</cell><cell></cell><cell></cell><cell>Element-</cell><cell>(Un)projection</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I INSTANTIATION</head><label>I</label><figDesc>OF THE PROPOSED MINET.</figDesc><table><row><cell>Module</cell><cell>Operation</cell><cell>k</cell><cell>c</cell><cell>s</cell><cell>t</cell><cell>Output size</cell></row><row><cell></cell><cell>Conv2d</cell><cell>3</cell><cell>4</cell><cell>1</cell><cell>5</cell><cell>64?2048</cell></row><row><cell></cell><cell>MobileBlock</cell><cell>3</cell><cell>20</cell><cell>1</cell><cell>1</cell><cell>64?2048</cell></row><row><cell></cell><cell>MobileBlock</cell><cell>3</cell><cell>24</cell><cell>2</cell><cell>1</cell><cell>32?512</cell></row><row><cell></cell><cell>MobileBlock</cell><cell>3</cell><cell>24</cell><cell>1</cell><cell>1</cell><cell>32?512</cell></row><row><cell></cell><cell>MobileBlock</cell><cell>5</cell><cell>40</cell><cell>2</cell><cell>1</cell><cell>16?256</cell></row><row><cell>MFM</cell><cell>MobileBlock MobileBlock</cell><cell>5 5</cell><cell>40 40</cell><cell>1 1</cell><cell>1 1</cell><cell>16?256 16?256</cell></row><row><cell></cell><cell>MobileBlock</cell><cell>3</cell><cell>80</cell><cell>1</cell><cell>1</cell><cell>16?256</cell></row><row><cell></cell><cell>MobileBlock</cell><cell>3</cell><cell>80</cell><cell>1</cell><cell>1</cell><cell>16?256</cell></row><row><cell></cell><cell>MobileBlock</cell><cell>3</cell><cell>80</cell><cell>1</cell><cell>1</cell><cell>16?256</cell></row><row><cell></cell><cell>MobileBlock</cell><cell>3</cell><cell>80</cell><cell>1</cell><cell>1</cell><cell>16?256</cell></row><row><cell></cell><cell>Conv2d</cell><cell>1</cell><cell>32</cell><cell>0</cell><cell>1</cell><cell>16?256</cell></row><row><cell></cell><cell>MobileBlock</cell><cell>3</cell><cell>64</cell><cell>1</cell><cell>1</cell><cell>16?256</cell></row><row><cell></cell><cell>MobileBlock</cell><cell>3</cell><cell>128</cell><cell>1</cell><cell>1</cell><cell>16?256</cell></row><row><cell></cell><cell>MobileBlock</cell><cell>3</cell><cell>128</cell><cell>1</cell><cell>1</cell><cell>16?256</cell></row><row><cell></cell><cell>MobileBlock</cell><cell>3</cell><cell>32</cell><cell>1</cell><cell>1</cell><cell>8?128</cell></row><row><cell></cell><cell>MobileBlock</cell><cell>3</cell><cell>64</cell><cell>1</cell><cell>1</cell><cell>8?128</cell></row><row><cell>MIM</cell><cell>MobileBlock</cell><cell>3</cell><cell>64</cell><cell>1</cell><cell>1</cell><cell>8?128</cell></row><row><cell></cell><cell>MobileBlock</cell><cell>3</cell><cell>128</cell><cell>1</cell><cell>1</cell><cell>8?128</cell></row><row><cell></cell><cell>MobileBlock</cell><cell>3</cell><cell>128</cell><cell>1</cell><cell>1</cell><cell>8?128</cell></row><row><cell></cell><cell>BasicBlock</cell><cell>3</cell><cell>64</cell><cell>1</cell><cell>1</cell><cell>4?64</cell></row><row><cell></cell><cell>BasicBlock</cell><cell>3</cell><cell>128</cell><cell>1</cell><cell>1</cell><cell>4?64</cell></row><row><cell></cell><cell>BasicBlock</cell><cell>3</cell><cell>128</cell><cell>1</cell><cell>1</cell><cell>4?64</cell></row><row><cell></cell><cell>Conv2d</cell><cell>3</cell><cell>32</cell><cell>1</cell><cell>1</cell><cell>16?512</cell></row><row><cell>UFM</cell><cell>Conv2d</cell><cell>3</cell><cell>32</cell><cell>1</cell><cell>1</cell><cell>64?2048</cell></row><row><cell></cell><cell>Conv2d</cell><cell>1</cell><cell>32</cell><cell>1</cell><cell>1</cell><cell>64?2048</cell></row></table><note>* Each module contains several components: Conv2d, MobileBlock, and BasicBlock. Conv2d denotes a convolutional layer followed by one batch normalization layer and ReLU activation. MobileBlock and BasicBlock are illustrated in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II IMPACT</head><label>II</label><figDesc>OF THE THREE MODULES.MobileBlocks. In our experiments, we show that increasing the computational operations as the resolution decreases leads to a higher efficiency compared to using the same blocks for all paths. While the number of parameters doubles compared to the proposed architecture if we use the BasicBlocks for all paths, the accuracy drops if only MobileBlocks are used.</figDesc><table><row><cell>No.</cell><cell>MFM</cell><cell>Interaction</cell><cell>UFM</cell><cell>mIoU</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell>50.9</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell>50.7</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell></cell><cell>50.6</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell><cell>51.8</cell></row><row><cell>5</cell><cell>w/o depth</cell><cell></cell><cell></cell><cell>49.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III IMPACT</head><label>III</label><figDesc>OF ADDITIONAL SUPERVISION.</figDesc><table><row><cell>No.</cell><cell>Top</cell><cell>MIM Middle</cell><cell>Bottom</cell><cell>UFM</cell><cell>mIoU</cell></row><row><cell>1</cell><cell>S</cell><cell>S</cell><cell></cell><cell>E</cell><cell>51.8</cell></row><row><cell>2</cell><cell></cell><cell>S</cell><cell></cell><cell>E</cell><cell>50.3</cell></row><row><cell>3</cell><cell>S</cell><cell></cell><cell></cell><cell>E</cell><cell>50.2</cell></row><row><cell>4</cell><cell>S</cell><cell>S</cell><cell></cell><cell></cell><cell>50.5</cell></row><row><cell>5</cell><cell></cell><cell></cell><cell></cell><cell>E</cell><cell>49.0</cell></row><row><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>48.4</cell></row><row><cell>7</cell><cell>S</cell><cell>S</cell><cell>S</cell><cell>E</cell><cell>50.9</cell></row><row><cell>8</cell><cell>S</cell><cell>S</cell><cell></cell><cell>S</cell><cell>50.8</cell></row><row><cell>9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV IMPACT</head><label>IV</label><figDesc>OF ? IN<ref type="bibr" target="#b6">(7)</ref>.</figDesc><table><row><cell>?</cell><cell>0</cell><cell>0.01</cell><cell>0.1</cell><cell>1.0</cell></row><row><cell>mIoU</cell><cell>49.0</cell><cell>49.3</cell><cell>51.8</cell><cell>51.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V ABLATION</head><label>V</label><figDesc>STUDY FOR DIFFERENT BLOCKS OF EACH PATH.</figDesc><table><row><cell>Top</cell><cell cols="5">Middle Bottom Param (M) GFLOP SPS mIoU</cell></row><row><cell cols="2">3?MB 5?MB 3?BB</cell><cell>1.0</cell><cell>6.20</cell><cell>59</cell><cell>51.8</cell></row><row><cell cols="2">5?MB 5?MB 3?BB</cell><cell>1.1</cell><cell>6.43</cell><cell>52</cell><cell>50.1</cell></row><row><cell cols="2">7?MB 5?MB 3?BB</cell><cell>1.1</cell><cell>6.62</cell><cell>51</cell><cell>49.7</cell></row><row><cell cols="2">9?MB 5?MB 3?BB</cell><cell>1.2</cell><cell>7.77</cell><cell>48</cell><cell>48.9</cell></row><row><cell cols="2">3?MB 3?MB 3?BB</cell><cell>1.0</cell><cell>6.20</cell><cell>60</cell><cell>50.8</cell></row><row><cell cols="2">3?MB 7?MB 3?BB</cell><cell>1.0</cell><cell>6.29</cell><cell>55</cell><cell>49.0</cell></row><row><cell cols="2">3?MB 9?MB 3?BB</cell><cell>1.2</cell><cell>6.57</cell><cell>51</cell><cell>50.2</cell></row><row><cell cols="2">3?MB 5?MB 5?BB</cell><cell>1.4</cell><cell>6.43</cell><cell>54</cell><cell>50.3</cell></row><row><cell cols="2">3?MB 5?MB 7?BB</cell><cell>1.8</cell><cell>6.62</cell><cell>52</cell><cell>51.5</cell></row><row><cell cols="2">3?MB 5?MB 9?BB</cell><cell>2.4</cell><cell>6.92</cell><cell>50</cell><cell>49.8</cell></row><row><cell cols="2">3?MB 3?MB 3?MB</cell><cell>0.6</cell><cell>6.00</cell><cell>61</cell><cell>49.7</cell></row><row><cell cols="2">5?MB 5?MB 5?MB</cell><cell>0.6</cell><cell>6.30</cell><cell>58</cell><cell>50.2</cell></row><row><cell cols="2">3?BB 3?BB 3?BB</cell><cell>2.0</cell><cell>11.04</cell><cell>33</cell><cell>51.2</cell></row></table><note>* "MB" denotes MobileBlock. "BB" denotes BasicBlock. "SPS" represents the number of processed scans per second.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI EVALUATION</head><label>VI</label><figDesc>RESULTS ON THE SEMANTICKITTI TEST SET FOR POINT-BASED, IMAGE-BASED, AND PROJECTION-BASED METHODS. .3 0.3 0.1 0.8 0.2 0.2 0.0 61.6 15.8 35.7 1.4 41.4 12.9 31.0 4.6 17.6 2.4 3.7 2 3.0 14.6 Pointnet++</figDesc><table><row><cell>Methods</cell><cell>Classes</cell><cell>car</cell><cell>bicycle</cell><cell>motorcycle</cell><cell>truck</cell><cell>other-vehicle</cell><cell>person</cell><cell>bicyclist</cell><cell>motorcyclist</cell><cell>road</cell><cell>parking</cell><cell>sidewalk</cell><cell>other-ground</cell><cell>building</cell><cell>fence</cell><cell>vegetation</cell><cell>trunk</cell><cell>terrain</cell><cell>pole</cell><cell>traffic-sign</cell><cell>SPS</cell><cell>Param (M)</cell><cell>mIoU</cell></row><row><cell>Pointnet [5]</cell><cell></cell><cell cols="2">46.3 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VII EVALUATION</head><label>VII</label><figDesc>RESULTS ON THE SEMANTICPOSS DATASET.</figDesc><table><row><cell></cell><cell></cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell cols="6">trunk plants traffic sign pole building fence</cell><cell>bike</cell><cell>road</cell><cell>mIoU</cell></row><row><cell cols="2">SqueezeSegV1 [8]</cell><cell>5.5</cell><cell>0.0</cell><cell>8.7</cell><cell>3.4</cell><cell>39.1</cell><cell>2.4</cell><cell>2.5</cell><cell>34.5</cell><cell>7.6</cell><cell>18.4</cell><cell>62.5</cell><cell>16.8</cell></row><row><cell cols="2">SqueezeSegV1 [8] + CRF</cell><cell>14.2</cell><cell>1.4</cell><cell>11.6</cell><cell>18.1</cell><cell>5.9</cell><cell>11.1</cell><cell>1.9</cell><cell>37.9</cell><cell>5.6</cell><cell>18.9</cell><cell>78.7</cell><cell>18.7</cell></row><row><cell cols="2">SqueezeSegV2 [9]</cell><cell>18.4</cell><cell>11.2</cell><cell>34.9</cell><cell>15.8</cell><cell>56.3</cell><cell>11.0</cell><cell>4.5</cell><cell>47.0</cell><cell>25.5</cell><cell>32.4</cell><cell>71.3</cell><cell>29.8</cell></row><row><cell cols="2">SqueezeSegV2 [9] + CRF</cell><cell>23.9</cell><cell>22.6</cell><cell>29.7</cell><cell>15.3</cell><cell>37.3</cell><cell>11.1</cell><cell>5.3</cell><cell>45.9</cell><cell>18.2</cell><cell>34.7</cell><cell>73.4</cell><cell>28.9</cell></row><row><cell cols="2">RangeNet53 [10]</cell><cell>10.0</cell><cell>6.2</cell><cell>33.4</cell><cell>7.3</cell><cell>54.2</cell><cell>5.5</cell><cell>2.6</cell><cell>49.9</cell><cell>18.4</cell><cell>28.6</cell><cell>63.5</cell><cell>25.4</cell></row><row><cell cols="2">RangeNet53 + k-NN</cell><cell>14.2</cell><cell>8.2</cell><cell>35.4</cell><cell>9.2</cell><cell>58.1</cell><cell>6.8</cell><cell>2.8</cell><cell>55.5</cell><cell>28.8</cell><cell>32.2</cell><cell>66.3</cell><cell>28.9</cell></row><row><cell>MINet</cell><cell></cell><cell>13.3</cell><cell>11.3</cell><cell>34.0</cell><cell>18.8</cell><cell>62.9</cell><cell>11.8</cell><cell>4.1</cell><cell>55.5</cell><cell>20.4</cell><cell>34.7</cell><cell>69.2</cell><cell>30.5</cell></row><row><cell cols="2">MINet + k-NN</cell><cell>20.1</cell><cell>15.1</cell><cell>36.0</cell><cell>23.4</cell><cell>67.4</cell><cell>15.5</cell><cell>5.1</cell><cell>61.6</cell><cell>28.2</cell><cell>40.2</cell><cell>72.9</cell><cell>35.1</cell></row><row><cell></cell><cell cols="2">TABLE VIII</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">PERFORMANCE ON AN EMBEDDED PLATFORM (JETSON AGX).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="5">Resolutions GFLOPs SPS mIoU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>64 ? 2048</cell><cell></cell><cell>360.5</cell><cell>7</cell><cell>49.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RangeNet53 [10]</cell><cell>64 ? 1024</cell><cell></cell><cell>180.3</cell><cell>11</cell><cell>45.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>64 ? 512</cell><cell></cell><cell>90.1</cell><cell>22</cell><cell>39.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>64 ? 2048</cell><cell></cell><cell>360.5</cell><cell>5</cell><cell>52.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RangeNet53 + k-NN [10]</cell><cell>64 ? 1024</cell><cell></cell><cell>180.3</cell><cell>8</cell><cell>48.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>64 ? 512</cell><cell></cell><cell>90.1</cell><cell>13</cell><cell>41.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>64 ? 2048</cell><cell></cell><cell>6.2</cell><cell>24</cell><cell>52.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MINet</cell><cell>64 ? 1024</cell><cell></cell><cell>3.2</cell><cell>47</cell><cell>49.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>64 ? 512</cell><cell></cell><cell>1.7</cell><cell>80</cell><cell>45.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>64 ? 2048</cell><cell></cell><cell>6.2</cell><cell>13</cell><cell>55.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MINet + k-NN</cell><cell>64 ? 1024</cell><cell></cell><cell>3.2</cell><cell>18</cell><cell>52.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>64 ? 512</cell><cell></cell><cell>1.7</cell><cell>21</cell><cell>48.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* The number of GFLOPs does not include the post-processing.</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Searching for MobileNetV3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ShuffleNet V2: Practical guidelines for efficient CNN architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">PointNet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">PointNet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">PointConv: Deep convolutional networks on 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SqueezeSeg: Convolutional neural nets with recurrent CRF for real-time road-object segmentation from 3D LiDAR point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICRA</title>
		<imprint>
			<biblScope unit="page" from="1887" to="1893" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SqueezeSegV2: Improved model structure and unsupervised domain adaptation for roadobject segmentation from a LiDAR point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICRA</title>
		<imprint>
			<biblScope unit="page" from="4376" to="4382" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">RangeNet++: Fast and accurate LiDAR semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ IROS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4213" to="4220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="7151" to="7160" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Richer convolutional features for edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1939" to="1946" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semantic edge detection with diverse deep supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02864</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="3887" to="3896" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="4558" to="4567" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">RandLA-Net: Efficient semantic segmentation of largescale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11236</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Projected-point-based segmentation: A new paradigm for LiDAR point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03928</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">YOLOv3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SuMa++: Efficient LiDAR-based semantic SLAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Palazzolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gigu?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ IROS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4530" to="4537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">OverlapNet: Loop closing for LiDAR-based SLAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>L?be</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>R?hling</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vysotska</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haag</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fraunhofer</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">MobileNets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MobileNetV2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="562" to="570" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="1857" to="1866" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">InstanceCut: from edges to instances with MultiCut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5008" to="5017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The lov?sz-softmax loss: A tractable surrogate for the optimization of the intersection-overunion measure in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Rannen</forename><surname>Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="4413" to="4421" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SemanticKITTI: A dataset for semantic scene understanding of LiDAR sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9297" to="9307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards 3d lidar-based semantic scene understanding of 3d point cloud sequences: The SemanticKITTI dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Semanticposs: A point cloud dataset with large quantity of dynamic instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09147</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SPLATNet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="2530" to="2539" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">PolarNet: An improved grid representation for online LiDAR point clouds semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR, 2020</title>
		<imprint>
			<biblScope unit="page" from="9601" to="9610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="801" to="818" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Pyramid attention network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10180</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">BiSeNet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="325" to="341" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">DenseASPP for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="3684" to="3692" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="2980" to="2988" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Moving Object Segmentation in 3D LiDAR Data: A Learning-based Approach Exploiting Sequential Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wiesmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<ptr target="http://www.ipb.uni-bonn.de/pdfs/chen2021ral-iros.pdf" />
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="6529" to="6536" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
