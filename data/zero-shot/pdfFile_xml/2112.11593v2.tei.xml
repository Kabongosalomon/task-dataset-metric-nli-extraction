<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AdaptPose: Cross-Dataset Adaptation for 3D Human Pose Estimation by Learnable Motion Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Gholami</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabab</forename><surname>Ward</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Jane</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AdaptPose: Cross-Dataset Adaptation for 3D Human Pose Estimation by Learnable Motion Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the problem of cross-dataset generalization of 3D human pose estimation models. Testing a pre-trained 3D pose estimator on a new dataset results in a major performance drop. Previous methods have mainly addressed this problem by improving the diversity of the training data. We argue that diversity alone is not sufficient and that the characteristics of the training data need to be adapted to those of the new dataset such as camera viewpoint, position, human actions, and body size. To this end, we propose AdaptPose, an end-to-end framework that generates synthetic 3D human motions from a source dataset and uses them to fine-tune a 3D pose estimator. AdaptPose follows an adversarial training scheme. From a source 3D pose the generator generates a sequence of 3D poses and a camera orientation that is used to project the generated poses to a novel view. Without any 3D labels or camera information AdaptPose successfully learns to create synthetic 3D poses from the target dataset while only being trained on 2D poses. In experiments on the Human3.6M, MPI-INF-3DHP, 3DPW, and Ski-Pose datasets our method outperforms previous work in cross-dataset evaluations by 14% and previous semi-supervised learning methods that use partial 3D annotations by 16%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Monocular 3D human pose estimation aims to reconstruct the 3D skeleton of the human body from 2D images. Due to pose and depth ambiguities, it is well known to be an inherently ill-posed problem. However, deep learning models are able to learn 2D to 3D correspondences and achieve impressively accurate results when trained and tested on similar datasets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b33">33]</ref>.</p><p>An often disregarded aspect is that the distribution of features in a dataset e.g. camera orientation and body poses differ from one dataset to another. Therefore, a pre-trained network underperforms when applied to images captured</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source Domain</head><p>Target Domain</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain?</head><p>?3d labels ?Camera intrinsics ?Camera extrinsics ?3d labels ?Camera intrinsics ?Camera extrinsics ?Sample videos</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthetic Motion</head><p>Motion Generator <ref type="figure">Figure 1</ref>. AdaptPose generates synthetic motions to improve the cross-dataset generalization. The source dataset has 3D labels and camera information, while the target dataset has only sample videos. The synthetic motions are generated to belong to the target dataset. Therefore fine-tuning the 3D pose estimator with synthetic motions improves the generalization of the model. from a different viewpoint or when they contain an activity that is not present in the training dataset <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b47">47]</ref>. As an example, <ref type="figure">Figure 1</ref> shows images from the Human3.6M <ref type="bibr" target="#b15">[16]</ref> dataset on the left and images from the Ski-Pose <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b36">36]</ref> dataset on the right which we define as source domain and target domain, respectively. Camera viewpoint, position, human action, speed of motion, and body size significantly differ between the source and target domain. This large domain gap causes 3D pose estimation models trained on the source domain to make unreliable predictions for the target domain <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b48">48]</ref>. We address this problem by generating synthetic 3D data that lies within the distribution of the target domain and fine-tuning the pose estimation network by the generated synthetic data. Our method does not require 3D labels or camera information from the target domain and is only trained on sample videos from the target domain.</p><p>To the best of our knowledge, there are only two approaches that generate synthetic 2D-3D human poses for cross-dataset generalization of 3D human pose estimators <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24]</ref>. Li et al. <ref type="bibr" target="#b23">[24]</ref> randomly generate new 2D-3D pairs of the source dataset by substituting parts of the human body in 3D space and projecting the new 3D pose to 2D. PoseAug <ref type="bibr" target="#b11">[12]</ref> proposes a differential data augmentation framework that is trained along with a pose estimator. Both, <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b11">[12]</ref>, merely improve the diversity of the source domain without considering the distribution of the target domain. Moreover, these methods are based on single images and do not consider temporal information.</p><p>We formulate the data augmentation process as a domain adaptation problem. <ref type="figure">Figure 2</ref> shows our training pipeline. Our goal is to generate plausible synthetic 2D-3D pairs that lie within the distribution of the target domain. Our framework, AdaptPose, introduces a human motion generator network that takes 3D samples from the source dataset and modifies them by a learned deformation to generate a sequence of new 3D samples. We project the generated 3D samples to 2D and feed them to a domain discriminator network. The domain discriminator is trained with real 2D samples from the target dataset and fake samples from the generator. We use the generated samples to fine-tune a pose estimation network. Therefore, our network adapts to any target using only images from the target dataset. 3D annotation from the target domain is not required. Unlike <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24]</ref>, this enables our network to generate plausible 3D poses from the target domain. Another contribution is the extension of the camera viewpoint generation from a deterministic approach to a probabilistic approach. We assume that the camera viewpoint of the target domain comes from a specific well-defined, but unknown distribution. Therefore, we propose to learn a distribution of camera viewpoints instead of learning to generate a deterministic rotation matrix. Our network rotates the generated 3D poses into a random camera coordinate system within the learned distribution. The generated sample is a sequence of 2D-3D pose pairs that entails plausibility in the temporal domain. We believe that the application of the proposed motion generator is not limited to improving only cross-dataset performance of 3D pose estimation, but it could also be used in other tasks such as human action recognition.</p><p>Contributions. 1) we propose to close the domain gap between the training and test datasets by a kinematics-aware domain discriminator. The domain discriminator is trained along with a human motion generator (HMG) that uses a source training dataset to generate human motions close to those in the target dataset. 2) We show that learning the distribution of the camera viewpoint is more effective than learning to generate a deterministic camera matrix. 3) To the best of our knowledge, this is the first approach that proposes generating human motions specifically for crossdataset generalization for 3D human pose estimation, unlike previous work that focuses on single-frame data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In the following, we discuss the related work with a focus on cross-dataset adaptation.</p><p>Weakly-supervised Learning.</p><p>Weakly supervised learning has been proposed to diminish the dependency of networks on 3D annotations. These methods rely on unpaired 3D annotation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b45">45]</ref>, multi-view images <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b41">41]</ref>, or cycle-consistency <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref>. Most related to our work is the adaptation of a network to the target domain via weakly supervised learning. Zhang et al. <ref type="bibr" target="#b48">[48]</ref> propose an online adaptation to target test data based on the weakly supervised learning method of <ref type="bibr" target="#b4">[5]</ref>. Yang et al. <ref type="bibr" target="#b45">[45]</ref> use unpaired 3D annotation to further fine-tune a network on in-the-wild images. Kundu et al. <ref type="bibr" target="#b22">[23]</ref> use a selfsupervised learning method to improve the generalization of a pre-trained network on images with occlusion.</p><p>Cross-dataset Generalization. Cross-dataset adaption of 3D pose estimators has recently gained attention. Guan et al. and Zhang et al. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b48">48]</ref> propose an online adaptation of the pose estimator during the inference stage over test data. Guan et al. <ref type="bibr" target="#b13">[14]</ref> use a temporal consistency loss and a 2D projection loss on the streaming test data to adapt the network to the target test dataset. Zhang et al. <ref type="bibr" target="#b48">[48]</ref> use a cycle consistency approach to optimize the network on every single test frame. Although the online-adaptation approach improves cross-dataset generalizability, it also increases the inference time, especially if the networks exploit temporal information. Wang et al. <ref type="bibr" target="#b43">[43]</ref> argues that estimating the camera viewpoint beside the 3D keypoints improves crossdataset generalization of the 3D pose estimator. However, the camera viewpoint is not the only criterion that differs between datasets. Split-and-Recombine <ref type="bibr" target="#b47">[47]</ref> proposes to split the human skeleton into different body parts so that different body parts of a rare pose from the target dataset could have been seen in the source dataset.</p><p>Data Augmentation is another way to diminish crossdataset errors. Previous methods perform data augmentation on images <ref type="bibr" target="#b35">[35]</ref>, 3D mesh models <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b49">49]</ref>, or 2D-3D pairs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24]</ref>. Most related to our work is augmenting 2D-3D pairs. Li et al. <ref type="bibr" target="#b23">[24]</ref> generate synthetic 3D human samples by substituting body parts from a source training set. The evolutionary process of <ref type="bibr" target="#b23">[24]</ref> is successful in generating new poses, however, the generation of natural camera viewpoints is overlooked. Instead, it randomly perturbs source camera poses. PoseAug <ref type="bibr" target="#b11">[12]</ref> proposes an end-to-end data augmentation framework that trains along with a pose estimator network. Although it improves the diversity of the training data, there is no guarantee that the generated samples are in the distribution of the target dataset. Moreover, according to the ablation studies of PoseAug, the main improvement comes from generating camera viewpoints instead of generating new poses. This means that PoseAug has limited abilities to effectively improve pose diversities  <ref type="figure">Figure 2</ref>. Overview of the proposed network. The input is a vector of 3D keypoints from the source dataset concatenated with Gaussian noise. The motion generator learns to generate a sequence of 3D keypoints X b 3D and the mean and standard deviation of a normal distribution N . A random rotation matrix is sampled from the learned normal distribution and X b 3D is transformed to X r 3D and projected to 2D. The domain discriminator is trained with X r 2D and 2D keypoints from the target domain. The lifting network is a pretrained pose estimator that estimates 3D from 2D. It is used to evaluate X r 2D , X r 3D , provide feedback to the motion generator, and to select a subset of samples for fine-tuning the lifting network. The pipeline is trained end-to-end.</p><p>in the training set. In contrast, we enforce the generated synthetic data to be in the distribution of the target data. Unlike PoseAug, we show that our motion generation network significantly improves cross-dataset results even without augmenting the camera-viewpoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Formulation</head><p>Let X src = (X src 2D , X src 3D ) be a pair of 2D and 3D poses from the source dataset and X tar = X tar 2D a 2D pose from the target dataset. The input to our model are sequences of frames with length n, X src</p><formula xml:id="formula_0">2D : [x 2D ] n t=0 , X src 3D : [x 3D ] n t=0 , and X tar 2D : [y 2D ] n t=0 where x 2D , y 2D ? R J?3 . AdaptPose consists of a generator function G(X src , z; ? G ) ? X fake ,<label>(1)</label></formula><p>with parameters ? G , that maps source samples X src and a noise vector z ? p z to a fake 2D-3D pair X fake = (X fake 2D , X fake 3D ). The fake samples (X fake</p><formula xml:id="formula_1">2D , X fake 3D ) are a se- quence of 2D-3D keypoints X fake 2D : [x fake 2D ] n t=0 , X fake 3D : [x fake 3D ] n t=0 .</formula><p>The generator G generates an adapted dataset X fake = G(X src , z) of any desired size. In order to adapt the source to the target domain in the absence of 3D target poses we introduce a domain discriminator D D and a 3D discriminator D 3D . The domain discriminator D D (x; ? D ) gives the likelihood d that the 2D input x is sampled from the target domain X tar 2D . The generator tries to generate fake samples X fake 2D as close as possible to target samples X tar 2D , while the discriminator tries to distinguish between them. Unlike a standard GAN network <ref type="bibr" target="#b12">[13]</ref> where generator is conditioned only on a noise vector, our generator is conditioned on both a noise vector and a sample from the source dataset which was shown to be effective in generating synthetic images <ref type="bibr" target="#b2">[3]</ref>. Additionally, the model is conditioned on a 3D discriminator D 3D (x; ? D ) that outputs the likelihood d that the generated 3D, X fake 3D , is sampled from the real 3D distribution. Ideally, we would like to condition on the target 3D dataset. Since 3D data from the target domain is not available we condition it on the source 3D dataset. However, conditioning the 3D discriminator D 3D directly on the source 3D poses restrains the motion generator to the source distribution. Instead, we condition the 3D discriminator D 3D on a perturbed version of data X psrc 3D = y + X src</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D</head><p>where y ? p y is a small noise vector. The noise vector y is selected such that X psrc 3D is a valid pose from the source distribution. The goal of AdaptPose is to optimize the following objective function</p><formula xml:id="formula_2">L = min ? G max (? D D ,? D 3D ) ?L(G, D D ) + ?L(G, D 3D ),<label>(2)</label></formula><p>where ? and ? are the weights of the losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Human Motion Generator</head><p>We name the generator of our GAN network Human Motion Generator (HMG). The HMG consists of two main components. 1) A bone generator that rotates the bone vectors and changes the bone length ratios. The bone generation operation produces new 3D keypoints X b 3D . 2) A camera generator that generates a new camera viewpoint {R, T}, where R ? R 3?3 is a rotation matrix and T is a translation vector. X b 3D is transformed to the generated camera viewpoint by</p><formula xml:id="formula_3">X fake 3D = RX b 3D + T,<label>(3)</label></formula><p>with the corresponding 2D keypoints</p><formula xml:id="formula_4">X fake 2D = ?(X fake 3D ),<label>(4)</label></formula><p>where ? is the perspective projection that uses the intrinsic parameters from the source dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Bone Generation</head><p>In this section, we analyze different methods of bone vector generation in the temporal domain. The main challenge is to keep the bone changes plausible for every single frame and temporally consistent in the time domain. We propose and analyze the three different methods BG1, BG2, and BG3 shown in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>BG1. The bone generation network accepts a sequence of 3D keypoints from the source dataset. The sequence of 3D keypoints is transformed into a bone vector repre-</p><formula xml:id="formula_5">sentation [ B src t ] t0+n t=t0 where B src t ? R (J?1)?3 and J is the number of keypoints. BG1 generates a displacement vector ? B ? R (J?1)?3 and a bone ratio ? ? R (J?1)?1 . The new bone vector is [ B fake t ] t0+n t=t0 where B fake t = B src t + ? B B src t + ? B B src t (1 + ?).<label>(5)</label></formula><p>? B may change the bone length instead of rotating to a new configuration as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. To avoid this, we divide the generated bones by B src t + ? B in Eq. 5. BG2. The bone generation network accepts a single sample of 3D keypoints from the source dataset and converts it to a bone representation B src t0 . BG2 generates ? B and ?. The new bone vector is</p><formula xml:id="formula_6">[ B fake t ] t0+n t=t0 where B fake t+j = B src t0 + j? B/n B src t0 + j? B/n B src t0 (1 + ?).<label>(6)</label></formula><p>BG3. The bone generation network generates the vector r ? R (J?1)?3 and the angle ? ? R (J?1)?1 . A sequence of rotation matrices [R t ] n t=0 is calculated by</p><formula xml:id="formula_7">R t+j = H( r r j? n ),<label>(7)</label></formula><p>where H transforms axis-angle rotation of (?, r) to rotation matrix representation via quaternions q = q r + q</p><formula xml:id="formula_8">x i + q y j + q z k by q = cos( ? 2 ) + r r sin( ? 2 ),<label>(8)</label></formula><formula xml:id="formula_9">R = v ? v + q 2 r I + 2q r [v] ? + [v] 2 ? ,<label>(9)</label></formula><p>where ? is the outer product, I is the identity matrix, and</p><formula xml:id="formula_10">[v] ? = ? ? 0 ?v 3 v 2 v 3 0 ?v 1 ?v 2 v 1 0 ? ? .<label>(10)</label></formula><p>{B } </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Camera Generation</head><p>In this section, we introduce two different methods of camera generation: 1) Deterministic, which generates a single camera rotation matrix and translation and 2) probabilistic. The network learns a distribution of rotation matrices. A random rotation matrix is sampled from the learned distribution. Additionally, we explore three different rotation representations: axis-angle, Euler-angles, and quaternions. In the following, we will discuss each of the procedures for each of the rotation representations.</p><p>Deterministic Axis-angle. The network generates an axis r and a translation T where the angle of rotation is r .</p><formula xml:id="formula_11">The rotation matrix R ? R 3?3 is produced by R = H( r)</formula><p>where H is explained in the equation 8.</p><p>Probabilistic Axis-angle. The network learns three separate normal distributions N 1 (? 1 , ? 1 ), N 2 (? 2 , ? 2 ) , and N 3 (? 3 , ? 3 ), an angle ?, and a translation T . The axis r = {r 1 , r 2 , r 3 } is sampled from the learned normal distributions and converted to a rotation matrix by</p><formula xml:id="formula_12">R = H( r r ?).<label>(11)</label></formula><p>Probabilistic Euler-angles. The network learns three Gaussian distributions N 1 , N 2 , and N 3 to sample the Eulerangles (?, ?, ?) from the specified distributions. The rotation matrix is obtained as follows:</p><formula xml:id="formula_13">R = R z (?)R y (?)R x (?),<label>(12)</label></formula><p>where R z (?), R y (?), and R x (?) are rotations of (?, ?, and ?) degrees around z, y, x axis, respectively. Probabilistic Quaternion. A quaternion represents a rotation around axis u = (u x , u y , u z ) with angle ? as</p><formula xml:id="formula_14">q = cos( ? 2 ) + u sin( ? 2 ).<label>(13)</label></formula><p>Therefore, q can be represented by four elements. Our network learns four distributions N 1,...,4 and randomly samples elements of q from the distributions. The quaternion q is then converted to a rotation matrix representation as explained in section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Domain and 3D Discriminators</head><p>We adopt the kinematic chain space (KCS) <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b40">40]</ref> in 2D space to generate a matrix of joint angles and limb lengths in the image plane. The domain discriminator has two branches that accept 2D keypoints and the KCS matrix, respectively. The diagonal of the KCS matrix contains the limb lengths in the image space. Other components of the KCS matrix represent angular relationships of the 2D pose. It is important to mention that we do not normalize input 2D keypoints relative to the root joint as it causes perspective ambiguities <ref type="bibr" target="#b46">[46]</ref>. Therefore, diag(KCS) is a function of position and body scale. On the contrary KCS ? diag(KCS) is a function of the camera viewpoint and scale of the person. Thus, the KCS matrix disentangles different parameters that the motion generator requires to learn. For the 3D discriminator, in order not to condition the 3D discriminator on the source domain, we first apply a random perturbation of ? degrees to the input bone vectors ? &lt; 10 ? and then feed the perturbed 3D to a part-wise KCS branch <ref type="bibr" target="#b11">[12]</ref> and the original 3D to a KCS branch. Further details about the 3D discriminator are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Selection</head><p>In order to stabilize the training of the lifting network we introduce a selection step by evaluating samples via the lifting network N . In this step, the lifting network receives (X src 2D , X src 3D ) and (X fake 2D , X fake 3D ) which are source and generated samples, respectively. We exclude samples that are either too simple or too hard using the following rule</p><formula xml:id="formula_15">selection= ? ? ? yes if ( L(N (X fake 2D )) L(N (X src 2D )) ? a) 2 &lt; b 2 no otherwise ,<label>(14)</label></formula><p>where L is an L 2 loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Training</head><p>Motion Generator. Our adversarial framework is trained using three losses for the motion generator and for the discriminators which are defined as</p><formula xml:id="formula_16">LD 3D = 1 2 E[(D(X src 3D ) ? 1) 2 ] + 1 2 E[D(X fake 3D ) 2 ],<label>(15)</label></formula><formula xml:id="formula_17">LD D = 1 2 E[(D(X src 2D ) ? 1) 2 ] + 1 2 E[D(X fake 2D ) 2 ],<label>(16)</label></formula><p>LG</p><formula xml:id="formula_18">adv = 1 2 E[(D(X fake 2D ) ? 1) 2 ],<label>(17)</label></formula><p>where (X src 3D , X fake 3D ) are 3D samples from the source dataset and synthetic generated samples, respectively. (X tar 2D , X fake 2D ) are 2D keypoints from the target dataset and the generated synthetic data, respectively. The generator also receives a feedback loss from the lifting network. The feedback loss has two components: 1) reprojection loss of the estimated 3D keypoints of the target domain 2) fixed hard ratio feedback loss adapted from <ref type="bibr" target="#b11">[12]</ref>. The lifting network N accepts X tar 2D from the target dataset and predicts X tar 3D . We define the reprojection loss as</p><formula xml:id="formula_19">L proj = X fake proj X fake proj ? X fake 2D X fake 2D 1 ,<label>(18)</label></formula><p>where 1 is the L 1 norm and</p><formula xml:id="formula_20">X fake proj = 1 0 0 0 1 0 N (X tar 2D ).<label>(19)</label></formula><p>The fixed hard ratio loss provides feedback depending on the difficulty of generated sample relative to the source samples as follows:</p><formula xml:id="formula_21">f = ( L(N (X fake 2D )) L(N (X src 2D )) ? c) 2 ,<label>(20)</label></formula><formula xml:id="formula_22">L hr = f if f &lt; d 2 0 otherwise ,<label>(21)</label></formula><p>where L is L 2 loss. The summation of the above mentioned losses is our generator loss</p><formula xml:id="formula_23">L G = L advG + L proj + L hr .<label>(22)</label></formula><p>Lifting Network. The lifting network N is trained using (X src 2D , X src 3D ) and (X fake 2D , X fake 3D ) which gives the lifting loss</p><formula xml:id="formula_24">LN = X src 3D ? N (X src 2D ) 2 + X fake 3D ? N (X fake 2D ) 2 .<label>(23)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We perform extensive experiments to evaluate the performance of AdaptPose for cross-dataset generalization. We further conduct ablation studies on the different elements of our network. In the following, we discuss different datasets and subsequently baselines and metrics.</p><p>? Human3.6M (H3.6M) contains 3D and 2D data from seven subjects captured in 50 fps. We use the training set of H3.6M (S1, S5, S6, S7, S8) as our source dataset for cross-dataset evaluations. While performing experiments on the H3.6M dataset itself we will use S1 as the source dataset and S5, S6, S7, and S8 as the target.</p><p>? MPI-INF-3DHP (3DHP) contains 3D and 2D data from 8 subjects and covers 8 different activities. We will use the 2D data from the training set of 3DHP <ref type="bibr" target="#b28">[29]</ref> as our target dataset when evaluating 3DHP. The test set of 3DHP includes more than 24K frames. However, some of the previous work use a subset of test data which includes 2,929 frames for evaluation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21]</ref>. The 2,929 version has temporal inconsistency which is fine for the single-frame networks. We use the official test set of 3DHP and compare our results against the previous work's results on the official test set of 3DHP for a fair comparison.</p><p>? 3DPW contains 3D and 2D data captured in an outdoor environment. The camera is moving in some of the trials. 3DPW <ref type="bibr" target="#b38">[38]</ref> is captured in 25fps and has more variability than 3DHP and H3.6M in terms of camera poses. We use the training set of 3DPW as our target dataset when experimenting on this dataset.</p><p>? Ski-Pose PTZ-Camera (Ski) includes 3D and 2D labels from 5 professional ski athletes in a ski resort. The dataset is captured in 30 fps and frames are cropped in 256 ? 256. The cameras are moving and there is a major domain gap between Ski and previous datasets in terms of the camera pose/position.</p><p>Evaluation Metrics. We use mean per joint position error (MPJPE) and Procrustes aligned MPJPE (P-MPJPE) as our main evaluation metrics. P-MPJPE measures MPJPE after performing Procrustes alignment of the predicted pose and the target pose. We also report the percentage of correct keypoint (PCK) with a threshold of 150 mm and area under the curve (AUC) for evaluation on 3DHP following previous arts.</p><p>Baseline. We use VideoPose3D <ref type="bibr" target="#b33">[33]</ref> (VPose3D) as the baseline pose estimator model. VPose3D is a lifting network that regresses 3D keypoints from input 2D keypoints. We use 27 frames as the input in our experiments. As preprocessing for H3.6M, 3DHP, and 3DPW datasets we normalize image coordinate such that [0, w] is mapped to [?1, 1]. Note that the 3DPW dataset has some portrait frames with a height greater than width. In these cases, we pad the width so that height is equal to width to avoid the 2D keypoint coordinates being larger than the image frame after normalization. Our experiments show that this preprocessing has lower cross-dataset error compared with root centering and Frobenius normalization of 2D keypoints. While performing experiments on the Ski dataset we use root centering and Frobenius normalization of 2D keypoints since the image frames are already cropped to 256 ? 256 with the person in the center of the image. Since there is an fps difference and also motion speed difference between our source dataset and target datasets, we also perform random downsampling in our data loader for training the baseline network. Specifically, our data loader samples {x r(t?n) , ..., x r(t+n) } from the source dataset and r is a random number sampled from a uniform distribution of <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5]</ref> . <ref type="table" target="#tab_2">Table 5</ref> shows that the baseline model has a cross-dataset MPJPE of 96.4 mm using 3DHP as the target dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Quantitative Evaluation</head><p>H3.6M. We compare our results with previous semisupervised learning methods that only use 3D labels from S1 and 2D annotations from the remaining subjects for training <ref type="bibr" target="#b33">[33]</ref> as well as data augmentation methods. Our results improve upon the previous state-of-the-art by 16%. We use ground truth 2D keypoints and therefore compare with previous work with the same setting. Since the camera pose does not change much between subjects, we hypothesize that the comparison in the current setting compares our bone generation method against previous work.</p><p>3DHP. <ref type="table" target="#tab_1">Table 2</ref> gives MPJPE, AUC, and PCK on test set of 3DHP. We report the results of PoseAug's released pre-trained model on the complete test set of 3DHP. Our results have a 14% margin in terms of MPJPE compared with previous methods that report cross-dataset evaluation results <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b47">47]</ref>. This includes the comparison to <ref type="bibr" target="#b48">[48]</ref> that uses information from the target test data to perform test-time optimization.</p><p>3DPW. <ref type="table">Table 3</ref> provides MPJPE and PA-MPJPE on the test set of 3DPW. Our method outperforms previous methods by 12 mm in PA-MPJPE. This includes previous methods that particularly were designed for cross-dataset generalization <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14]</ref> and those that use temporal information <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20]</ref>. In comparison with test-time optimization methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b48">48]</ref>, ours also has an advantage of fast inference.</p><p>SKI. <ref type="table">Table 4</ref> gives the cross-dataset results on the Ski dataset. Skiing is fast and sequences of the Ski dataset are as short as 5s. This provides little training data for temporal models and, therefore, we use a single-frame input model. We report the performance of VPose3D with single-frame input in a cross-dataset scenario to compare as a baseline model. Moreover, our results compared with Rhodin et al. <ref type="bibr" target="#b34">[34]</ref> and CanonPose <ref type="bibr" target="#b41">[41]</ref> that use multi-view data from the training set of Ski show 28mm improvement in MPJPE and 2mm in PA-MPJPE.  <ref type="figure">Figure 4</ref> shows qualitative evaluation on Ski, 3DHP, and 3DPW datasets. The predictions of the baseline and AdaptPose are depicted vs. the ground truth. We observe that AdaptPose successfully enhances the baseline predictions. <ref type="figure">Figure 5</ref> provides some examples of the generated motion and the input 3D keypoints. Generated motions are smooth and realistic. We provide further qualitative examples in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Qualitative Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Ablation Studies</head><p>Ablation on Components of AdaptPose. We ablate components of our framework including bone generation, camera generation, domain discriminator, and selection. <ref type="table" target="#tab_2">Table 5</ref> provides the performance improvements by adding any of the components starting from the baseline. All of the components have a major contribution to the results. Comparing bone generation and camera generation, the latter has larger effects on the performance. However, in contrast to PoseAug <ref type="bibr" target="#b11">[12]</ref>, our bone generation method is significantly contributing to the results (10 mm vs 1 mm). A3 shows that a combination of bone and camera generation is as good as camera generation alone. Therefore, A4 excludes bone generation from the pipeline that causes a 9 mm performance drop in MPJPE. A3 and A5 give the role of domain adaptation that is 10 mm improvements. Ablation on bone generation methods. In this section we compare the performance of three different bone generation methods that were explained in Section 4.1. <ref type="table" target="#tab_3">Table 6</ref> gives performance of BG1, BG2, and BG3 while performing cross-dataset evaluation on 3DHP. We observe that using an axis-angle representation for rotating bone vectors is superior to generating bone directions. We hypothesize that learning ? B is a harder task since there are infinitely many ? B that can generate [ B ] N t=0 from B t . On the contrary, there are only two axis-angles that map B t to [ B ] N t=0 . Ablation on camera generation methods In this section we perform analysis on three different camera generation methods that were introduced in Section 4.2. In terms of rotation representation, axis-angle outperforms quaternions and Euler-angles. Euler-angles are sensitive to the order of rotations and can lead to degenerate solutions. Comparing probabilistic and deterministic methods, the former obtains 5 mm more accurate results. Ablation on temporal information. <ref type="table">Table 8</ref> shows the performance of the network while excluding temporal information from the input and generating single 2D-3D pairs. Our cross-dataset MPJPE is 86.4 mm which still improves over previous methods (86.4 mm vs. 92.2). Therefore, although using temporal information is highly contributing  to our framework, our network still excels in non-temporal settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Are we really adapting to new datasets?</head><p>To evaluate our claim that we are adapting poses and camera views to the target dataset we visualize some samples of generated motions for 3DHP and 3DPW datasets in <ref type="figure">Figure 6</ref>. The ceiling viewpoint in the first row is from 3DHP that is out of the distribution of our source dataset. While the 2D input is from a chest view camera the generated sample is from a ceiling view, similar to the target samples. We observe that our approach generates qualitatively similar camera poses. The second and third rows also provide examples of new poses that are out of the distribution of source poses and similar to samples in the target dataset. input generated sample from target <ref type="figure">Figure 6</ref>. Sample of input images from the source dataset and the generated 3D keypoints. For visualization purposes, we only plot the middle frame from the sequence of generated frames. We manually select the images on the right from the target that matched the generated.</p><p>We provide further qualitative examples in the supplementary material. <ref type="table" target="#tab_2">Table 5</ref> also provides numbers regarding the importance of domain discriminators in our framework (A5 vs A3). It is important to mention that we substitute the domain discriminator with a 2D discriminator from the source dataset when excluding the domain discriminator in <ref type="table" target="#tab_2">Table  5</ref>. Thus, the performance drop while excluding the domain discriminator is essentially attributed to the lack of adaptation to the target space and not because of excluding the 2D discriminator. The supplementary material provides further experiments on the domain adaption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>We proposed an end-to-end framework that adapts a pre-trained 3D human pose estimation model to any target dataset by generating synthetic motions by only looking at 2D target poses. AdaptPose outperforms previous work on four public datasets by a large margin (&gt; 10%). Our proposed solution can be applied to applications where limited motion data is available. Moreover, our method is able to generate synthetic human motion for other tasks such as human action recognition. The major limitation of our work is that it underperforms when there is a large body scale difference between source and train set. Although we have defined a parameter that learns to adjust the body bone lengths we observe a 10mm difference between normalized MPJPE and actual MPJPE when there is a large scale difference between source and target body scales (cross-dataset on 3DPW). Future work should address the scale ambiguity between source and target domains.</p><p>In this section, we provide further visualization of the performance of the domain discriminator. <ref type="figure">Figure 7</ref> shows the distribution of camera viewpoints of the source, target, and generated datasets. Human3.6M and 3DHP are the source and target datasets, respectively. Human3.6M includes four chest-view cameras, while 3DHP includes 14 cameras that cover chest-view, top-view, and bottom-view. <ref type="figure">Figure 7</ref> shows that the camera viewpoints of the target dataset are more diverse than those of the source dataset. We define the viewpoint by the relative rotation matrix between the subject and the camera. <ref type="figure">Figure 7</ref> shows that AdaptPose successfully generates camera viewpoints that follow the distribution of the target camera viewpoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source domain</head><p>Generated data Target domain <ref type="figure">Figure 7</ref>. Camera viewpoints of the source (Human3.6M), target (3DHP), and generated data. The generated data follows the diversity and pattern of viewpoints of the target dataset. ?, ?, and ? are Euler-angles in radiant. The viewpoint is defined by the relative rotation matrix between the person and the camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation: 2D Detections</head><p>In this section, we perform experiments on the influence of 2D detection. Using ground truth 2D for cross-dataset evaluation is the fairest comparison since most of the previous studies use the same data <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>. Therefore, we used ground truth 2D in our evaluations and compared our results with previous work using the same setting. However, ground truth 2D is not always available. In this section, we employ AlphaPose <ref type="bibr" target="#b9">[10]</ref> to obtain 2D poses of the target dataset. The model is pre-trained on MPII <ref type="bibr" target="#b0">[1]</ref> and is not fine-tuned on the target dataset. To obtain directly comparable numbers we use the same 2D detection to evaluate 3D pose estimators of Pavllo et al. <ref type="bibr" target="#b32">[32]</ref> and Gong et al. <ref type="bibr" target="#b11">[12]</ref>. <ref type="table" target="#tab_5">Table 9</ref> provides the cross-dataset evaluation results while using ground truth 2D and detected 2D. In this experiment, the source and target datasets are Human3.6M and 3DHP, respectively. AdaptPose outperforms other methods using both ground truth 2D and detected 2D. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation: 3D Discriminator</head><p>In this section we first perform an ablation on the influence of 3D discriminator. Excluding the 3D discriminator results in <ref type="figure" target="#fig_3">Figure 8</ref> shows the structure of the 3D discriminator. A small perturbation (&lt; 10 deg) is applied to the bone vectors of input 3D and then the perturbed version is fed to the part-wise KCS matrices of right/left arm and right/left leg. The original 3D pose is also fed to a KCS matrix. The perturbation branch enables the model to explore plausible 3D poses out of the source domain. <ref type="figure" target="#fig_4">Figure 9</ref> shows the convergence curve of AdaptPose with and without perturbation of the source dataset. Without perturbation, the cross-dataset error of the model decreases to 77.2 in the first 9 epochs and then slightly increases in the following epochs. After applying the perturbation, the error decreases slower and convergence of the model is more stable.    <ref type="bibr" target="#b33">[33]</ref>, green is PoseAug <ref type="bibr" target="#b11">[12]</ref>, red is AdaptPose, and blue is the ground truth.  <ref type="figure" target="#fig_5">Figure 10</ref> provides further qualitative comparisons between AdaptPose, VideoPose3D <ref type="bibr" target="#b33">[33]</ref>, PoseAug <ref type="bibr" target="#b11">[12]</ref>, and ground truth 3D. AdaptPose significantly outperforms the previous methods. <ref type="figure" target="#fig_5">Figure 10</ref> shows that in the case of body occlusions, AdaptPose is more accurate than other methods. One of the main limitations of our method is scale error. AdaptPose under-performs if there is a large difference between source and target body scales. Such scale ambiguity is inevitable when using only monocular views and no 3D supervision of the target domain is available. <ref type="figure" target="#fig_6">Figure  11</ref> provides some examples of scale error for cross-dataset evaluation on 3DPW dataset. Code will be publicly available upon publication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Further Qualitative Results</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Bone generation methods. Blue vectors indicate bone vectors before rotation and green vectors are bone vectors after rotation. ? B is rotating bone direction produced by the network. r and ? are the axis and angle of the rotation, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>3D human pose predictions (red) vs. ground truth (blue) for samples of Ski and 3DPW. input generated Samples of generated motions and the corresponding input 3D keypoints. Motions are smooth and realistic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 .</head><label>8</label><figDesc>The 3D discriminator of AdaptPose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 .</head><label>9</label><figDesc>The evaluation error of AdaptPose while training with and without adding perturbation to the 3D discriminator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 .</head><label>10</label><figDesc>Further qualitative examples from 3DPW (right) and 3DHP (left) datasets. Yellow is Pavllo et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 .</head><label>11</label><figDesc>Scale error while performing cross-dataset evaluation on 3DPW dataset. Source: Human3.6M, target: 3DPW.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Cross-scenario learning on H3.6M. Source: S1.</figDesc><table><row><cell>Target:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Cross-dataset evaluation on 3DHP dataset.</figDesc><table><row><cell>Source:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table><row><cell cols="3">Ablation study on supervision elements of the proposed</cell></row><row><cell cols="2">model. Source: H3.6M-target:3DHP</cell><cell></cell></row><row><cell>Index</cell><cell cols="2">BG Cam DD Select PMPJPE MPJPE</cell></row><row><cell>Baseline</cell><cell>66.5</cell><cell>96.4</cell></row><row><cell>A1</cell><cell>61.7</cell><cell>90.1</cell></row><row><cell>A2</cell><cell>62.0</cell><cell>88.2</cell></row><row><cell>A3</cell><cell>61.8</cell><cell>88.1</cell></row><row><cell>A4</cell><cell>59.3</cell><cell>86.5</cell></row><row><cell>A5</cell><cell>54.0</cell><cell>78.6</cell></row><row><cell>AdaptPose</cell><cell>53.6</cell><cell>77.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table><row><cell cols="3">Ablation study on bone generation strategies</cell></row><row><cell>Method</cell><cell cols="2">PMPJPE MPJPE</cell></row><row><cell>BG1</cell><cell>59.3</cell><cell>85.1</cell></row><row><cell>BG2</cell><cell>56.2</cell><cell>80.0</cell></row><row><cell>BG3</cell><cell>53.6</cell><cell>77.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>Ablation study on camera generation strategies</figDesc><table><row><cell>Method</cell><cell cols="3">Representation PMPJPE MPJPE</cell></row><row><cell>Deterministic</cell><cell>Axis-Angle</cell><cell>58.0</cell><cell>82.8</cell></row><row><cell>Probabilistic</cell><cell>Axis-Angle</cell><cell>53.6</cell><cell>77.2</cell></row><row><cell>Probabilistic</cell><cell>Quaternion</cell><cell>58.7</cell><cell>83.5</cell></row><row><cell>Probabilistic</cell><cell>Euler-Angle</cell><cell>60.9</cell><cell>85.3</cell></row><row><cell cols="4">Table 8. Ablation study on temporal information</cell></row><row><cell>Input</cell><cell cols="3">PCK AUC MPJPE</cell></row><row><cell>1 frame</cell><cell>84.6</cell><cell>50.3</cell><cell>86.4</cell></row><row><cell>27 frames</cell><cell>88.4</cell><cell>54.2</cell><cell>77.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 9 .</head><label>9</label><figDesc>Experiment on 2D detection. Source: Human3.6M, target: 3DHP. P2 is mean per joint position error (MPJPA) and P1 is MPJPA after Procrustes alignment of the estimated and ground truth 3D.</figDesc><table><row><cell></cell><cell cols="2">AlphaPose 2D</cell><cell></cell><cell>GT 2D</cell></row><row><cell>Method</cell><cell>P2</cell><cell>P1</cell><cell>P2</cell><cell>P1</cell></row><row><cell>Pavllo et al. [33]</cell><cell>86.9</cell><cell>127.1</cell><cell cols="2">66.5 96.4</cell></row><row><cell>PoseAug [12]</cell><cell>87.2</cell><cell>125.7</cell><cell cols="2">59.0 92.6</cell></row><row><cell>Ours</cell><cell>83.4</cell><cell>120.5</cell><cell cols="2">53.6 77.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>This Appendix provides ablations on the domain discriminator, 2D detections, and 3D discriminator. We also provide further qualitative results that compare AdaptPose against previous methods. Moreover, some failure cases of AdaptPose are visualized.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised pixellevel domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><forename type="middle">Magnenat</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised 3d pose estimation with geometric selfsupervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Rohith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d human pose estimation using spatio-temporal networks with explicit occlusion training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10631" to="10638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Occlusion-aware networks for 3d human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wending</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sim2real transfer learning for 3d human pose estimation: motion to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Can 3d pose be learned from 2d projections alone?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Rohith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong Phuoc</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">RMPE: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Tripose: A weakly-supervised 3d human pose estimation via triangulation from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Rezaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabab</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z. Jane</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Poseaug: A differentiable pose augmentation framework for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bilevel online adaptation for outof-domain human mesh reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Shanyan Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imtiaz</forename><surname>Mir Rayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weaklysupervised 3d human pose learning via multi-view images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Exemplar fine-tuning for 3d human model fitting towards in-thewild 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salih</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-supervised 3d human pose estimation via part guided novel image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jogendra</forename><surname>Nath Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mugalodi</forename><surname>Rakesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised crossdataset adaptation via probabilistic amodal 3d human pose completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jogendra</forename><surname>Nath Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M V</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Patravali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhakrishnan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cascaded deep monocular 3d human pose estimation with evolutionary training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mesh graphormer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="12939" to="12948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention mechanism exploits temporal contexts: Real-time 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
	<note>Sen-ching Cheung, and Vijayan Asari</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Orinet: A fully convolutional network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Loddon</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV</title>
		<imprint>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<title level="m">Fifth International Conference on</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="120" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning monocular 3d human pose estimation from multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Sp?rri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mocap-guided data augmentation for 3d pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?gory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3116" to="3124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Research dedicated to sports injury prevention -the &apos;sequence of prevention&apos; on the example of alpine ski racing. Habilitation with Venia Docendi in &quot;Biomechanics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Sp?rri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>Department of Sport Science and Kinesiology, University of Salzburg.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gul</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A kinematic chain space for monocular motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanno</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Canonpose: Self-supervised monocular 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petrissa</forename><surname>Zell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Motion guided 3d pose estimation from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="764" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Predicting camera viewpoint improves cross-dataset generalization for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daeyun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 Workshops</title>
		<editor>Adrien Bartoli and Andrea Fusiello</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publish</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>ing. 1, 2, 6</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<title level="m">Booktitle=3D Vision (3DV). Synthesizing training images for boosting human 3d pose estimation</title>
		<editor>Year= 2015 Wenzheng Chen and Huan Wang and Yangyan Li and Hao Su and Zhenhua Wang and Changhe Tu and Dani Lischinski and Daniel Cohen-Or and Baoquan Chen</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pcls: Geometry-aware neural reconstruction of 3d pose with perspective crop layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="9064" to="9073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Srnet: Improving generalization in 3d human pose estimation with a split-and-recombine approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Inference stage optimization for cross-scenario 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Body meshes as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Hao Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
