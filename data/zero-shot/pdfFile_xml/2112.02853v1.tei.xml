<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reliable Propagation-Correction Modulation for Video Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohao</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huangzhong University of Science &amp; Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
							<email>yanlu@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Reliable Propagation-Correction Modulation for Video Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Error propagation is a general but crucial problem in online semi-supervised video object segmentation. We aim to suppress error propagation through a correction mechanism with high reliability. The key insight is to disentangle the correction from the conventional mask propagation process with reliable cues. We introduce two modulators, propagation and correction modulators, to separately perform channel-wise re-calibration on the target frame embeddings according to local temporal correlations and reliable references respectively. Specifically, we assemble the modulators with a cascaded propagation-correction scheme. This avoids overriding the effects of the reliable correction modulator by the propagation modulator. Although the reference frame with the ground truth label provides reliable cues, it could be very different from the target frame and introduce uncertain or incomplete correlations. We augment the reference cues by supplementing reliable feature patches to a maintained pool, thus offering more comprehensive and expressive object representations to the modulators. In addition, a reliability filter is designed to retrieve reliable patches and pass them in subsequent frames. Our model achieves state-of-the-art performance on YouTube-VOS18/19 and DAVIS17-Val/Test benchmarks. Extensive experiments demonstrate that the correction mechanism provides considerable performance gain by fully utilizing reliable guidance. Code is available at: https://github.com/JerryX1110/RPCMVOS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Probabilistic model of frame-to-frame VOS</head><p>Given all available observations x 1:t up to t-th frame, the label up to t-th frame y 1:t is predicted by maximum a posterior</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Semi-supervised video object segmentation (VOS), also known as mask tracking, aims at segmenting target objects in a video sequence given the ground truth mask at the first (or reference) frame. Recently, sequence-to-sequence methods <ref type="bibr" target="#b34">(Vaswani et al. 2017;</ref><ref type="bibr" target="#b9">Duke et al. 2021</ref>) achieve impressive results but suffer from relatively high cost. Online methods <ref type="bibr" target="#b26">Oh et al. 2018;</ref><ref type="bibr" target="#b38">Wang et al. 2018</ref><ref type="bibr" target="#b39">Wang et al. , 2019</ref>, taking only the current frame with image-wise references as input, are more practical for fast and streaming applications. We focus on online methods in this paper.</p><p>The Semi-supervised VOS problem is usually formulated as a maximum a posterior (MAP) problem, conditioning on the target frame, preceding and reference frames and labels. <ref type="bibr">*</ref> The work was done when Xiaohao Xu was an intern at MSRA. Copyright ? 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Considering the probabilistic model of online VOS, the current label can be predicted from a frame-by-frame propagation path or a direct translation path from the reliable reference label. To exploit local temporal consistency, many methods <ref type="bibr" target="#b26">(Oh et al. 2018;</ref><ref type="bibr" target="#b28">Perazzi et al. 2017;</ref><ref type="bibr" target="#b13">Hu, Huang, and Schwing 2017;</ref><ref type="bibr" target="#b36">Voigtlaender and Leibe 2017;</ref><ref type="bibr" target="#b8">Cheng et al. 2017</ref><ref type="bibr" target="#b7">Cheng et al. , 2018</ref><ref type="bibr" target="#b13">Hu, Huang, and Schwing 2017)</ref> follow the propagation path to perform mask propagation, but errors may accumulate over time due to the inevitable prediction uncertainty in each iteration. The reference frame with a ground truth label provides reliable object cues, thus having the potential to suppress error propagation <ref type="bibr" target="#b18">(Li et al. 2020)</ref>. Recent methods <ref type="bibr" target="#b35">(Voigtlaender et al. 2019;</ref><ref type="bibr" target="#b47">Yang, Wei, and Yang 2020)</ref> demonstrate that even naively manipulating references by feature concatenation and matching could improve the VOS performance. This encourages us to make full use of reliable reference cues to correct errors during the mask propagation. However, the target frame may turn out to be very different from the reference when time goes by, losing explicit correspondences to the reference. For example, in <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>, the reference only containing part of the object is not comprehensive to represent the whole object, i.e., the foot in reference. In this case, estimating correlations between the reference and target is uncertain and incomplete, which may lead to negative impacts on the VOS task. Network modulation, which recalibrates feature embeddings with additional conditions, has achieved great success in VOS <ref type="bibr" target="#b47">Yang, Wei, and Yang 2020)</ref>. The modulation operation is light-weight and can be performed per frame, which fulfils the streaming requirement. The key to modulation is to construct expressive conditional weights and extract highly correlated embeddings. For the VOS task, modulation weights need to be representative for reference objects and embeddings also need to encode reliable correlations between the reference and the target. In this paper, we propose a new end-to-end framework for VOS with reliable propagation-correction modulation, which can provide representative object proxies weights for modulation and consolidate target embeddings in a cascaded assembly of propagation and correction modulators.</p><p>To perform correction with high reliability, we augment the translation path from the reference to a more comprehensive correction path. Since object cues in the reference frame may be incomplete, we progressively supplement them with reliable information in each iteration. A reliable patch memory pool is maintained to store the historical reliable feature patches, which is further utilized in subsequent frames. The reliable patch pool is for two usages, i.e., augmenting the object proxy with comprehensive information to obtain expressive modulation weights and consolidating frame embedding with more reliable correlations. As for the network design, we introduce two types of modulator, i.e., propagation and correction modulator, which separately augment embedding according to the local temporal correlation cues in the propagation path and the reliable reference cues from the correction path. To avoid overriding the correction effect, the correction modulator is inserted after the propagation modulator. We also propose a reliability filter to assess the prediction quality. From the example reliability map in <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>, regions with large appearance change from reference are predicted as uncertain while other reliable regions can be passed to the following frames for correction. Our experiments demonstrate that the assembly of propagation and correction modulators has a considerable impact on VOS performance. Our contributions are three-fold.</p><p>? We propose a new reliable propagation-correction modulation model for VOS, which significantly suppresses error propagation (see precision decay curve in <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>). Our model achieves the state-of-the-art performance on both YouTube-VOS and DAVIS17 benchmarks. ? We disentangle the reliable correction from the conventional erroneous mask propagation process with separate memory modulators. ? We augment both the object proxy and target embedding with comprehensive reliable cues to reinforce the correction modulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Propagation-based VOS. Propagation-based methods utilize the semantic or spatial cues from the previous frame to predict the mask of the current frame. Early methods <ref type="bibr" target="#b0">Caelles et al. 2017;</ref><ref type="bibr" target="#b13">Hu, Huang, and Schwing 2017;</ref><ref type="bibr" target="#b16">Khoreva et al. 2019</ref>) utilize online-learning method to eliminate the drifting problem but is timeconsuming. Optical flow <ref type="bibr" target="#b33">(Tsai, Yang, and Black 2016;</ref><ref type="bibr" target="#b13">Hu, Huang, and Schwing 2017;</ref><ref type="bibr" target="#b8">Cheng et al. 2017;</ref><ref type="bibr" target="#b44">Xu et al. 2018b</ref>) and object tracking also prove to be useful guidance for mask propagation. Although propagation-based models can secure good temporal consistency , these propagation-based methods are prone to error accumulation, which may largely degrade the VOS performance especially for long video clips <ref type="bibr" target="#b20">(Liang et al. 2020b</ref>).</p><p>Matching-based VOS. Matching-based methods learn an embedding space for target objects. <ref type="bibr" target="#b4">(Chen et al. 2018b;</ref><ref type="bibr" target="#b14">Hu, Huang, and Schwing 2018;</ref><ref type="bibr" target="#b50">Zeng et al. 2019</ref>) directly construct the correspondence between the current frame with the first frame. <ref type="bibr" target="#b22">(Lin, Qi, and Jia 2019;</ref><ref type="bibr" target="#b35">Voigtlaender et al. 2019;</ref><ref type="bibr" target="#b39">Wang et al. 2019;</ref><ref type="bibr" target="#b47">Yang, Wei, and Yang 2020)</ref> further leverage both the first and the previous frames. Several recent methods <ref type="bibr" target="#b14">(Hu, Huang, and Schwing 2018;</ref><ref type="bibr" target="#b19">Liang et al. 2020a;</ref><ref type="bibr" target="#b9">Duke et al. 2021</ref>) turn to use several latest frames to further improve the local temporal guidance. Moreover, STM-based networks <ref type="bibr" target="#b27">(Oh et al. 2019;</ref><ref type="bibr" target="#b31">Seong, Hyun, and Kim 2020;</ref><ref type="bibr" target="#b23">Lu et al. 2020;</ref><ref type="bibr">Liang et al. 2020b,c;</ref><ref type="bibr" target="#b6">Cheng, Tai, and Tang 2021b;</ref><ref type="bibr" target="#b37">Wang et al. 2021;</ref><ref type="bibr" target="#b42">Xie et al. 2021;</ref><ref type="bibr" target="#b12">Hu et al. 2021;</ref><ref type="bibr" target="#b32">Seong et al. 2021</ref>) boost the performance with memory networks that memorize information from past frames for further reuse, which relieve the error propagation to some extent. However, how to reduce uncertainty propagation is still a hard problem and hasn't been tackled perfectly. Our method disentangles the guidance by reliability and further suppresses uncertainty with a carefully designed scheme.</p><p>Conditional modulation. Recently, <ref type="bibr" target="#b47">Yang, Wei, and Yang 2020;</ref><ref type="bibr" target="#b17">Li et al. 2021</ref>) introduce conditional modulation methods to tackle video-related tasks like video object segmentation for instance-based, or spatial guidance. However, the modulation weights or embeddings for mask propagation in the video are inevitably volunteered to error. To collaboratively suppress error propagation and make full use of the power of conditional modulation, we propose a new conditional modulation method called reliable conditional modulation to tackle VOS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminaries</head><p>We first review the probabilistic model of frame-to-frame VOS and analyze it from two aspects, i.e., the frame-byframe propagation path and the correction path from reliable reference. We also introduce a widely used measurement for prediction reliability.   <ref type="figure">Figure 2</ref>: Overview of the proposed framework. We disentangle the correction mechanism from the frame-to-frame mask propagation process. We assemble a cascaded scheme of propagation-correction modulators to leverage local temporal correlations and reliable references in order. We also augment the reference cues by supplementing reliable feature patches to a maintained pool, thus offering more comprehensive and expressive object representations to the modulators. A reliability filter is introduced to filter out uncertain patches for subsequent frames.</p><p>(MAP) estimation:</p><formula xml:id="formula_0">p(y1:t|x1:t) = p(x1:t|y1:t)p(yt) p(x1:t) ? p(x1:t|y1:t)p(yt)<label>(1)</label></formula><p>Here p(x 1:t |y 1:t ) is the observation model, which is usually estimated by the likelihood p(x 1:t |D), where D denotes the training data. p(y 1:t?1 |x 1:t?1 ) is the posterior up to previous frame. p(y 1:t ) is the prior model and could be unfolded with the first-order markov assumptions:</p><formula xml:id="formula_1">p(y1:t) = p(yt|yt?1)p(y1:t?1) = p(y1)? t 2 p(yi|yi?1)<label>(2)</label></formula><p>We then instantiate Equation 1 with the propagation and correction path respectively.</p><p>Propagation path. We assume that the observation model is conditionally independent given the states, i.e., p(x 1:t |y 1:t ) = ? t 1 p(x i |y i ). The posterior takes the form</p><formula xml:id="formula_2">p(y1:t|x1:t) ? ? t 1 p(xi|yi)? t 2 p(yi|yi?1)p(y1) ? ? t 1 p(xi|yi)? t 2 p(yi|yi?1)<label>(3)</label></formula><p>Note p(y 1 ) is omitted since the label of the first frame y 1 is given. Therefore, we observe that prediction uncertainty of p(y t |y t?1 ) will accumulate over time, which lead to error propagation.</p><p>Correction path. We first consider the direct translation from the reliable reference frame x 1 to the t-th frame x t for correction. The posterior takes the form</p><formula xml:id="formula_3">p(y1, yt|x1, xt) = p(yt, xt|y1, x1)p(x1, y1) p(x1, xt) ? p(yt, xt|y1, x1)<label>(4)</label></formula><p>Again, p(x 1 , y 1 ) is omitted since x 1 and y 1 are given. The joint condition probability p(y t , x t |y 1 , x 1 ) corresponds to the joint similarity of observations and labels between target and reference frames. Since labels represent object masks, the joint similarity can be considered as an object-aware similarity. Thus, prediction in the correction path is highly correlated to the object-aware similarity between target and reference frames. Since reference in the first frame may not be comprehensive, we will augment it during the iterations. Prediction reliability. Prediction uncertainty of deep neural networks is difficult to estimate accurately, while it is highly correlated with information entropy <ref type="bibr" target="#b10">(Feder and Merhav 1994)</ref>. Here, we employ the Shannon entropy to measure the reliability of prediction in each iteration:</p><formula xml:id="formula_4">H(I) = ? N +1 i=1 P ( e p i N +1 j=1 e p j ) log P ( e p i N +1 j=1 e p j )<label>(5)</label></formula><p>where p i , i ? {1, ..., N + 1} indicate the probability maps of N objects and background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reliable Propagation-Correction Modulation</head><p>In this section, we first describe the overview of our pipeline and then elaborate on the reliable modulation mechanism. Finally, the network design is detailed. <ref type="figure">Fig. 2</ref> illustrates the overview of our framework. Our goal is to predict the object mask? t of the target frame x t , given the first frame x 1 and its object mask y 1 , as well as the previous frame x t?1 and predicted object mask? t?1 . We first extract features maps f 1 , f t?1 , f t of frames x 1 , x t?1 , x t from a shared backbone respectively. To make features object-aware, we mask the features with corresponding object masks,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pipeline Overview</head><formula xml:id="formula_5">f o t = f t y t , t = {1, t ? 1}.</formula><p>For the propagation path, we define an object proxy w p representing a image-level object feature by applying a global average pooling (GAP) on f o t?1 . Object-aware inter-frame correlation for propagation path is then represented with a similarity map s t?1,t between the target feature f t and masked previous feature f o t?1 . For the correction path, we also define an object proxy w c and a similarity map s c,t ; instead of directly computing them from a reference frame feature, we maintain a reliable feature patch pool containing reliable object feature patches from historical frames. The object proxy and correlation cues from both paths are then further encoded with a memory encoder to form a compact embedding e t for the target frame. Given two object proxies w p and w c , the current embedding is modulated with the cascaded propagation and correction modulator respectively. The final probability p for each object is decoded from the modulated embedding with a decoder. In turn, we provide the reliability map r t for updating the reliable patch pool P for the next frame with the reliability filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object proxy for Correction Path</head><p>Reliable object patch pool. For correction path, the object semantics in the first frame is often incomplete. Specifically, the mask may cover only a part of an object. This will highly degrade the correction effects from the reference frame. Thus, we need to augment the object proxy by supplementing new features in the process of mask propagation. However, previous study ) has shown that mask propagation with multiple historical frames may be vulnerable to the influence of inaccurate information, leading to error propagation. To augment object proxy with historical cues while suppressing error propagation, we introduce a reliable object patch pool for updating useful feature patches for object representation augmentation and a reliability filter to filter out uncertain feature patches.</p><p>Specifically, we use a reliability map r to filter out uncertain patches, i.e., f c = f y r. During mask propagation, we only recall the highly reliable patches of the object and supplement them to the reliable object patch pool P, thus augmenting the semantic cues of target objects. Details are described in Algorithm 1.</p><p>Reliable object proxy. Given the reliable object patch pool P containing a set of highly reliable patches, we construct the reliable object proxy w c by apply GAP on all elements in P, i.e., w c = GAP({f y r}, f ? P).</p><p>Algorithm 1: Reliable object proxy augmentation Input: Embedding of current frame f t , embedding f t?1 , mask? t?1 and reliability map r t?1 of previous frame, reliable object patch pool P and its updating time interval ? , timestamp t ? {2, ...}. Output: Augmented reliable object proxy w c and similarity map from the correction path s c,t 1: if t == 2 then 2:</p><p>Let</p><formula xml:id="formula_6">r 1 = I, P = ?. 3: end if 4: if ((t ? 2) mod ? ) == 0 then 5: f o t?1 = f t?1 ? t?1 r t?1 6: P = P f o t?1 . 7: end if 8: w c = f ?P f |P| 9: s c,t = max S(P, f t ),</formula><p>where S is a similarity measure. 10: return w c , s c,t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Propagation-Correction Modulation</head><p>While most methods <ref type="bibr" target="#b27">(Oh et al. 2019;</ref><ref type="bibr" target="#b31">Seong, Hyun, and Kim 2020;</ref><ref type="bibr" target="#b23">Lu et al. 2020</ref>) employ reference and previous cues equivalently, they do not distinguish their influence on the prediction of the current frame. We address that mask propagation along the propagation path can preserve local temporal consistency, while the reference provides more reliable information. The reliable reference is more suitable to perform a correction role after propagation, and thus such two kinds of cues should be handle in a disentangled way to be made full use of.</p><p>We maintain two types of external memory modules to selectively memorize information from the propagation and correction paths, namely, propagation memory storing the previous embedding e t?1 and updating at each frame, and correction memory storing the embedding of reference frame e 1 . Accordingly, we build two modulator ? prop and ? corr to modulate the current embedding with corresponding object proxy w p and w c . Intuitively, we set the correlation modulator after the propagation memory as illustrated in <ref type="figure" target="#fig_2">Fig. 3 (a)</ref> because correlation is reliable and should not be overridden by the propagation modulator.</p><p>Our key observation is that the order of modulators in cascaded schemes matters since the uncertainty also relies on the depth of relevant layers in the neural network <ref type="bibr" target="#b11">(Goldfeld et al. 2019)</ref>. We verified this observation with detailed analysis in the experiment section and confirmed a cascade with propagation-correction order (i.e. P 2C) outperforms other cascade variants as well as parallel approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Design</head><p>We detail the implementation of each network module.</p><p>Correlation head. The correlation head for calculating similarities between features consists of a set of linear operations in local windows as <ref type="bibr" target="#b35">(Voigtlaender et al. 2019)</ref>, and then the similarity maps are concatenated with the previous mask and projected into a high-dimensional space. Note that we follow <ref type="bibr" target="#b47">(Yang, Wei, and Yang 2020)</ref> to split features into the foreground and background-masked ones according to corresponding masks and concatenated them together for further processing.</p><p>Memory encoder. The memory encoder ? me maps the concatenated features [f t , s c,t , s t?1,t ] to a lower dimension and compact embedding e t . Meanwhile, it consists of a 1 ? 1 convolution layer followed by a series of channel reweighting operations.</p><formula xml:id="formula_7">e t = ? me ([f t ; s c,t ; s t?1,t ], [w p ; w c ]), where [; ] denotes concatenation.</formula><p>Modulator block. <ref type="figure" target="#fig_2">Fig. 3 (b)</ref> illustrates the structure of a basic memory modulator block. A memory modulator block maintains a memory buffer e i ? R H?W ?C for later usage. During each forward, the memory modulator block inputs memory embedding features e t from the current frame, reads out the buffered memory embedding (i.e., e t?1 for propagation modulator or e 1 for correction modulator), concatenates them together and then performs a reweighting operation along channel dimension with w p or w c . Inspired by the theoretical proof <ref type="bibr">(Chen et al. 2011</ref>) that low-rank matrix can recover from errors and erasures, we implicitly force the memory embedding to simultaneously keep the low-rank property and encode more useful information by compressing the reweighted embedding to a lower dimension via a 1 ? 1 convolution layer.</p><p>Reliability filter. We first compute the Shannon entropy H t from probability maps {p i } with Equation 5. Then the reliability map r t is estimated by applying a threshold function ? ? (?) on the Shannon entropy H t to remain the reliable regions in the final mask prediction for the update of the reliable patch pool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment</head><p>Datasets. We evaluate our model mainly on two widely used VOS benchmarks with multiple objects, YouTube-VOS <ref type="bibr" target="#b43">(Xu et al. 2018a</ref>) and DAVIS17 , and a small-scale single object VOS benchmark DAVIS16 <ref type="bibr" target="#b29">(Perazzi et al. 2016</ref>). The unseen object categories make YouTube-VOS more proper to measure the generalization ability of algorithms. So we conduct our experiments on YouTube-VOS to evaluate various methods accurately.</p><p>Metrics. We adopt the evaluation metrics from the DAVIS benchmark <ref type="bibr" target="#b29">(Perazzi et al. 2016)</ref>: the region accuracy J, which calculates the intersection-over-union (IoU) of the estimated masks and the ground truth masks, the boundary accuracy F, which measures the accuracy of boundaries via bipartite matching between the boundary pixels. Implementation details. We use the DeepLabv3+ <ref type="bibr" target="#b1">(Chen et al. 2018a</ref>) architecture as the backbone of our network. Unless otherwise specified, we use ResNet-101 as the backbone network of DeepLabv3+.</p><p>The training is conducted with an SGD optimizer with a momentum of 0.9 using the cross-entropy loss. For YouTube-VOS experiments, we only use YouTube-VOS without any external datasets. We first use a learning rate of 0.02 for 200k steps with a batch size of 8, then change to a learning rate of 0.01 for another 200k steps. During inference, we restrict the long-edge of each frame to no more than 1040 pixels and apply a scale set of [1.0, 1.3, 1.5] for multiscale testing on YouTube-VOS. For DAVIS17 and DAVIS16 experiments, we finetune the trained model on YouTube-VOS for 20k steps with both DAVIS and YouTube-VOS in a ratio of 2:1.</p><p>All the experiments are performed on an NVIDIA DGX-1 Linux workstation (OS: Ubuntu 16.04.4 LTS, GPU: 8? Tesla V100). Our code is implemented with PyTorch 1.4.1 and is partly leveraged from <ref type="bibr">(Yang, Wei, and Yang)</ref>. To ensure the validity of experiments, our main results are averaged for 3 runs. For hyper-parameters, we set the update interval ? for reliable patch candidate pool P as 5 and the parameter ? in the reliability filter as 1 without tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Results</head><p>Quantitative comparison. We compare our method with multiple state-of-the-art methods on YouTube-VOS18 validation (YV18-Val), YouTube-VOS19 validation (YV19-Val) and DAVIS benchmarks in <ref type="table" target="#tab_1">Table 1 and Table 2</ref>. With-   <ref type="figure" target="#fig_3">Fig. 4</ref> shows the qualitative comparison between state-of-the-art methods and our model on the YouTube-VOS validation set. Thanks to our reliable correction mechanism, our model suppresses the error propagation better, achieving better results when the targets become different from the reference. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study and Discussion</head><p>In addition to the state-of-the-art performance, we provide the following insights with our detailed ablation.</p><p>Modulator assembly variants. Apart from propagation modulator P and correction modulator C, an auxiliary selfmodulator S that uses current embedding both as both memory and input serves as a reference. For simplicity, "2" and "&amp;" between characters stand for cascaded and parallel assembly schemes. The variants can be categorized into propagation-based (S2P , P 2S, P 2P and P 2P 2P ), correction-based (S2C, C2S and C2C) and propagation &amp; correction (C2P , P 2C and P &amp;C) schemes. Note that P &amp;C is a variant where the propagation and correction modulators are assembled in parallel streams and followed by a self-modulator to fuse the outputs.</p><p>Modulation with propagation and correction guidance.</p><p>To study how the propagation and correction guidance influ- ences VOS results, we conduct ablation experiments on the YV18-Val split with different modulator variants. We also list the SOTA model CFBI that uses naive local and global guidance for comparison. The results are shown in <ref type="table" target="#tab_3">Table 3</ref>. Can our modulator improve the performance only with direct propagation guidance? Yes. From <ref type="table" target="#tab_3">Table 3</ref>, we can find that our propagation-based variants all outperform CFBI. The reasons are two-fold. First, while CFBI utilizes image-level features from previous frames for improving performance, it does not have explicit designs to suppress error propagation. Instead, our model utilizes a compressed and memorized embedding throughout the whole sequence, which consolidates the local spatial correlation. Secondly, the modulator is a plug-and-play module with a resolutionkeeping design, which may help protect the detail from loss.</p><p>Can more propagation guidance from different layers further boost the performance? No. Although modulators are light-weight, simply stacking more propagation-based modulators (P 2S, P 2P and P 2P 2P ) only has a marginal gain, which indicates that simply incorporating propagation guidance is not enough.</p><p>Is correction guidance effective for uncertainty suppression? Yes. Thanks to the highly reliable correction memory, simply leveraging the correction modulator (C2S, S2C, C2C) can boost the performance especially in unseen categories compared with the one without using correction guidance (S2S). Among those injections, assembling the correction modulator in deeper layers (S2C) brings the largest gain since it encodes cues with high reliability and its correction impact will not be overridden by others.</p><p>Can propagation and correction be collaboratively leveraged? Yes. The propagation-correction cascaded assembly (P 2C) stands out among all the modulator variants, which verifies our insight that propagation modulator relying on local coherence fits shallow layers while correction modulator relying on high-level semantics fits deeper layers.</p><p>How does modulator affect the embedding? To answer this question, we visualize the transformation process of embedding in P 2C over different layers with principal components analysis in <ref type="figure" target="#fig_4">Fig. 5</ref>. We can observe that P 2C modulates the embedding by progressively separating the foreground and background features in embedding space. Modulation with reliable object proxy augmentation.</p><p>To study how the reliable object proxy augmentation influences VOS results, we conduct a set of ablations on YV19-Val split with different proxy construction settings, with P 2C as a baseline here. The results are shown in <ref type="table" target="#tab_4">Table 4</ref>. Does object proxy augmentation make contribution? Yes. <ref type="table" target="#tab_4">Table 4</ref> shows that no matter uncertainty patches are filtered or not, using object proxy augmentation always performs better, for both calculating modulation weight and correlation. The reason is that direct propagation may introduce uncertainty while correction from the first frame may be limited due to incomplete semantic guidance. Thus, apart from these two guidance, augmenting object proxy representation especially with reliability helps to complete the semantic concept of an object.</p><p>Is reliability-based filtering method beneficial? Yes. Comparing P 2C +OA(W +S)+RF and P 2C +OA(W + S), we can notice that object proxy augmentation with reliability filter outperforms, which indicates that such consideration of reliability during proxy augmentation can further complement with propagation-correction modulators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We present a new modulation-based model that can effectively suppress error propagation in semi-supervised VOS.</p><p>The key is to disentangle the correction from the frame-byframe mask propagation, which provides reliable and comprehensive object proxies as modulation weights and assembles modulators carefully to further consolidate the target embedding. The object proxy is augmented by supplementing new reliable feature patches from the reliability filter in each iteration, evolving comprehensively. The target embedding encoding the current frame and correlations with references is also consolidated due to the supplemented reliable patches. The assembly of modulators is critical, and our experiments demonstrate that the cascaded propagationcorrection scheme performs the best. The main reason is that correction modulation contains global reliable information that could correct errors, and its impact should not be overridden by other modulation.</p><p>We also introduce a reliability filter to facilitate the modulation by assessing prediction quality and selecting reliable feature patches. The experiments show impressive gain from the reliable propagation-correction modulation for VOS.</p><p>In the appendix, we first demonstrate the detailed network structure of Modulator Block. Then, we provide inference speed analysis on DAVIS16 and ablation study for reliability measures. What's more, we provide the qualitative result of a challenging case from the Test-dev split of DAVIS17.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detailed Modulator Block</head><p>As shown in <ref type="figure">Fig.A, a</ref> modulator block is the basic module to construct various modulator assembly, such as cascade and parallel ones in <ref type="figure">Fig.A(b)</ref> and <ref type="bibr">(c)</ref> <ref type="figure">. Fig.B</ref> demonstrates the detailed network structure of a basic modulator block. A modulator block maintains a memory buffer, which stores memory embedding feature e i at timestamp i for afterward usage. During each forward operation, the memory modulator block inputs memory embedding features e t from the current frame, reads out the buffered memory embedding (i.e., e 1 for correction modulator or e t?1 for propagation modulator) and concatenates them together. Then, several instance heads are introduced at several intermediate layer to re-weight the embedding feature channel-wisely. The instance head includes a fully connected (FC) layer and a nonlinear activation function to construct a gate for the embedding feature to be re-weighted. Notably, the spatial resolution of the embedding feature is maintained in the memory modulator block for object detail preservation. What's more, the channel dimension of the input and output feature embedding are also the same. Such design enforces the network to transform and compress the mixture of the feature embedding from current frame and memory bank into a more compact representation. We use Group Normalization (Group Norm) <ref type="bibr" target="#b41">(Wu and He 2018)</ref> and Gated Channel Transformation (GCT)  in the bottleneck unit for stable training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inference Time Analysis on DAVIS16</head><p>As previous studies <ref type="bibr" target="#b47">(Yang, Wei, and Yang 2020;</ref><ref type="bibr" target="#b27">Oh et al. 2019;</ref><ref type="bibr" target="#b42">Xie et al. 2021;</ref><ref type="bibr" target="#b27">Oh et al. 2019)</ref>, we first compare the inference time of our model with previous state-of-the-art models on DAVIS16 <ref type="bibr" target="#b29">(Perazzi et al. 2016</ref>). Then we make an inference time analysis of our reliable proxy augmentation to evaluate whether this component is efficient or not. Our inference protocol mainly follows <ref type="bibr" target="#b47">(Yang, Wei, and Yang 2020)</ref>, uses one Tesla V100 GPU and set batch size as one.</p><p>Comparison with state-of-the-art models. The inference time comparison of our whole model and previous state-ofthe-art models is shown in <ref type="table" target="#tab_5">Table C</ref>. Compared to the previous state-of-the-art model CFBI <ref type="bibr" target="#b47">(Yang, Wei, and Yang 2020)</ref>, whose setting is similar to ours and achieves a good balance of both speed and accuracy, our proposed model not only achieves much better J&amp;F (90.6% vs. 89.4%) but also maintains a faster inference speed (0.172s vs.0.18s). Under the full resolution inference setting, we can further improve the performance from 90.6% to 91.5% with little extra time cost ( 0.09s).</p><p>Ablation for reliable proxy augmentation To validate the efficiency of our reliable proxy augmentation, we further make an inference time ablation study. From <ref type="table" target="#tab_5">Table A,   Table A</ref>: Time cost ablation study for reliable proxy augmentation (RP A). Here, P 2C stands for our propagationcorrection modulator scheme. We report the average inference time under two inference resolution settings (480p and Full-resolution F R) on DAVIS16 <ref type="bibr" target="#b29">(Perazzi et al. 2016</ref>). We first calculate the time t P 2C of simply using P 2C and the time t P 2C+RP A of both using P 2C and RP A. Then, we calculate the increased time ?t as t RP A by subtracting t P 2C from t P 2C+RP A . we can notice that the additional time cost by incorporating the reliable proxy augmentation is really slight (about 1% relative increase under 480p resolution inference and 0.1% relative increase under Full-resolution inference), which further proves the efficiency and effectiveness of this algorithm.</p><formula xml:id="formula_8">Time (s) 480p F R t P 2C 0.1705 0.2625 t P 2C+RP A 0.1722 0.2628 t RP A (?t) 0.0017 0.0003</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study for Reliability Measures</head><p>Shannon Entropy (SE) is used as a measure of prediction reliability and incorporated in the reliable proxy consolidation. <ref type="table" target="#tab_5">Table B</ref> shows the ablation study of different measures of prediction reliability. Apart from using Shannon Entropy, we also tried using the logit map l t of each object to directly indicate reliability (logit), which brings minor gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Result on DAVIS17 Test-dev</head><p>In <ref type="figure">Fig.C, we</ref> show a qualitative comparison between our model and previous state-of-the-art models on a very challenging case (carousel) on DAVIS17 (Pont-Tuset et al. 2017) Test-dev split. Considering the large appearance transition from the reference to targets and interference from similar objects, our model achieves much better results compared to STM <ref type="bibr" target="#b27">(Oh et al. 2019)</ref> and MiVOS (Cheng, Tai, and Tang 2021a) (red rectangles bound error regions). The binary reliability maps indicate reliable (while) and uncertain (black) patches. From this figure, we can observe that the error regions in a frame of STM and MiVOS tend to propagate into larger ones in the following frames. However, for our model, even if some tiny error regions occur, it can be quickly suppressed in the following frames with the help of reliable guidance.   <ref type="bibr" target="#b29">(Perazzi et al. 2016)</ref>. Y denotes additionally using YouTube-VOS for training. Superscript F R denotes full-resolution testing. Otherwise, methods are all tested on 480p. F t and S separately denote finetuning at test time and using simulated data in the training process. We mainly borrow the table from <ref type="bibr" target="#b47">(Yang, Wei, and Yang 2020)</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our model can suppress error propagation in VOS with a reliable propagation-correction mechanism. (a) Average overall performance (J&amp;F) on YouTube-VOS 19 validation set over time. Ours has the least performance decay. (b) Considering the large appearance transition from the reference to targets, our model achieves much better results compared to CFBI (Yang, Wei, and Yang 2020) and MiVOS (Cheng, Tai, and Tang 2021a) (red rectangles bound error regions). The binary reliability maps indicate reliable (white) and uncertain (black) patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) Cascaded propagation-correction (P 2C) modulator scheme. (b) Basic modulator block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative comparison to several state-of-the-art methods, CFBI<ref type="bibr" target="#b47">(Yang, Wei, and Yang 2020)</ref>, JOINT<ref type="bibr" target="#b25">(Mao et al. 2021)</ref>, MiVOS (Cheng, Tai, and Tang 2021a) on YouTube-VOS 19 validation set. With the reliable correction mechanism, our model can reduce the error regions in the mask propagation process. Error regions are highlighted with blue bounding boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>(a) Pixel-level embedding evolution through a P 2C modulator. Features of foreground and background are colored in blue and purple. The visualization is conducted by reducing the embedding to 2-dims with PCA. (b) Predicted mask with input frame blended. Zoom in to view better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>modulator blockFigure A: (a) Basic modulator block. (b) Cascade modulator assembly scheme. S2S is an assembly with two cascaded selfmodulator. P 2C stands for propagation-correction cascade scheme. (b) Parallel modulator assembly scheme. We show P &amp;C here for illustration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>object proxy augmentation GAP Global average pooling Feature map Weights ? Element-wise multiplication Corr. head Legend?</head><label></label><figDesc></figDesc><table><row><cell>t?1 o</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>{ i }</cell><cell>?</cell></row><row><cell></cell><cell>Propagation</cell><cell>Correction</cell></row><row><cell></cell><cell>memory</cell><cell>memory</cell><cell>Decoder</cell></row><row><cell>Corr.</cell><cell>modulator</cell><cell>modulator</cell></row><row><cell>head</cell><cell></cell><cell></cell></row><row><cell>Update for t+1</cell><cell></cell><cell>Reliability</cell></row><row><cell>1 o</cell><cell></cell><cell>filter</cell></row><row><cell>Reliable patch pool ( )</cell><cell></cell><cell></cell></row></table><note>t Reliable</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Quantitative comparisons on YouTube-VOS. Subscript s and u denote scores in seen and unseen categories. denotes using external training datasets. Superscript M S and F denotes using multi-scale and flip testing in evaluation respectively.</figDesc><table><row><cell>Methods</cell><cell></cell><cell cols="3">YouTube-VOS 2018 Validation</cell><cell></cell><cell></cell><cell cols="3">YouTube-VOS 2019 Validation</cell><cell></cell></row><row><cell></cell><cell>J&amp;F</cell><cell>J s</cell><cell>J u</cell><cell>F s</cell><cell>F u</cell><cell>J&amp;F</cell><cell>J s</cell><cell>J u</cell><cell>F s</cell><cell>F u</cell></row><row><cell>AGAME (Johnander et al. 2019)</cell><cell>66.1</cell><cell>67.8</cell><cell>60.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PReM (Luiten, Voigtlaender, and Leibe 2018)</cell><cell>66.9</cell><cell>71.4</cell><cell>56.5</cell><cell>75.9</cell><cell>63.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>STM (Oh et al. 2019)</cell><cell>79.4</cell><cell>79.7</cell><cell>72.8</cell><cell>84.2</cell><cell>80.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CFBI (Yang, Wei, and Yang 2020)</cell><cell>81.4</cell><cell>81.1</cell><cell>75.3</cell><cell>85.8</cell><cell>83.4</cell><cell>81.0</cell><cell>80.6</cell><cell>75.2</cell><cell>85.1</cell><cell>83.0</cell></row><row><cell>RMN (Xie et al. 2021)</cell><cell>81.5</cell><cell>82.1</cell><cell>75.7</cell><cell>85.7</cell><cell>82.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SST (Duke et al. 2021)</cell><cell>81.7</cell><cell>81.2</cell><cell>76.0</cell><cell>-</cell><cell>-</cell><cell>81.8</cell><cell>80.9</cell><cell>76.6</cell><cell>-</cell><cell>-</cell></row><row><cell>LCM (Hu et al. 2021)</cell><cell>82.0</cell><cell>82.2</cell><cell>75.7</cell><cell>86.7</cell><cell>83.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MiVOS+km (Cheng, Tai, and Tang 2021b)</cell><cell>82.6</cell><cell>81.1</cell><cell>77.7</cell><cell>85.6</cell><cell>86.2</cell><cell>82.8</cell><cell>81.6</cell><cell>77.7</cell><cell>85.8</cell><cell>85.9</cell></row><row><cell>HMMN (Seong et al. 2021)</cell><cell>82.6</cell><cell>82.1</cell><cell>76.8</cell><cell>87.0</cell><cell>84.6</cell><cell>82.5</cell><cell>81.7</cell><cell>77.3</cell><cell>86.1</cell><cell>85.0</cell></row><row><cell>CFBI+ (Yang, Wei, and Yang 2021)</cell><cell>82.8</cell><cell>81.8</cell><cell>77.1</cell><cell>86.6</cell><cell>85.6</cell><cell>82.9</cell><cell>80.6</cell><cell>78.9</cell><cell>85.2</cell><cell>86.8</cell></row><row><cell>JOINT (Mao et al. 2021)</cell><cell>83.1</cell><cell>81.5</cell><cell>78.7</cell><cell>85.9</cell><cell>86.5</cell><cell>82.8</cell><cell>80.8</cell><cell>79.0</cell><cell>84.8</cell><cell>86.6</cell></row><row><cell>Ours</cell><cell>84.0</cell><cell>83.1</cell><cell>78.5</cell><cell>87.7</cell><cell>86.7</cell><cell>83.9</cell><cell>82.6</cell><cell>79.1</cell><cell>86.9</cell><cell>87.1</cell></row><row><cell>CFBI M S,F (Yang, Wei, and Yang 2020)</cell><cell>82.7</cell><cell>82.2</cell><cell>76.9</cell><cell>86.8</cell><cell>85.0</cell><cell>82.4</cell><cell>81.8</cell><cell>76.9</cell><cell>86.1</cell><cell>84.8</cell></row><row><cell>Ours M S</cell><cell>84.3</cell><cell>83.3</cell><cell>78.9</cell><cell>87.9</cell><cell>86.9</cell><cell>84.2</cell><cell>83.0</cell><cell>79.4</cell><cell>87.3</cell><cell>87.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Quantitative comparisons on DAVIS. denotes using external training datasets besides YouTubeVOS and DAVIS. Superscript F R denotes full-resolution testing. Otherwise, methods are all tested on 480p.</figDesc><table><row><cell></cell><cell cols="7">CFBI SST MiVOS RMN LCM JOINT Ours Ours F R</cell></row><row><cell></cell><cell></cell><cell cols="5">DAVIS16 Validation (single object, easy)</cell><cell></cell></row><row><cell cols="2">J&amp;F 89.4</cell><cell>-</cell><cell>91.0</cell><cell>88.8 90.7</cell><cell>-</cell><cell>90.6</cell><cell>91.5</cell></row><row><cell>J</cell><cell>88.3</cell><cell>-</cell><cell>89.7</cell><cell>88.9 89.9</cell><cell>-</cell><cell>87.1</cell><cell>88.3</cell></row><row><cell>F</cell><cell>90.5</cell><cell>-</cell><cell>92.1</cell><cell>88.7 91.4</cell><cell>-</cell><cell>94.0</cell><cell>94.7</cell></row><row><cell></cell><cell></cell><cell cols="5">DAVIS17 Validation (multi-object, medium)</cell><cell></cell></row><row><cell cols="3">J&amp;F 81.9 82.5</cell><cell>83.3</cell><cell cols="3">83.5 83.5 83.5 83.7</cell><cell>84.8</cell></row><row><cell>J</cell><cell cols="2">79.1 79.9</cell><cell>80.6</cell><cell cols="3">81.0 80.5 80.8 81.3</cell><cell>82.5</cell></row><row><cell>F</cell><cell cols="2">84.6 85.1</cell><cell>85.1</cell><cell cols="3">86.0 86.5 86.2 86.0</cell><cell>87.2</cell></row><row><cell></cell><cell></cell><cell cols="4">DAVIS17 Test-dev (multi-object, hard)</cell><cell></cell><cell></cell></row><row><cell cols="2">J&amp;F 74.8</cell><cell>-</cell><cell>76.5</cell><cell>75.0 78.1</cell><cell>-</cell><cell>79.2</cell><cell>81.0</cell></row><row><cell>J</cell><cell>71.1</cell><cell>-</cell><cell>72.7</cell><cell>71.9 74.4</cell><cell>-</cell><cell>75.8</cell><cell>77.6</cell></row><row><cell>F</cell><cell>78.5</cell><cell>-</cell><cell>80.2</cell><cell>78.1 81.8</cell><cell>-</cell><cell>82.6</cell><cell>84.3</cell></row><row><cell cols="8">out using any bells and whistles (e.g., fine-tuning at test</cell></row><row><cell cols="8">time, top-k filtering, pre-training on external training dataset</cell></row><row><cell cols="8">BL30K or simulated training data), our model significantly</cell></row><row><cell cols="8">outperforms nearly all the contemporary work and previous</cell></row><row><cell cols="8">SOTA methods. Our model stands out in most of the evalu-</cell></row><row><cell cols="8">ation metrics, especially on unseen categories of YouTube-</cell></row><row><cell cols="8">VOS, which further demonstrates the generalization ability.</cell></row><row><cell cols="8">On the challenging DAVIS17 Test-dev split, the overall per-</cell></row><row><cell cols="8">formance can be further promoted to 81% J&amp;F with full-</cell></row><row><cell cols="8">resolution testing thanks to the good scalability of input res-</cell></row><row><cell cols="4">olution of our model.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Qualitative comparison.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation on memory modulator scheme variants on YouTube-VOS 18 validation set. ? indicates improvement over our compared method CFBI.</figDesc><table><row><cell cols="2">Method</cell><cell>J&amp;F</cell><cell>Js</cell><cell>Ju</cell><cell>Fs</cell><cell>Fu</cell></row><row><cell></cell><cell>CFBI</cell><cell>81.4</cell><cell>81.1</cell><cell>75.3</cell><cell>85.8</cell><cell>83.4</cell></row><row><cell></cell><cell>S2S</cell><cell cols="5">81.9(0.5?) 81.6(0.5?) 76.1(0.8?) 86.1(0.3?) 83.9(0.5?)</cell></row><row><cell></cell><cell>S2P</cell><cell cols="5">81.9(0.5?) 81.4(0.3?) 76.2(0.9?) 85.9(0.1?) 84.3(0.9?)</cell></row><row><cell></cell><cell>P 2S</cell><cell cols="5">82.2(0.8?) 81.8(0.7?) 76.4(1.1?) 86.5(0.7?) 84.3(0.9?)</cell></row><row><cell>Ours</cell><cell>P 2P</cell><cell cols="5">82.3(0.9?) 81.8(0.7?) 76.3(1.0?) 86.5(0.7?) 84.4(1.0?)</cell></row><row><cell></cell><cell>C2S</cell><cell cols="5">82.1(0.7?) 81.8(0.7?) 76.2(0.9?) 86.3(0.5?) 84.1(0.7?)</cell></row><row><cell></cell><cell>S2C</cell><cell cols="5">82.6(1.2?) 81.7(0.6?) 77.5(2.2?) 86.1(0.3?) 85.2(1.8?)</cell></row><row><cell></cell><cell>C2C</cell><cell cols="5">82.3(0.9?) 81.6(0.5?) 76.8(1.5?) 86.1(0.3?) 84.7(1.3?)</cell></row><row><cell></cell><cell>C2P</cell><cell cols="5">82.5(1.1?) 82.0(0.9?) 76.7(1.4?) 86.7(0.9?) 84.4(1.0?)</cell></row><row><cell></cell><cell>P 2C</cell><cell cols="5">82.9(1.5?) 82.9(1.8?) 76.9(1.6?) 87.4(1.6?) 84.5(1.1?)</cell></row><row><cell></cell><cell>P &amp;C</cell><cell cols="5">82.5(1.1?) 82.1(1.0?) 76.7(1.4?) 86.7(0.9?) 84.3(0.9?)</cell></row><row><cell></cell><cell cols="6">P 2P 2P 82.3(0.9?) 82.0(0.9?) 76.4(0.9?) 86.5(0.7?) 84.1(0.7?)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study for reliable object proxy augmentation on YouTube-VOS 19 validation set. P 2C denotes the propagation-correction modulator assembly, OA and RF denotes using object proxy augmentation and reliability filter. W denotes using OA for calculate modulation weight only. W + S denotes using OA for calculating weights and correlation (similarity).</figDesc><table><row><cell>P 2C</cell><cell>OA</cell><cell>RF</cell><cell>J&amp;F</cell><cell>J s</cell><cell>J u</cell><cell>F s</cell><cell>F u</cell></row><row><cell></cell><cell></cell><cell></cell><cell>82.7</cell><cell>82.1</cell><cell>77.4</cell><cell>86.3</cell><cell>84.9</cell></row><row><cell></cell><cell>(W )</cell><cell></cell><cell>82.9</cell><cell>82.3</cell><cell>77.7</cell><cell>86.5</cell><cell>85.2</cell></row><row><cell></cell><cell>(W + S)</cell><cell></cell><cell>83.6</cell><cell>82.4</cell><cell>78.7</cell><cell>86.8</cell><cell>86.5</cell></row><row><cell></cell><cell>(W + S)</cell><cell></cell><cell>83.9</cell><cell>82.6</cell><cell>79.1</cell><cell>86.9</cell><cell>87.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table B :</head><label>B</label><figDesc>Ablation study of reliability measures (M r ) on YouTube-VOS19. logit denotes directly using value of logit map to indicate uncertainty while SE denotes using Shanoon entropy.</figDesc><table><row><cell>M r</cell><cell>J&amp;F</cell><cell>J s</cell><cell>J u</cell><cell>F s</cell><cell>F u</cell></row><row><cell>-</cell><cell>82.69</cell><cell>82.17</cell><cell>77.40</cell><cell>86.42</cell><cell>84.76</cell></row><row><cell>logit</cell><cell>82.75</cell><cell>82.00</cell><cell>77.50</cell><cell>86.30</cell><cell>85.18</cell></row><row><cell>SE</cell><cell>83.92</cell><cell>82.68</cell><cell>79.06</cell><cell>86.85</cell><cell>87.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table C :</head><label>C</label><figDesc>Quantitative comparison on DAVIS16</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>for inference speed comparison.</figDesc><table><row><cell>Methods</cell><cell>F t</cell><cell>S</cell><cell>J&amp;F</cell><cell>J</cell><cell>F</cell><cell>t/s</cell></row><row><cell>OSMN (Yang et al. 2018)</cell><cell></cell><cell></cell><cell>-</cell><cell>74.0</cell><cell>-</cell><cell>0.14</cell></row><row><cell>PML(Chen et al. 2018b)</cell><cell></cell><cell></cell><cell>77.4</cell><cell>75.5</cell><cell>79.3</cell><cell>0.28</cell></row><row><cell>VideoMatch (Hu, Huang, and Schwing 2018)</cell><cell></cell><cell></cell><cell>80.9</cell><cell>81</cell><cell>80.8</cell><cell>0.32</cell></row><row><cell>FEELVOS(Y) (Voigtlaender et al. 2019)</cell><cell></cell><cell></cell><cell>81.7</cell><cell>81.1</cell><cell>82.2</cell><cell>0.45</cell></row><row><cell>RGMP (Oh et al. 2018)</cell><cell></cell><cell></cell><cell>81.8</cell><cell>81.5</cell><cell>82.0</cell><cell>0.14</cell></row><row><cell>A-GAME(Y) (Johnander et al. 2019)</cell><cell></cell><cell></cell><cell>82.1</cell><cell>82.2</cell><cell>82.0</cell><cell>0.07</cell></row><row><cell>OnAVOS (Voigtlaender and Leibe 2017)</cell><cell></cell><cell></cell><cell>85.0</cell><cell>85.7</cell><cell>84.2</cell><cell>13</cell></row><row><cell>PReMVOS (Luiten, Voigtlaender, and Leibe 2018)</cell><cell></cell><cell></cell><cell>86.8</cell><cell>84.9</cell><cell>88.6</cell><cell>32.8</cell></row><row><cell>STMVOS (Oh et al. 2019)</cell><cell></cell><cell></cell><cell>86.5</cell><cell>84.8</cell><cell>88.1</cell><cell>0.16</cell></row><row><cell>RMNet(Y) (Xie et al. 2021)</cell><cell></cell><cell></cell><cell>88.8</cell><cell>88.9</cell><cell>88.7</cell><cell>0.084</cell></row><row><cell>STMVOS(Y) (Oh et al. 2019)</cell><cell></cell><cell></cell><cell>89.3</cell><cell>88.7</cell><cell>89.9</cell><cell>0.16</cell></row><row><cell>CFBI (Y) (Yang, Wei, and Yang 2020)</cell><cell></cell><cell></cell><cell>89.4</cell><cell>88.3</cell><cell>90.5</cell><cell>0.18</cell></row><row><cell>Ours(Y)</cell><cell></cell><cell></cell><cell>90.6</cell><cell>87.1</cell><cell>94.0</cell><cell>0.172</cell></row><row><cell>Ours F R (Y)</cell><cell></cell><cell></cell><cell>91.5</cell><cell>88.3</cell><cell>94.7</cell><cell>0.263</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The authors would like to thank Xiang Li, Zhaoyang Jia, and Linfeng Qi for meaningful discussion. The authors would also like to thank Rex Cheng for sharing his insightful viewpoints about VOS.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="221" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jalali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanghavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Caramanis</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<idno type="arXiv">arXiv:1104.0354</idno>
		<title level="m">Low-rank Matrix Recovery from Errors and Erasures</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Blazingly fast video object segmentation with pixelwise metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1189" to="1198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modular Interactive Video Object Segmentation: Interaction-to-Mask, Propagation and Difference-Aware Fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07941</idno>
		<title level="m">Modular Interactive Video Object Segmentation: Interaction-to-Mask, Propagation and Difference-Aware Fusion</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast and accurate online video object segmentation via tracking parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7415" to="7424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Segflow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="686" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Aarabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.08833</idno>
		<title level="m">SSTVOS: Sparse Spatiotemporal Transformers for Video Object Segmentation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Relations between entropy and error probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Merhav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="259" to="266" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Estimating Information Flow in Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Goldfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greenewald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Melnyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Polyanskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>Chaudhuri, K.</editor>
		<editor>and Salakhutdinov, R.</editor>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="2299" to="2308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning Position and Target Consistency for Memory-based Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04329</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MaskRNN: Instance Level Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Videomatch: Matching based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="54" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A generative appearance model for end-to-end video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brissman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8953" to="8962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lucid data dreaming for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1175" to="1197" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01695</idno>
		<title level="m">Hybrid Instanceaware Temporal Fusion for Online Video Instance Segmentation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Delving into the Cyclic Mechanism in Semi-supervised Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Larochelle, H.</editor>
		<editor>Ranzato, M.</editor>
		<editor>Hadsell, R.</editor>
		<editor>Balcan, M. F.</editor>
		<editor>and Lin, H.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1218" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">WaterNet: An adaptive matching pipeline for segmenting water with volatile appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video Object Segmentation with Adaptive Feature Bank and Uncertain-Region Refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video Object Segmentation with Adaptive Feature Bank and Uncertain-Region Refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Larochelle, H.</editor>
		<editor>Ranzato, M.</editor>
		<editor>Hadsell, R.</editor>
		<editor>Balcan, M. F.</editor>
		<editor>and Lin, H.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3430" to="3441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Agss-vos: Attention guided single-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3949" to="3957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Video object segmentation with episodic graph memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07020</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Premvos: Proposal-generation, refinement and merging for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="565" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.03679</idno>
		<title level="m">Joint Inductive and Transductive Learning for Video Object Segmentation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by reference-guided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7376" to="7385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9226" to="9235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2663" to="2672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="724" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Kernelized Memory Network for Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="629" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.11404</idno>
		<title level="m">Hierarchical Memory Matching Network for Video Object Segmentation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3899" to="3908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Attention is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Curran Associates Inc</publisher>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="6000" to="6010" />
			<pubPlace>Red Hook, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>ISBN 9781510860964</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Feelvos: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9481" to="9490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09364</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04604</idno>
		<title level="m">SwiftNet: Real-time Video Object Segmentation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semi-supervised video object segmentation with supertrajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="985" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ranet: Ranking attention network for fast video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3978" to="3987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Memory Selection Network for Video Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Efficient Regional Memory Network for Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12934</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Youtube-vos: Sequence-to-sequence video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="585" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dynamic video segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6556" to="6565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Efficient Video Object Segmentation via Network Modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<ptr target="https://github.com/z-x-yang/CFBI" />
		<title level="m">CFBI github repo</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Collaborative video object segmentation by foreground-background integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="332" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2010.06349</idno>
		<title level="m">Collaborative Video Object Segmentation by Multi-Scale Foreground-Background Integration</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Gated channel transformation for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11794" to="11803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dmm-net: Differentiable mask-matching network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3929" to="3938" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
