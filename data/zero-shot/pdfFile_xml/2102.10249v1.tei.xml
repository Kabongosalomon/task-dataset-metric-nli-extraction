<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Entity Structure Within and Throughout: Modeling Mention Dependencies for Document-Level Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benfeng</forename><surname>Xu</surname></persName>
							<email>benfeng@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhu</surname></persName>
							<email>zhuyong@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
							<email>zdmao@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Entity Structure Within and Throughout: Modeling Mention Dependencies for Document-Level Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Entities, as the essential elements in relation extraction tasks, exhibit certain structure. In this work, we formulate such structure as distinctive dependencies between mention pairs. We then propose SSAN, which incorporates these structural dependencies within the standard self-attention mechanism and throughout the overall encoding stage. Specifically, we design two alternative transformation modules inside each self-attention building block to produce attentive biases so as to adaptively regularize its attention flow. Our experiments demonstrate the usefulness of the proposed entity structure and the effectiveness of SSAN. It significantly outperforms competitive baselines, achieving new state-of-the-art results on three popular document-level relation extraction datasets. We further provide ablation and visualization to show how the entity structure guides the model for better relation extraction. Our code is publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction aims at discovering relational facts from raw texts as structured knowledge. It is of great importance to many real-world applications such as knowledge base construction, question answering, and biomedical text analysis. Although early studies mainly limited this problem under an intra-sentence and single entity pair setting, many recent works have made efforts to extend it into documentlevel texts <ref type="bibr" target="#b10">(Li et al. 2016a;</ref><ref type="bibr" target="#b34">Yao et al. 2019)</ref>, making it a more practical but also more challenging task. Document-level texts entail a large quantity of entities defined over multiple mentions, which naturally exhibit meaningful dependencies in between. <ref type="figure" target="#fig_0">Figure 1</ref> gives an example from the recently proposed document-level relation extraction dataset DocRED <ref type="bibr" target="#b34">(Yao et al. 2019)</ref>, which illustrates several mention dependencies: 1) Coming Down Again and the Rolling Stones that both reside in the 1st sentence are closely related, so we can identify R1: Performer (blue link) based on their local context; 2) Coming Down Again from the 1st sentence, It from the 2nd sentence, and The song from the 5th sentence refer to the same entity (red link), so it is necessary to consider and reason with them together; 3) the Rolling Stones from the 1st sentence and Mick Jagger from the 2nd sentence, though not display direct connections, can be associated via two coreferential mentions: Coming Down Again and it, which is essential to predict the target relation R2: Member of (green link) between the two entities. Similar dependency also exists between the Rolling Stones and Nicky Hopkins, which helps identify R3: Member of between them. Intuitively, such dependencies indicate rich interactions among entity mentions, and thereby provide informative priors for relation extraction.</p><p>Many previous works have tried to exploit such entity structure, in particular the coreference dependency. For example, it is a commonly used trick to simply encode coreferential information as extra features, and integrate them into the initial input word embeddings. <ref type="bibr" target="#b29">Verga, Strubell, and McCallum (2018)</ref> propose an adapted version of multiinstance learning to aggregate the predictions from coreferential mentions. Others also directly apply average pooling to the representations of coreferential mentions <ref type="bibr" target="#b34">(Yao et al. 2019)</ref>. In summary, these heuristic techniques only use entity dependencies as complementary evidence in the pre-or post-processing stage, and thus bear limited modeling ability. Besides, most of them fail to include other meaningful dependencies in addition to coreference.</p><p>More recently, graph-based methods have shown great advantage in modeling entity structure <ref type="bibr" target="#b22">(Sahu et al. 2019;</ref><ref type="bibr" target="#b2">Christopoulou, Miwa, and Ananiadou 2019;</ref><ref type="bibr" target="#b14">Nan et al. 2020)</ref>. Typically, these methods rely on a general-purpose encoder, usually LSTM, to first obtain contextual representations of an input document. Then they introduce entity structure by constructing a delicately designed graph, where entity representations are updated accordingly through propagation. This kind of approach, however, isolates the context reasoning stage and structure reasoning stage due to the heterogeneity between the encoding network and graph network, which means the contextual representations cannot benefit from structure guidance in the first place.</p><p>Instead, we argue that structural dependencies should be incorporated within the encoding network and throughout the overall system. To this end, we first formulate the aforementioned entity structure under a unified framework, where we define various mention dependencies that cover the interactions in between. We then propose SSAN (Structured Self-Attention Network), which is equipped with a novel extension of self-attention mechanism <ref type="bibr" target="#b28">(Vaswani et al. 2017</ref>), to effectively model these dependencies within its building blocks and through all network layers bottom-to-up. Note that although this paper only focus on entity structure for document-level relation extraction, the method developed here is readily applicable to all kinds of Transformer-based pretrained language models to incorporate any structural dependencies.</p><p>To demonstrate the effectiveness of the proposed approach, we conduct comprehensive experiments on Do-cRED <ref type="bibr" target="#b34">(Yao et al. 2019</ref>), a recently proposed entity-rich document-level relation extraction dataset, as well as two biomedical domain datasets, namely CDR <ref type="bibr" target="#b10">(Li et al. 2016a)</ref> and GDA <ref type="bibr" target="#b33">(Wu et al. 2019</ref>). On all three datasets, we observe consistent and substantial improvements over competitive baselines, and establish the new state-of-the-art. Our contribution can be summarized as follows: ? We summarize various kinds of mention dependencies exhibited in document-level texts into a unified framework. By explicitly incorporating such structure within and throughout the encoding network, we are able to perform context reasoning and structure reasoning simultaneously and interactively, which brings substantially improved performance on relation extraction tasks. ? We propose SSAN that extends the standard self-attention mechanism with structural guidance. ? We achieve new state-of-the-art results on three document-level relation extraction datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>This section elaborates on our approach. We first formalize entity structure in section 2.1, then detail the proposed SSAN model in section 2.2 and section 2.3, and finally introduce its application to document-level relation extraction in section 2.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Entity Structure</head><p>Entity structure describes the distribution of entity instances over texts and the dependencies among them. In the specific scenario of document-level texts, we consider the following two structures.</p><p>? Co-occurrence structure: Whether or not two mentions reside in the same sentence. ? Coreference structure: Whether or not two mentions refer to the same entity.</p><p>Both structures can be described as True or False. For cooccurrence structure, we segment documents into sentences, and take them as minimum units that exhibit mention interactions. So True or False distinguishes intra-sentential interactions which depend on local context from intersentential ones that require cross sentence reasoning. We denote them as intra and inter respectively. For coreference structure, True indicates that two mentions refer to the same entity and thus should be investigated and reasoned with together, while False implies a pair of distinctive entities that are possibly related under certain predicates. We denote them as coref and relate respectively. In summary, these two structures are mutually orthogonal, resulting in four distinctive and undirected dependencies, as shown in table 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coreference True False</head><p>Co-occurence True intra+coref intra+relate False inter+coref inter+relate Besides the dependencies between entity mentions, we further consider another type of dependency between entity mentions and its intra-sentential non-entity (NE) words. We denote it as intraNE. For other inter-sentential non-entity words, we assume there is no crucial dependency, and categorize it as NA. The overall structure is thus formulated into an entity-centric adjacency matrix with all its elements from a finite dependency set: {intra+coref, inter+coref, in-tra+relate, inter+relate, intraNE, NA} (see <ref type="figure">figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SSAN</head><p>SSAN inherits the architecture of Transformer <ref type="bibr" target="#b28">(Vaswani et al. 2017</ref>) encoder, which is a stack of identical building blocks, wrapped up with feedforward network, residual connection, and layer normalization. As its core component, we propose structured self-attention mechanism with two alternative transformation modules.</p><p>Given an input token sequence x = (x 1 , x 2 , ..., x n ), following the above formulation, we introduce S = {s ij } to represent its structure, where i, j ? {1, 2, ..., n} and s ij ?{intra+coref, inter+coref, intra+relate, inter+relate, intraNE, NA} is a discrete variable denotes the dependency from x i to x j . Note that here we extend dependency from mention-level to token-level for practical implementation. If <ref type="figure">Figure 2</ref>: The overall architecture of SSAN. Left illustrates structured self-attention as its basic building block. Right explains our entity structure formulation. This minimum example consists of two sentences: S1, S2, and three entities: E1, E2 and E3. N denotes non-entity tokens. Element in row i and column j represents the dependency from query token x i to key token x j , we distinguish dependencies using different colors. mention instance consists of multiple subwords (E3 in figure 2, S2), we assign dependencies for each token accordingly. Within each mention, subword pairs should conform with intra+coref and thus are assigned as such.</p><p>In each layer l, the input representation x l i ? R din is first projected into query / key / value vector respectively:</p><formula xml:id="formula_0">q l i = x l i W Q l , k l i = x l i W K l , v l i = x l i W V l (1) where W Q l ,W K l ,W V l ? R din?dout .</formula><p>Based on these inputs and entity structure S, we compute unstructured attention score and structured attentive bias, and then aggregate them together to guide the final self-attention flow.</p><p>The unstructured attention score is produced by query-key product as in standard self-attention:</p><formula xml:id="formula_1">e l ij = q l i k l j T ? d<label>(2)</label></formula><p>Parallel to it, we employ an additional module to model the structural dependency conditioned on their contextualized query / key representations. We parameterize it as transformations which project s ij along with query vector q l i and key vector k l j into attentive bias, then impose it upon e l ij :</p><formula xml:id="formula_2">e l ij = e l ij + transf ormation(q l i , k l j , s ij ) ? d<label>(3)</label></formula><p>The proposed transformation module regulates the attention flow from x i to x j . As a consequence, the model benefits from the guidance of structural dependencies.</p><p>After we obtain the regulated attention scores? l ij , a softmax operation is applied, and the value vectors are aggre-gated accordingly: <ref type="figure">Figure 2</ref> gives the overview of SSAN. In the next section, we describe the transformation module.</p><formula xml:id="formula_3">z l+1 i = n j=1 exp? l ij n k=1 exp? l ik v l j (4) here z l+1 i ? R dout is the updated contextual representation of x l i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Transformation Module</head><p>To incorporate the discrete structure s ij into an end-to-end trainable deep model, we instantiate each s ij as neural layers with specific parameters, train and apply them in a compositional fashion. As a result, for each input structure S composed of s ij , we have a structured model composed of corresponding layer parameters. As for the specific design of these neural layers, we propose two alternatives: Biaffine Transformation and Decomposed Linear Transformation:</p><formula xml:id="formula_4">bias l ij = Biaf f ine(s ij , q l i , k l j ) or = Decomp(s ij , q l i , k l j )<label>(5)</label></formula><p>Biaffine Transformation Biaffine Transformation computes the bias as:</p><formula xml:id="formula_5">bias l ij = q l i A l,sij k l j T + b l,sij<label>(6)</label></formula><p>here we parameterize dependency s ij as trainable neural layer A l,sij ? R dout?1?dout , which attends to the query and key vector simultaneously and directionally, and projects them into a single-dimensional bias. As for the second term b l,sij , we directly model prior bias for each dependency independent to its context. Decomposed Linear Transformation Inspired by how <ref type="bibr" target="#b3">Dai et al. (2019)</ref> decompose the word embedding and position embedding in Transformer, we propose to introduce bias upon query and key vectors respectively, the bias is thus decomposed as:</p><formula xml:id="formula_6">bias l ij = q l i K T l,sij + Q l,sij k l j T + b l,sij<label>(7)</label></formula><p>where K l,sij ,Q l,sij ? R d are also trainable neural layers. Intuitively, these three terms respectively represent: 1) bias conditioned on query token representation, 2) bias conditioned on key token representation, and 3) prior bias. So the overall computation of structured self-attention is:</p><formula xml:id="formula_7">e l ij = q l i k l j T + transf ormation(q l i , k l j , s ij ) ? d = q l i k l j T + q l i A l,sij k l j T + b l,sij ? d or = q l i k l j T + q l i K T l,sij + Q l,sij k l j T + b l,sij ? d<label>(8)</label></formula><p>As these transformation layers model structural dependencies adaptively according to context, we do not share them across different layers or different attention heads. Previously, <ref type="bibr" target="#b24">Shaw, Uszkoreit, and Vaswani (2018)</ref> have proposed to model relative position information of input token pair within the Transformer. They first map the relative distance into embedding, then add them with key vectors before computing the attention score. Technically, such design can be seen as a simplified version of our Decomposed Linear Transformation, with query conditioned bias only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">SSAN for Relation Extraction</head><p>The proposed SSAN model takes document text as input, and builds its contextual representations under the guidance of entity structure within and throughout the overall encoding stage. In this work, we simply use it for relation extraction with minimum design. After the encoding stage, we construct a fixed dimensional representation for each target entity via average pooling, which we denote as e i ? R de . Then, for each entity pair, we compute the probability of relation r from the pre-specified relation schema as:</p><formula xml:id="formula_8">P r (e s , e o ) = sigmoid(e s W r e o )<label>(9)</label></formula><p>where W r ? R de?de . The model is trained using cross entropy loss: 3 Experimental Setup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We evaluate the proposed approach on three popular document-level relation extraction datasets, namely Do-cRED <ref type="bibr" target="#b34">(Yao et al. 2019)</ref>, CDR <ref type="bibr" target="#b10">(Li et al. 2016a</ref>) and GDA <ref type="bibr" target="#b33">(Wu et al. 2019)</ref>, all involving challenging relational reasoning over multiple entities across multiple sentences. We summarize their information in Appendix A.</p><p>DocRED DocRED is a large scale dataset constructed from Wikipedia and Wikidata. It provides comprehensive human annotations including entity mentions, entity types, relational facts, and the corresponding supporting evidence. There are 97 target relations in total and approximately 26 entities on average in each document. The data scale is 3053 documents for training, 1000 for development set, and 1000 for test. Besides, DocRED also collects distantly supervised data for alternative research. It utilizes a finetuned BERT model to identify entities and link them to Wikidata. Then the relation labels are obtained via distant supervision, producing 101873 document instances at scale.</p><p>CDR The Chemical-Disease Reactions dataset is a biomedical dataset constructed using PubMed abstracts. It contains 1500 human-annotated documents in total that are equally split into training, development, and test sets. CDR is a binary classification task that aims at identifying induced relation from chemical entity to disease entity, which is of significant importance to biomedical research.</p><p>GDA Like CDR, the Gene-Disease Associations dataset is also a binary relation classification task that identify Gene and Disease concepts interactions, but with a much more massive scale constructed by distant supervision using MEDLINE abstracts. It consists of 29192 documents as the training set and 1000 as the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pretrained Transformers</head><p>We initialize SSAN with different pretrained language models including BERT <ref type="bibr" target="#b4">(Devlin et al. 2019)</ref>, RoBERTa  and SciBERT <ref type="bibr" target="#b1">(Beltagy, Lo, and Cohan 2019)</ref>.</p><p>BERT BERT is one of the first works that find the success of Transformer in pretraining language models on large scale corpora. Specifically, it is pretrained using Masked Language Model and Next Sentence Prediction on BooksCorpus and Wikipedia. BERT is pretrained under two configurations, Base and Large, respectively contains 12 and 24 self-attention layers. It can be easily finetuned on various downstream tasks, producing competitive baselines.</p><p>RoBERTa RoBERTa is an optimized version of BERT, which removes the Next Sentence Prediction task and adopts way larger text corpora as well as more training steps. It is currently one of the superior pretrained language models that outperforms BERT in various downstream NLP tasks.</p><p>SciBERT SciBERT adopts the same model architecture as BERT, but is trained on scientific text instead. It demonstrates considerable advantage in a series of scientific domain tasks. In this paper, we provide SciBERT-initialized SSAN on the two biomedical domain datasets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation Detail</head><p>On each dataset, we give comprehensive results of SSAN initialized with different pretrained language models along with their corresponding baselines for fair comparisons. The parameters in newly introduced transformation modules are learned from scratch. All results are obtained using grid search for hyper-parameters (see appendix B for detail) on the development set, then the best model is selected to produce results on the test set. On DocRED, following the official baseline implementation <ref type="bibr" target="#b34">(Yao et al. 2019</ref>), we utilize naive features including entity type and entity coreference, which is added to the input word embedding. We also concatenate entity relative distance embedding of each entity pair before the final classification. We preprocess CDR and GDA dataset following <ref type="bibr" target="#b2">Christopoulou, Miwa, and Ananiadou (2019)</ref>. On CDR, after the best hyper-parameter is set, we merge the training set and dev set to train the final model, on GDA, we split 20% of the training set for development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DocRED Results</head><p>We conduct comprehensive and comparable experiments on DocRED dataset. We report both F1 and Ign F1 according     <ref type="table">Table 6</ref>: Ablation for bias terms of two transformation modules on DocRED dev set. Refer to equation 6 and equation 7 for specifics, we have removed the layer index l because the ablation is implemented across all layers.</p><p>ability to model structural dependencies. We compare our model with previous works that either do not consider entity structure or do not explicitly model them within and throughout encoders. Specifically, ContexAware <ref type="bibr" target="#b34">(Yao et al. 2019)</ref>, BERT Two-Phase <ref type="bibr" target="#b30">(Wang et al. 2019a</ref>) and HINBERT <ref type="bibr" target="#b27">(Tang et al. 2020</ref>) do not consider the structural dependencies among entities. EOG  and LSR <ref type="bibr" target="#b14">(Nan et al. 2020</ref>) utilize graph methods to perform structure reasoning, but only after the BiLSTM or BERT encoder. CorefBERT and CorefRoBERTa <ref type="bibr" target="#b36">(Ye et al. 2020</ref>) further pretrain BERT and RoBERTa with a coreference prediction task to enable implicit reasoning of coreference structure. Results in table 2 shows that SSAN performs better than these methods. Our best model, SSAN Biaffine built upon RoBERTa Large, is +2.41 / +1.79 Ign F1 better on dev / test set than CorefRoBERTa Large <ref type="bibr" target="#b36">(Ye et al. 2020)</ref>, and +1.80 / +1.04 Ign F1 better than our baseline. In general, these results demonstrate both the usefulness of entity structure and the effectiveness of SSAN.</p><p>Although SSAN is well compatible with pretrained Transformer models, there still exists a distribution gap between parameters in newly introduced transformation layers and those already pretrained ones, thus impedes the improvements of SSAN to a certain extent. In order to alleviate such distribution deviation, we also utilize the distantly supervised data from DocRED, which shares identical format with the trainset, to first pretrain SSAN before finetuning on the annotated training set for better adaptation. Here we choose our best model, SSAN Biaffine built upon RoBERTa Large, and denote it as +Adaptation in table 2 (see appendix B for hyperparameters setting). The resulting performance are greatly improved, achieving 63.78 Ign F1 and 65.92 F1 on test set as well as the 1st position on the leaderboard 3 at the time of submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CDR and GDA Results</head><p>On CDR and GDA datasets, besides BERT, we also adopts SciBERT for its superiority when dealing with biomedical domain texts. On CDR test set (see <ref type="table" target="#tab_4">Table 3</ref>), SSAN obtains +1.3 F1/+1.7 F1 gain based on BERT Base/Large and +2.9 F1 gain based on SciBERT, which significantly outperform the baselines and all existing works. On GDA (see <ref type="table" target="#tab_5">Table 4</ref>), similar improvements can also be observed. These results demonstrate the strong applicability and generality of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>We perform ablation studies of the proposed approach on DocRED. Again, we consider SSAN Biaffine built upon RoBERTa Large. <ref type="table" target="#tab_6">Table 5</ref> gives the results of SSAN when each structural dependency is excluded. It is clear that all five dependencies contribute to the final improvements. We can arrive at the conclusion that the proposed entity structure formulation is indeed helpful priors for document-level relation extraction. We can also see that intra+coref effects the most among all dependencies.</p><p>We also look into the design of two transformation modules by testing each bias term respectively. As shown in table 6, all bias terms can improve the result over baseline, including the prior bias +b sij that is only individual values. Among all bias terms, biaffine bias +q i A sij k T j is the most effective, brings +1.38 Ign F1 improvements solely. For Decomposed Linear Transformation, key conditioned bias +Q sij k T j produces better results than query conditioned bias +q i K T sij , which implies that the key vectors might be associated with more entity structure information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Visualization of Attentive Biases</head><p>As a key feature of SSAN is to formulate entity structure priors into attentive biases, it would be instructive to explore how such attentive biases regulate the propagation of selfattention bottom-to-up. To this purpose, we collect all attentive biases produced by SSAN Biaffine (built upon RoBERTa Large) for DocRED dev instances, categorized according to dependency types, and averaged across all attention heads and all instances. <ref type="figure" target="#fig_2">Figure 3 (a)</ref> is the resultant heatmap, where each cell indicates the value of averaged bias at each layer (horizontal axis) for each entity dependency type (vertical axis). We can observe meaningful patterns: 1) Along the horizontal axis, the bias is relatively small at bottom layers, where the self-attention score will be mainly decided by unstructured semantic contexts. It then grows gradually and reaches the maximum at the top-most layers, where the self-attention score will be greatly regulated by the structural priors. 2) Along the vertical axis, at the top-most layers (inside the dotted bounding box), bias from inter+coref is significantly positive. This conforms with human intuition that coreferential mention pairs might act as a bridge for cross-sentence reasoning, thus should enable more information passing. While biases from intra+relate and in-ter+relate appear in contrast.</p><p>Based on the discussion, we further investigate the effect of different layers to impose attentive biases. As shown in <ref type="figure" target="#fig_2">Figure 3 (b)</ref>, with only the top 4 layers (1/6 of the total layers) integrated with entity structure, SSAN can keep +0.89 F1 gain, which confirms that these top-most layers with larger biases indeed impact more significantly. In the meantime, with more layers included, the performance still improves, and reaches the best of +1.50 F1 with all 24 layers equipped with structured self-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Document-level RE Recent years have seen growing interests for relation extraction beyond single sentence <ref type="bibr" target="#b20">(Quirk and Poon 2017;</ref><ref type="bibr" target="#b17">Peng et al. 2017a</ref>). Among the most influential works, many have proposed to introduce intra-sentential and inter-sentential syntactic dependencies <ref type="bibr" target="#b18">(Peng et al. 2017b;</ref><ref type="bibr" target="#b26">Song et al. 2018;</ref><ref type="bibr" target="#b7">Gupta et al. 2019)</ref>. More recently, document-level relation extraction tasks have been proposed <ref type="bibr" target="#b10">(Li et al. 2016a;</ref><ref type="bibr" target="#b34">Yao et al. 2019)</ref>, where the goal is to identify relations of multiple entity pairs from the entire document text, and rich entity interactions are thereby involved. In order to model these interactions, many graph based methods are proposed <ref type="bibr" target="#b22">(Sahu et al. 2019;</ref><ref type="bibr" target="#b2">Christopoulou, Miwa, and Ananiadou 2019;</ref><ref type="bibr" target="#b14">Nan et al. 2020</ref>). However, these graph networks are built upon their contextual encoder, which is different from our approach that model entity interactions within and throughout the system. Entity Structure Entity structure has been shown to be useful in many NLP tasks. In early works, <ref type="bibr" target="#b0">Barzilay and Lapata (2008)</ref> propose an entity-grid representation for discourse analysis, where the document is summarized into a set of entity transition sequences that record distributional, syntactic, and referential information. <ref type="bibr" target="#b8">Ji et al. (2017)</ref> introduce a set of symbolic variables and state vectors to encode the mentions and their coreference relationships for language modeling task. <ref type="bibr" target="#b5">Dhingra et al. (2018)</ref> propose Coref-GRU, which incorporates mention coreference information for reading comprehension tasks. In general, many works have utilized entity structure in various formulation for different tasks.</p><p>For document-level relation extraction, entity structure also is essential prior. For example, <ref type="bibr">Verga, Strubell, and Mc-Callum (2018)</ref> propose to merge predictions from coreferential mentions. <ref type="bibr" target="#b14">Nan et al. (2020)</ref> propose to model entity interactions via latent structure reasoning. And Christopoulou, Miwa, and Ananiadou (2019) construct a graph of mention nodes, entity nodes, and sentence nodes, then connect them using mention-mention coreference, mention-sentence residency etc., such design provides much more comprehensive entity structure information. Based on the graph, they further utilize an edge-oriented method to iteratively refine the relation representation between target entity pairs, which is quite different from our approach.</p><p>Structured Networks Neural networks that incorporate structural priors have been extensively explored. In previous works, many have investigated how to infuse the treelike syntax structure into the classical LSTM encoder <ref type="bibr" target="#b9">(Kim et al. 2017;</ref><ref type="bibr" target="#b25">Shen et al. 2019;</ref><ref type="bibr" target="#b18">Peng et al. 2017b</ref>). For Transformer encoder, it is also a challenging and thriving research direction. <ref type="bibr" target="#b24">Shaw, Uszkoreit, and Vaswani (2018)</ref> propose to incorporate relative position information of input tokens in the form of attentive bias, which inspired part of this work. <ref type="bibr" target="#b31">Wang et al. (2019b)</ref> further extend this method to relation extraction task, where the relative position is adjusted into entity-centric form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this work, we formalize entity structure for documentlevel relation extraction. Based on it, we propose SSAN to effectively incorporate such structural priors, which performs both contextual reasoning and structure reasoning of entities simultaneously and interactively. The resulting performance on three datasets demonstrates the usefulness of entity structure and the effectiveness of the SSAN model.</p><p>For future works, we give two promising directions: 1) apply SSAN to more tasks such as reading comprehension, where the structure of entities or syntax is useful prior information. 2) extend the entity structure formulation to include more meaningful dependencies, such as more complex interactions based on discourse structure.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example excerpted from DocRED. Different mention dependencies are distinguished by colored edges, with the target relations listed in below.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>r (e s , e o ), y r (e s , e o )) (10) and y is the target label. Given N entities and a relation schema of size M , equation 9 should be computed N ? N ? M times to give all predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a): Visualization on the learned attentive bias from different layers and different mention dependencies. Results are averaged over the entire dev set and different attention heads. (b): Ablation on number of layers to impose attentive biases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The formulation of entity structure.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Results on DocRED. Subscript Decomp and Biaffine refer to Decomposed Linear Transformation and Biaffine Transformation. Test results are obtained by submitting to official Codalab. Result with* is from Nan et al. (2020).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results on CDR dev set and test set.</figDesc><table><row><cell>Model</cell><cell>Dev F1</cell><cell>Test F1</cell><cell>Intra-/ Inter-Test F1</cell></row><row><cell>EoG (2019)</cell><cell>78.7</cell><cell>81.5</cell><cell>85.2 / 49.3</cell></row><row><cell>LSR (2020)</cell><cell>-</cell><cell>79.6</cell><cell>83.1 / 49.6</cell></row><row><cell>LSR w/o MDP (2020)</cell><cell>-</cell><cell>82.2</cell><cell>85.4 / 51.1</cell></row><row><cell>BERT Base Baseline</cell><cell>79.8</cell><cell>81.2</cell><cell>84.7 / 60.3</cell></row><row><cell>SSAN Decomp</cell><cell>81.5</cell><cell>83.4</cell><cell>86.7 / 62.3</cell></row><row><cell>SSAN Biaffine</cell><cell>81.6</cell><cell>82.1</cell><cell>86.1 / 56.8</cell></row><row><cell>BERT Large Baseline</cell><cell>80.4</cell><cell>81.6</cell><cell>84.9 / 61.5</cell></row><row><cell>SSAN Decomp</cell><cell>82.0</cell><cell>83.8</cell><cell>86.6 / 65.0</cell></row><row><cell>SSAN Biaffine</cell><cell>82.2</cell><cell>83.9</cell><cell>86.9 / 63.9</cell></row><row><cell>SciBERT Baseline</cell><cell>81.4</cell><cell>83.6</cell><cell>87.2 / 61.8</cell></row><row><cell>SSAN Decomp</cell><cell>82.5</cell><cell>83.2</cell><cell>87.0 / 60.0</cell></row><row><cell>SSAN Biaffine</cell><cell>82.8</cell><cell>83.7</cell><cell>86.6 / 65.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results on GDA dev set and test set.to<ref type="bibr" target="#b34">Yao et al. (2019)</ref>. Ign F1 is computed by excluding relational facts that already appeared in the training set.As shown in table 2, SSAN with both Biaffine and Decomp transformation can consistently outperform their baselines with considerable margin. In most of the results, Biaffine brings more considerable performance gain compared to Decomp, which demonstrates that the former is of greater</figDesc><table><row><cell>Dependency</cell><cell>Ign F1</cell><cell>F1</cell></row><row><cell>SSAN Biaffine (RoBERTa Large)</cell><cell>60.25</cell><cell>62.08</cell></row><row><cell>? intra+coref</cell><cell>59.59</cell><cell>61.57</cell></row><row><cell>? intra+relate</cell><cell>59.92</cell><cell>61.91</cell></row><row><cell>? inter+coref</cell><cell>59.87</cell><cell>61.74</cell></row><row><cell>? inter+relate</cell><cell>59.92</cell><cell>61.84</cell></row><row><cell>? intraNE</cell><cell>59.96</cell><cell>61.97</cell></row><row><cell>? all</cell><cell>58.45</cell><cell>60.58</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation for entity structure formulation on Do-cRED dev set. Results when each dependency is excluded, and "-all" degenerates to RoBERTa Large baseline.</figDesc><table><row><cell>Bias Term</cell><cell>Ign F1</cell><cell>F1</cell></row><row><cell>RoBERTa Large baseline (w/o bias)</cell><cell>58.45</cell><cell>60.58</cell></row><row><cell>+b sij</cell><cell>58.62</cell><cell>60.59</cell></row><row><cell>+Q sij k T j</cell><cell>58.79</cell><cell>60.65</cell></row><row><cell>+q i K T sij</cell><cell>59.26</cell><cell>61.31</cell></row><row><cell>+q i K T sij + Q sij k T j + b sij</cell><cell>59.54</cell><cell>61.50</cell></row><row><cell>+q i A sij k T j</cell><cell>59.83</cell><cell>61.75</cell></row><row><cell>+q i A sij k T j + b sij</cell><cell>60.25</cell><cell>62.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Summary of DocRED, CDR and GDA datasets. For column Mention / Sent, we exclude sentences that do not contain any entity mention.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="2">DocRED</cell><cell>CDR</cell><cell cols="2">GDA</cell></row><row><cell>Model</cell><cell>Base</cell><cell>Large</cell><cell>Distant Pretrain</cell><cell>-</cell><cell>Base</cell><cell>Large</cell></row><row><cell cols="3">learning rate 5e ? 5 3e ? 5</cell><cell>2e ? 5</cell><cell>5e ? 5</cell><cell cols="2">5e ? 5 3e ? 5</cell></row><row><cell>epoch</cell><cell cols="2">{40, 60, 80, 100}</cell><cell>10</cell><cell>{10, 20, 30, 40}</cell><cell cols="2">{2, 4, 6}</cell></row><row><cell>batch size</cell><cell></cell><cell>4</cell><cell></cell><cell>4</cell><cell cols="2">{4, 8}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Hyper-parameters Setting.</figDesc><table /><note>B Hyper-parameters Setting</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8</head><label>8</label><figDesc>details our hyper-parameters setting. All experiment results are obtained using grid search on the development set. All comparable results share the same search scope.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://competitions.codalab.org/competitions/20717#results</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank all anonymous reviewers for their valuable comments. This work is supported by the National Key Research and Development Project of China (No.2018YFB1004300, No.2018AAA0101900), and the National Natural Science Foundation of China (No.61876223, No.U19A2057).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Datasets</head><p>Table 7 details statistics of entities along with other related information of three selected datasets. We can see that all three datasets entail more than two dozen mentions per document on average, with each sentence contains approximately three mentions on average. These statistics further demonstrate the complexity of entity structure in documentlevel relation extraction tasks.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modeling local coherence: An entity-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SciBERT: A Pretrained Language Model for Scientific Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno>doi:10.18653/ v1/D19-1371</idno>
		<ptr target="https://www.aclweb.org/anthology/D19-1371" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3615" to="3620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1498</idno>
		<ptr target="https://www.aclweb.org/anthology/D19-1498" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4925" to="4936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive Language Models beyond a Fixed-Length Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1285</idno>
		<ptr target="https://www.aclweb.org/anthology/P19-1285" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural Models for Reasoning over Multiple Mentions Using Coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2007</idno>
		<ptr target="https://www.aclweb.org/anthology/N18-2007" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="42" to="48" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Chemicalinduced disease relation extraction via convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural relation extraction within and across sentence boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sch?tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Runkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6513" to="6520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic Entity Representations in Neural Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Martschat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1195</idno>
		<ptr target="https://www.aclweb.org/anthology/D17-1195" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1830" to="1839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structured Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HkE0Nvqlg" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">BioCreative V CDR task corpus: a resource for chemical disease relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">CIDExtractor: A chemical-induced disease relation extraction system for biomedical literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="994" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Document-Level Biomedical Relation Extraction Leveraging Pretrained Self-Attention Structure and Entity Replacement: Algorithm and Pretreatment Method Validation Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMIR Medical Informatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">17644</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reasoning with Latent Structure Refinement for Document-Level Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sekulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.141</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.141" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1546" to="1557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for chemical-disease relation extraction are improved with character-based word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Verspoor</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-2314</idno>
		<ptr target="https://www.aclweb.org/anthology/W18-2314" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the BioNLP 2018 workshop</title>
		<meeting>the BioNLP 2018 workshop<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting graph kernels for high performance biomedical relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Panyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Verspoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramamohanarao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical semantics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cross-Sentence N-ary Relation Extraction with Graph LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<idno>2307-387X</idno>
		<ptr target="https://transacl.org/ojs/index.php/tacl/article/view/1028" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="101" to="115" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cross-sentence n-ary relation extraction with graph lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="101" to="115" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving chemical disease relation extraction with rich features and weakly labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cheminformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">53</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distant Supervision for Relation Extraction beyond the Sentence Boundary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1171" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spain</forename><surname>Valencia</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/E17-1110" />
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Inter-sentence Relation Extraction with Document-level Graph Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th</title>
		<meeting>the 57th</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<idno type="DOI">10.18653/v1/P19-1423</idno>
		<ptr target="https://www.aclweb.org/anthology/" />
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="19" to="1423" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Self-Attention with Relative Position Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2074</idno>
		<ptr target="https://www.aclweb.org/anthology/N18-2074" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1l6qiR5F7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">N-ary Relation Extraction using Graph-State LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gildea</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1246</idno>
		<ptr target="https://www.aclweb.org/anthology/D18-1246" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2226" to="2235" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">HIN: Hierarchical Inference Network for Document-Level Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="197" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Simultaneously Self-Attending to All Mentions for Full-Abstract Biological Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1080</idno>
		<ptr target="https://www.aclweb.org/anthology/N18-1080" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Long Papers; New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="872" to="884" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fine-tune Bert for Docred with two-step process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Focke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sylvester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11898</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Extracting Multiple-Relations in One-Pass with Pre-Trained Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Potdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th</title>
		<meeting>the 57th</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<idno type="DOI">10.18653/v1/P19-1132</idno>
		<ptr target="https://www.aclweb.org/anthology/P19-1132" />
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="1371" to="1377" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Renet: A deep learning approach for extracting gene-disease associations from literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-F</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Research in Computational Molecular Biology</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="272" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">DocRED: A Large-Scale Document-Level Relation Extraction Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th</title>
		<meeting>the 57th</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<idno type="DOI">10.18653/v1/P19-1074</idno>
		<ptr target="https://www.aclweb.org/anthology/" />
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="19" to="1074" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.06870</idno>
		<title level="m">Coreferential Reasoning Learning for Language Representation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An effective neural model extracting document level chemical-induced disease relations from biomedical literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
