<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multitask AET with Orthogonal Tangent Regularity for Dark Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziteng</forename><surname>Cui</surname></persName>
							<email>cuiziteng@sjtu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
							<email>guojunq@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Innopeak Technology</orgName>
								<orgName type="institution">Seattle Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gu</surname></persName>
							<email>lin.gu@riken.jp</email>
							<affiliation key="aff2">
								<orgName type="department">RIKEN AIP</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
							<email>s.you@uva.nl</email>
							<affiliation key="aff4">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenghui</forename><surname>Zhang</surname></persName>
							<email>zenghui.zhang@sjtu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
							<email>harada@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff2">
								<orgName type="department">RIKEN AIP</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghai</forename><surname>Jiao</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multitask AET with Orthogonal Tangent Regularity for Dark Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dark environment becomes a challenge for computer vision algorithms owing to insufficient photons and undesirable noise. To enhance object detection in a dark environment, we propose a novel multitask auto encoding transformation (MAET) model which is able to explore the intrinsic pattern behind illumination translation. In a self-supervision manner, the MAET learns the intrinsic visual structure by encoding and decoding the realistic illumination-degrading transformation considering the physical noise model and image signal processing (ISP). Based on this representation, we achieve the object detection task by decoding the bounding box coordinates and classes. To avoid the over-entanglement of two tasks, our MAET disentangles the object and degrading features by imposing an orthogonal tangent regularity. This forms a parametric manifold along which multitask predictions can be geometrically formulated by maximizing the orthogonality between the tangents along the outputs of respective tasks. Our framework can be implemented based on the mainstream object detection architecture and directly trained end-to-end using normal target detection datasets, such as VOC and COCO. We have achieved the state-of-theart performance using synthetic and real-world datasets. Code is available at https://github.com/cuiziteng/MAET.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Low-illumination environment poses significant challenges in computer vision. Computational photography community has proposed many human-vision-oriented algorithms to recover normal-lit images <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b7">8]</ref>. Unfortunately, the restored image does not necessarily benefit the high-level visual understanding tasks. As the enhancement/restoration approaches are optimized for human visual perception, they may generate artifacts (see <ref type="figure" target="#fig_0">Fig. 1</ref> for an example), which are misleading for con-  <ref type="bibr" target="#b27">[28]</ref>, Zhang et al. <ref type="bibr" target="#b51">[51]</ref>, Guo et al. <ref type="bibr" target="#b7">[8]</ref> respectively, on which YOLOv3 failed to make detection. sequent vision tasks.</p><p>Another line of research focuses on the robustness of specific high-level vision algorithms. They either train models on a large volume of real-world data <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b48">48]</ref> or rely on carefully designed task-related features <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>However, existing methods suffer from two major inconsistencies: target inconsistency and data inconsistency (in the existing research). Target inconsistency refers to the fact that most methods focus on their own target, either human vision or machine vision. Each line follows their routes separately without benefiting each other under a general framework.</p><p>In the meantime, data inconsistency complicates the assumption that the training data should resemble the one used for evaluation. For example, pre-trained object detection models are usually trained on clear and normal lit images. To adapt to the poor light condition, they rely on the augmented dark images to fine-tune the models without exploring the intrinsic structure under the illumination variance. Just like Happy families are all alike; every unhappy family is unhappy in its own way, even with the existing datasets <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b15">16]</ref>, the varied distribution of real-world conditions can hardly be covered by the training set.</p><p>Here, we aim to bridge above two gaps under a unified framework. As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, the normal lit image can be parametrically transformed (t deg ) into their degraded low-illumination counterparts. Based on this transformation, we propose a novel multitask autoencoding transformation (MAET) to extract the transformation equivariant convolutional features for object detection in dark images. We train the MAET based on two tasks: <ref type="bibr" target="#b0">(1)</ref> to learn the intrinsic representation by decoding the low-illuminationdegrading transformation based on unlabeled data and <ref type="bibr" target="#b1">(2)</ref> to decode object position and categories based on labeled data. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, we train our MAET to encode the pair of normal lit and low-light images with siamese encoder E and decode its degrading parameters, such as noise level, gamma correction, and white balance gains, using decoder D deg . This allows our model to capture the intrinsic visual structure that is equivariant to illumination variance. Compared with <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b41">41]</ref>, who conducted oversimplified synthesis, we design our degradation model considering the physical noise model of sensors and image signal processing (ISP). Then, we perform the object detection task by decoding the bounding box coordinates and classes with the decoder D obj based on the representation encoded by E ( <ref type="figure" target="#fig_1">Fig. 2)</ref>.</p><p>Although MAET regularizes network training by predicting low-light degrading parameters, the joint training of object detection and transformation decoding are overentangled through a shared backbone network. While this improves the detection of dark objects using MAET regu-larity, it may also risk overfitting the object-level representation into self-supervisory imaging signals. To this end, we propose to disentangle the object detection and transformation decoding tasks by imposing an orthogonal tangent regularity. It assumes that the multivariate outputs of above two tasks form a parametric manifold, and disentangling the multitask outputs along the manifold can be geometrically formulated by maximizing the orthogonality among the tangents along the output of different tasks. The framework can be directly trained end-to-end using standard target detection datasets, such as COCO <ref type="bibr" target="#b22">[23]</ref> and VOC <ref type="bibr" target="#b5">[6]</ref>, and make it detect low-light images. Although we consider YOLOv3 <ref type="bibr" target="#b39">[39]</ref> for illustration, the proposed MAET is a general framework that can be easily applied to other mainstream object detectors, e.g., <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b54">54]</ref>.</p><p>Our contributions to this study are as follows:</p><p>? By exploring physical noise models of sensors and the ISP pipeline, we leverage a novel MAET framework to encode the intrinsic structure, which can decode low-light-degrading transformation. Then, we perform the object detection by decoding bounding box coordinates and categories based on this robust representation. Our MAET framework is compatible with mainstream object architectures.</p><p>? Moreover, we present the disentangling of multitask outputs to avoid the overfitting of the learned objectdetection features into the self-supervisory degrading parameters. This can be naturally performed from a geometric perspective by maximizing the orthogonality along the tangents corresponding to the output of different tasks.</p><p>? Based on comprehensive evaluation and compared with other methods, our method shows superior performance pertaining to low-light object detection tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Low Illumination Datasets</head><p>Several datasets have been proposed for the low-light object detection task: Neumann et al. <ref type="bibr" target="#b30">[31]</ref> proposed NightOwls dataset for pedestrians detection in the night. Nada et al. <ref type="bibr" target="#b29">[30]</ref> collected an unconstrained face detection dataset (UFDD) considering various adverse conditions, such as rain, snow, haze, and low illumination. In recent times, the UG 2 + challenge <ref type="bibr" target="#b48">[48]</ref> has included several tracks for vision tasks under different poor visibility environments. Among them, DARK FACE dataset with 10,000 images (includes 6,000 labeled and 4,000 unlabeled images). For the multi-class dark object detection task, Loh et al. <ref type="bibr" target="#b24">[25]</ref> proposed exclusively dark (ExDark) dataset, which includes 7363 images with 12 object categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Low-Light Vision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Enhancement and Restoration Methods</head><p>Low-light vision tasks focus on the human visual experience by restoring details and correcting the color shift. Early attempts are either Retinex theory based approaches <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b8">9]</ref> or histogram equalization (HE) based approaches <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b18">19]</ref>. Nowadays, with the development of deep learning, CNN based methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b7">8]</ref> and GAN based methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b11">12]</ref> have achieved a significant improvement in this task. Like Wei et al. <ref type="bibr" target="#b46">[46]</ref> combined the Retinex theory <ref type="bibr" target="#b17">[18]</ref> with deep network for low-light image enhancement. Jiang et al. <ref type="bibr" target="#b11">[12]</ref> used an unsupervised GAN to solve this problem. Very recently, Guo et al. <ref type="bibr" target="#b7">[8]</ref> proposed a self-supervised method, which could learn without normal light images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">High-Level Task</head><p>To adopt the high-level task for a dark environment, a straightforward strategy is casting the aforementioned enhancement methods as a post-processing step <ref type="bibr" target="#b53">[53,</ref><ref type="bibr" target="#b7">8]</ref>. Other ones rely on augmented real-world data <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b47">47]</ref> or some oversimplified synthetic data <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b41">41]</ref>.Recent real noisy image benchmarks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b35">35]</ref> show that sometimes hand-crafted algorithms may even outperform deep learning models. To combine the strength of computational photography, we develop a framework with transformationequivariant representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Transformation-Equivariant Representation Learning</head><p>Several self-supervised representation learning methods have been proposed to learn image features either through solving Jigsaw Puzzles <ref type="bibr" target="#b32">[33]</ref> or impainting the missing region of an image <ref type="bibr" target="#b34">[34]</ref>. Recently, a series of auto-encoding transformations (AETs), such as AET <ref type="bibr" target="#b50">[50]</ref>, AVT <ref type="bibr" target="#b37">[37]</ref>, EnAET <ref type="bibr" target="#b45">[45]</ref>, have demonstrated state-of-the-art performances for several self-supervised tasks. As the AET is flexible and not restricted to any specific convolutional structure, we extend it to our multitask AET for object detection in dark images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multitask Autoencoding Transformation (MAET)</head><p>In this section, we first briefly introduce auto-encoding transformation (AET) <ref type="bibr" target="#b50">[50]</ref>, based on which we propose multitask AET (MAET). Then, we discuss the ISP pipeline in camera to design the degrading transformations to be leveraged by our MAET. Finally, we explain the MAET architecture and training and testing details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Background: From AET to MAET</head><p>AET <ref type="bibr" target="#b50">[50]</ref> learns representative latent features that decode or recover the parameterized transformation from the original image (x) and the transformed counterpart (t(x)) based on transformation t:</p><formula xml:id="formula_0">x T ? ?? ? t(x).<label>(1)</label></formula><p>The AET comprises a siamese representation encoder (E) and a transformation decoder (D). The encoder E extracts features from x and its transformation t(x), which should capture intrinsic visual structures to explain the transformation t (e.g., the low-illumination degrading transformations in the next section). Then, the decoder D uses the encoded E(x) and E(t(x)) to decode the estimationt for t:</p><formula xml:id="formula_1">t = D ? [E(x), E(t(x))].<label>(2)</label></formula><p>The AET, specifically the representation encoder E and transformation decoder D, can be trained by minimizing the deviation loss ? of the original transform t and the predicted resultt:</p><formula xml:id="formula_2">L aet ? k ? k (t k , t k ),<label>(3)</label></formula><p>where ? k denotes type k transformation loss computed using the mean-squared error (MSE) loss between the predicted transformationt k and ground truth transformation t k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-Task AET with Orthogonal Regularity</head><p>In this study, we further extend the AET to the MAET by simultaneously solving multiple tasks. As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, the proposed MAET model consists of two parts: a representation encoder (E) and multi-task decoders. For the task of illumination-degrading transformation t deg , we use decoder D deg to decode the degrading parameters. The task of object detection is realised by the decoder D obj to predict the bounding box location and object categories directly from illumination-degenerated images. Although the two tasks are correlated, their outputs reflect very different aspects of input images: the illumination conditions for D deg and the object locations and categories for D obj . This suggests that an orthogonal regularity can be imposed to decouple the unnecessary interdependence between the outputs of different tasks.</p><p>To this end, the orthogonal objective of the proposed MAET is to minimize the absolute value of cosine similarity below:</p><formula xml:id="formula_3">L ort ? k,l | cos ? k,l | = k,l | ?E ?D k deg T ? ?E ?D l obj | ? ?E ?D k deg ? ? ? ?E ?D l obj ? ,<label>(4)</label></formula><p>where ?E ?D k deg and ?E ?D l obj are the tangents of the representation manifold formed by the encoder E along the kth and lth output coordinates of the illumination-degrading transformation and object detection tasks, respectively. In other words, these two tangents depict the directions along which the representation moves with the change of the decoder outputs D k deg and D l obj , respectively. Minimizing the absolute value of the cosine similarity will push the two tangents as orthogonal as possible. Based on the geometric point of view, this will disentangle the two tasks so that the change of the predicted coordinates for one task will have a minimal impact on the coordinates for the other task. In Sections 3.3, we will discuss the details about how to define the low-illumination-degrading transformation. The idea of imposing orthogonality between tasks was explored in literature <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b49">49]</ref>. However, here we implement it in the context of AET, where the orthogonal directions are defined in terms of decoder tangents along the encoder-induced manifold, which differs from the previous works.</p><p>Therefore, the total loss for our low-light object detection consists of three parts: degradation transformation loss L deg , object detection loss L obj and orthogonal regularity loss L ort (cf. Eq. (4)), the total loss used for training can be represented as</p><formula xml:id="formula_4">L total = L ort + ? 1 ? L obj + ? 2 ? L deg .<label>(5)</label></formula><p>The object detection loss L obj is specific for different object detectors <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b39">39]</ref>. In this experiments, L obj is the loss function of YOLOv3 <ref type="bibr" target="#b39">[39]</ref>, which includes location loss, classification loss and confidence loss. The degradation transformation loss L deg is the AET loss (cf. Eq. (3)) with the low-illumination degrading transformation t deg , and ? 1 and ? 2 are the fixed balancing hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Low-Illumination Degrading Transformations</head><p>Given a normal lit noise-free image x, we aim to design a low-illumination-degrading transformation t deg to transform x into a dark image t deg (x) that matches the real photo captured under low-light conditions, i.e., by turning off the light. Most of existing methods conduct an oversimplified synthesis, e.g., invert gamma correction (sometimes with additive mixed Gaussian noise) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b52">52]</ref> or retinex theory based synthesis method <ref type="bibr" target="#b19">[20]</ref>. The ignorance of the physics of sensors and on-chip image signal processing (ISP) makes these methods generalise poorly to realworld dark images. Here, we first systematically describe the ISP pipeline between the sensor measurement system and the final photo. Based on this pipeline, we parametrically model the low light-degrading transformation t deg .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Image Signal Processing (ISP) Pipeline</head><p>The camera is designed to render the photo to be as pleasant and accurate as possible based on the perspective of a human eye. For this reason, the RAW data captured by the camera sensor requires ISP (several steps) before becoming the final photo. Much research has been done to simulate this ISP process <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b20">21]</ref>. For example, Karaimer and Brown <ref type="bibr" target="#b13">[14]</ref> step-by-step detailed the ISP process and showed its high potential pertaining to computer vision.</p><p>We adopt a simplified ISP and its unprocessing procedure from <ref type="bibr" target="#b2">[3]</ref>  <ref type="figure" target="#fig_2">(Fig. 3</ref>). Particularly, we ignore several steps including the demosaicing process <ref type="bibr" target="#b0">[1]</ref>. Although these processes are important for precise ISP algorithm, most images on the Internet are of various sources and do not follow the perfect ISP procedure. We ignore these steps for the tradeoff between precision and generability. We have made a detailed analysis of the demosaicing's influence in supplementary materials Appendix.B.2. Next, we introduce our ISP process in detail.</p><p>Quantization is the analog voltage signal step that quantizes the analog measurement x into discrete codes y quan using an analog-to-digital converter (ADC). The quantization step maps a range of analog voltages to a single value and generates a uniformly distributed quantization noise. To simulate the quantization step, the quantization noise x quan related to B bits has been added. In our degrading model, B is randomly chosen from 12, 14, and 16 bits.</p><formula xml:id="formula_5">x quan ? U (? 1 2B ,<label>1 2B )</label></formula><formula xml:id="formula_6">y quan = x + x quan .<label>(6)</label></formula><p>White Balance simulates the color constancy of human vision system (HVS) to map "white" colors with the white object <ref type="bibr" target="#b38">[38]</ref>. The captured image is the product of the color of light and material reflectance. The white-balance step in the camera pipeline estimates and adjusts the red channel gain g r and blue channel gain g b to make image appearing to be lit under "neutral" illumination.</p><formula xml:id="formula_7">? ? y r y g y b ? ? = ? ? g r 0 0 0 1 0 0 0 g b ? ? ? ? ? x r x g x b ? ?<label>(7)</label></formula><p>Based on <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b2">3]</ref>, g r is randomly chosen from (1.9, 2.4), and g b is randomly chosen from (1.5, 1.9); both follow an uniform distribution and are independent of each other. The inverse process considers the reciprocal of the red and blue gains 1/g.</p><p>Color Space Transformation converts the whitebalanced signal from camera internal color space cRGB to sRGB color space. This step is essential in ISP pipeline as camera color space are not identical to the sRGB space <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b13">14]</ref>. The converted signal y sRGB can be obtained with a 3 ? 3 color correction matrix (CCM) M ccm : </p><formula xml:id="formula_8">y sRGB = M ccm ? y cRGB ,<label>(8)</label></formula><formula xml:id="formula_9">y cRGB = M ?1 ccm ? y sRGB .<label>(9)</label></formula><p>Gamma Correction has also been widely used in the ISP pipeline for the non-linearity of humans perception on dark areas <ref type="bibr" target="#b36">[36]</ref>. Here we use the standard gamma curve <ref type="bibr" target="#b35">[35]</ref> as:</p><formula xml:id="formula_10">y gamma = max(x, ?) 1 ?<label>(10)</label></formula><p>and its' invert process is:</p><formula xml:id="formula_11">y invert gamma = max(x, ?) ? .<label>(11)</label></formula><p>The gamma curve parameter ? could be randomly sampled from an uniform distribution ? ? U (2, 3.5) and ? is a very small value (? = 1e ?5 ) to prevent numerical instability during training.</p><p>Tone Mapping aims to match the "characteristic curve" of film. For the sake of computational complexity, we perform a "smoothstep" curve <ref type="bibr" target="#b2">[3]</ref> as</p><formula xml:id="formula_12">y tone = 3x 2 ? 2x 3<label>(12)</label></formula><p>and we could also perform the inverse with:</p><formula xml:id="formula_13">y invert tone = 1 2 ? sin( sin ?1 (1 ? 2x) 3 ).<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Degrading Transformation Model</head><p>After defining each step of our ISP pipeline, we can present our low-illumination-degradation transform t deg that synthesizes realistic dark light image t deg (x) based on its normal light counterparts x. At first, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, we <ref type="figure">Figure 4</ref>. Examples of our degrading transformation on SID dataset <ref type="bibr" target="#b3">[4]</ref>. The long-exposure RAW images and their ground truth short-exposure RAW images are transformed into the sRGB format using Adobe Lightroom, separately shown in the first and second columns. The third column shows the images generated from our pipeline.</p><p>have to use an inverse processing procedure <ref type="bibr" target="#b2">[3]</ref> to transform the normal lit image x into sensor measurement or RAW data. Then, we linearly attenuate the RAW image and corrupt it with shot and read noise. Finally, we continue applying the pipeline to turn the low-lit sensor measurement to the photo t deg (x).</p><p>Unprocessing Procedure: Based on <ref type="bibr" target="#b2">[3]</ref>, the unprocessing part aims to translate the input sRGB images into their RAW format counterparts, which are linearly proportional to the captured photons. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, we unprocess the input images by (a) invert tone mapping, (b) invert gamma correction, (c) transformation of image from sRGB space to cRGB space, and (d) invert white balancing, here we call (a), (b), (c), (d) together as t unprocess . Based on these parts, we synthesize realistic RAW format images, and the resulting synthetic RAW image is used for low-light corruption process.</p><p>Low Light Corruption: When light photons are projected through a lens on a capacitor cluster, considering the same exposure time, aperture size, and automatic gain control, each capacitor develops an electric charge corresponding to the lux of illumination of the environment.</p><p>Shot noise is a type of noise generated by the random arrival of photons in a camera, which is a fundamental limitation. As the time of photon arrival is governed by Poisson statistics, uncertainty in the number of photons collected during a given period is ? s = ? S, where ? s is the shot noise and S is the signal of the sensor.</p><p>Read noise occurs during the charge conversion of electrons into voltage in the output amplifier, which can be approximated using a Gaussian random variable with zero mean and fixed variance.</p><p>Shot and read noises are common in a camera imaging system; thus, we model the noisy measurement x noise <ref type="bibr" target="#b6">[7]</ref> on the sensor:</p><formula xml:id="formula_14">x noise ? N (? = kx, ? 2 = ? 2 r + ? s kx) y noise = kx + x noise ,<label>(14)</label></formula><p>where the true intensity of each pixel x from the unprocessing procedure. We linearly attenuate it with parameter k. To simulate different lighting conditions, the parameter of light intensity k is randomly chosen from a truncated Gaussian distribution, in range of (0.01, 1.0), with mean 0.1 and variance 0.08. The parameter range of ? r and ? s follows <ref type="bibr" target="#b35">[35]</ref>, which is shown in <ref type="table">Table 1</ref>.</p><p>ISP Pipeline: RAW images often pass through a series of transformations, before we see it in the RGB format; therefore, we apply RAW image processing after the lowlight corruption process. Based on <ref type="bibr" target="#b2">[3]</ref>, our transformations are in the following order: (e) add quantization noise (f) white balancing, (g) from cRGB to sRGB, and (h) gamma correction, we call (f), (g), (h) together as t ISP .</p><p>Finally, we can obtain the degraded low-light images t deg (x) from the noise-free x, as it shown in Eq. <ref type="bibr" target="#b14">15</ref>. Some examples of the original images, generated images, and ground truth are shown in <ref type="figure">Fig. 4</ref>. We summarise the parameters and their ranges involved in t deg <ref type="table">(Table 1)</ref>:</p><formula xml:id="formula_15">t deg (x) = t ISP (k ? t unprocess (x) + x noise + x quan ). (15)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Architecture</head><p>The architecture of the proposed MAET is shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. Our network comprises representation encoder E and decoder D. For illustration, we implement the MAET based on the architecture of YOLOv3 <ref type="bibr" target="#b39">[39]</ref>. Moreover, this can be replaced by other mainstream detection frameworks, e.g. , <ref type="bibr" target="#b40">[40]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b54">[54]</ref>.</p><p>E adopts a siamese structure with shared weights. During the training process, the normal lit image x is fed into the left path of E (denoted in orange), while its degraded counterparts t deg (x) go through the right path or dark path (denoted in blue). Here, the encoder adopts DarkNet-53 network <ref type="bibr" target="#b39">[39]</ref> as the backbone.</p><p>As we solve two tasks, degrading transformation decoding and object detection tasks, decoder D can be divided into degrading transformation decoder D deg and object detection decoder D obj . The former focuses on decoding the parameters of low-light-degrading transformation (t deg ). The latter decodes the target information, i.e., target class and location. As shown in <ref type="figure" target="#fig_3">Fig. 5</ref>, the encoded latent features E(x) and E(t deg (x)) are concatenated together and passed to decoder D deg to estimate the corresponding degrading transformation t deg . This self-supervision training helps the MAET learn the intrinsic visual structure under various illumination degrading transformations with unlabeled data. The object detection decoder D obj only decodes the representation E(t deg (x)) from the dark path (denoted in blue) to predict the parameters of object detection. In the testing time, we directly feed the low-light images to the dark path of the MAET encoder to decode the detection results: target categories and locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training Details</head><p>We realize our work based on the open-source object detection toolbox MMDetection <ref type="bibr" target="#b4">[5]</ref>. The loss weight components, ? 1 and ? 2 , in Eq. 5 are set to 1 and 10, respectively. In this experiment, L obj represents the loss function of YOLO Head output branch in D obj , L deg represents the </p><formula xml:id="formula_16">- Quantization f (x) = x + U (? 1 2B , 1 2B ) B ? [12, 14, 16] 1 B White Balance f (x) = ? ? g r 0 0 0 1 0 0 0 g b ? ? ? x g r ? U (1.9, 2.4) g b ? U (1.5, 1.9) 1 gr , 1 gb Color Correction cRGB ? uRGB ? sRGB f (x) = M cu ? M us ? x</formula><p>Mixture of four color correction matrices (CCMs): Sony A7R, Olympus E-M10, Sony RX100 IV, Huawei Nexus 6P in <ref type="bibr" target="#b2">[3]</ref> -Tone Mapping <ref type="table">Table 1</ref>. Details of low-illumination-degrading transformation parameters, the first column denotes the names of the transformations, the second column denotes the transformation process, the third column is the parameter range, and the last line denotes the parameters to be predicted in our MAET model's degrading transformation decoder.   MSE loss of transformation parameters between the prediction of D deg and the known ground truth, as listed in the last line of <ref type="table">Table 1</ref>: (k, 1 B , 1 gr , 1 g b , 1 ? ), each parameter is normalized in its corresponding category as a pre-process step, and their weights in L deg are set to 5 : 1 : 1 : 1 : 1.</p><formula xml:id="formula_17">f (x) = 3x 2 ? 2x 3 - - Gamma Correction f (x) = max(x, ?) 1 ? , ? = 1e ?5 ? ? U (2, 3.5) 1 ?</formula><p>All the input images have been cropped and resized to 608 ? 608 pixel size. The backbone, DarkNet-53, is initialized with an ImageNet pre-trained model. We adopt stochastic gradient descent (SGD) as an optimizer and set the image batch size to 8. We set the weight decay to 5e-4 and momentum to 0.9. The learning rate of the encoder (E) and object detection decoder (D obj ) is initially set to 5e-4, and the learning rate of degrading transformation decoder (D deg ) is initially set to 5e-5. Both the rates adopt a Multi-StepLR policy for learning rate decay. For the VOC dataset, we trained our network with a single Nvidia GeForce RTX 3090 GPU for 50 epochs, and the learning rate decreased by 10 at 20 and 40 epochs, and for the COCO dataset, we trained our network with four Nvidia GeForce RTX 3090 GPUs for 273 epochs, and the learning rate decreased by 10 at 218 and 246 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Synthetic Evaluation</head><p>Pascal VOC <ref type="bibr" target="#b5">[6]</ref> is a well-known dataset with 20 categories. We train our model based on the VOC 2007 and VOC 2012 train and validation sets, and test the model based on the VOC 2007 test set. For VOC evaluation, we report mean average precision (mAP) rate at IOU threshold of 0.5.</p><p>COCO <ref type="bibr" target="#b22">[23]</ref> is another widely used dataset with 80 categories and over 10,0000 images. We train our model based on the COCO 2017 train set and test the model based on the COCO 2017 validation set. For COCO evaluation, we evaluated each index of COCO dataset. The quantitative results of VOC and COCO datasets are listed in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>In this part, we train and test the YOLOv3 model <ref type="bibr" target="#b39">[39]</ref> based on the VOC and COCO datasets for normal-lit and synthetic low-lit images as reference. Then, we use the YOLOv3 model trained for normal-lit to test on the set recovered by different low-light enhancement methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b7">8]</ref>  <ref type="bibr" target="#b0">1</ref> . To verify the effectiveness of the orthogonal loss, we train the MAET model with/without orthogonal loss function as MAET (w/o ort) and MAET (w ort) and directly test these models on the low-lit images with no pre-processing. To ensure fairness, all the methods in the training process are set to same setting parameters, i.e., the data augmentation methods (expand, random crop, mul- tisize, and random flip), input size, learning rate, learning strategy, and training epochs. Experimental configurations and results are listed in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>The experimental results in <ref type="table" target="#tab_1">Table 2</ref> show that our MAET has significantly improved the baseline detection framework based on the synthetic low-light dataset. Compared with the enhancement methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b7">8]</ref>, the proposed MAET shows superior performance considering all evaluation indexes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Real-World Evaluation</head><p>To evaluate the performance in a real-world scenario, we have evaluated our trained model (explained in Sec. 4.2) using the exclusively dark (ExDark) dataset <ref type="bibr" target="#b24">[25]</ref>. The dataset includes 7,363 low-light images, ranging from extremely dark environments to twilight with 12 object categories. The local object-bounding boxes are annotated for each image. As EXDark is divided based on different categories, 80% samples of each category are used for fine-tuning on COCO pre-trained model (Sec.4.2) for 25 epochs with a learning rate of 0.001, and the remaining 20% are used for evaluation; we calculate the average precision (AP) of each category (see <ref type="table">Table.</ref> 3 for more details) and calculate the overall mean average precision (mAP). Moreover, we have provided some examples in Appendix.A. As listed in <ref type="table">Table  3</ref>, we can see that the proposed MAET method achieves satisfactory performance considering most of the classes and overall mAP. This result affirms that our degrading transformation is in accordance with real-world conditions. Furthermore, we have evaluated our methods with the UG 2 + DARK FACE dataset <ref type="bibr" target="#b48">[48]</ref>; UG 2 + is a low-light face detection dataset, which contains 6,000 labeled lowlight face images, where 5400 images are used for finetuning on the COCO pretrained model (Sec.4.2) for 20 epochs with a learning rate of 0.001. The other 600 images are used for evaluation; the experiment results are listed in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose MAET, a novel framework to explore the intrinsic representation that is equivariant to degradation caused by changes in illumination. The MAET decodes this self-supervised representation to detect objects in a dark environment. To avoid the over-entanglement of object and degrading features, our method develops a parametric manifold along which multitask predictions can be geometrically formulated by maximizing the orthogonality among the tangents along the output of respective tasks. Throughout the experiment, the proposed algorithm outperforms the state-of-the-art models pertaining to real-world and synthetic dark image datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Display of Object Detection Results in Real-World Situation</head><p>In this section we would give more examples on real data sets. <ref type="figure" target="#fig_6">Fig.6</ref> shows a case in UG 2 + DARK FACE <ref type="bibr" target="#b48">[48]</ref> dataset. <ref type="figure" target="#fig_7">Fig.7</ref> shows the detection results of ExDark <ref type="bibr" target="#b24">[25]</ref> dataset. Our MAET has shown better detection performance when facing actual dark light detection tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Other Low-Light Image Synthesis Methods</head><p>Although the purpose of this paper is not for a perfect low-light image synthesis pipeline, we have also compared with existing low light image synthesis methods (from normal light sRGB to low light sRGB) and evaluate their impact on low-light object detection tasks.</p><p>The work in <ref type="bibr" target="#b19">[20]</ref> proposed to use the RetiNex model <ref type="bibr" target="#b17">[18]</ref> to generate low light counterpart by normal light images:</p><formula xml:id="formula_18">I(x) = R(x) ? L,<label>(16)</label></formula><p>in this equation, R(x) is the clear normal-lit images (same as x in Eq.15), I(x) is the generated low-lit counterpart (same as t deg (x) in Eq.15) and L is the random fixed illumination value, here L, same as our parameter k's range. The work in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b52">52]</ref> proposed to use an invert gamma correction with additional noises to generate low light degraded image from normal light counterpart, the equation shown as follow:  </p><formula xml:id="formula_19">t deg (x) = x ? + n.<label>(17)</label></formula><formula xml:id="formula_20">t deg (x) = x ? k.<label>(18)</label></formula><p>here ? is the gamma curve parameter and n is the additional noise (Poisson noise or Gaussian-Poisson noise model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Demosacing's Influence</head><p>Demosacing is an essential part in camera image signal processing pipeline <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b2">3]</ref>, which aims to recover intermediate gray-scale image to the R/G/B value by interpolating the missing values in Bayer pattern. Unlike the previous work <ref type="bibr" target="#b2">[3]</ref>, we ignore this step for simplicity in our ISP procedure. In supplementary material, we show an example after adding mosaicing process after invert WB process ((d) in <ref type="figure" target="#fig_2">Fig.3</ref>) and demosacing process after WB process ((f) in <ref type="figure" target="#fig_2">Fig.3</ref>).</p><p>To evaluate effects of different low light data generation methods: Retinex based generation method L R , inverse gamma curve L G , inverse gamma curve with additional possion noise L GP , inverse gamma curve with additional mixed Gaussian-Poisson noise model L Gmix , our proposed data generation method in Sec.3.3 L ours , and our proposed data generation method with mosaicing and demosacing process L ours+m . We measured the performance of using different dark light data on the real world datasets <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b48">48]</ref>, shown as YOLO (L) in <ref type="table" target="#tab_4">Table 5</ref>, the training configurations and strategies are same as Sec.4, it could be seen that our synthetic method is of greatest help in improving the detection performance of real datasets <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b48">[48]</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Detection and enhancement results of image taken by Sony DSC-RX100M7 camera at night with 0.1s exposure time and 3200 ISO. (a) is detection result on the original image by MAET (YOLOv3) and (b), (c), (d) are the enhanced images by Lv et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Structure of the multitask autoencoding transformations (MAET) framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>General view of the low-illumination-degrading pipeline, an sRGB "sheep" image from PASCAL VOC2007 dataset<ref type="bibr" target="#b5">[6]</ref> passed by unprocessing procedure, low-light corruption, and image signal processing(ISP) process to get the final degraded low-light counterpart the inversion of this process is:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Architecture of the proposed MAET model base on YOLOv3 framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 r</head><label>2</label><figDesc>f (x) = k ? x k ? N (? = 0.1, ? = 0.08) 0.01 ? k ? 1.0 k Shot Noise and Read Noise f (x) = x + N (? = x, ? 2 = ? 2 r + ? s x) log? s ? U (?4, ?2) log? log?s ? N (? = 2.18log? s + 0.12, ? = 0.26)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>training set testing set pre-process VOC(AP 50 ) COCO(AP) COCO(AP 50 ) COCO(AP 75 ) COCO(AP S ) COCO(AP M ) COCO(AP L )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Detection results of UG 2 + DARK FACE dataset<ref type="bibr" target="#b48">[48]</ref>.(a)/(b)/(c) is the detection result of YOLO based on the dataset pre-processed by MBLLEN [28]/KIND [51]/Zero-DCE [8] and (d) is the is the detection result of MAET-YOLO model on original dataset, yellow and green boxes are ground truth boxes and prediction results, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Detection results of ExDark [25] dataset: (a) is the detection result of YOLO using the original ExDark dataset, (b)/(c)/(d) is the detection result of YOLO based on the ExDark dataset pre-processed by MBLLEN [28]/KIND [51]/Zero-DCE [8], and (e) is the detection result of the proposed MAET-YOLO method based on the original ExDark dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table /><note>The experimental results on VOC [6] dataset and COCO [23] dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .Table 4 .</head><label>44</label><figDesc>The proposed MAET method has achieved better results compared with the other methods. The experiment results on the UG 2 + DARK FACE<ref type="bibr" target="#b48">[48]</ref> dataset.</figDesc><table><row><cell></cell><cell>mAP</cell></row><row><cell>YOLO (N)</cell><cell>0.483</cell></row><row><cell cols="2">MBLLEN [28] + YOLO(N) 0.516</cell></row><row><cell>KIND [51] + YOLO(N)</cell><cell>0.516</cell></row><row><cell cols="2">Zero-DCE [8] + YOLO(N) 0.542</cell></row><row><cell>YOLO (L)</cell><cell>0.540</cell></row><row><cell>MAET (w/o ort)</cell><cell>0.542</cell></row><row><cell>MAET (w ort)</cell><cell>0.558</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>The experiment results on ExDark<ref type="bibr" target="#b24">[25]</ref> dataset and UG 2+ DARK FACE<ref type="bibr" target="#b48">[48]</ref> datasets by using different kinds of synthetic low light COCO<ref type="bibr" target="#b22">[23]</ref> dataset. The detection results verify the reliability of our synthesis method.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">ExDark DARK FACE</cell></row><row><cell></cell><cell>L R</cell><cell>0.698</cell><cell>0.511</cell></row><row><cell></cell><cell>L G</cell><cell>0.709</cell><cell>0.528</cell></row><row><cell>YOLO (L)</cell><cell>L GP L Gmix</cell><cell>0.712 0.713</cell><cell>0.532 0.535</cell></row><row><cell></cell><cell>L ours+m</cell><cell>0.706</cell><cell>0.530</cell></row><row><cell></cell><cell>L ours</cell><cell>0.716</cell><cell>0.540</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Here<ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b51">51]</ref> have been retrained on the normal-lit image and synthetic low-lit image pairs and<ref type="bibr" target="#b7">[8]</ref> only trained on low-lit images, because<ref type="bibr" target="#b7">[8]</ref> is a self-supervised model which do not need normal-lit ground truth.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Chapter 5 -comparison of color demosaicing methods</title>
	</analytic>
	<monogr>
		<title level="m">Advances in Imaging and electron Physics</title>
		<editor>Peter W. Hawkes</editor>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="173" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Renoir -a dataset for real low-light image noise reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josue</forename><surname>Anaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Barbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="144" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unprocessing images for learned raw denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11036" to="11045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to see in the dark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3291" to="3300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">MMDetection: Open mmlab detection toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Practical Poissonian-gaussian noise modeling and fitting for single-image raw-data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trimeche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1737" to="1754" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Zero-reference deep curve estimation for lowlight image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1777" to="1786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lime: Low-light image enhancement via illumination map estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="982" to="993" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">FlexISP: A flexible camera image processing framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Heide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2014-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning the image processing pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Wandell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5032" to="5042" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">EnlightenGAN: Deep light enhancement without paired supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2340" to="2349" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A multiscale retinex for bridging the gap between color images and the human observation of scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Jobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Woodell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="965" to="976" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A software platform for manipulating the camera imaging pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Hakki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Karaimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Low-lightGAN: Lowlight enhancement via advanced generative adversarial network with task-driven training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2811" to="2815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Object detection in images with low light condition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Kvyetnyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maslii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Photonics Applications in Astronomy, Communications, Industry, and High Energy Physics Experiments</title>
		<editor>Ryszard S. Romaniuk and Maciej Linczuk</editor>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">10445</biblScope>
			<biblScope unit="page" from="250" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">An alternative technique for the computation of the designator in the retinex theory of color vision. Proceedings of the National Academy of Sciences of the United States of America</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Land</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="3078" to="3080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Contrast enhancement based on layered difference representation of 2D histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5372" to="5384" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lightennet: A convolutional neural network for weakly illuminated image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jichang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="15" to="22" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Handheld mobile photography in very low light</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orly</forename><surname>Liba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Ta</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Karnad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiurui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Geiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">W</forename><surname>Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yael</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2019-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Compact feature learning for multi-domain image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7186" to="7194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Getting to know lowlight images with the exclusively dark dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chee Seng</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Llnet: A deep autoencoder approach to natural low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adedotun</forename><surname>Kin Gwn Lore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumik</forename><surname>Akintayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="650" to="662" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention guided lowlight image enhancement with a large scale low-light simulation dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifan</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2175" to="2193" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MBLLEN: Low-light image/video enhancement using CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifan</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongsoon</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Object recognition under various lighting conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Takizawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiaki</forename><surname>Shirai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobutaka</forename><surname>Shimada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Josef Bigun and Tomas Gustavsson</title>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="899" to="906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pushing the limits of unconstrained face detection: a challenge dataset and baseline results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 9th International Conference on Biometrics Theory, Applications and Systems (BTAS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">NightOwls: A pedestrians at night dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Karg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Scharfenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Piegert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Mistr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Prokofyeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Thiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automatic ISP image quality tuning using nonlinear optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nishimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gerasimow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sushma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sutic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Michael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2471" to="2475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Benchmarking denoising algorithms with real photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Plotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="2750" to="2759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Poynton</surname></persName>
		</author>
		<title level="m">Inc Books24x7, and Engineering Information Inc. Digital Video and HD: Algorithms and Interfaces. Computer Graphics</title>
		<imprint>
			<publisher>Elsevier Science</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">AVT: Unsupervised learning of transformation equivariant representations by autoencoding variational transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Guo-Jun Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8130" to="8139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Color image processing pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Drew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="43" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">YOLOv3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>abs/1804.02767</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
	<note>NIPS&apos;15</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Yolo in the dark -domain adaptation method for merging multiple models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukihiro</forename><surname>Sasagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Nagahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -European Conference on Computer Vision</title>
		<meeting>-European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Msr-net:low-light image enhancement using deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Suteu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yike</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06844</idno>
		<title level="m">Regularizing deep multitask networks using orthogonal gradients</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bilateral filtering for gray and color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271)</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="839" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">EnAET: Self-trained ensemble autoencoding transformations for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09265</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep retinex decomposition for low-light enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Exploring image enhancement for salient object detection in low light images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimin</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.16124</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Advancing image understanding in poor visibility environments: A collective benchmark study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5737" to="5752" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Gradient surgery for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhe</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.06782</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">AED: Unsupervised representation learning by auto-encoding transformations rather than data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2547" to="2555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Kindling the darkness: A practical low-light image enhancer. MM &apos;19</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Association for Computing Machinery</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Optical flow in the dark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6748" to="6756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fork-GAN: Seeing into the rainy night</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinran</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
