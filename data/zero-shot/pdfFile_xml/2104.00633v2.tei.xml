<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RePOSE: Fast 6D Object Pose Refinement via Deep Texture Rendering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Iwase</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rawal</forename><surname>Khirodkar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rio</forename><surname>Yokota</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RePOSE: Fast 6D Object Pose Refinement via Deep Texture Rendering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present RePOSE, a fast iterative refinement method for 6D object pose estimation. Prior methods perform refinement by feeding zoomed-in input and rendered RGB images into a CNN and directly regressing an update of a refined pose. Their runtime is slow due to the computational cost of CNN, which is especially prominent in multiple-object pose refinement. To overcome this problem, RePOSE leverages image rendering for fast feature extraction using a 3D model with a learnable texture. We call this deep texture rendering, which uses a shallow multilayer perceptron to directly regress a view-invariant image representation of an object. Furthermore, we utilize differentiable Levenberg-Marquardt (LM) optimization to refine a pose fast and accurately by minimizing the distance between the input and rendered image representations without the need of zooming in. These image representations are trained such that differentiable LM optimization converges within few iterations. Consequently, RePOSE runs at 92 FPS and achieves state-of-the-art accuracy of 51.6% on the Occlusion LineMOD dataset -a 4.1% absolute improvement over the prior art, and comparable result on the YCB-Video dataset with a much faster runtime. The code is available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In many applications of 6D object pose estimation like robotic grasping and augmented reality (AR), fast runtime is critical. State-of-the-art 6D object pose estimation methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b28">29]</ref> demonstrate that iterative 6D object pose refinement improves the accuracy largely. Nevertheless, since recent 6D object pose refinement methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b19">20]</ref> directly regress an update of a pose to align a zoomed-in input image of an object against a template image (e.g., 3D rendering of that object) using a Convolutional Neural Network (CNN), we presume that the CNN's computational cost of zoomedin inputs can be a bottleneck toward the real-time 6D object pose estimation.</p><p>We have mainly two choices of refinement strategies. As described, the former one is CNN-based direct regression, which generally requires large computational cost. The latter one is a classical non-linear optimization <ref type="bibr" target="#b24">[25]</ref> which iteratively updates a pose by minimizing the photometric error between input and template images. Their runtime per iteration is quite fast. Since the photometric error explicitly considers each pixel, they can obtain enough details for accurate optimization without the need of zooming in. However, they can fail under diverse illumination or gross pose differences. Although non-linear least squares methods such as inverse compositional image alignment <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23]</ref> or active appearance models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24]</ref> are extremely efficient, straightforward implementations of such methods can be unstable under significant illumination or pose changes. In addition, their runtime can be slower if many iterations are performed until convergence.</p><p>We leverage and improve the latter method to realize both quick and accurate refinement. In this paper, we propose RePOSE, a new feature-based non-linear optimization framework for 6D object pose refinement. The main tech-nical insight presented in this work is that one can learn an image feature representation which is both robust for alignment and fast to compute. As stated earlier, the main impediment of CNN-based refinement methods is that the deep feature must be extracted during the refinement process iteratively. To remove this, we show that it is possible to directly render deep features using a simple graphics render. The rendering process decouples the shape of the object from the texture. At the time of rendering, texture is mapped to the 3D shape and then projected as a 2D image. Instead of mapping an RGB valued texture to the object, we can alternatively render a deep feature texture. Then, the rendered object can be directly aligned to the deep features of the input image. By retaining the deep feature representation during rendering, the pose alignment is robust and the refinement process becomes very efficient.</p><p>RePOSE refines an object pose by minimizing the distance between the deep features of the input and rendered images. Since the input image is fixed during iterative refinement, its feature is only computed once using a CNN. In contrast, the deep feature of the template image are directly generated using a simple computer graphics renderer. The rendering process takes less than a millisecond which greatly increases the speed of the iterative refinement process. The deep feature representation is learned such that nonlinear optimization can be easily performed through a differentiable LM optimization network <ref type="bibr" target="#b24">[25]</ref>. We experimentally found 5 iterations are enough to converge, which contributes to fast 6D object pose refinement.</p><p>RePOSE has several practical advantages over recent CNN-based regression methods: 1) RePOSE can be exceptionally fast. -In the case of 1 iteration, RePOSE runs at 181 FPS for 5 objects and 244 FPS for 1 object, 2) RePOSE is data efficient. -Since RePOSE considers projective geometry explicitly, there is no need to learn the mapping of the deep feature into an object's pose from training data. In our experiments, we show that RePOSE achieves better or comparable performance with much fewer number of training images than prior methods, and 3) RePOSE does not request RGB textures of a 3D model. -It has been known that RGB texture scanning has troubles with metalic, darkcolored, or transparent objects even with the latest 3D scanner <ref type="bibr" target="#b0">[1]</ref>. We believe that the requirement of RGB textures of recent CNN-based regression methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b19">20]</ref> makes the implementation in the real world more challenging.</p><p>We evaluate RePOSE on three popular 6D object estimation datasets -LineMOD <ref type="bibr" target="#b15">[16]</ref>, the challenging Occlusion LineMOD <ref type="bibr" target="#b5">[6]</ref>, and YCB-Video <ref type="bibr" target="#b40">[41]</ref>. RePOSE sets a new state of the art on the Occlusion LineMOD (51.6%) <ref type="bibr" target="#b5">[6]</ref> dataset and achieves comparable performance on the other datasets with much faster speed (80 to 92 FPS with 5 iterations). Additionally, we perform ablations to validate the effectiveness of our proposed methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Two-stage pose estimation methods Recently, Oberweger <ref type="bibr" target="#b26">[27]</ref>, PVNet <ref type="bibr" target="#b27">[28]</ref>, DPOD <ref type="bibr" target="#b41">[42]</ref>, and HybridPose <ref type="bibr" target="#b34">[35]</ref> have shown excellent performance on 6D object pose estimation using a two-stage pipeline to estimate a pose: (i) estimating a 2D representation (e.g. keypoints, dense correspondences, edge vectors, symmetry correspondences), (ii) PnP algorithm <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b10">11]</ref> for pose estimation. DOPE <ref type="bibr" target="#b37">[38]</ref> and BB8 <ref type="bibr" target="#b28">[29]</ref> estimate the corners of the 3D bounding box and run a PnP algorithm. Instead of regarding the corners as keypoints, PVNet <ref type="bibr" target="#b27">[28]</ref> places the keypoints on the object surface via the farthest point sampling algorithm. PVNet also shows that their proposed voting-based keypoint detection algorithm is effective especially for occluded objects. HybridPose <ref type="bibr" target="#b34">[35]</ref> uses multiple 2D representations including keypoints, edge vectors, and symmetry correspondences and demonstrates superior performance through constraint optimization. DPOD <ref type="bibr" target="#b41">[42]</ref> takes advantage of the dense correspondences using a UV map as a 2D representation. However, since the PnP algorithm is sensitive to small errors in the 2D representation, it is still challenging to estimate the object pose especially under occlusion. RePOSE adopts PVNet <ref type="bibr" target="#b27">[28]</ref> as the initial pose estimator using the official implementation.</p><p>Pose refinement networks Recent works <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b21">22]</ref> have demonstrated that using a pose refinement network after the initial pose estimator is effective for 6D object pose estimation. For practical applications, the runtime of the pose refinement network is crucial. PoseCNN <ref type="bibr" target="#b40">[41]</ref> and AAE <ref type="bibr" target="#b35">[36]</ref> incorporates an ICP algorithm <ref type="bibr" target="#b42">[43]</ref> using depth information to refine the pose with a runtime of around 200 ms. SSD6D <ref type="bibr" target="#b17">[18]</ref> and HybridPose <ref type="bibr" target="#b34">[35]</ref> proposed to refine the pose by optimizing a modification of reprojection error. DeepIM <ref type="bibr" target="#b21">[22]</ref>, DPOD <ref type="bibr" target="#b41">[42]</ref>, and CosyPose <ref type="bibr" target="#b19">[20]</ref> introduce a CNN-based refinement regression network using the zoomed-in input image and a rendered object image. Their methods require a high-quality texture map of a 3D model to compare the images. However, it is still challenging to obtain accurate texture scans of metalic, darkcolored, or transparent objects. NeMO <ref type="bibr" target="#b39">[40]</ref> proposes a pose refinement method using the standard differentiable rendering and learning the texture of a 3D model via contrastive loss. However, gradient descent is used for optimization, hence, it takes more than 8s for inference and is not fast enough for real-time applications.</p><p>Non-linear least squares optimization Non-linear least squares optimization is widely used in machine learning. In computer vision, it is often utilized to find an optimal pose which minimizes the reprojection error or photometric error <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33]</ref>. Recently some works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b11">12]</ref> incorporate   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RePOSE: Fast 6D Object Pose Refinement</head><p>Given an input image I with a ground-truth object pose P gt and the template 3D model M, RePOSE predicts pos? P of model M which matches P gt in I. We extract a feature F inp from image I using a CNN ? i.e. F inp = ?(I). RePOSE then refines the initial pose estimate P ini = ?(I)</p><p>where ? is any pose estimation method like PVNet <ref type="bibr" target="#b27">[28]</ref> and PoseCNN <ref type="bibr" target="#b40">[41]</ref> in real time using differentiable Levenberg-Marquardt (LM) optimization <ref type="bibr" target="#b24">[25]</ref>. RePOSE renders the template 3D model with learnable deep textures in pose P to extract feature F rend . The pose refinement is performed by minimizing the distance between F inp and F rend . We now describe in detail (1) F inp extraction, (2) F rend extraction and finally (3) the pose refinement using LM optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature Extraction of an Input Image F inp</head><p>We adopt a U-Net <ref type="bibr" target="#b29">[30]</ref> architecture for the CNN ?. The decoder outputs a deep feature map for every pixel in I. The per-pixel feature F inp ? R w?h?d is extracted by the decoder. <ref type="figure" target="#fig_0">Figure 1</ref> (b) provides a visual illustration of F inp extracted from the input image I. Note that the channel depth d is a flexible parameter but we found d = 3 to be optimal. The pre-trained weights of PVNet <ref type="bibr" target="#b27">[28]</ref> or PoseCNN <ref type="bibr" target="#b40">[41]</ref> are used for the encoder and only the decoder is trained while training RePOSE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Template 3D Model Rendering F rend</head><p>The template 3D model M with pose P = {R, t} where R is 3D rotation and t is 3D translation, is projected to 2D to render the feature F rend . Let the template 3D model M = {V, C, F} be represented by a triangular watertight mesh consisting of N vertices V = {V n } N n=1 where V n ? R 3 , faces F and deep textures C. V n is the 3D coordinate of the vertex in the coordinate system centered on the object. Each vertex V n has a corresponding vertex learnable texture</p><formula xml:id="formula_0">C n ? R d , C = {C n } N n=1</formula><p>, which is learned. Note that the dimensions of the vertex learnable texture d must match depth dimension of input image feature F inp so that they can be compared during alignment.</p><p>RePOSE projects the 3D mesh onto to the image plane using a pinhole camera projection function ? (homogeneous to inhomogeneous coordinate conversion). Specifically, we <ref type="figure">Figure 3</ref>: Rasterization of deep textures into a pixel p n as the weighted sum of C i n using w i n as weights in the barycentric coordinate system,</p><formula xml:id="formula_1">! " ! # ! $ ! ! # ! " ! $ ! # ! " ! $</formula><formula xml:id="formula_2">3 i w i n = 1. map the vertex V n to v ? R 2 using eq 1. v n = ? V n R + t ? n<label>(1)</label></formula><p>The vertex deep textures C n ? R 3 are learnable and computed using a 2-layer fully-connected network. The deep texture at each pixel is calculated by rasterization using the deep textures C n in barycentric coordinates w as shown in <ref type="figure">Figure 3</ref>. This operation can be parallelized using a GPU. Our custom implementation of the <ref type="bibr" target="#b16">[17]</ref>'s renderer takes less than 1 ms to render F rend . F rend (x, y) at a pixel location (x, y) is computed as follows:</p><formula xml:id="formula_3">F rend (x, y) = 3 i=1 w i n C i n<label>(2)</label></formula><p>where the triangular face index n corresponding to the pixel p n at (x, y) is found by ray tracing and w i is the normalized barycentric weight corresponding to the coordinates (x, y) inside the triangle <ref type="figure">(Figure 3</ref>). Simply put, the rendered deep feature F rend (x, y) is a linear combination of deep textures of the three projected vertices. F rend is end-to-end learnable by backpropagation. The gradient of F rend with respect to the three deep textures of the triangle {C i n } 3 i=1 is as follows:</p><formula xml:id="formula_4">?F rend (x, y) ?C i n = w i n .<label>(3)</label></formula><p>Note that F rend is the output of a non-linear function ? of the template 3D model M and its pose P, i.e., F rend = ?(P, M) where ? is the deep texture renderer <ref type="figure" target="#fig_3">(Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Levenberg-Marquardt (LM) Optimization</head><p>After computing F inp (Section 3.1) and F rend (Section 3.2), the optimal poseP is calculated by minimizing the following objective function:</p><formula xml:id="formula_5">e = vec(F inp ) ? vec(F rend ),<label>(4)</label></formula><formula xml:id="formula_6">P = arg min P k ||e k || 2 2 ,<label>(5)</label></formula><p>where e k denotes the k th element of the error e ? R whd and is the element-wise difference between the flattened values of F inp and F rend . To perform optimization efficiently, we only use the error e in the pixel where the mask of F rend exists.</p><p>We solve this non-linear least squares problem using the iterative Levenberg-Marquardt (LM) algorithm. The update rule for the pose P is as follows:</p><formula xml:id="formula_7">?P = (J T (e) J + ?I) ?1 J T (e) e,<label>(6)</label></formula><formula xml:id="formula_8">P i+1 = P i + ?P,<label>(7)</label></formula><p>where J is the Jacobian of the objective with respect to the pose P, and ? is a learnable step size. The Jacobian J can be decomposed as:</p><formula xml:id="formula_9">J = ?F rend ?P = ?F rend ?x ?x ?P<label>(8)</label></formula><p>where x is a vector of all 2D image coordinate. We compute ?Frend ?x using a finite difference approximation and ?x ?P is computed analytically. Please refer supplemental for the details.</p><p>We minimize a loss function L ADD(-S) based on the ADD(-S) score:</p><formula xml:id="formula_10">L ADD(-S) = S ADD(-S) (P, P gt )<label>(9)</label></formula><p>where S is the function used to calculate the distance used in the ADD(-S) score. Additionally, we also minimize a loss function L diff which ensures the value of the objective function is minimized when the pose P is equal to P gt :</p><formula xml:id="formula_11">d = vec(F inp ) ? vec(?(P gt , M)),<label>(10)</label></formula><formula xml:id="formula_12">L diff = k ||d k || 2 2 .<label>(11)</label></formula><p>The minimization of these two loss functions through LM optimization allows our refinement network to learn representations of the input image as well as the rendered object image, which helps in predicting the optimal pose.</p><formula xml:id="formula_13">L = L ADD(-S) + ?L diff<label>(12)</label></formula><p>where ? is a hyperparameter. We show the RePOSE framework in Algorithm 1. Note, all the operations inside the LM optimization (Equations (6) and <ref type="formula" target="#formula_8">(7)</ref>) are differentiable allowing us to learn deep textures C and ? using backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We train our model using Adam optimizer <ref type="bibr" target="#b18">[19]</ref> with a learning rate of 1 ? 10 ?3 , decayed by 0.5 every 100 epochs.  The number of channels d in F inp and F rend is set to 3 using grid search, and iterations t in LM optimization is set to 5. We used pretrained PVNet <ref type="bibr" target="#b27">[28]</ref> on the LineMOD and Occlusion LineMOD datasets, and PoseCNN <ref type="bibr" target="#b40">[41]</ref> on the YCB-Video <ref type="bibr" target="#b40">[41]</ref> dataset as the initial pose estimator ?. The encoder of U-Net <ref type="bibr" target="#b29">[30]</ref> consisting of ResNet-18 <ref type="bibr" target="#b14">[15]</ref> shares its weights with the PVNet, and PoseCNN and only the weights of the decoder are trained. Therefore, RePOSE simply can reuse the deep features extracted from the initial pose estimator, which reduces the computational cost.</p><p>Following <ref type="bibr" target="#b27">[28]</ref>, we also add 500 synthetic and fused images for LineMOD and 20K synthetic images for YCB-Video to avoid overfitting during training. In accordance with the convention, to evaluate the scores on the Occlusion LineMOD dataset, we use the model trained by using only the LineMOD dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets</head><p>All experiments are performed on the LineMOD <ref type="bibr" target="#b15">[16]</ref>, Occlusion LineMOD <ref type="bibr" target="#b5">[6]</ref>, and YCB-Video <ref type="bibr" target="#b40">[41]</ref> datasets. The LineMOD dataset contains images of small textureless objects in a cluttered scene under different illumination. High-quality template 3D models of the objects in the images are also provided for render and compare based pose estimation. The Occlusion LineMOD dataset is a subset of the LineMOD dataset focused mainly on the occluded objects. YCB-Video <ref type="bibr" target="#b40">[41]</ref> dataset contains images of objects from the YCB-object set <ref type="bibr" target="#b9">[10]</ref>. We use ADD (-S) <ref type="bibr" target="#b15">[16]</ref> and AUC of ADD(-S) scores as our evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Metrics</head><p>ADD(-S) score. ADD(-S) score <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b40">41]</ref> is a standard metric which calculates the average distance between objects transformed by the predicted poseP = {R,t}, and the ground-truth pose P gt = {R gt , t gt } using vertices V i of the template 3D model M. The distance is calculated as follows; <ref type="table">Table 1</ref>: Results on the YCB-Video dataset using RGB only. The results for DeepIM <ref type="bibr" target="#b21">[22]</ref> are computed using the official pre-trained model, and the score inside the parentheses are the reported results from the paper. Refinement FPS denotes FPS of running only a pose refinement network. RePOSE w/ track includes the runtime for CNN feature extraction of a real image. FPS is reported with refinement of 5 objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric</head><p>PoseCNN <ref type="bibr" target="#b40">[41]</ref> DeepIM <ref type="bibr" target="#b21">[22]</ref> PVNet <ref type="bibr" target="#b27">[28]</ref>   </p><formula xml:id="formula_14">1 N N i || R V i +t ? (R gt V i + t gt ) ||<label>(13)</label></formula><p>For symmetric objects such as eggbox and glue, we use the following distance metric,</p><formula xml:id="formula_15">1 N N i min 0?j?N || R V i +t ? (R gt V j + t gt ) || (14)</formula><p>The predicted pose is considered correct if this distance is smaller than 10% of the target object's diameter. AUC of ADD(-S) computes the area under the curve of the distance used in ADD(-S). The pose predictions with distance larger than 0.1m are not included in computing the AUC. We use AUC of ADD(-S) to evaluate the performance on the YCB-Video dataset <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Quantitative Evaluations</head><p>Results on the LineMOD and Occlusion LineMOD datasets. As shown in <ref type="table" target="#tab_1">Tables 2 and 3</ref>, RePOSE achieves the state of the art ADD(-S) scores on the Occlusion  <ref type="bibr" target="#b27">[28]</ref>, DPOD <ref type="bibr" target="#b41">[42]</ref>, and HybridPose <ref type="bibr" target="#b34">[35]</ref> using the ADD(-S) score. Note, we exclude EfficientPose <ref type="bibr" target="#b8">[9]</ref> as it is trained on the Occlusion LineMOD dataset. # of wins denotes in how many objects the method achieves the best score. LineMOD dataset. In comparison to PVNet <ref type="bibr" target="#b27">[28]</ref>, RePOSE successfully refines the initial pose estimate in all the objects, achieving an improvement of 9.8% and 10.8% on the LineMOD and Occlusion LineMOD dataset respectively. On the LineMOD dataset, our score is comparable to the state-of-the art EfficientPose <ref type="bibr" target="#b8">[9]</ref>. The key difference is mainly on ape and duck where our initial pose estimator PVNet <ref type="bibr" target="#b27">[28]</ref> performs poorly. Interestingly, for small objects like ape and duck in the Occlusion LineMOD dataset, we show a significant improvement of 10.2 and 15.1 respectively over the prior art HybridPose <ref type="bibr" target="#b34">[35]</ref>.</p><p>Results on the YCB-Video dataset. <ref type="table">Table 1</ref> shows the result on the YCB-Video dataset <ref type="bibr" target="#b40">[41]</ref>. We also performed experiments using RePOSE as a 6D object tracker using the tracking algorithm proposed in <ref type="bibr" target="#b21">[22]</ref>. RePOSE achieves comparable performance with other methods with a 4 times faster runtime of 80 FPS for refinement of 5 objects. Further, the result with tracking demonstrates that RePOSE is useful as a real-time 6D object tracker. Note, the scores are heavily affected by the use and amount of synthetic data and various data augmentation <ref type="bibr" target="#b19">[20]</ref>. For instance, Cosy-Pose <ref type="bibr" target="#b19">[20]</ref> used one million synthetic images during train-  ing, making it hard to compare against fairly. However, our method achieves comparable performance using 500 times less training images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>All ablations for RePOSE are conducted on the LineMOD and Occlusion LineMOD datasets using PVNet <ref type="bibr" target="#b27">[28]</ref> as an initial pose estimator. We report the results in <ref type="table" target="#tab_4">Tables 4 and 5.</ref> RGB vs Deep Texture. Instead of using learnable deep textures C, we perform experiments using an original RGB image and rendered image with scanned colors. The inference is all the same except we are using photometric error between two images. The experimental result reported in <ref type="table" target="#tab_4">Tables 4 and 5</ref> show that the ADD(-S) score drops significantly after optimization in all the objects using RGB representation. As illustrated in <ref type="figure" target="#fig_6">Figure 5</ref>, the LineMOD dataset has three main challenges which makes the pose refine- Furthermore, RGB images may have the region with the same color as the object. This background noise becomes an obstacle in terms of convergence properties. These texture-less objects make it challenging to compute the image gradient which is essential to optimize a pose. ment using the photometric error difficult -1) Illumination changes between the input RGB image and synthetic rendering, 2) Poor image gradients due to texture-less objects, 3) Background confusion i.e. the background color is similar to the object's color. The ADD(-S) scores drop largely due to these key reasons. On the contrary, RePOSE with learnable deep textures is able to converge within few iterations because of the robustness of deep textures to the above challenges. <ref type="table" target="#tab_4">Tables 4 and 5</ref> clearly demonstrate the effectiveness of our learnable deep textures over using scanned colors for the template 3D model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN with Feature Warping vs Feature Rendering.</head><p>Feature warping (FW) is commonly used to minimize photometric or feature-metric error through a non-linear least squares such as Gauss-Newton or Levenberg-Marquardt method <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref>. We conduct an experiment to compare a CNN with feature warping and our proposed feature rendering using the deep texture renderer. In a CNN with feature warping, F rend is extracted in the same fashion as the F inp using a CNN on a normalized synthetic rendering of the template 3D model. This is done just once, following which the feature is warped based on the updated pose at each iteration. The result is shown in <ref type="table" target="#tab_4">Tables 4 and 5</ref>. On the LineMOD dataset, we observed on average small improvments by the feature warping. The ADD(-S) score only allows the pose estimator to have an mean vertex distance error of 10% of the object's diameter. In this task, this means only 2 to 3 pixel displacement error in 2D image space is allowed especially for small objects. However, it is challenging to train a CNN to extract features with accurate image gradients required for fine-grained pose refinement. On the contrary, our deep texture renderer can compute accurate gradients as the neighborhood vertices on the template 3D model are not strongly correlated. This local constraint is critical for fast and accurate pose refinement. Furthermore, we perform additional experiments to verify the effect of feature warping. To this end, we warp the feature extracted by deep texture renderer based on the updated pose (Ours w/ FW). The result in <ref type="table" target="#tab_4">Table 4</ref> shows that Ours w/ FW achieves 9.2% absolute improvement from PVNet <ref type="bibr" target="#b27">[28]</ref> on the LineMOD dataset <ref type="bibr" target="#b15">[16]</ref>. However, Table 5 demonstrates the limited ability on the Occlusion LineMOD dataset <ref type="bibr" target="#b5">[6]</ref>. From this result, we figure out that warping has an inferior influence on refinement of occluded objects. We conjecture that this difference comes from the fact that warping can not deal with large pose error because unlike our proposed RePOSE, feature warping can only consider the visible surface at the first step. Being different from the methods using feature warping, our iterative deep texture rendering method can generate a feature with a complete shape. We believe this characteristics of feature rendering leads to successful pose refinement.</p><p>Comparison with the latest refinement network on the LineMOD dataset. We compare our refinement network with the latest fully CNN-based refinement network proposed in the paper of DPOD <ref type="bibr" target="#b41">[42]</ref>. In this experiment, we use the same initial pose estimator <ref type="bibr" target="#b27">[28]</ref>. Since DPOD is fully CNN-based, we increased the amount of the dataset by twice. The refinement network of DPOD outputs a refined pose based on a cropped input RGB image and a synthetic rendering with an initial pose estimate. The experimental result in <ref type="table" target="#tab_4">Tables 4 and 5</ref> shows DPOD fails to refine pose well when trained with the small amount of the dataset. The refinement network of DPOD estimates a refined pose directly and do not consider projective geometry explicitly. This means their network needs to learn not only deep features but also mapping of the deep feature into an object's pose from training data. Several papers <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b30">31]</ref> report that learning a less complex task can achieve better accuracy and generalization in a 6D camera localization task. Also, we assume the low ADD(-S) score on Occlusion LineMOD dataset implies its low generalization performance to occluded objects. Our network only trains deep features and a refined object's pose is acquired by solving minimization problem based on projective geometry. From this experimental result, we believe the same principle proposed in the field of 6D camera localization is still valid in 6D object pose estimation. <ref type="table">Table 6</ref>: Comparison of number of iterations and refinement runtime. ADD(-S) on the Occlusion LineMOD dataset is reported in this table. Our proposed network is trained by using a pose loss for 5 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Iteration ADD(-S) Score Runtime AAE <ref type="bibr" target="#b41">[42]</ref> --200 ms SSD6D <ref type="bibr" target="#b41">[42]</ref> - Number of iteration and run time analysis. Our proposed refinement network, RePOSE can adjust the tradeoff between the accuracy and run time by changing the number of iterations. We show the ADD(-S) score and the run time on the Occlusion LineMOD dataset with each iteration count in <ref type="table">Table 6</ref>. On a machine equipped with Nvidia RTX2080 Super GPU and Ryzen 7 3700X CPU, our method takes 1.7 ms per iteration (deep texture rendering + pose update through LM optimization <ref type="bibr" target="#b24">[25]</ref>). This result shows our method achieves higher performance with the faster or comparable runtime than prior art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Real-time pose estimation needs accurate and fast pose refinement. Our proposed method, RePOSE uses efficient deep texture renderer to perform pose refinement at 92 FPS and has practical applications as a real-time 6D object tracker. Our experiments show that learnable deep textures coupled with the efficient non-linear optimization results in accurate 6D object poses. Further, our ablations highlight the fundamental limitations of a convolutional neural network to extract critical information useful for pose refinement. We believe that the concept of using efficient renderers with learnable deep textures instead of a CNN for pose refinement is an important conceptual change and will inspire a new research direction for real-time 6D object pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgment</head><p>This work is funded in part by the Department of Homeland Security award 2017-DN-077-ER0001, JST AIP Acceleration, Grant Number JPMJCR20U1, and JST CREST, Grant Number JPMJCR19F5, Japan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>RePOSE uses the initial estimated pose of PVNet on the LineMOD and Occlusion LineMOD dataset, and of PoseCNN on the YCB-Video dataset. PoseCNN has VGG-16 <ref type="bibr" target="#b33">[34]</ref> as the backbone network. However, since RePOSE reuses the deep feature from PoseCNN, its slow runtime of VGG-16 can be problematic in the case of tracking. Instead, we use ResNet-18 as the backbone network of PoseCNN. RePOSE makes use of the deep feature from ResNet-18, however, we still rely on the initial pose of the original PoseCNN with VGG-16. For tracking, the U-Net decoder of RePOSE also outputs the segmentation mask and use it as an additional signal to detect whether an object is lost or not. RePOSE learns its parameters separately per object on the LineMOD and Occlusion LineMOD dataset. In contrast to that, RePOSE learns the parameters for all the objects simultaneously on the YCB-Video dataset. We use pvnet-rendering 1 to generate synthetic images. However, we may be able to improve accuracy if images generated by photorealistic rendering are used for training. The neural textures, which are the outputs of the MLP that takes learnable parameters as input (ref. <ref type="figure" target="#fig_0">Fig.1 of the main paper)</ref>, are trained along with the MLP and input learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Derivation of Camera Jacobian Matrix</head><p>Notation: R ? SO(3) denotes a rotation matrix, t ? R 3 denotes a translation vector, w ? so(3) denotes a rotation vector, P = w t ? R 6 is a pose representation using a rotation and translation vector, f x and f y are focal lengths, p x and p y are the principal point offsets, X and x denote a homogeneous and inhomogeneous 2D image coordinate, subscript w, c denotes a coordinate is defined in the world, and camera coordinate system respectively, and Id ? R 3?3 is an 3 ? 3 identity matrix. R and w represents the same rotation.</p><p>Derivation: Jacobian matrix J of the objective function with respect to a pose P is required to perform deep featurebased pose optimization. We show the detailed derivation of a camera jacobian matrix ?x ?P at each pixel in Equation 9. The camera jacobian matrix can be decomposed more as follows;</p><formula xml:id="formula_16">?x ?P = ? ? ? ? ? ?R ?w ?x ?R ?x ?t ? ? ? ? ? ? R 6?2<label>(15)</label></formula><p>Using the derivative calculation method of a rotation matrix with respect to a rotation vector proposed in <ref type="bibr" target="#b13">[14]</ref>, the 1 https://github.com/zju3dv/pvnet-rendering following equation is acquired.</p><formula xml:id="formula_17">?R ?w = ?R ?w 1 ?R ?w 2 ?R ?w 3 T = ? ? ? ? ? ? ? ? ? ? ? vec w 1 [w] ? + [w ? (Id ? R)e 1 ] ? w 2 R vec w 2 [w] ? + [w ? (Id ? R)e 2 ] ? w 2 R vec w 3 [w] ? + [w ? (Id ? R)e 3 ] ? w 2 R ? ? ? ? ? ? ? ? ? ? ? ? R 3?9<label>(16)</label></formula><p>where vec is a vectorize operation, [x] ? is a conversion from a 3-d vector to a skew-symmetric matrix, e i is a ith 3-d basis vector. R is regarded as a 9-d vector for simplicity. A camera and image coordinate x c and x can be calculated as follows;</p><formula xml:id="formula_18">x c = Rx w + t (17) x = ? ? ? ? f x x c z c + p x f y y c z c + p y ? ? ? ?<label>(18)</label></formula><p>Using Equations <ref type="formula" target="#formula_2">(17)</ref> and <ref type="formula" target="#formula_2">(18)</ref>, the following derivatives can be obtained.  </p><formula xml:id="formula_19">?x ?R = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</formula><formula xml:id="formula_20">?x ?t = ? ? ? ? ? ? ? ? f x z c 0 0 f y z c ? f x x c z 2 c ? f y y c z 2 c ? ? ? ? ? ? ? ? ? R 3?2<label>(20)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Ablation Study</head><p>Ablation on effect of warping. We show the median of absolute angular and relative translation error on the LineMOD and Occlusion LineMOD datasets in <ref type="table" target="#tab_7">Tables 7  and 8</ref>. The relative translation error is computed with respect to object diameter. The initial pose error on the LineMOD dataset <ref type="bibr" target="#b15">[16]</ref> is relatively small and the methods using warping improve score properly. On the contrary, the initial pose error is large on the Occlusion LineMOD dataset. In that case, feature warping becomes less effective. Being different from the methods using feature warping, our iterative deep feature rendering method can generate a feature with a complete shape. We believe this characteristics of feature rendering leads to successful reduction of the error of rotation and translation on both datasets.</p><p>Ablation on the number of channels in the neural textures We vary the number of channels in the neural textures. We report the results on the LineMOD dataset in Ta- <ref type="table">Table 10</ref>: Runtime comparison among recent methods. In CNN with feature warping (FW) the initial feature is obtained using CNN and then warp the feature based on an updated pose. We assume the number of iterations is 5 for RePOSE and FW denotes feature warping. The FPS is reported with refinement of 5 objects.  Runtime Ablation on Feature Warping. DeepIM <ref type="bibr" target="#b21">[22]</ref> and CosyPose <ref type="bibr" target="#b19">[20]</ref> run a CNN every iteration on a concatenated image of a zoomed input and rendered images to compare these two images and output a pose directly. According to the ablation study by <ref type="bibr" target="#b21">[22]</ref>, high-resolution zoomed-in is a key and it improves the ADD(-S) score by 23.4. However, as shown in <ref type="table">Table 10</ref>, extracting image features from zoomed images multiple times leads to a slow runtime. Instead, RePOSE runs a CNN once for an input image with the original resolution. Additionally, an image representation of a rendered image can be extracted within 1ms because of deep texture rendering. This makes the runtime of RePOSE faster than prior methods while keeping a comparable accuracy to the prior methods. We also measure the runtime of RePOSE using different feature extraction methods for a rendered image. As shown in <ref type="table">Table 10</ref> in the supplemental and <ref type="table" target="#tab_4">Table 4</ref> and 5 in the main paper, RePOSE with deep texture rendering (Ours) achieves the fastest and highest accuracy among these three variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Runtime Ablation on the number of objects We investigate the trade-off between FPS and number of objects. As shown in 6, RePOSE is the only method which runs at over than 60 FPS even with refinement of 10 objects. 6D pose refinement is always performed after initial pose estimation. Thus, it is crucial to make sure it runs at faster than real-time <ref type="bibr">(30 FPS)</ref>. DeepIM causes an out of memory error when the number of object is more than 6 with a NVIDIA RTX2080 which has 8GB memory.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>RePOSE Framework: (a) 3D model with deep texture is projected to obtain (b) the rendered image representation with the deep texture renderer. (c) The pose is refined iteratively by minimizing the projection error of the rendered image representation and (e) the CNN feature extracted from (d) the input image via Levenberg-Marquardt (LM) optimization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " C Q P j 8 + s t w 4 7 6 q h g 0 w 1 j Y X H a k I Y o = " &gt; A A A C A 3 i c b Z D L S g M x F I Y z 9 V b r b d S d b o J F c F V m q q D L i l 2 4 c F H B X q A d h k y a t q F J Z k g y Q h k G 3 P g q b l w o 4 t a X c O f b m G l H 0 N Y f A h / / O Y e c 8 w c R o 0 o 7 z p d V W F p e W V 0 r r p c 2 N r e 2 d + z d v Z Y K Y 4 l J E 4 c s l J 0 A K c K o I E 1 N N S O d S B L E A 0 b a w f g q q 7 f v i V Q 0 F H d 6 E h G P o 6 G g A 4 q R N p Z v H / Q 4 0 i P J k 5 v U T 3 7 4 s l 5 P 0 5 J v l 5 2 K M x V c B D e H M s j V 8 O 3 P X j / E M S d C Y 4 a U 6 r p O p L 0 E S U 0 x I 2 m p F y s S I T x G Q 9 I 1 K B A n y k u m N 6 T w 2 D h 9 O A i l e U L D q f t 7 I k F c q Q k P T G e 2 p Z q v Z e Z / t W 6 s B x d e Q k U U a y L w 7 K N B z K A O Y R Y I 7 F N J s G Y T A w h L a n a F e I Q k w t r E l o X g z p + 8 C K 1 q x T 2 t V G / P y r V q H k c R H I I j c A J c c A 5 q 4 B o 0 Q B N g 8 A C e w A t 4 t R 6 t Z + v N e p + 1 F q x 8 Z h / 8 k f X x D T n I l 9 M = &lt; / l a t e x i t &gt; P ref &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R W X Y P R l i j K A L Q / N Z L z t c x d J M i m g = " &gt; A A A C A 3 i c b Z D L S s N A F I Z P v N Z 6 i 7 r T z W A R X J W k C r o s u H F Z w V 6 g D W U y n b R D Z 5 I w M x F K C L j x V d y 4 U M S t L + H O t 3 G S Z q G t P w x 8 / O c c 5 p z f j z l T 2 n G + r Z X V t f W N z c p W d X t n d 2 / f P j j s q C i R h L Z J x C P Z 8 7 G i n I W 0 r Z n m t B d L i o X P a d e f 3 u T 1 7 g O V i k X h v Z 7 F 1 B N 4 H L K A E a y N N b S P B w L r i R + k r W y Y F i x F K m m Q Z d W h X X P q T i G 0 D G 4 J N S j V G t p f g 1 F E E k F D T T h W q u 8 6 s f Z S L D U j n G b V Q a J o j M k U j 2 n f Y I g F V V 5 a 3 J C h M + O M U B B J 8 0 K N C v f 3 R I q F U j P h m 8 5 8 S 7 V Y y 8 3 / a v 1 E B 9 d e y s I 4 0 T Q k 8 4 + C h C M d o T w Q N G K S E s 1 n B j C R z O y K y A R L T L S J L Q / B X T x 5 G T q N u n t R b 9 x d 1 p q N M o 4 K n M A p n I M L V 9 C E W 2 h B G w g 8 w j O 8 w p v 1 Z L 1 Y 7 9 b H v H X F K m e O 4 I + s z x / N K p g 0 &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>F inp &lt; l a t e x i t s h a 1 _k ke k k 2 2 &lt;</head><label>12</label><figDesc>b a s e 6 4 = " D / N / T 9 j Y d H x t h G 6 O o i O S h b 7 G c m 4 = " &gt; A A A C A 3 i c b V D L S s N A F J 3 U V 6 2 v q D v d D B b B V U m q o M u C I C 4 r 2 A e 0 I U y m k 3 b o z C T M T I Q S A m 7 8 F T c u F H H r T 7 j z b 5 y k W W j r g Y H D O f c y 9 5 w g Z l R p x / m 2 K i u r a + s b 1 c 3 a 1 v b O 7 p 6 9 f 9 B V U S I x 6 e C I R b I f I E U Y F a S j q W a k H 0 u C e M B I L 5 h e 5 3 7 v g U h F I 3 G v Z z H x O B o L G l K M t J F 8 + 2 j I k Z 4 E Y X q T + W n B J U + p i L O s 5 t t 1 p + E U g M v E L U k d l G j 7 9 t d w F O G E E 6 E x Q 0 o N X C f W X o q k p p i R r D Z M F I k R n q I x G R g q E C f K S 4 s M G T w 1 y g i G k T R P a F i o v z d S x J W a 8 c B M 5 l e q R S 8 X / / M G i Q 6 v v D x S o o n A 8 4 / C h E E d w b w Q O K K S Y M 1 m h i A s q b k V 4 g m S C G t T W 1 6 C u x h 5 m X S b D f e 8 0 b y 7 q L e a Z R 1 V c A x O w B l w w S V o g V v Q B h 2 A w S N 4 B q / g z X q y X q x 3 6 2 M + W r H K n U P w B 9 b n D 8 y V m D Q = &lt; / l a t e x i t &gt; P ini &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + z Y e 4 A D s 9 3 s U d H S Q A O R w H Q L N 3 5 c = " &gt; A A A C A 3 i c b V D L S s N A F L 3 x W e s r 6 k 4 3 g 0 V w V Z I q 6 L L g x m U F + 4 A 2 l M l 0 0 g 6 d S c L M R C g h 4 M Z f c e N C E b f + h D v / x k m a h b Y e G D i c c y 9 z z / F j z p R 2 n G 9 r Z X V t f W O z s l X d 3 t n d 2 7 c P D j s q S i S h b R L x S P Z 8 r C h n I W 1 r p j n t x Z J i 4 X P a 9 a c 3 u d 9 9o F K x K L z X s 5 h 6 A o 9 D F j C C t Z G G 9 v F A Y D 3 x g 7 S V D d O C S 5 G y k G V Z d W j X n L p T A C 0 T t y Q 1 K N E a 2 l + D U U Q S Q U N N O F a q 7 z q x 9 l I s N S O c Z t V B o m i M y R S P a d / Q E A u q v L T I k K E z o 4 x Q E E n z Q o 0 K 9 f d G i o V S M + G b y f x K te j l 4 n 9 e P 9 H B t W c i x Y m m I Z l / F C Q c 6 Q j l h a A R k 5 R o P j M E E 8 n M r Y h M s M R E m 9 r y E t z F y M u k 0 6 i 7 F / X G 3 W W t 2 S j r q M A J n M I 5 u H A F T b i F F r S B w C M 8 w y u 8 W U / W i / V u f c x H V 6 x y 5 w j + w P r 8 A d G 2 m D c = &lt; / l a t e x i t &gt; e = F inp F rend &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / c K c + e P 6 Y w q n / B k 6 y q n r 0 Z 8 n l s w = " &gt; A A A C J H i c b V D L S g N B E J z 1 b X x F P X o Z D I I X w 2 4 U F E Q Q B P E Y w T w g C W F 2 0 q u D s 7 P L T K 8 Y l v 0 Y L / 6 K F w 8 + 8 O D F b 3 E 2 i a C J B Q M 1 V d 1 0 d / m x F A Z d 9 9 O Z m p 6 Z n Z t f W C w s L a + s r h X X N + o m S j S H G o 9 k p J s + M y C F g h o K l N C M N b D Q l 9 D w b 8 9 y v 3 E H 2 o h I X W E / h k 7 I r p U I B G d o p W 7 x u B 0 y v P G D F D J 6 Q n 8 + 5 1 m 3 j X C P q V B x R v c m d Q 2 q l 3 W L J b f s D k A n i T c i J T J C t V t 8 a / c i n o S g k E t m T M t z Y + y k T K P g E r J C O z E Q M 3 7 L r q F l q W I h m E 4 6 O D K j O 1 b p 0 S D S 9 i m k A / V 3 R 8 p C Y / q h b y v z b c 2 4 l 4 v / e a 0 E g 6 N O f m m C o P h w U J B I i h H N E 6 M 9 o Y G j 7 F v C u B Z 2 V 8 p v m G Y c b a 4 F G 4 I 3 f v I k q V f K 3 n 6 5 c n l Q O q 2 M 4 l g g W 2 S b 7 B K P H J J T c k G q p E Y 4 e S B P 5 I W 8 O o / O s / P u f A x L p 5 x R z y b 5 A + f r G 5 M 9 p f o = &lt; / l a t e x i t &gt; arg min P X l a t e x i t s h a 1 _ b a s e 6 4 = " z e J A H x 8 w u B h S u / Q 2 L b S O m t N 8 L 2 w = " &gt; A A A C V 3 i c b V H L T u M w F H X C q 5 T H F F i y s a i Q E I s q C Q t m i c S G Z U e a A l J d o h v 3 p l i 1 n W A 7 S F U I H z m a D b 8 y G y Z p i 8 T r S J a O z 7 n X v j 5 O c i m s C 4 I X z 1 9 Z X V v f a G 2 2 t 7 Z 3 d n 9 0 9 v a v b V Y Y j g O e y c z c J m B R C o 0 D J 5 z E 2 9 w g q E T i T T K 9 b P y b R z R W Z P q 3 m + U 4 U j D R I h U c X C 3 F H c 0 S n A h d 4 o M G Y 2 B 2 W r V L p s D d Z 3 n J j K J g J s 9 K 6 C p e q E l a 9 q u K S a G E s x V l t l D x l L I n + u Z i t d j H E b 2 j U Z u h H r 8 / O + 5 0 g 1 4 w B / 1 K w i X p k i X 6 c e c P G 2 e 8 U K g d l 2 D t M A x y N y r B O M E l V m 1 W W M y B T 2 G C w 5 p q U G h H 5 T y X i h 7 X y p i m m a m X d n S u v u 8 o Q V k 7 U 0 l d 2 Y x v P 3 u N + J 0 3 L F z 6 c 1 Q K n R c O N V 9 c l B a S u o w 2 I d O x M M i d n N U E u B H 1 r J T f g w H u 6 q 9 o Q g g / P / k r u Y 5 6 4 V k v + h V 1 L 6 J l H C 1 y S I 7 I C Q n J O b k g V 6 R P B o S T v + S f t + K t e i / e q 7 / u t x a l v r f s O S A f 4 O / 9 B 4 R V t b I = &lt; / l a t e x i t &gt;Texture ParametersLast iteration Only</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FFigure 2 :</head><label>2</label><figDesc>rend &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 G h a Z S r 0 c 7 b u v m P u y O j H f x 8 Q S e g = " &gt; A A A B / 3 i c b V D L S s N A F J 3 4 r P U V F d y 4 C R b B V U m q o M u C I C 4 r 2 A e 0 p U y m N + 3 Q y S T M 3 I g l d u G v u H G h i F t / w 5 1 / 4 6 T N Q l s P D B z O u Z d 7 5 v i x 4 B p d 9 9 t a W l 5 Z X V s v b B Q 3 t 7 Z 3 d u 2 9 / Y a O E s W g z i I R q Z Z P N Q g u o Y 4 c B b R i B T T 0 B T T 9 0 V X m N + 9 B a R 7 J O x z H 0 A 3 p Q P K A M 4 p G 6 t m H n Z D i 0 A / S 6 0 m v g / C A q Q L Z n / T s k l t 2 p 3 A W i Z e T E s l R 6 9 l f n X 7 E k h A k M k G 1 b n t u j N 2 U K u R M w K T Y S T T E l I 3 o A N q G S h q C 7 q b T / B P n x C h 9 J 4 i U e R K d q f p 7 I 6 W h 1 u P Q N 5 N Z W j 3 v Z e J / X j v B 4 L K b c h k n C J L N D g W J c D B y s j K c P l f A U I w N o U x x k 9 V h Q 6 o o Q 1 N Z 0 Z T g z X 9 5 k T Q q Z e + s X L k 9 L 1 U r e R 0 F c k S O y S n x y A W p k h t S I 3 X C y C N 5 J q / k z X q y X q x 3 6 2 M 2 u m T l O w f k D 6 z P H / U I l q g = &lt; / l a t e x i t &gt; times (1.7 ms / iter) Overview of the RePOSE refinement network. Given an input image I and the template 3D model M with deep textures, U-Net and deep texture renderer output features F inp and F rend respectively. We use Levenberg-Marquardt optimization [25] to obtain the refined pose P ref . The refined pose P ref after N iterations is used to compute the loss L ADD(?S) . The pre-trained encoder of the initial pose estimator is used. The decoder of U-Net and deep textures (seed parameters, and fc layers) are trained to minimize L ADD(-S) and L diff .non-linear least squares algorithms like Gauss-Newton and Levenberg-Marquardt<ref type="bibr" target="#b24">[25]</ref> into a deep learning network for efficient feature optimization in VisualSLAM. RePOSE is inspired by similar formulation as in<ref type="bibr" target="#b36">[37]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :Algorithm 1 :</head><label>41</label><figDesc>Example results on the Occlusion LineMOD dataset<ref type="bibr" target="#b5">[6]</ref>. We show an input RGB image, refined poses, and ground-truth pose from the top to bottom. The color of 3D bounding boxes are changed from purple to lightgreen as optimization progresses. RePOSE Training V = VERTICESOF3DMODEL(); F = FACESOF3DMODEL(); C = INITIALIZETEXTUREPARAMETERS(); # Iterate over Training Data for Pini, Pgt, I do Finp = UNET (I); P = Pini; for t times do Frend = DEEPTEXTURERENDER (P, V, F, C); e = vec(Finp) ? vec(Frend); J = JACOBIAN (Frend, P, V); ?P = POSEUPDATE (e, J, P); P = P + ?P; # Update Pose P ref = P; L = LOSS (Pref, Pgt, V); UPDATEPARAMETERS(L, C, UNET);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of object's appearance between an input RGB image and rendered image. Difference of illumination makes pose refinement in RGB space challenging.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Trade-off between FPS and number of objects of recent methods. ble 9. Note, the results are comparable, however, setting number of channels to 3 results in the best performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="6">Comparison of RePOSE on Linemod dataset with</cell></row><row><cell cols="6">recent methods including PVNet [28], DPOD [42], Hybrid-</cell></row><row><cell cols="6">Pose [35], and EfficientPose [9] using the ADD(-S) score.</cell></row><row><cell cols="6"># of wins denotes in how many objects the method achieves</cell></row><row><cell cols="2">the best score.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Object</cell><cell cols="5">PVNet DPOD HybridPose EfficientPose RePOSE</cell></row><row><cell>Ape</cell><cell>43.6</cell><cell>87.7</cell><cell>63.1</cell><cell>89.4</cell><cell>79.5</cell></row><row><cell>Benchvise</cell><cell>99.9</cell><cell>98.5</cell><cell>99.9</cell><cell>99.7</cell><cell>100</cell></row><row><cell>Camera</cell><cell>86.9</cell><cell>96.1</cell><cell>90.4</cell><cell>98.5</cell><cell>99.2</cell></row><row><cell>Can</cell><cell>95.5</cell><cell>99.7</cell><cell>98.5</cell><cell>99.7</cell><cell>99.8</cell></row><row><cell>Cat</cell><cell>79.3</cell><cell>94.7</cell><cell>89.4</cell><cell>96.2</cell><cell>97.9</cell></row><row><cell>Driller</cell><cell>96.4</cell><cell>98.8</cell><cell>98.5</cell><cell>99.5</cell><cell>99.0</cell></row><row><cell>Duck</cell><cell>52.6</cell><cell>86.3</cell><cell>65.0</cell><cell>89.2</cell><cell>80.3</cell></row><row><cell>Eggbox</cell><cell>99.2</cell><cell>99.9</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>Glue</cell><cell>95.7</cell><cell>98.7</cell><cell>98.8</cell><cell>100</cell><cell>98.3</cell></row><row><cell>Holepuncher</cell><cell>81.9</cell><cell>86.9</cell><cell>89.7</cell><cell>95.7</cell><cell>96.9</cell></row><row><cell>Iron</cell><cell>98.9</cell><cell>100</cell><cell>100</cell><cell>99.1</cell><cell>100</cell></row><row><cell>Lamp</cell><cell>99.3</cell><cell>96.8</cell><cell>99.5</cell><cell>100</cell><cell>99.8</cell></row><row><cell>Phone</cell><cell>92.4</cell><cell>94.7</cell><cell>94.9</cell><cell>98.5</cell><cell>98.9</cell></row><row><cell>Average</cell><cell>86.3</cell><cell>95.2</cell><cell>91.3</cell><cell>97.4</cell><cell>96.1</cell></row><row><cell># of wins</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>6</cell><cell>8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Comparison of RePOSE on Occlusion LineMOD dataset with recent methods including PVNet</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study of feature representation, feature warping, and a refinement network on the LineMOD dataset. RGB denotes pose refinement using photometric error. FW denotes feature warping after extraction from a CNN or deep texture rendering following first iteration. DPOD denotes using DPOD's refinement network and PVNet as an initial pose estimator. FW, DPOD, and RePOSE are trained with the same dataset, we report the ADD(-S) scores.</figDesc><table><row><cell>Object</cell><cell cols="6">PVNet [28] RGB CNN w/ FW DPOD Ours w/ FW Ours</cell></row><row><cell>Ape</cell><cell>43.6</cell><cell>5.81</cell><cell>65.4</cell><cell>51.2</cell><cell>75.9</cell><cell>79.5</cell></row><row><cell>Benchvise</cell><cell>99.9</cell><cell>75.6</cell><cell>99.8</cell><cell>99.5</cell><cell>100</cell><cell>100</cell></row><row><cell>Camera</cell><cell>86.9</cell><cell>7.06</cell><cell>96.3</cell><cell>91.1</cell><cell>98.2</cell><cell>99.2</cell></row><row><cell>Can</cell><cell>95.5</cell><cell>3.05</cell><cell>99.1</cell><cell>95.7</cell><cell>99.4</cell><cell>99.8</cell></row><row><cell>Cat</cell><cell>79.3</cell><cell>3.00</cell><cell>88.6</cell><cell>92.4</cell><cell>92.7</cell><cell>97.9</cell></row><row><cell>Driller</cell><cell>96.4</cell><cell>80.9</cell><cell>7.6</cell><cell>98.2</cell><cell>98.7</cell><cell>99</cell></row><row><cell>Duck</cell><cell>52.6</cell><cell>0.00</cell><cell>76.2</cell><cell>71.3</cell><cell>84.6</cell><cell>80.3</cell></row><row><cell>Eggbox</cell><cell>99.2</cell><cell>8.64</cell><cell>96.4</cell><cell>99.9</cell><cell>100</cell><cell>100</cell></row><row><cell>Glue</cell><cell>95.7</cell><cell>5.40</cell><cell>97.2</cell><cell>97.6</cell><cell>98.2</cell><cell>98.3</cell></row><row><cell>Holepuncher</cell><cell>81.9</cell><cell>18.7</cell><cell>77.2</cell><cell>89.7</cell><cell>95.1</cell><cell>96.9</cell></row><row><cell>Iron</cell><cell>98.9</cell><cell>40.7</cell><cell>98.7</cell><cell>97.9</cell><cell>99.7</cell><cell>100</cell></row><row><cell>Lamp</cell><cell>99.3</cell><cell>34.9</cell><cell>91.8</cell><cell>95.5</cell><cell>100</cell><cell>99.8</cell></row><row><cell>Phone</cell><cell>92.4</cell><cell>14.6</cell><cell>94.9</cell><cell>97.2</cell><cell>98.7</cell><cell>98.9</cell></row><row><cell>Average</cell><cell>86.3</cell><cell>23.0</cell><cell>90.7</cell><cell>90.5</cell><cell>95.5</cell><cell>96.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of feature representation, feature warping, and a refinement network on the Occlusion LineMOD dataset. We report the ADD(-S) scores, all other details are same as inTable 4.</figDesc><table><row><cell>Object</cell><cell cols="6">PVNet [28] RGB CNN w/ FW DPOD Our w/ FW Ours</cell></row><row><cell>Ape</cell><cell>15.8</cell><cell>4.96</cell><cell>22.7</cell><cell>22.0</cell><cell>25.8</cell><cell>31.1</cell></row><row><cell>Can</cell><cell>63.3</cell><cell>5.22</cell><cell>66.4</cell><cell>71.1</cell><cell>61.3</cell><cell>80.0</cell></row><row><cell>Cat</cell><cell>16.7</cell><cell>0.17</cell><cell>11.7</cell><cell>21.9</cell><cell>19.4</cell><cell>25.6</cell></row><row><cell>Driller</cell><cell>65.7</cell><cell>61.7</cell><cell>72.1</cell><cell>68.3</cell><cell>71.1</cell><cell>73.1</cell></row><row><cell>Duck</cell><cell>25.2</cell><cell>1.80</cell><cell>36.5</cell><cell>30.8</cell><cell>40.8</cell><cell>43.0</cell></row><row><cell>Eggbox</cell><cell>50.2</cell><cell>7.75</cell><cell>45.4</cell><cell>42.4</cell><cell>47.7</cell><cell>51.7</cell></row><row><cell>Glue</cell><cell>49.6</cell><cell>1.88</cell><cell>45.6</cell><cell>41.3</cell><cell>49.4</cell><cell>54.3</cell></row><row><cell>Holepuncher</cell><cell>39.7</cell><cell>21.5</cell><cell>40.8</cell><cell>43.3</cell><cell>40.2</cell><cell>53.6</cell></row><row><cell>Average</cell><cell>40.8</cell><cell>13.1</cell><cell>42.9</cell><cell>42.6</cell><cell>44.5</cell><cell>51.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Comparison of the median of absolute angular and relative translation error on the LindMOD dataset<ref type="bibr" target="#b15">[16]</ref>. We do not report the rotation error of symmetric objects (eggbox, and glue) because of its non-unique rotation representation.</figDesc><table><row><cell>Object</cell><cell cols="2">PVNet [28]</cell><cell cols="2">CNN w/ FW</cell><cell cols="2">Ours w/ FW</cell><cell>Ours</cell><cell></cell></row><row><cell></cell><cell cols="8">Rotation Translation Rotation Translation Rotation Translation Rotation Translation</cell></row><row><cell>Ape</cell><cell>2.213 ?</cell><cell>0.119</cell><cell>1.849 ?</cell><cell>0.069</cell><cell>1.446 ?</cell><cell>0.055</cell><cell>1.197 ?</cell><cell>0.051</cell></row><row><cell>Benchvise</cell><cell>1.030 ?</cell><cell>0.022</cell><cell>0.857 ?</cell><cell>0.022</cell><cell>0.644 ?</cell><cell>0.014</cell><cell>0.757 ?</cell><cell>0.010</cell></row><row><cell>Cam</cell><cell>1.183 ?</cell><cell>0.045</cell><cell>0.896 ?</cell><cell>0.030</cell><cell>1.322 ?</cell><cell>0.073</cell><cell>0.713 ?</cell><cell>0.023</cell></row><row><cell>Can</cell><cell>0.958 ?</cell><cell>0.032</cell><cell>1.238 ?</cell><cell>0.033</cell><cell>0.995 ?</cell><cell>0.040</cell><cell>0.674 ?</cell><cell>0.017</cell></row><row><cell>Cat</cell><cell>1.260 ?</cell><cell>0.050</cell><cell>1.177 ?</cell><cell>0.041</cell><cell>0.941 ?</cell><cell>0.036</cell><cell>0.857 ?</cell><cell>0.035</cell></row><row><cell>Driller</cell><cell>1.008 ?</cell><cell>0.029</cell><cell>0.812 ?</cell><cell>0.023</cell><cell>1.057 ?</cell><cell>0.060</cell><cell>0.773 ?</cell><cell>0.014</cell></row><row><cell>Duck</cell><cell>1.701 ?</cell><cell>0.078</cell><cell>1.701 ?</cell><cell>0.055</cell><cell>1.531 ?</cell><cell>0.042</cell><cell>1.481 ?</cell><cell>0.053</cell></row><row><cell>Eggbox</cell><cell>-</cell><cell>0.056</cell><cell>-</cell><cell>0.074</cell><cell>-</cell><cell>0.048</cell><cell>-</cell><cell>0.025</cell></row><row><cell>Glue</cell><cell>-</cell><cell>0.050</cell><cell>-</cell><cell>0.049</cell><cell>-</cell><cell>0.040</cell><cell>-</cell><cell>0.036</cell></row><row><cell>Holepuncher</cell><cell>1.265 ?</cell><cell>0.052</cell><cell>1.548 ?</cell><cell>0.058</cell><cell>1.295 ?</cell><cell>0.060</cell><cell>1.009 ?</cell><cell>0.029</cell></row><row><cell>Iron</cell><cell>1.205 ?</cell><cell>0.027</cell><cell>1.223 ?</cell><cell>0.027</cell><cell>0.927 ?</cell><cell>0.020</cell><cell>0.911 ?</cell><cell>0.015</cell></row><row><cell>Lamp</cell><cell>1.050 ?</cell><cell>0.029</cell><cell>1.235 ?</cell><cell>0.042</cell><cell>1.054 ?</cell><cell>0.019</cell><cell>0.902 ?</cell><cell>0.020</cell></row><row><cell>Phone</cell><cell>1.208 ?</cell><cell>0.040</cell><cell>1.122 ?</cell><cell>0.032</cell><cell>0.925 ?</cell><cell>0.019</cell><cell>0.889 ?</cell><cell>0.025</cell></row><row><cell>Average</cell><cell>1.280 ?</cell><cell>0.048</cell><cell>1.242 ?</cell><cell>0.043</cell><cell>1.103 ?</cell><cell>0.040</cell><cell>0.924 ?</cell><cell>0.027</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Comparison of the median of absolute angular and relative translation error on the Occlusion LindMOD dataset<ref type="bibr" target="#b5">[6]</ref>. We do not report the rotation error of symmetric objects (eggbox, and glue) because of its non-unique rotation representation.</figDesc><table><row><cell>Object</cell><cell cols="2">PVNet [28]</cell><cell cols="2">CNN w/ FW</cell><cell cols="2">Ours w/ FW</cell><cell>Ours</cell><cell></cell></row><row><cell></cell><cell cols="8">Rotation Translation Rotation Translation Rotation Translation Rotation Translation</cell></row><row><cell>Ape</cell><cell>4.103 ?</cell><cell>0.210</cell><cell>4.222 ?</cell><cell>0.185</cell><cell>4.015 ?</cell><cell>0.171</cell><cell>3.871 ?</cell><cell>0.157</cell></row><row><cell>Can</cell><cell>2.722 ?</cell><cell>0.068</cell><cell>2.879 ?</cell><cell>0.069</cell><cell>2.793 ?</cell><cell>0.067</cell><cell>2.704 ?</cell><cell>0.043</cell></row><row><cell>Cat</cell><cell>6.012 ?</cell><cell>0.239</cell><cell>6.480 ?</cell><cell>0.363</cell><cell>6.552 ?</cell><cell>0.335</cell><cell>5.852 ?</cell><cell>0.274</cell></row><row><cell>Driller</cell><cell>2.774 ?</cell><cell>0.072</cell><cell>2.567 ?</cell><cell>0.131</cell><cell>2.762 ?</cell><cell>0.120</cell><cell>2.545 ?</cell><cell>0.056</cell></row><row><cell>Duck</cell><cell>6.923 ?</cell><cell>0.156</cell><cell>6.795 ?</cell><cell>0.115</cell><cell>6.537 ?</cell><cell>0.108</cell><cell>6.533 ?</cell><cell>0.102</cell></row><row><cell>Eggbox</cell><cell>-</cell><cell>0.325</cell><cell>-</cell><cell>0.295</cell><cell>-</cell><cell>0.284</cell><cell>-</cell><cell>0.318</cell></row><row><cell>Glue</cell><cell>-</cell><cell>0.275</cell><cell>-</cell><cell>0.246</cell><cell>-</cell><cell>0.235</cell><cell>-</cell><cell>0.266</cell></row><row><cell>Holepuncher</cell><cell>3.969 ?</cell><cell>0.121</cell><cell>3.917 ?</cell><cell>0.116</cell><cell>3.918 ?</cell><cell>0.126</cell><cell>3.629 ?</cell><cell>0.088</cell></row><row><cell>Average</cell><cell>4.417 ?</cell><cell>0.183</cell><cell>4.477 ?</cell><cell>0.190</cell><cell>4.430 ?</cell><cell>0.181</cell><cell>4.189 ?</cell><cell>0.163</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Ablation of the channel size on the LineMOD dataset</figDesc><table><row><cell>Channel Size</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>7</cell><cell>9</cell></row><row><cell>ADD(-S)</cell><cell cols="5">95.2 96.1 95.7 94.9 94.5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Einscan Pro 2x</surname></persName>
		</author>
		<ptr target="https://www.einscan.com/handheld-3d-scanner/einscan-pro-2x/.2" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bundle adjustment -a modern synthesis</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Vision Algorithms: Theory and Practice</title>
		<meeting>the International Workshop on Vision Algorithms: Theory and Practice</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bundle adjustment in the large</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Photometric bundle adjustment for visionbased slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brett</forename><surname>Hatem Said Alismail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Browning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lucas-kanade 20 years on: A unifying framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning 6d object pose estimation using 3d object coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DSAC -Differentiable RANSAC for Camera Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning less is more -6d camera localization via 3d surface regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Efficientpose: An efficient, accurate and scalable end-to-end 6d multi object pose estimation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Bukschat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Vetter</surname></persName>
		</author>
		<editor>CoRR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The ycb object and model set: Towards common benchmarks for manipulation research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berk</forename><surname>Calli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Walsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">M</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICAR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">End-to-end learnable geometric vision by backpropagating pnp optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jun</forename><surname>Chin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to solve nonlinear least squares for monocular stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bloesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Czarnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A compact formula for the derivative of a 3-d rotation in exponential coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Yezzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Kurt Konolige, and Nassir Navab. Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural 3d mesh renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroharu</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ssd-6d: Making rgbbased 3d detection and 6d pose estimation great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cosypose: Consistent multi-view multi-object 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Labbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carpentier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Epnp: An accurate o(n) solution to the pnp problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deepim: Deep iterative matching for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An Iterative Image Registration Technique with an Application to Stereo Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Active appearance models revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<idno>CMU-RI-TR-03-02</idno>
		<imprint>
			<date type="published" when="2003" />
			<pubPlace>Pittsburgh, PA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The levenberg-marquardt algorithm: Implementation and theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><forename type="middle">J</forename><surname>Mor?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Numerical Analysis</title>
		<imprint>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">ORB-SLAM: A Versatile and Accurate Monocular SLAM System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Tard?s</surname></persName>
		</author>
		<editor>T-RO</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Making deep heatmaps robust to partial occlusions for 3d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pvnet: Pixelwise voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bb8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient &amp; effective prioritized matching for large-scale imagebased localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kobbelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Understanding the limitations of cnn-based absolute camera pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qunjie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Structure-frommotion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hybridpose: 6d object pose estimation under hybrid representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Implicit 3d orientation learning for 6d object detection from rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Zoltan-Csaba Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolph</forename><surname>Brucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">BA-Net: Dense Bundle Adjustment Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengzhou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>To</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakumar</forename><surname>Sundaralingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Birchfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CoRL</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">GN-Net: The Gauss-Newton Loss for Multi-Weather Relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Stumberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Nemo: Neural mesh models of contrastive features for robust 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angtian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanner</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatraman</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dpod: 6d pose object detector and refiner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Shugurov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">In Computer Vision: A Reference Guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Iterative Closest Point (ICP)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
