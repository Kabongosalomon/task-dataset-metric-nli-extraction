<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Breaking with Fixed Set Pathology Recognition through Report-Guided Contrastive Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantin</forename><surname>Seibold</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Rei?</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saquib Sarfraz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Kleesiek</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University Medicine Essen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Breaking with Fixed Set Pathology Recognition through Report-Guided Contrastive Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When reading images, radiologists generate text reports describing the findings therein. Current state-of-the-art computer-aided diagnosis tools utilize a fixed set of predefined categories automatically extracted from these medical reports for training. This form of supervision limits the potential usage of models as they are unable to pick up on anomalies outside of their predefined set, thus, making it a necessity to retrain the classifier with additional data when faced with novel classes. In contrast, we investigate direct text supervision to break away from this closed set assumption. By doing so, we avoid noisy label extraction via text classifiers and incorporate more contextual information. We employ a contrastive global-local dual-encoder architecture to learn concepts directly from unstructured medical reports while maintaining its ability to perform free form classification. We investigate relevant properties of open set recognition for radiological data and propose a method to employ currently weakly annotated data into training. We evaluate our approach on the large-scale chest X-Ray datasets MIMIC-CXR, CheXpert, and ChestX-Ray14 for disease classification. We show that despite using unstructured medical report supervision, we perform on par with direct label supervision through a sophisticated inference setting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Radiologists interpret a vast amount of imaging data and summarize their insights as medical reports. This documentation accumulates large databases of radiological imaging and accompanying findings, i.e., millions of collected chest radiographs annually <ref type="bibr" target="#b0">[1]</ref>. Computer-aided-diagnosis (CAD) systems utilize these databases to streamline the clinical workflow and save time <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b7">8]</ref>. Modern CAD tools often rely on deep learning models <ref type="bibr" target="#b16">[17]</ref> using large-scale data sets such as MIMIC-CXR <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b1">2]</ref> for training. Training for such tasks requires hand-designed supervision, typically by extracting a fixed set of predefined labels from the reports using rule-or deep learning-based models <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20]</ref>. Such training typically requires hand-designed supervision in the form of extracting a set of labels from the reports using rule-or deep learning-based models <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20]</ref>. While these tools can deliver acceptable performance on a subset of diseases <ref type="bibr" target="#b24">[25]</ref>, arXiv:2205.07139v1 [cs.CV] 14 May 2022 they lack generalization capabilities for diseases that were not part of the fixed label-set used for training. To add disease classes requires substantial effort annotating the data with extra labels and retraining the system. To circumvent this, one can approach training in a class-agnostic manner, however, it becomes unclear how models can still be applied to classify diseases.</p><p>Recent methods based on contrastive language-image pre-training <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b23">24]</ref> indicate that by large-scale multi-modal representation learning, object recognition can be detached from prior fixed-set, hand-designed class definitions. These models learn joint feature spaces between images and textual descriptions and utilize text prompts to transform recognition from learned fix-set classification to a matching task between text and image embeddings. Radiological reports, in contrast to natural-image captions, have an inherently different structure, as they encompass multiple distinct sentences such that their entirety describes all relevant information. This shift makes a direct application of existing methods non-trivial.</p><p>In this work, we see our contributions as the following:</p><p>1. We address training through report supervision by considering radiological reports in one of two ways: The local level, assuming each sentence conveys a distinct concept relevant for the patient, and secondly, the global report view, which encodes the entirety of the findings. 2. We propose a novel inference setting that allows us to query any desired finding, and the CAD system generates a binary decision regarding its presence in the given radiological imagery.</p><p>3. We provide an extensive study on various factors impacting the performance of multi-modal training and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Global-Local Contrastive Learning</head><p>We illustrate our method for report-based training of a vision model and inference protocol specifically designed for disease recognition in <ref type="figure" target="#fig_0">Figure 1</ref>. To tackle the complexity of medical reports, we split representations into a sentence-and report-level from a shared visual and language encoder. We consider embeddings for both the presence and absence of a pathology for its prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Overview</head><p>Contrastive language and image pretraining (CLIP) has shown immense potential for object recognition in natural images <ref type="bibr" target="#b17">[18]</ref> through learning from imagecaption pairs. In contrast to textual descriptions in CLIP-based models, medical reports typically consists of multiple sentences focusing on different parts in the image. As each sentence contains specific subset of information, we aim to capture sentence-(local) and report-level (global) context in our representations. Thus, our model builds on separate image-and text encoders ? and ?, which embed an image I via z I = ?(I) and a sentence by z s = ?(s), respectively. In training, for a given report R, we capture the local context by splitting the full report into its sentences R = {s 1 , . . . , s n } and subsequently extract sentencelevel embeddings z si = ?(s i ), s i ? R. To generate global embeddings that contain the full information of the whole report, the sentence-level embeddings are aggregated through attention pooling: z R = Attn([z s1 , . . . , z s |R| ]) <ref type="bibr" target="#b20">[21]</ref>. To embed z I , z s and z R into shared multi-modal representations, we project sentences and reports via linear transformations p S and p R into two feature spaces. As the image encoder has access to global image information for report-level prediction as well as to local image patterns for selective sentence-dependent prediction, we project z I twice: into a global representation p G (z I ) which shall align with p R (z R ) and a representation for local patterns p L (z I ) for alignment with p S (z s ).</p><p>During training, we are provided with a dataset of image-report pairs (I i , R i ) ? (I 1 , R 1 ), . . . , (I N , R N ). For brevity and clarity in subsequent formulas, we will write projections, e.g. the global projection of an image I i as p G i instead of p G i (?(I i )), for the projection of the k th sentence from report R i , we write p S ik .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training Objectives</head><p>Local Contrast: While radiological reports describe the assessment of a patient's health, not every sentence is directly linked to specific findings, some sentences mention clinical procedures or required follow-up examinations. However, we can assume that all clinically relevant information is present in a subset of sentences in the report due to the doctors' obligation to document the findings. This property is the core of the multiple-instance learning (MIL) assumption.</p><p>Therefore, it might seem natural to choose MILNCE <ref type="bibr" target="#b13">[14]</ref> as the MIL-based objective for integrate sentences in training, yet, this assumption only holds when normalizing over sentences, as not every sentence has to match the image. However, if a sentence fits an image, it should match strictly that image, thus, we hold the regular formulation when normalizing over images. As such, MILNCE does not quite fit this use-case and we redesign it by splitting its symmetry:</p><formula xml:id="formula_0">L L (I i , R i ) = ? log n k=1 exp(?(p L i , p S ik )/? L ) N j=1 n m=1 exp(?(p L i , p S jm )/? L ) ? n k=1 log exp(?(p L i , p S ik )/? L ) N j=1 exp(?(p L j , p S ik )/? L ) ,<label>(1)</label></formula><p>with ? L being a learned parameter and ?(?, ?) denoting the cosine similarity.</p><p>Global Contrast: For our batch we assume that an image-report pair is unique and formulate the following objective leveraging the attention-fused reports via:</p><formula xml:id="formula_1">L G (I i , R i ) = ? log exp(?(p G i , p R i )/? G ) N j=1 exp(?(p G i , p R j )/? G ) ? log exp(?(p G i , p R i )/? G ) N j=1 exp(?(p G j , p R i )/? G )<label>(2)</label></formula><p>Self-Supervision: CLIP has been established as a data-hungry algorithm <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b15">16]</ref>. Several recent methods combine intrinsic supervision signals with the CLIP objective to make full use of the available data <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref>. As we have access to severely smaller datasets in the medical domain as compared to the natural image domain, we follow Li et al. <ref type="bibr" target="#b11">[12]</ref> and integrate SimSiam <ref type="bibr" target="#b3">[4]</ref>. For this, we generate two augmented versions of the input image A 1 (I) and A 2 (I) and add a threelayer encoder-head p E and a two-layer prediction-head p P on top of the visual backbone ? to enforce similarity between the two views:</p><formula xml:id="formula_2">L S (A 1 (I), A 2 (I)) = ??(p P A1(I) , detach(p E A2(I) )) ? ?(p P A2(I) , detach(p E A1(I) )) (3)</formula><p>Furthermore, we utilize the augmented images used for the self-supervised objective to mirror our text-image objectives to the augmented samples.</p><formula xml:id="formula_3">L M (I i , R i ) = L G (A 1 (I i ), R i ) + L L (A 1 (I i ), R i ) + L G (A 2 (I i ), R i ) + L L (A 2 (I i ), R i )<label>(4)</label></formula><p>Our final objective for report-based contrastive learning amounts to:</p><formula xml:id="formula_4">L(I i , R i ) = ? 1 * L L (I i , R i )+? 2 * L G (I i , R i )+? 3 * L S (I i , R i )+? 4 * L M (I i , R i ) (5)</formula><p>with ? 1 = ? 2 = ? 3 = 0.5 and ? 4 = 0.25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model Inference</head><p>For fixed set classification models, the inference process is straightforward: A given image I passed through a network with an activation in the final layer, returning class-wise pseudo probabilities. When model architecture and training procedure do not permit a classification layer, methods often resort to zeroshot-like inference <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24]</ref> where a nearest neighbor search in semantic space is conducted <ref type="bibr" target="#b4">[5]</ref>. In our considered design a text-based query is used to infer the presence or absence of a given disease. In similar CLIP-like models, the text embeddings (e.g., of the disease names) can be matched to an image embedding. The query with the maximum similarity can then be retained as matched.</p><p>Such a matching based disease discovery is feasible for detecting single disease class. The underlying assumption of having exclusively one dominant class to predict does not hold for chest radiographs as pathologies are not mutually exclusive. Similarly, modeling co-occurrence as individual classes is also infeasible due to the exponentially rising number of possible class combinations. As such, inference for a multi-label classification needs to be formulated for such contrastively trained methods.</p><p>We perform this, by querying an image with class-related textual prompts and interpreting their similarity scores as prediction probabilities for the respective disease class. In practice, we notice that a single query for class presence is ambiguous since the text embeddings of words and their negations may fall close to one another in the feature space. Due to this proximity of opposing semantics a query could be mistaken with the negation of its class.</p><p>To overcome this issue, we propose to perform inference over two sets of queries (q p c , q n c ) for each class. While the query embedding q p c indicates the occurrence of a class c, q n c indicates its absence, e.g. opacities consistent with pneumonia for presence as opposed to the lungs are clear for its absence. </p><p>where ? is the respective learned scaling factor depending on the used projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Prompt Engineering</head><p>Several works on zero-shot classification perform their inference by extracting the features of the class name through a word embedding model <ref type="bibr" target="#b21">[22]</ref>. While sufficient for most zero-shot applications a lot of context regarding the class is lost. In order to effectively utilize language-vision models it is necessary to align the downstream task to the training <ref type="bibr" target="#b17">[18]</ref>. As such we model a set of positive and negative prompts applicable for pathologies to enrich our matching process between visual and textual projections. While for our basic approach we consider ('{class}', 'No {class}') prompts, we found that a more detailed prompt design can overall deliver improved performance. As such we consider a set of prompts following the  Prompt-based Dataset Extension: Despite medical reports being the more common resource in the practical field, currently the majority of large-scale datasets are only publically available with fixed sets of labels. In order to investigate the effect of additional data in training of our method, we reverse our proposed prompt engineering to generate synthetic reports for the datasets Pad-Chest and ChestX-Ray14 based on their class-labels. Through this procedure, we are able to sample sentences indicating presence or absence of a class and generate more than 200k added image-report pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets:</p><p>-MIMIC-CXR: It contains 377,110 chest X-rays taken from 65179 patients with 14 disease labels and 227,835 reports. We use the splits provided by <ref type="bibr" target="#b9">[10]</ref>. Unless further specified all models were trained on this dataset. -CheXpert: It contains 224,316 chest X-rays taken from 65,240 patients with 14 disease labels. The labels are shared with MIMIC-CXR. We only consider the validation split provided by <ref type="bibr" target="#b6">[7]</ref>. -ChestX-ray14: It contains 112,120 frontal-view chest X-rays taken from 30,805 patients with 14 disease labels. We use the splits provided by <ref type="bibr" target="#b18">[19]</ref>. -PadChest: It consists 160k chest X-rays of 67k patients with 174 findings.   <ref type="table" target="#tab_1">Table 2</ref> we consider validation performance. Labels with value -2 and -1 are ignored for the calculation of the metric as their state is not certain. For all ablations, we use the "basic"-prompting scheme, while for further experiments the "detailed"-scheme is used. Implementation Details: For all experiments we use the same ResNet50 and Transformer as Redford et al. <ref type="bibr" target="#b17">[18]</ref> as backbones. We optimize with AdamW <ref type="bibr" target="#b12">[13]</ref>, a learning rate of 0.0001 and a cosine schedule. We trained classification models with a learning rate of 0.0005 as this has shown slightly better performance. During training, we resize the images to the inference size of 320 ? 320 and randomly crop by 288x288. For specific further augmentations, we follow SimSiam <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Ablation -Effect of Heads: We investigate the impact of both prediction heads during inference. We start by showing the individual head performance and then go over to different fusion approaches on the left of Table1. For feature fusion we consider the concatenation of local and global features of the same modality. For score-fusions, we calculate scores as described above and aggregate the class predictions based on their maximum or average. We see that for our method both the global and local head show nearly the same performance. While performing max-score fusion the across-datasetperformance drops by 0.4%, where mean-score fusion improves by 0.6%. Ablation -Effect of Losses: On the right of Table1, we show the impact of the objective functions. We see that adding local contrast improves the model by 2%. Adding the self-supervised and mirrored objective worsen performance by 0.45%. It can be noted that the self-supervised loss achieves the best performance on ChestX-Ray14 by more than 2%. whereas adding both simultaneously improves across dataset performance by 0.6%. Multi-label Inference and Prompt Engineering: We show the impact of our proposed inference scheme and prompts in <ref type="figure" target="#fig_3">Figure 2</ref>. We see that performance overall improves with significant improvements for some classes such as fractures which were unable to be categorized just using cosine-similarity. When the detailed prompt the mean performance further improves. Data size Impact: We show the impact of using additional prompt-based reports during training in <ref type="figure" target="#fig_4">Figure 3</ref>. We see that including artificial training data for ChestX-Ray14 significantly improves its validation performance. In general it seems that while for some classes performance seems to worsen, the overall performance improves when adding additional data.</p><p>Comparison with Other Approaches: We compare against the same vision network trained with label supervision on its respective dataset. All other methods were trained using the MIMIC-CXR dataset. SLIP <ref type="bibr" target="#b14">[15]</ref> refers to a version of CLIP, which incorporates self-supervision in form of a SIMCLR-like objective <ref type="bibr" target="#b2">[3]</ref>. M ILN CE local refers to our local branch trained with the MILNCE <ref type="bibr" target="#b13">[14]</ref> objective alone. LoCo and GloCo refer to our method trained with either just the local or global objective respectively. We evaluate using the "detailed"-prompt scheme. We show the results in <ref type="table" target="#tab_1">Table 2</ref>. We see that our formulation of the local contrastive loss outperforms the MILNCE version across all datasets. Our proposed method outperforms the considered contrastive language-image pretraining baselines in the form of CLIP and SLIP and manages to achieve similar performance as the supervised ResNet for domains similar to MIMIC, however, underperforms for the ChestX-Ray14 dataset. When adding the additional report datasets of PadChest and ChestX-Ray14 we manage to beat label-supervised performance across all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed an approach to make networks less reliant to label supervision through contrastive language-image pre-training on report level. In order to still maintain competitive levels of performance we introduced a novel way of constructing inference. Doing so we are able to offset issues stemming from explicit class similarities. We show that despite using unstructured medical report supervision, we perform on par with explicit label supervision through a sophisticated inference setting across different datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of our proposed method. Training on the left, inference on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Then, the cosine similarity between the image and both queries ?(p I , q p c ) and ?(p I , q n c ) is computed with the final prediction wrt. class c being defined as: P (c, I) = exp(?(p I , q p c )/? ) exp(?(p I , q p c )/? ) + exp(?(p I , q n c )/? ) ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>templates ('{adverb}{indication verb} {effect} * {location} * {class synonym}', '{adverb} {indication verb} {absence} {class synonym}'). Hereby, we utilize all combinations over a small set of categories to gather a variety of different settings. During inference, features of all queries of the same set are averaged.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Performance changes based on differences in prompt generation. Class wise performance on the left. Mean performance to the right. Models trained on MIMIC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Contributions of data scaling for chest radiograph dataset. Performance change of adding additional chest X-ray datasets with prompt-based captions. Evaluation Setup: We evaluate the multi-label classification ability of all networks via the Area Under the ROC-curve (AUROC) and show the performance over MIMIC-CXR, CheXpert and ChestX-Ray14. For all experiments expect</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Left: Impact of chosen scores for inference. Right: Ablation of model parts.</figDesc><table><row><cell>Inf.</cell><cell cols="3">MIMIC CheXpert CXR14 Avg.</cell><cell>Parts</cell><cell>MIMIC CXpert CXR14 Avg.</cell></row><row><cell cols="2">Local 77.81</cell><cell>78.09</cell><cell>71.72 75.87</cell><cell>LG</cell><cell>75.47 77.24 69.22 73.97</cell></row><row><cell cols="2">Global 76.24</cell><cell>80.42</cell><cell>71.00 75.88</cell><cell>LG+LL</cell><cell>76.20 82.24 69.26 75.90</cell></row><row><cell>Max Cat</cell><cell>76.85 77.29</cell><cell>71.29 80.30</cell><cell>78.22 75.45 71.72 76.43</cell><cell cols="2">LG+LL+LS 76.10 76.08 74.24 75.47 LG+LL+LM 77.03 77.36 71.72 75.37</cell></row><row><cell cols="2">Mean 77.06</cell><cell>81.08</cell><cell>71.50 76.54</cell><cell>Ours</cell><cell>77.06 81.08 71.50 76.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Classification performance on MIMIC, CheXpert and Chest-XRay14. * indicates that the model was trained with additional PadChest and ChestX-Ray14 data.</figDesc><table><row><cell></cell><cell cols="2">MIMIC-CXR</cell><cell>CheXpert</cell><cell cols="2">ChestX-Ray14</cell></row><row><cell>Method</cell><cell cols="5">(in-domain) (out-of-domain) (out-of-domain)</cell></row><row><cell></cell><cell>val</cell><cell>test</cell><cell>val</cell><cell>val</cell><cell>test</cell></row><row><cell cols="3">Label-Supervised 77.26 77.42</cell><cell>78.90</cell><cell cols="2">79.70 76.47</cell></row><row><cell>CLIP</cell><cell cols="2">73.23 70.25</cell><cell>75.85</cell><cell cols="2">68.03 63.34</cell></row><row><cell>SLIP</cell><cell cols="2">72.45 72.44</cell><cell>78.49</cell><cell cols="2">71.45 67.55</cell></row><row><cell>M ILN CE local</cell><cell cols="2">69.30 69.18</cell><cell>74.98</cell><cell cols="2">67.56 63.06</cell></row><row><cell>LoCo</cell><cell cols="2">77.03 78.15</cell><cell>81.71</cell><cell cols="2">71.92 68.14</cell></row><row><cell>GloCo</cell><cell cols="2">75.47 76.58</cell><cell>77.24</cell><cell cols="2">69.22 65.86</cell></row><row><cell>Ours</cell><cell cols="2">78.46 79.40</cell><cell>78.86</cell><cell cols="2">75.77 71.23</cell></row><row><cell>Ours*</cell><cell cols="2">78.30 80.40</cell><cell>83.24</cell><cell cols="2">79.90 78.33</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head><p>The present contribution is supported by the Helmholtz Association under the joint research school "HIDSS4Health -Helmholtz Information and Data Science School for Health" and by the Helmholtz Association Initiative and Networking Fund on the HAICORE@KIT partition.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<ptr target="https://www.england.nhs.uk" />
	</analytic>
	<monogr>
		<title level="j">National health service</title>
		<imprint>
			<biblScope unit="page" from="2022" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Padchest: A large chest x-ray image dataset with multi-label annotated reports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bustos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pertusa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De La Iglesia-Vay?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page">101797</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gloria: A multimodal global-local representation learning framework for label-efficient medical image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3942" to="3951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ciurea-Ilcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chute</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haghgoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shpanskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="590" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Identifying pneumonia in chest x-rays: a deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Rodrigues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Measurement</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="511" to="518" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4904" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Berkowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Greenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Horng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep learning in medical imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurospine</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">657</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05208</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9879" to="9889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.12750</idno>
		<title level="m">Slip: Self-supervision meets languageimage pre-training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.10050</idno>
		<title level="m">Combined scaling for zero-shot transfer learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Computer-aided detection in chest radiography based on artificial intelligence: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical engineering online</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-guided multiple instance learning for weakly supervised disease classification and localization in chest radiographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Seibold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleesiek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Schlemmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="617" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pareek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.09167</idno>
		<title level="m">Chexbert: combining automatic labelers and expert annotations for accurate radiology report labeling using bert</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A survey of zero-shot learning: Settings, methods, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>TIST)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2097" to="2106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Self-supervised image-text pretraining with mixed data in chest x-rays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16022</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karargyris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saboury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Boyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Comparison of chest radiograph interpretations by artificial intelligence algorithm vs radiology residents</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2022779" to="2022779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00747</idno>
		<title level="m">Contrastive learning of medical visual representations from paired images and text</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
