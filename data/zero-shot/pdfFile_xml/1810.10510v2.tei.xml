<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neighbourhood Consensus Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria ? CIIRC</orgName>
								<orgName type="institution" key="instit2">CTU in Prague ? DeepMind * Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria ? CIIRC</orgName>
								<orgName type="institution" key="instit2">CTU in Prague ? DeepMind * Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria ? CIIRC</orgName>
								<orgName type="institution" key="instit2">CTU in Prague ? DeepMind * Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria ? CIIRC</orgName>
								<orgName type="institution" key="instit2">CTU in Prague ? DeepMind * Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria ? CIIRC</orgName>
								<orgName type="institution" key="instit2">CTU in Prague ? DeepMind * Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria ? CIIRC</orgName>
								<orgName type="institution" key="instit2">CTU in Prague ? DeepMind * Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neighbourhood Consensus Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of finding reliable dense correspondences between a pair of images. This is a challenging task due to strong appearance differences between the corresponding scene elements and ambiguities generated by repetitive patterns. The contributions of this work are threefold. First, inspired by the classic idea of disambiguating feature matches using semi-local constraints, we develop an end-to-end trainable convolutional neural network architecture that identifies sets of spatially consistent matches by analyzing neighbourhood consensus patterns in the 4D space of all possible correspondences between a pair of images without the need for a global geometric model. Second, we demonstrate that the model can be trained effectively from weak supervision in the form of matching and non-matching image pairs without the need for costly manual annotation of point to point correspondences. Third, we show the proposed neighbourhood consensus network can be applied to a range of matching tasks including both category-and instance-level matching, obtaining the state-of-the-art results on the PF Pascal dataset and the InLoc indoor visual localization benchmark. ? WILLOW project, D?partement d'informatique de l'?cole normale sup?rieure,</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Finding visual correspondences is one of the fundamental image understanding problems with applications in 3D reconstruction <ref type="bibr" target="#b0">[2]</ref>, visual localization <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b39">41]</ref> or object recognition <ref type="bibr" target="#b19">[21]</ref>. In recent years, significant effort has gone into developing trainable image representations for finding correspondences between images under strong appearance changes caused by viewpoint or illumination variations <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b42">44]</ref>. However, unlike in other visual recognition tasks, such as image classification or object detection, where trainable image representations have become the de facto standard, the performance gains obtained by trainable features over the classic hand-crafted ones have been only modest at best <ref type="bibr" target="#b33">[35]</ref>.</p><p>One of the reasons for this plateauing performance could be the currently dominant approach for finding image correspondence based on matching individual image features. While we have now better local patch descriptors, the matching is still performed by variants of the nearest neighbour assignment in a feature space followed by separate disambiguation stages based on geometric constraints. This approach has, however, fundamental limitations. Imagine a scene with textureless regions or repetitive patterns, such as a corridor with almost textureless walls and only few distinguishing features. A small patch of an image, depicting a repetitive pattern or a textureless area, is indistinguishable from other portions of the image depicting the same repetitive or textureless pattern. Such matches will be either discarded <ref type="bibr" target="#b21">[23]</ref> or incorrect. As a result, matching individual patch descriptors will often fail in such challenging situations.</p><p>In this work we take a different direction and develop a trainable neural network architecture that disambiguates such challenging situations by analyzing local neighbourhood patterns in a full set of dense correspondences. The intuition is the following: in order to disambiguate a match on a repetitive pattern, it is necessary to analyze a larger context of the scene that contains a unique non-repetitive feature. The information from this unique match can then be propagated to the neighbouring uncertain matches. In other words, the certain unique matches will support the close-by uncertain ambiguous matches in the image.</p><p>This powerful idea goes back to at least 1990s <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b44">46]</ref>, and is typically known as neighbourhood consensus or more broadly as semi-local constraints. The neighbourhood consensus has been typically carried out on sparsely detected local invariant features as a filtering step performed after a hard assignment of features by nearest neighbour matching using the Euclidean distance in the feature space. Furthermore, the neighbourhood consensus has been evaluated by manually engineered criteria, such as a certain number of locally consistent matches <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b36">38]</ref>, or consistency in geometric parameters including distances and angles between matches <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b44">46]</ref>.</p><p>In this work, we go one step further and propose a way of learning neighbourhood consensus constraints directly from training data. Moreover, we perform neighbourhood consensus before hard assignment of feature correspondence; that is, on the complete set of dense pair-wise matches. In this way, the decision on matching assignment is done only after taking into account the spatial consensus constraints, hence avoiding errors due to early matching decisions on ambiguous, repetitive or textureless matches.</p><p>Contributions. We present the following contributions. First, we develop a neighbourhood consensus network -a convolutional neural network architecture for dense matching that learns local geometric constraints between neighbouring correspondences without the need for a global geometric model. Second, we show that parameters of this network can be trained from scratch using a weakly supervised loss-function that requires supervision at the level of image pairs without the need for manually annotating individual correspondences. Finally, we show that the proposed model is applicable to a range of matching tasks producing high-quality dense correspondences, achieving state-of-the-art results on both category-and instance-level matching benchmarks. Both training code and models are available at [1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>This work relates to several lines of research, which we review below.</p><p>Matching with hand-crafted image descriptors. Traditionally, correspondences between images have been obtained by hand crafted local invariant feature detectors and descriptors <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b40">42]</ref> that were extracted from the image with a controlled degree of invariance to local geometric and photometric transformations. Candidate (tentative) correspondences were then obtained by variants of nearest neighbour matching. Strategies for removing ambiguous and non-distinctive matches include the widely used second nearest neighbour ratio test <ref type="bibr" target="#b21">[23]</ref>, or enforcing matches to be mutual nearest neighbours. Both approaches work well for many applications, but have the disadvantage of discarding many correct matches, which can be problematic for challenging scenes, such as indoor spaces considered in this work that include repetitive and textureless areas. While successful, handcrafted descriptors have only limited tolerance to large appearance changes beyond the built-in invariance.</p><p>Matching with trainable descriptors. The majority of trainable image descriptors are based on convolutional neural networks (CNNs) and typically operate on patches extracted using a feature detector such as DoG <ref type="bibr" target="#b21">[23]</ref>, yielding a sparse set of descriptors <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b35">37]</ref> or use a pre-trained image-level CNN feature extractor <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b30">32]</ref>. Others have recently developed trainable methods that comprise both feature detection and description <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b41">43]</ref>. The extracted descriptors are typically compared using the Euclidean distance, but an appropriate similarity score can be also learnt in a discriminative manner <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b42">44]</ref>, where a trainable model is used to both extract descriptors and produce a similarity score. Finding matches consistent with a geometric model is typically performed in a separate post-processing stage <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b41">43]</ref>.</p><p>Trainable image alignment. Recently, end-to-end trainable methods have been developed to produce correspondences between images according to a parametric geometric model, such as an affine, perspective or thin-plate spline transformation <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b27">29]</ref>. In these works, all pairwise feature matches are computed and used to estimate the geometric transformation parameters using a CNN. Unlike previous methods that capture only a sparse set of correspondences, this geometric estimation CNN captures interactions between a full set of dense correspondences. However, these methods currently only estimate a low complexity parametric transformation, and therefore their application is limited to only coarse image alignment tasks. In contrast, we target a more general problem of identifying reliable correspondences between images of a general 3D scene. Our approach is not limited to a low dimensional parametric model, but outputs a generic set of locally consistent image correspondences, applicable to a wide range of computer vision problems ranging from category-level image alignment to camera pose estimation. The proposed method builds on the classical ideas of neighbourhood consensus, which we review next.</p><p>Match filtering by neighbourhood consensus. Several strategies have been introduced to decide whether a match is correct or not, given the supporting evidence from the neighbouring matches. The early examples analyzed the patterns of distances <ref type="bibr" target="#b44">[46]</ref> or angles <ref type="bibr" target="#b32">[34]</ref> between neighbouring matches. Later work simply counts the number of consistent matches in a certain image neighbourhood <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b36">38]</ref>, which can be built in a scale invariant manner <ref type="bibr" target="#b28">[30]</ref> or using a regular image grid <ref type="bibr" target="#b3">[5]</ref>. While simple, these techniques have been remarkably effective in removing random incorrect matches and disambiguating local repetitive patterns <ref type="bibr" target="#b28">[30]</ref>. Inspired by this simple yet powerful idea we develop a neighbourhood consensus network -a convolutional neural architecture that (i) analyzes the full set of dense matches between a pair of images and (ii) learns patterns of locally consistent correspondences directly from data.</p><p>Flow and disparity estimation. Related are also methods that estimate optical flow or stereo disparity such as <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b37">39]</ref>, or their trainable counterparts <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b38">40]</ref>. These works also aim at establishing reliable point to point correspondences between images. However, we address a more general matching problem where images can have large viewpoint changes (indoor localization) or major changes in appearance (category-level matching). This is different from optical flow where image pairs are usually consecutive video frames with small viewpoint or appearance changes, and stereo where matching is often reduced to a local search around epipolar lines. The optical flow and stereo problems are well addressed by specialized methods that explicitly exploit the problem constraints (such as epipolar line constraint, small motion, smoothness, etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed approach</head><p>In this work, we combine the robustness of neighbourhood consensus filtering with the power of trainable neural architectures. We design a model which learns to discriminate a reliable match by recognizing patterns of supporting matches in its neighbourhood. Furthermore, we do this in a fully differentiable way, such that this trainable matching module can be directly combined with strong CNN image descriptors. The resulting pipeline can then be trained in an end-to-end manner for the task of feature matching. An overview of our proposed approach is presented in <ref type="figure" target="#fig_0">Fig. 1</ref>. There are five main components: (i) dense feature extraction and matching, (ii) the neighbourhood consensus network, (iii) a soft mutual nearest neighbour filtering, (iv) extraction of correspondences from the output 4D filtered match tensor, and (v) weakly supervised training loss. These components are described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dense feature extraction and matching</head><p>In order to produce an end-to-end trainable model, we follow the common practice of using a deep convolutional neural network (CNN) as a dense feature extractor.</p><p>Then, given an image I, this feature extractor will produce a dense set of descriptors, {f I ij } ? R d , with indices i = 1, . . . , h and j = 1, . . . , w, and (h, w) denoting the number of features along image height and width (i.e. the spatial resolution of the features), and d the dimensionality of the features.  While classic hand-crafted neighbourhood consensus approaches are applied after a hard assignment of matches is done, this is not well suited for developing a matching method that is differentiable and amenable for end-to-end training. The reason is that the step of selecting a particular match is not differentiable with respect to the set of all the possible features. In addition, in case of repetitive features, assigning the match to the first nearest neighbour might result in an incorrect match, in which case the hard assignment would lose valuable information about the subsequent closest neighbours.</p><p>Therefore, in order to have an approach that is amenable to end-to-end training, all pairwise feature matches need to be computed and stored. For this we use an approach similar to <ref type="bibr" target="#b26">[28]</ref>. Given two sets of dense feature descriptors f A = {f A ij } and f B = {f B ij } corresponding to the images to be matched, the exhaustive pairwise cosine similarities between them are computed and stored in a 4-D tensor c ? R h?w?h?w referred to as correlation map, where:</p><formula xml:id="formula_0">c ijkl = f A ij , f B kl f A ij 2 f B kl 2 .</formula><p>(1)</p><p>Note that, by construction, elements of c in the vicinity of index ijkl correspond to matches between features that are in the local neighbourhoods N A and N B of descriptors f A ij in image A and f B kl in image B, respectively, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>; this structure of the 4-D correlation map tensor c will be exploited in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Neighbourhood consensus network</head><p>The correlation map contains the scores of all pairwise matches. In order to further process and filter the matches, we propose to use 4-D convolutional neural network (CNN) for the neighbourhood consensus task (denoted by N (?)), which is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>Determining the correct matches from the correlation map is, a priori, a significant challenge. Note that the number of correct matches are of order of hw, while the size of the correlation map is of the order of (hw) 2 . This means that the great majority of the information in the correlation map corresponds to matching noise due to incorrectly matched features.</p><p>However, supported by the idea of neighbourhood consensus presented in Sec. 1, we can expect correct matches to have a coherent set of supporting matches surrounding them in the 4-D space. These geometric patterns are equivariant with translations in the input images; that is, if the images are translated, the matching pattern is also translated in the 4-D space by an equal amount. This property motivates the use of 4-D convolutions for processing the correlation map as the same operations should be performed regardless of the location in the 4-D space. This is analogous to the motivation for using 2-D convolutions to process individual images -it makes sense to use convolutions, instead of for example a fully connected layer, in order to profit from weight sharing and keep the number of trainable parameters low. Furthermore, it facilitates sample-efficient training as a single training example provides many error signals to the convolutional weights, since the same weights are applied at all positions of the correlation map. Finally, by processing matches with a 4D convolutional network we establish a strong locality prior on the relationships between the matches. That is, by design, the network will determine the quality of a match by examining only the information in a local 2D neighbourhood in each of the two images.</p><p>The proposed neighbourhood consensus network has several convolutional layers, as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, each followed by ReLU non-linearities. The convolutional filters of the first layer of the proposed CNN span a local 4-D region of the matches space, which corresponds to the Cartesian product of local neighbourhoods N A and N B in each image, respectively. Therefore, each 4-D filter of the first layer can process and detect patterns in all pairwise matches of these two neighbourhoods. This first layer has N 1 filters that can specialize in learning different local geometric deformations, producing N 1 output channels, that correspond to the agreement with these local deformations at each 4-D point of the correlation tensor. These output channels are further processed by subsequent 4-D convolutional layers. The aim is that these layers capture more complex patterns by combining the outputs from the previous layer, analogously to what has been observed for 2-D CNNs <ref type="bibr" target="#b43">[45]</ref>. Finally, the neighbourhood consensus CNN produces a single channel output, which has the same dimensions as the 4D input matches.</p><p>Finally, in order to produce a method that is invariant to the particular order of the input images, that is, that it will produce the same matches regardless of whether an image pair is input to the net as (I A , I B ) or (I B , I A ), we propose to apply the network twice in the following way:</p><formula xml:id="formula_1">c = N (c) + N (c T ) T ,<label>(2)</label></formula><p>where by c T we mean swapping the pair of dimensions corresponding to the first and second images:</p><formula xml:id="formula_2">c T ijkl = c klij .</formula><p>This final output constitutes the filtered matchesc using the neighbourhood consensus network, where matches with inconsistent local patterns are downweighted or removed. Further filtering can be done by means of a global filtering strategy, as presented next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Soft mutual nearest neighbour filtering</head><p>Although the proposed neighbourhood consensus network can suppress and amplify matches based on the supporting evidence in their neighbourhoods -that is, at a semi-local level -it cannot enforce global constraints on matches, such as being a reciprocal match, where matched features are required to be mutual nearest neighbours:</p><formula xml:id="formula_3">(f A ab , f B cd ) mutual N.N. ?? (a, b) = arg min ij f A ij ? f B cd (c, d) = arg min kl f A ab ? f B kl .<label>(3)</label></formula><p>Filtering the matches by imposing the hard mutual nearest neighbour condition expressed by <ref type="formula" target="#formula_3">(3)</ref> would eliminate the great majority of candidate matches, which makes it unsuitable for usage in an end-to-end trainable approach, as this hard decision is non-differentiable.</p><p>We therefore propose a softer version of the mutual nearest neighbour filtering (M (?)), both in the sense of softer decision and better differentiability properties, that can be applied on dense 4-D match scores:</p><formula xml:id="formula_4">? = M (c), where? ijkl = r A ijkl r B ijkl c ijkl ,<label>(4)</label></formula><p>and r A ijkl and r B ijkl are the ratios of the score of the particular match c ijkl with the best scores along each pair of dimensions corresponding to images A and B respectively:</p><formula xml:id="formula_5">r A ijkl = c ijkl max ab c abkl , and r B ijkl = c ijkl max cd c ijcd .<label>(5)</label></formula><p>This soft mutual nearest neighbour filtering operates as a gating mechanism on the input, downweighting the scores of matches that are not mutual nearest neighbours. Note that the proposed formulation is indeed a softer version of the mutual nearest neighbours criterion as? ijkl equals the matching score</p><formula xml:id="formula_6">c ijkl iff (f A ij , f B kl )</formula><p>are mutual nearest neighbours, and is decreased to a value in [0, c ijkl ) otherwise. On the contrary, the "hard" mutual nearest neighbour matching would assign? ijkl = 0 in the latter case.</p><p>While this filtering step has no trainable parameters, it can be inserted in the CNN pipeline at both training and evaluation stages, and it will help to enforce the global reciprocity constraint on matches. In the proposed approach, the soft mutual nearest neighbour filtering is used to filter both the correlation map, as well as the output of the neighbourhood consensus CNN, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Extracting correspondences from the correlation map</head><p>Suppose that we want to match two images I A and I B . Then, the output of our model will produce a 4-D filtered correlation map c, which contains (filtered) scores for all pairwise matches. However, for various applications, such as image warping, geometric transformation estimation, pose estimation, visualization, etc, it is desirable to obtain a set of point-to-point image correspondences between the two images. To achieve this, a hard assignment can be performed in either of two possible directions, from features in image A to features in image B, or vice versa. .</p><p>Note that the scores are: (i) positive, (ii) normalized using the soft-max function, which makes ab s B ijab = 1. Hence we can interpret them as discrete conditional probability distributions of f A ij , f B kl being a match, given the position (i, j) of the match in A or (k, l) in B. If we denote (I, J, K, L) the discrete random variables indicating the position of a match (a priori unknown), and (i, j, k, l) the particular position of a match, then:</p><formula xml:id="formula_8">P (K = k, L = l | I = i, J = j) = s B ijkl and P (I = i, J = j | K = k, L = l) = s A ijkl . (7)</formula><p>Then, the hard-assignment in one direction can be done by just taking the most likely match (the mode of the distribution):</p><formula xml:id="formula_9">f B kl assigned to a given f A ij ?? (k, l) = arg max cd P (K = c, L = d | I = i, J = j) = arg max cd s B ijcd ,<label>(8)</label></formula><p>and analogously to obtain the matches f A ij assigned to a given f B kl . This probabilistic intuition allows us to model the match uncertainty using a probability distribution and will be also useful to motivate the loss used for weakly-supervised training, which will be described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Weakly-supervised training</head><p>In this section we define the loss function used to train our network. One option is to use a stronglysupervised loss, but this requires dense annotations consisting of all pairs of corresponding points for each training image pair. Obtaining such exhaustive ground-truth is complicated -dense manual annotation is impractical, while sparse annotation followed by an automatic densification technique typically results in imprecise and erroneous training data. Another alternative is to resort to synthetic imagery which would provide point correspondences by construction, but this has the downside of making it harder to generalize to larger appearance variations encountered in real image pairs we wish to handle. Therefore, it is desirable to be able to train directly from pairs of real images, and using as little annotation as possible.</p><p>For this we propose to use a training loss that only requires a weak-level of supervision consisting of annotation on the level of image pairs. These training pairs (I A , I B ) can be of two types, positive pairs, labelled with y = +1, or negative pairs, labelled with y = ?1. Then, the following loss function is proposed:</p><formula xml:id="formula_10">L(I A , I B ) = ?y s A +s B ,<label>(9)</label></formula><p>wheres A ands B are the mean matching scores over all hard assigned matches as per <ref type="formula" target="#formula_9">(8)</ref>  Note that the minimization of this loss maximizes the scores of positive and minimizes the scores of negative image pairs, respectively. As explained in 3.4, the hard-assigned matches correspond to the modes of the distributions of <ref type="bibr" target="#b5">(7)</ref>. Therefore, maximizing the score forces the distribution towards a Kronecker delta distribution, having the desirable effect of producing well-identified matches in positive image pairs. Similarly, minimizing the score forces the distribution towards the uniform one, weakening the matches in the negative image pairs. Note that while the only scores that directly contribute to the loss are the ones coming from hard-assigned matches, all matching scores affect the loss because of the normalization in <ref type="bibr" target="#b4">(6)</ref>. Therefore, all matching scores will be updated at each training step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental results</head><p>The proposed approach was evaluated on both instance-and category-level matching problems. The same approach is used to obtain reliable matches for both problems, which are then used to solve two completely different tasks: (i) camera pose estimation in the challenging scenario of indoor localization, in the instance-level matching case, and (ii) semantic object alignment in the categorylevel matching case. Next we present the implementation details, followed by the results on the two tasks.</p><p>Implementation details. The model was implemented in PyTorch <ref type="bibr" target="#b25">[27]</ref>, and a ResNet-101 network <ref type="bibr" target="#b12">[14]</ref> initialized on ImageNet was used for feature extraction (up to the conv4_23 layer). The neighbourhood consensus network N (?) contains three layers of 5 ? 5 ? 5 ? 5 filters or two layers of 3 ? 3 ? 3 ? 3 filters for category level and instance level matching, respectively. In both cases, the intermediate results have 16 channels (N 1 = N 2 = 16). A feature resolution of 25 ? 25 was used for training. As accurately localized features are needed for the pose estimation task, we extract correspondence for pose estimation at test time at a higher resolution resulting in a 200 ? 150 feature map, which is downsampled using 4-D max+argmax pooling operation after computing the correlation map to 100 ? 75 for increased efficiency. The model is initially trained for 5 epochs using Adam optimizer <ref type="bibr" target="#b18">[20]</ref>, with a learning rate of 5 ? 10 ?4 and keeping the feature extraction layer weights fixed. For category level matching, the model is then subsequently finetuned for 5 more epochs, training both the feature extraction and the neighbourhood consensus network, with a learning rate of 1 ? 10 ?5 . In the case of instance level matching, finetuning the feature extraction did not improve the performance. Additional implementation details are given in appendices A and B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Category-level matching</head><p>The proposed method was evaluated on the task of category-level matching, where, given two images containing different instances from the same category (e.g. two different cat images) the goal is to match similar semantic parts.</p><p>Dataset and evaluation measure. We report results on the PF-Pascal <ref type="bibr" target="#b9">[11]</ref> dataset, which contains 1,351 semantically related image pairs from the 20 object categories of the PASCAL VOC <ref type="bibr" target="#b7">[9]</ref> dataset. We follow the same evaluation protocol as <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b27">29]</ref>, and use the split from <ref type="bibr" target="#b10">[12]</ref> which divides the Method PCK HOG+PF-LOM <ref type="bibr" target="#b9">[11]</ref> 62.5 SCNet-AG+ <ref type="bibr" target="#b10">[12]</ref> 72.2 CNNGeo <ref type="bibr" target="#b26">[28]</ref> 71.9 WeakAlign <ref type="bibr" target="#b27">[29]</ref> 75.8 NC-Net 78.9  dataset into approximately 700 pairs for training, 300 for validation and 300 for testing. In order to train the network in a weakly-supervised manner using the proposed loss (9), the 700 training pairs are used as positive training pairs, and negative pairs are generated by randomly pairing images of different categories, such as a car with a dog image. The performance is measured using the percentage of correct keypoints (PCK), that is, number of correctly matched annotated keypoints.</p><p>Results. Quantitative results are presented in <ref type="table" target="#tab_1">Table 1</ref>. The proposed neighbourhood consensus network (NC-Net) obtains~3% improvement over the state-of-the-art methods on this dataset <ref type="bibr" target="#b27">[29]</ref>. An example of semantic keypoint transfer is shown in <ref type="figure" target="#fig_4">Figure 3</ref> and demonstrates how our approach can correctly match semantic object parts in challenging situations with large changes of appearance and non-rigid geometric deformations. See appendix C for additional examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Instance-level matching</head><p>Next we show that our method is also suitable for instance level matching and consider specifically the application to indoor visual localization, where the goal is to estimate an accurate 6DoF camera pose of a query photograph given a large-scale 3D model of a building. This is an extremely challenging instance-level matching task as indoor spaces are often self-similar and contain large textureless areas.</p><p>We compare our method with the recently introduced indoor localization approach of <ref type="bibr" target="#b39">[41]</ref>, which is a strong baseline that outperforms several state-of-the-art methods, and introduces a challenging dataset for large scale indoor localization.</p><p>Dataset and evaluation measure. We evaluate on the InLoc dataset <ref type="bibr" target="#b39">[41]</ref>, which consists of 10K database images (perspective cutouts) extracted from 227 RGBD panoramas, and an additional set of 356 query images captured with a smart-phone camera at a different time from the database images. We follow the same evaluation protocol and report the percentage of correctly localized queries at a given camera position error threshold. As the InLoc dataset was designed for evaluation and does not contain a training set, we collected an Indoor Venues Dataset, consisting of user-uploaded photos, captured at public places such as restaurants, cafes, museums or cathedrals, by crawling Google Maps. It features similar appearance variations as the InLoc dataset, such as illumination changes, and scene modifications due to the passage of time. This dataset contains 3861 positive image pairs from 89 different venues in 6 different cities, split into train: 3481 pairs (80 places) and validation: 380 pairs (from the remaining 9 places). The design and collection procedures are described in appendix E and the dataset is available at <ref type="bibr">[1]</ref>. As in the case of category-level matching, negative pairs were generated by randomly sampling images from different places.</p><p>Results. We plug-in our trainable neighbourhood consensus network (NC-Net) as a correspondence module into the InLoc indoor localization pipeline <ref type="bibr" target="#b39">[41]</ref>. We evaluate two variants of the approach. In the first variant, denoted DensePE+NC-Net, the DensePE <ref type="bibr" target="#b39">[41]</ref> method is used for generating candidate image pairs, and then our network (NC-Net) is used to produce the correspondences that are employed for pose estimation. In the second variant, denoted InLoc+NC-Net, we use the full InLoc pipeline, including pose-verification by view synthesis. In this case, matches produced by NC-Net are used as input for pose estimation for each of the top N = 10 candidate pairs from DensePE, and the resulting candidate poses are re-ranked using pose-verification by view-synthesis. As an ablation study, these two experiments are also performed when NC-Net is replaced with hard mutual nearest neighbours matching (MNN), using the same base CNN network (ResNet-101). Results  are summarised in <ref type="table" target="#tab_2">Table 2</ref> and clearly demonstrate benefits of our approach (DensePE+NC-Net) compared to both sparse keypoint (DoG+SIFT) matching (SparsePE) and the CNN feature matching used in <ref type="bibr" target="#b39">[41]</ref> (DensePE). When inserted into the entire localization pipeline, our approach (InLoc + NC-Net) obtains state-of-the-art results on the indoor localization benchmark. An example of obtained correspondences on a challenging indoor scene with repetitive structures and texture-less areas is shown in <ref type="figure" target="#fig_5">figure 4</ref>. Additional results are shown in appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Limitations</head><p>While our method identifies correct matches in many challenging cases, some situations remain difficult. The two typical failure modes include: repetitive patterns combined with large changes in scale, and locally geometrically consistent groups of incorrect matches. Furthermore, the proposed method has quadratic O(N 2 ) complexity with respect to the number of image pixels (or CNN features) N . This limits the resolution of the images that we are currently able to handle to 1600 ? 1200px (or 3200 ? 2400px if using the 4-D max+argmax pooling operation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have developed a neighbourhood consensus network -a CNN architecture that learns local patterns of correspondences for image matching without the need for a global geometric model. We have shown the model can be trained effectively from weak supervision and obtains strong results outperforming state-of-the-art on two very different matching tasks. These results open up the possibility for end-to-end learning of other challenging visual correspondence tasks, such as 3D category-level matching <ref type="bibr" target="#b16">[18]</ref>, or visual localization across day/night illumination <ref type="bibr" target="#b29">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>In these appendices we provide additional technical details, experimental results, and details about the dataset used. First, in appendices A and B we present additional implementation details concerning 4D convolutions and the relocalization of features. Then, in appendices C and D we present additional experimental results for category-level and instance-level matching, respectively. Finally, in appendix E, we provide additional details about the Indoor Venues Dataset (IVD), which we used for instance-level matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation of 4D convolutions</head><p>As 4D convolutions are not currently supported by the deep learning framework which we used (i.e. PyTorch <ref type="bibr" target="#b25">[27]</ref>), we implemented them by aggregating the results of multiple 3D convolutions. The same idea had been used in the past to obtain 3D convolutions by aggregating the results of multiple 2D convolutions, before 3D convolutions where natively supported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Feature relocalization</head><p>Here we present the details of the technique used for improving the accuracy of the localization of the features, which is particularly important in the case of instance-level matching for the task of camera pose estimation.</p><p>The localization precision of the extracted features f I ij depends on the spatial resolution h ? w of the dense feature map f I . For some tasks, such as pose estimation, precisely localized features are needed. However, in some cases, given hardware constraints, one cannot increase the spatial resolution h ? w to obtain the required precision, as increasing h and w by a factor of two results in a sixteen time increase in the memory consumption and computation time. Therefore, we devise a method to increase the localization precision, with a less severe impact on the memory consumption and computation time.</p><p>In this approach, the correlation map c from Eq. (1) is computed with higher resolution features 2h ? 2w leading to a 2h ? 2w ? 2h ? 2w correlation map resolution. However, this correlation map c is then downsampled to resolution h ? w ? h ? w before further processing by the neighbourhood consensus network. This downsampling is performed by a 4-D max-pooling operation, with the kernel of size 2: </p><formula xml:id="formula_11">c * abcd = max i?[2a,</formula><formula xml:id="formula_12">c ijkl .<label>(10)</label></formula><p>The downsampled correlation map c * is then processed and used to compute the final matches, which are localized with a precision given by the downsampled resolution h ? w. However, one can re-localize these features, and reduce the localization error, by simply extracting the following relocalization shifts from the 4-D max-pooling operation: </p><formula xml:id="formula_13">? a , ? b , ? c , ? d = arg max i?[2a,</formula><formula xml:id="formula_14">c ijkl .<label>(11)</label></formula><p>Then for a match (f  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional results for category-level matching</head><p>In <ref type="figure">Figures 5 and 6</ref> we present additional qualitative results for the task of keypoint transfer. We show a set of randomly sampled examples from the test set of the PF-Pascal dataset. In some cases the ground truth annotations are ambiguous. For instance, in the cat example in <ref type="figure">Fig. 5</ref>, where one of the keypoints could be both on the right side of the face or the back of the cat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional results for instance-level matching</head><p>We illustrate situations in which our proposed method successfully localizes the query images within 2 meters and 10 ? with respect to the reference pose, while the baseline method fails. We show qualitative examples of the results obtained by the proposed method InLoc+NC-Net versus DensePE (figure 7), InLoc (figure 8), and InLoc+MNN (figure 9) baselines. The matches (green dots) obtained by the proposed method tend to cover a larger space in the scene that enables a more accurate pose estimation in comparison with the baseline methods. Furthermore, these matches tend to be more geometrically consistent than those produced by the baselines. <ref type="figure" target="#fig_0">Figure 10</ref> shows the detailed results of camera localization that correspond to <ref type="table" target="#tab_2">Table 2</ref> in the main paper. It clearly shows the proposed NC-Net consistently improves over the state of the art DensePE and InLoc methods, as well as over the strong InLoc+MNN baseline which uses the same ResNet-101 dense features as NC-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E The Indoor Venues Dataset (IVD)</head><p>We collected the Indoor Venues Dataset (IVD) to train our network (NC-Net) for instance-level matching, as the InLoc <ref type="bibr" target="#b39">[41]</ref> dataset used for evaluation does not provide any training data. The IVD consists of 3861 positive image pairs, collected from 89 venues (restaurants, cafes, museums), from six European cities (Amsterdam, Brussels, Copenhagen, Edinburgh, Paris and Prague), keeping 10% for validation, and using the rest for training. These positive image pairs consist of user-uploaded images from the same indoor scene or location (i.e. same room) captured at different points in time and which, despite the variations in viewpoint, depict the same 3D structures. The image pairs contain a challenging level of variability in both viewpoint and illumination, as well as small variations in the scene due to the passage of time. These variations match well the challenging settings in the InLoc dataset. Examples of such images are shown in <ref type="figure" target="#fig_0">Figure 11</ref>.</p><p>The images were gathered from Google Maps 3 , which lists 100 million places, has 25 million daily updates, and 1 billion monthly active users <ref type="bibr" target="#b2">4</ref> . The availability of such a large number of user submitted content (reviews, photos, panoramas) makes it a suitable source for realistic indoor images. We selected 6 touristic cities in Europe, for which we used Google Maps API to search for venues in the selected cities, based on city centre GPS coordinates, and a search radius of 5-10 km. Only venues with at least 150 submitted reviews where used.</p><p>However, a large fraction (&gt; 50 %) of these images where non-relevant for our task (images of food, selfies, outdoor images, photographs of the menu, etc). Therefore, a classifier was trained for curating the downloaded images in an automatic way. We used four classes, based on the most frequent categories of images present: food, indoor, outdoor and other (for less frequent types of images, like menus or selfies). For this, we used a ResNet-50 model <ref type="bibr" target="#b12">[14]</ref>, pretrained on ImageNet as fixed feature extractor, and trained only the fully connected layer, on a subset of manually picked images (~1000/class).</p><p>From the curated indoor images, we then sampled 20 query images for each venue, and matched them against all the indoor images from the same place, using a NC-Net model trained on the PF-Pascal dataset. The resulting image pairs were manually inspected and filtered to make sure they depict the same part of the scene.</p><p>The dataset is publicly available at http://www.di.ens.fr/willow/research/ncnet/.</p><p>Ours Ground-truth <ref type="figure">Figure 5</ref>: Additional examples of semantic keypoint transfer. The left column shows the (manually annotated) keypoints automatically transferred from the left to the right image using our NC-Net. The right column shows the ground-truth correspondence annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Ground-truth <ref type="figure">Figure 6</ref>: Additional examples of semantic keypoint transfer. The left column shows the (manually annotated) keypoints automatically transferred from the left to the right image using our NC-Net. The right column shows the ground-truth correspondence annotations.  <ref type="figure">Figure 8</ref>: Examples of query images that are more accurately localized using the proposed approach (left) than with the InLoc baseline (right). The numbers below each pair of images show the localization error (meters, degrees) with respect to the reference camera pose. Green dots show inlier matches used for camera pose estimation.  <ref type="figure">Figure 9</ref>: Examples of query images that are more accurately localized using the proposed approach (left) than with the InLoc+MNN baseline (right). In this ablation study, both methods use the same CNN features. The numbers below each pair of images show the localization error (meters, degrees) with respect to the reference camera pose. Green dots show inlier matches used for camera pose estimation.  <ref type="bibr" target="#b29">[31]</ref>. Plots show the fraction of correctly localized queries (y-axis) within a certain distance (x-axis) whose rotation error is at most 10 ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Brussels, Belgium</head><p>Prague, Czech Republic Copenhagen, Denmark</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paris, France</head><p>Edinburgh, UK Amsterdam, Netherlands <ref type="figure" target="#fig_0">Figure 11</ref>: Indoor Venues Dataset (IVD). We show some examples of venues across 6 European cities, from which we collected images submitted by users on Google Maps. Note the variability in illumination, viewpoint, scene layout and occluders (people).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of the proposed method. A fully convolutional neural network is used to extract dense image descriptors f A and f B for images IA and IB, respectively. All pairs of individual feature matches f A ij and f B kl are represented in the 4-D space of matches (i, j, k, l) (here shown as a 3-D perspective for illustration), and their matching scores stored in the 4-D correlation tensor c. These matches are further processed by the proposed soft-nearest neighbour filtering and neighbourhood consensus network (see Figure 2) to produce the final set of output correspondences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Neighbourhood Consensus Network (NC-Net). A neighbourhood consensus CNN operates on the 4D space of feature matches. The first 4D convolutional layer filters span NA ? NB, the Cartesian product of local neighbourhoods NA and NB in images A and B respectively. The proposed 4D neighbourhood consensus CNN can learn to identify the matching patterns of reliable and unreliable matches, and filter the matches accordingly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>For</head><label></label><figDesc>this purpose, two scores are defined from the correlation map, by performing soft-max in the dimensions corresponding to images A and B: s A ijkl = exp(c ijkl ) ab exp(c abkl ) and s B ijkl = exp(c ijkl ) cd exp(c ijcd )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>of a given image pair (I A , I B ) in both matching directions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Semantic keypoint transfer. The annotated (ground truth) keypoints in the left image are automatically transferred to the right image using the dense correspondences between the two images obtained from our NC-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Instance-level matching. Top row: inlier correspondences (shown as green dots) obtained by our approach (InLoc+NC-Net). Bottom row: Baseline inlier correspondences (InLoc+MNN). Our method provides a much larger and locally consistent set of matches, even in low-textured regions. Both methods use the same CNN features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>A ab , f B cd ), the final re-localized feature positions (a , b ) and (c , d ) are computed by: a = a + ? a /2, b = b + ? b /2, c = c + ? c /2, d = d + ? d /2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Comparison with the state of the art large-scale indoor localization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results for semantic</figDesc><table><row><cell cols="6">Dist. SparsePE DensePE DensePE DensePE InLoc InLoc</cell><cell>InLoc</cell></row><row><cell>(m)</cell><cell>[41]</cell><cell>[41]</cell><cell cols="4">+ MNN + NC-Net [41] + MNN + NC-Net</cell></row><row><cell cols="2">0.25 21.3</cell><cell>35.3</cell><cell>31.9</cell><cell>37.1</cell><cell>38.9 37.1</cell><cell>44.1</cell></row><row><cell cols="2">0.50 30.7</cell><cell>47.4</cell><cell>50.5</cell><cell>53.5</cell><cell>56.5 60.2</cell><cell>63.8</cell></row><row><cell cols="2">1.00 42.6</cell><cell>57.1</cell><cell>62.0</cell><cell>62.9</cell><cell>69.9 72.0</cell><cell>76.0</cell></row><row><cell cols="2">2.00 48.3</cell><cell>61.1</cell><cell>64.7</cell><cell>66.3</cell><cell>74.2 76.3</cell><cell>78.4</cell></row><row><cell>keypoint transfer. We show the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>rate (%) of correctly transferred</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>keypoints within thresh. ? = 0.1.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of indoor localization methods. We show the rate (%) of correctly localized queries within a given distance (m) and 10 ? angular error.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://maps.google.com/ 4 According to https://cloud.google.com/maps-platform/places/, as of October 2018.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Building rome in a day</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="105" to="112" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.05030</idno>
		<title level="m">PN-Net: Conjoined triple deep network for learning local image descriptors</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning local feature descriptors with triplets and shallow convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ponsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">GMS: Grid-based motion statistics for fast, ultra-robust feature correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large displacement optical flow: descriptor matching in variational motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Universal correspondence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">FlowNet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2011/workshop/index.html" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2011 (VOC2011) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.5769</idno>
		<title level="m">Descriptor matching with convolutional neural networks: a comparison to SIFT</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Proposal flow: Semantic correspondences from object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SCNet: Learning Semantic Correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">MatchNet: Unifying feature and metric learning for patch-based matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschm?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learned local descriptors for recognition and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jahrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Winter Workshop</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning category-specific mesh reconstruction from image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sift-flow: Dense correspondence across different scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Do convnets learn correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Distinctive image features from scale-invariant keypoints. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An affine invariant interest point detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large-scale image retrieval with attentive deep local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<ptr target="http://pytorch.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Convolutional neural network architecture for geometric matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">End-to-end weakly-supervised semantic alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SCRAMSAC: Improving RANSAC&apos;s Efficiency with a Spatial Consistency Filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kobbelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Benchmarking 6dof urban visual localization in changing conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Toft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hammarstrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stenborg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Safari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Matching neural paths: transfer from recognition to correspondence search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automated scene matching in movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schaffalitzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image and Video Retrieval</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Local grayvalue invariants for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mohr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Comparative evaluation of hand-crafted and learned local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hardmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Discriminative learning of deep convolutional feature point descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning local feature descriptors using convex optimisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Video Google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Secrets of optical flow estimation and their principles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">InLoc: Indoor visual localization with dense matching and view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Taira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Local invariant feature detectors: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Computer Graphics and Vision</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="177" to="280" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">LIFT: Learned invariant feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A robust technique for matching two uncalibrated images through the recovery of the unknown epipolar geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deriche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-T</forename><surname>Luong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
	<note>Ours (InLoc+NC-Net</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Baseline (DensePE)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Examples of query images that are more accurately localized using the proposed approach (left) than with the densePE baseline (right). The numbers below each pair of images show the localization error (meters, degrees) with respect to the reference camera pose. Green dots show inlier matches used for camera pose estimation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
