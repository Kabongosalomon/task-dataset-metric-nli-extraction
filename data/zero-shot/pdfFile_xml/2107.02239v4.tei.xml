<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VISION XFORMERS: EFFICIENT ATTENTION FOR IMAGE CLASSIFICATION PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Jeevan</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Sethi</surname></persName>
							<email>asethi@iitb.ac.in</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Bombay Mumbai</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering Indian Institute of Technology Bombay Mumbai</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VISION XFORMERS: EFFICIENT ATTENTION FOR IMAGE CLASSIFICATION PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Transformer ? Image classification ? Linear Attention ? Deep Learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although transformers have become the neural architectures of choice for natural language processing, they require orders of magnitude more training data, GPU memory, and computations in order to compete with convolutional neural networks for computer vision. The attention mechanism of transformers scales quadratically with the length of the input sequence, and unrolled images have long sequence lengths. Plus, transformers lack an inductive bias that is appropriate for images. We tested three modifications to vision transformer (ViT) architectures that address these shortcomings. Firstly, we alleviate the quadratic bottleneck by using linear attention mechanisms, called X-formers (such that, X ? {Performer, Linformer, Nystr?mformer}), thereby creating Vision X-formers (ViXs). This resulted in up to a seven times reduction in the GPU memory requirement. We also compared their performance with FNet and multi-layer perceptron mixers, which further reduced the GPU memory requirement. Secondly, we introduced an inductive bias for images by replacing the initial linear embedding layer by convolutional layers in ViX, which significantly increased classification accuracy without increasing the model size. Thirdly, we replaced the learnable 1D position embeddings in ViT with Rotary Position Embedding (RoPE), which increases the classification accuracy for the same model size. We believe that incorporating such changes can democratize transformers by making them accessible to those with limited data and computing resources.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformers have revolutionised the natural language processing (NLP) domain with their ability to handle context over long sequences of data and achieving human-level accuracy for various tasks, such as language translation, text summarization, question answering, language modeling, and text generation <ref type="bibr" target="#b0">Vaswani et al. [2017]</ref>, <ref type="bibr" target="#b1">Devlin et al. [2019]</ref>. On the other hand, in recent years, vision application have been almost completely dominated by the convolutional neural network (CNN) architectures, which can exploit the two-dimensional (2D) structure of images using inductive priors, such as translational equivariance due to convolutional weight sharing and partial scale invariance due to pooling operations. Even though CNNs possess these advantages in handling image data, they cannot scale up receptive fields without increasing network depth and require several layers, especially pooling-type mechanisms, to capture long range dependencies. While the effective weights are dynamically calculated based on the inputs in attention mechanisms for transformers, popular CNN architectures, such as ResNet <ref type="bibr" target="#b2">He et al. [2015]</ref> and InceptionNet <ref type="bibr" target="#b3">Szegedy et al. [2014]</ref> lack an attention mechanism and use static weights for each input, although there are exceptions, such as squeeze and excitation nets <ref type="bibr" target="#b4">Iandola et al. [2016]</ref>, <ref type="bibr" target="#b5">Hu et al. [2019]</ref>. Thus, CNNs can further benefit from carefully crafted attention mechanisms to capture information from all spatial locations of an input. These advantages of attention mechanisms over vanilla CNNs have motivated the research into transformer architectures suitable for vision applications <ref type="bibr" target="#b6">Khan et al. [2021]</ref>.</p><p>In spite of their advantages over CNNs, transformers with their default attention mechanisms have found limited use in vision due to their quadratic complexity with respect to sequence length. Images are fed into transformers as long unrolled 1D sequences of pixels or patches. For example, an image of size 256?256 pixels becomes a sequence of length 65,536 pixels. The computational and memory requirements also scale quadratically as we use images of higher resolutions. Transformers also lack inductive biases to exploit the 2D structure of the images. A general architecture without a strong inductive bias makes transformers suitable for applications in multiple domains, provided one has enough data and computational resources. However, the required data, resources, and time for training transformers to achieve accuracy comparable performance with CNNs on images is not suitable for resource-constrained scenarios <ref type="bibr" target="#b6">Khan et al. [2021]</ref>. The inductive bias of 2D convolutions in the latter allows them to learn from relatively smaller datasets.</p><p>We propose three specific improvements to vision transformers to reduce the resources required to process images: (i) replacing the quadratic attention in ViT with linear attention mechanisms such as Linformer <ref type="bibr" target="#b7">Wang et al. [2020]</ref>, Performer <ref type="bibr" target="#b8">Choromanski et al. [2021]</ref> and Nystr?mformer <ref type="bibr" target="#b9">Xiong et al. [2021]</ref> creating the Vision X-former (ViX), (ii) replacing the the initial linear embedding layer by convolutional layers in ViX creating a hybrid ViX, and (iii) replacing the popular learnable 1D position embedding with Rotary Position Embedding (RoPE) <ref type="bibr" target="#b10">Su et al. [2021]</ref>. While, individually, some of the sub-quadratic attention mechanisms have been tested on images, none of the testing started with images in mind for a thorough comparison of various attention mechanisms for vision. Our idea of using convolutional embedding layers was also developed in parallel by another group , while the use of RoPE for vision transformers has not been proposed before. These improvements help us use smaller network sizes for images, which result in the processing of longer sequences at lesser cost. Our experiments suggest that each of these changes independently contribute to the increase in image classification accuracy on relatively smaller datasets using supervised learning. We also demonstrate these improvements across alternative transformer models, such as LeViT, CCT (Compact Convolutional Transformer), CvT (Convolutional vision Transformer) and PiT (Pooling-based Vision Transformer) when we tested them after making the proposed changes to their architectures. We also observed similar trends in the performance of FNet and MLP mixer-based vision transformers, when these are modified as proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Vision transformers</head><p>Transformers have been applied to unrolled pixel sequences <ref type="bibr" target="#b12">Parmar et al. [2018]</ref>. With Vision Transformer (ViT), it was proposed that applying the original transformer model to a sequence of sub-images (a.k.a. patches) rather than individual pixels of an image shortens the sequence length , as shown in <ref type="figure">Figure 1</ref>. Two-dimensional image patches are flattened into a vector and fed to the transformer as a sequence. These vectorized patches are then projected onto a patch embedding using a linear layer and 1D learnable position embedding is attached with it to encode location information. A learnable class embedding is prepended to the sequence of embedded patches. The output representation corresponding to this position is used as the global image representation for the image classification task. ViT achieved excellent results on image classification compared to state-of-the-art CNN models.</p><p>However, architecture that use unrolled pixel or even patch sequences have to be pre-trained on a large dataset (e.g., 300 million images ) and yet they show poor performance on medium-size datasets due to the lack of inductive biases for images. A larger number of images is required to discover the knowledge rules when a suitable inductive bias, e.g. 2D translational equivariance, is not used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Linear transformers</head><p>Recently, there has been an influx of proposals for efficient transformers (X-formers) to tackle the problem of quadratic complexity of the number of attention weights with respect to the sequence length <ref type="bibr" target="#b14">Tay et al. [2020a]</ref>. These X-formers, such as Linformer <ref type="bibr" target="#b7">Wang et al. [2020]</ref>, Performer <ref type="bibr" target="#b8">Choromanski et al. [2021]</ref>, Linear Transformer <ref type="bibr" target="#b15">Katharopoulos et al. [2020]</ref> and Nystr?mformer <ref type="bibr" target="#b9">Xiong et al. [2021]</ref> claim linear complexity in computing attention, which makes them ideal candidates for analyzing long pixel sequences encountered in vision tasks. The Linformer projects the length dimension of keys and values to a lower dimensional representation <ref type="bibr" target="#b7">Wang et al. [2020]</ref>. Performer <ref type="bibr" target="#b8">Choromanski et al. [2021]</ref> and Linear transformer <ref type="bibr" target="#b15">Katharopoulos et al. [2020]</ref> use kernels to cleverly re-write the attention mechanism to avoid computing the N?N attention matrix. Nystr?mformer repurposes the Nystr?m method for approximating self-attention using landmark (or Nystr?m) points to reconstruct the softmax matrix in self-attention <ref type="bibr" target="#b9">Xiong et al. [2021]</ref>. The Long Range Arena (LRA) benchmark was established to compare the performance and efficiency of these X-formers on various long sequence tasks, including image classification <ref type="bibr" target="#b16">Tay et al. [2020b]</ref>. Most of the applications of linear transformers discussed above have been limited to the NLP domain. We exploit the linear complexity of these attention mechanisms to create a more efficient transformer for vision applications that can handle long sequences and smaller patch sizes with reduced GPU memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Convolutional embedding layers for ViT</head><p>It was observed in <ref type="bibr" target="#b17">Child et al. [2019]</ref> and  that early layers in the transformer learn locally connected patterns which resemble convolutions. This led to the development of many hybrid architectures inspired by transformers and CNNs. LeViT is such a hybrid architecture for fast inference image classification using both convolutional layers and attention <ref type="bibr" target="#b18">Graham et al. [2021]</ref>. LeViT uses convolutional embedding instead of patch-wise projection and uses 2D relative positional biases instead of initial absolute positional bias used in ViT. It also downsamples the image in stages, uses an extra non-linearity in attention and also replaces layer-norm with batch-norm. CvT improves upon ViT by using a hierarchy of transformers containing a new convolutional token embedding, and a convolutional transformer block <ref type="bibr" target="#b19">Wu et al. [2021]</ref>. CCT eliminates the requirement for class token and positional embedding through a novel sequence pooling strategy and the use of convolutions <ref type="bibr" target="#b20">Hassani et al. [2021]</ref>. PiT uses the spatial dimension reduction principle of CNN to improve model capability and generalization performance compared to <ref type="bibr">ViT Heo et al. [2021]</ref>. LeViT, CvT and CCT uses convolution layers to down-sample the image size to reduce the sequence length. Additionally, they added multiple convolutional layers between the attention blocks, which increases the number of model parameters since CNNs have more parameters than transformers. Therefore, we used a single convolutional block in our architecture replacing the initial linear embedding layer of ViT to provide the required inductive bias to our model, without down-sampling the image size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Alternative positional embedding</head><p>Rotary position embedding (RoPE) encodes the absolute positional information with a rotation matrix and incorporates an explicit relative position dependency in the self-attention mechanism <ref type="bibr" target="#b10">Su et al. [2021]</ref>. It captures decaying inter-token dependency with increasing relative distances and can be used easily with linear self-attention mechanisms with relative position encoding. The previous works using RoPE has been limited to NLP and molecular domains . We observed the change in classification accuracy when the standard learnable 1D positional embedding is replaced with RoPE in the ViT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Alternative mixing models</head><p>Recently, new architectures were introduced that replace attention with other mechanisms that enable mixing of information between tokens. FNet replaces the self-attention layer in the transformer with the Fourier Transform that does not need to be learned Lee-Thorp et al. <ref type="bibr">[2021]</ref>. FNet was tested mostly in NLP tasks and its performance was compared to language models like BERT. MLP-mixer replaces attention using two types of multi-layer perceptrons (MLPs) which are applied independently first to image patches and then across patches <ref type="bibr" target="#b24">Tolstikhin et al. [2021]</ref>. In this paper, we compare the performance and GPU consumption of these models in supervised image classification by replacing attention in ViT with these alternative mixing mechanisms. We also observe the improvement in accuracy when we add convolutional layers to create the embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Architectures for ViX</head><p>We replace the regular quadratic attention in ViT with linear attention such as Linformer, Performer and Nystr?mformer, creating the Vision X-former (ViX), where X ? {Performer, Linformer, Nystr?mformer}, and compare their accuracy and computational requirements for image classification. We set the patch size of the ViT to one so that each pixel is considered as a separate token and the entire image is unrolled to create the full-length image token sequence. This is done to observe the performance in long sequence image classification. We also test the performance of FNet and MLP mixer architectures of similar depth to compare them with ViX. The ViX architecture is shown in <ref type="figure">Figure 2</ref>. The lack of inductive priors restricts attention from exploiting the 2D nature and symmetries of the images in the low data regime. We therefore use convolutional layers to replace the initial linear embedding layer to extract 2D information from the images. We propose hybrid ViX -a combination of linear attention and convolutional layers for image classification -and compare its performance with standard ViT. We replace the linear embedding layer in ViX by three convolutional layers, creating the same number of feature maps as the embedding dimension. The size of each feature map is the same as that of the input image. The hybrid ViX architecture is shown in <ref type="figure" target="#fig_2">Figure 3</ref>. The convolutional layers were also added to FNet and MLP mixer models to observe their performance.</p><p>We also compare the performance of other transformer architectures such as LeViT, CvT, PiT, and CCT by replacing quadratic attention with linear ones, creating LeViX, CCX, CvX, and PiX. In our notation, we replace the X in X-former models with the name of the linear attention used, for example, CCT model that uses Performer becomes CCP (Compact Convolutional Performer). Addition of the prefix hybrid means that the convolutional layer has replaced the linear embedding layer, i.e, hybrid PiX is created by replacing the linear embedding layer with convolutional ones. LeViT adds relative positional bias when it computes the full attention matrix. Since, linear attention mechanism does not </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The CIFAR-10 Krizhevsky <ref type="bibr">[2009]</ref> and the Tiny ImageNet Le and Yang <ref type="bibr">[2015]</ref> datasets were used for our experiments.</p><p>Since the image size in the former is 32?32, we have a sequence length of 1024 upon unrolling the images and an additional class token was added for classification, making the final sequence length 1025. All models used in our experiments have 128-dimensional embedding, 4 layers, 4 heads, 256 as MLP dimension, and 0 dropout to ensure uniformity in comparison in line with previous studies. <ref type="bibr">1</ref> We used 64 benchmark points in Nystr?mformer, a local window size of 256 with ReLU nonlinearity as the kernel function in Performer, and used the same projection for keys and values in Linformer. The Adam optimizer with ? = 0.001 (learning rate), ? 1 = 0.9 and ? 2 = 0.999 were used for computing running averages of gradient and its square, = 10 ?8 , and 0.01 as weight decay coefficient were used. We used automatic mixed precision in PyTorch during training to make it faster and consume less memory. Experiments were done with 16 GB Tesla P100-PCIe and Tesla T4 GPUs available in Kaggle and Google Colab. GPU usage for a batch size of 64 is reported along with top-1 % and top-5% accuracy from best of three runs.</p><p>In the hybrid ViT and ViX models, we used three convolutional layers with 32, 64, and 128 3?3 kernels respectively in each stage with stride of one with padding to generate 128 feature maps of size 32?32. We used four-head, four-layer ViT architecture with patch dimension of 128, and MLP dimension of 256 for this experiment. For the comparison between LeViT and LeViX, we used a single stage model with three convolutional layers for generating the embedding. For CCT and CCX, we used learnable position embedding and 3?3 convolutional and pooling kernels having stride equal to 1 and padding. For CvT and CvX, we used one attention module in the first stage, one attention module in the second stage and 2 attention modules in the third stage with 3?3 convolutions in each stage. For the PiT, PiX, and hybrid PiX, we used a 4-layer transformer, down-sampling the sequence after every two layers.</p><p>We also used the Tiny ImageNet dataset to validate the results obtained from CIFAR-10. We set the patch size of ViX models and stride for the initial convolutional layer in hybrid ViX models as two so that the 64?64 images in the Tiny ImageNet dataset gets unrolled to a sequence of length 1024. The final classification layer was replaced to handle classification of 200 classes instead of 10 classes. All other design parameters were similar to those used for the CIFAR-10 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">ViX compared to ViT</head><p>Compared to the vanilla transformer (ViT), the Vision Nystr?mformer (ViN) performs ? 16% better, Vision Performer (ViP) performs ? 3% better, and Vision Linformer (ViL) performs ? 2% better for image classification of CIFAR-10 dataset, as show in <ref type="table">Table 1</ref>. Among the ViX architectures, ViP and ViN use almost the same number of parameters as the quadratic attention, but ViL uses a significantly lower number of parameters. ViT also requires the largest share of GPU memory and storage space among all models tested. All X-former models (blue) consume less than half the storage space used by ViT (red) and use less than one-third of ViT's GPU memory for achieving comparable performance as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. Similar improvements in performance at reduced GPU RAM consumption were also observed on classification of Tiny ImageNet dataset, as shown in <ref type="table" target="#tab_2">Table 2</ref>.  FNet has the lowest number of parameters since it completely replaces the attention module with the fast Fourier transform that does require trainable parameters. The four-layered FNet performs ? 10% worse than ViT but uses only half the number of parameters, one-third of its memory, and only one-tenth of the GPU RAM for image classification on the CIFAR-10 dataset, as shown in <ref type="table">Table 1</ref>. The 4 layered MLP mixer performs ? 7% better, but uses 16 times the number of parameters that a ViT uses. However, an MLP mixer uses less than half the memory and one-tenth of the GPU RAM used by a ViT, making it a more efficient alternative to transformers. FNet performs ? 4 percentage points better than MLP mixer in classification of Tiny ImageNet dataset using one-tenth the GPU RAM used by ViT as shown in <ref type="table" target="#tab_2">Table 2</ref>. The fractional GPU RAM usage by these models will enable us to use deeper models compared to ViT for a fixed GPU budget.  As we can see from <ref type="table">Table 1</ref>, replacing the initial linear embedding layer with convolutional layers increases the number of parameters slightly for all base architectures. We also observe a slight increase in the model size and a negligible increase in the GPU usage. However, these small increases in parameters and computational resources is compensated by an appreciable improvement in performance of ? 20% across all models for classification of CIFAR-10 dataset. Performer (Hybrid ViP) and transformer (Hybrid ViT) models both gained ? 28% accuracy, while the Nystr?mformer (Hybrid ViN) gained ? 16%. The Hybrid ViP and the Hybrid ViN outperformed the Hybrid ViT by about 1 and 2 percentage points, respectively, using only one-third of the GPU RAM and half the memory of a ViT. Even though the Hybrid Vision Linformer (ViL) underperformed compared to the other mechanisms, its computational cost is one-sixth of ViT and it also has the lowest number of parameters. The Hybrid ViN performed better than the other models in terms of the top-1 accuracy.</p><p>Adding convolutional layers to FNet improves its performance by ? 28% for CIFAR-10 dataset while it improves by ? 18% for Tiny ImageNet dataset. It has the smallest number of parameters and consumes less memory than other ViX models, making it an attractive alternative compared to transformer models. The classification accuracy of the MLP mixer increased only marginally with the addition of convolutional layers for CIFAR-10 dataset, but increased by 4 percentage points for Tiny ImageNet dataset. <ref type="figure" target="#fig_3">Figure 4</ref> shows clearly that all ViX (blue squares) and hybrid (triangles) models outperform the ViT (red). The linear attention of ViX models (blue) using Performer (ViP) and Nystr?mformer (ViN) outperform the quadratic attention of ViT. Further, the graph shows that all hybrid models (green and purple), which use convolutional layers for generating the embedding, outperform the models that use a linear layer (red and blue) by a large margin. Even when the convolutional layers replace the linear embedding layers, the ViX models (green) outperform normal attention in ViT (purple). This significant increase in accuracy may be attributed to the convolutional layers providing low-level features such as edges, corners, and bars to the attention layers in the low data regime. The availability of these inductive priors from convolutional layers helps the transformer learn to classify images using less data.</p><p>Adding convolutional layers significantly improved the performance of models in classification of Tiny ImageNet dataset. We can see from <ref type="table" target="#tab_2">Table 2</ref> that hybrid ViP and hybrid ViN achieve the accuracy of hybrid ViT with less than one-third the GPU RAM. These results holds potential for using hybrid X-formers for processing high resolution images at smaller patch sizes without exhausting GPU memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">ViX compared to other Vision Transformers</head><p>We can see from <ref type="table" target="#tab_4">Table 3</ref> that LeViX models using Performer (LeViP) and Nystr?mformer (LeViN) outperform LeViT by about 1% and 3% respectively for classification of CIFAR-10 dataset. Even though all the models have a similar number of parameters, the LeViT model takes more than twice the memory and thrice the GPU RAM compared to LeViP and LeViN models. Similar performance gain is observed in the case of CCT and CvT, where Performer (CCP and CvP) and Nystr?mformer (CCN and CvN) perform better than CCT and CvT using less than half its memory and one-third of GPU RAM. The same trend is observed in    We also observe from <ref type="table" target="#tab_6">Table 5</ref> that replacing the quadratic attention in pooling-based image transformer (PiT) by ViX improves its performance. Nystr?mformer (PiN) and Performer (PiP) improves the accuracy by about 9% and 3% respectively using one-third the GPU RAM and half the memory compared to PiT. We also observe the similar trend from previous sections where a significant increase in accuracy was achieved by replacing the linear embedding layer with convolutional ones. An average increase of 10% in classification accuracy was observed in Hybrid PiX models compared to PiX models.  <ref type="table">Table 6</ref> shows that replacing the 1D learnable position embedding with Rotary Position Embedding (RoPE) further increases the performance of ViX and hybrid ViX models. We observed that using RoPE did not change the number of parameters in these models, but slightly increased the size and GPU usage. The trend of low GPU usage and memory consumption by ViX and hybrid ViX compared to ViT was observed again with RoPE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Discussion</head><p>We showed that using linear self-attention mechanisms such as Performer and Nystr?mformer in place of a vanilla attention mechanism with quadratic complexity in vision transformers can overcome a significant computational bottleneck that limits the application of transformers for long sequences formed by image pixels. More importantly, linear attention mechanisms in ViX can achieve image classification accuracy comparable to ViT while using far fewer computational resources, GPU RAM, and storage memory. This result was expected as linear attention mechanisms such as Nystr?mformer and Performer were able to reach the performance of vanilla transformer using half the GPU in LRA benchmark tasks which included image classification <ref type="bibr" target="#b16">Tay et al. [2020b]</ref>, <ref type="bibr" target="#b9">Xiong et al. [2021]</ref>. Our experiments confirm that replacing quadratic attention with these linear attention mechanisms in ViT architectures also leads to significant reduction in GPU RAM and computational costs without deteriorating their performance in image classification. This will enable the usage of transformer architectures for vision applications by practitioners who have resource constraints in terms of size of the GPUs (i.e., RAM and cores).</p><p>Additionally, we showed that replacing the initial fully connected embedding layer with the convolutional ones can independently improve the performance of vision transformers by providing an inductive bias that allows them to achieve higher accuracy with less training data. This result was obtained in parallel to ours by another group that showed similar trends by adding convolutional layers to vision transformer architectures, such as LeViT, CCT and CvT <ref type="bibr" target="#b18">Graham et al. [2021]</ref>, <ref type="bibr" target="#b19">Wu et al. [2021]</ref>, <ref type="bibr" target="#b20">Hassani et al. [2021]</ref>, <ref type="bibr" target="#b21">Heo et al. [2021]</ref>. We show that using convolutional layers to generate the embedding is enough to gain significant improvement in performance.</p><p>Furthermore, we showed that replacing the standard 1D learnable position embedding with rotary position embedding also showed independent improvement in classification accuracy. This trend was observed across different transformer architectures such as LeViT, CCT, CVT, and PiT when we implemented these changes in them. Alternative architectures that are efficient in token mixing, such as FNet and MLP mixer, also hold promise in vision applications as they require only one-tenth of the GPU RAM used by a vanilla ViT. Using ViX, FNet or MLP mixer will enable the use of deeper models with a fixed GPU RAM budget compared to vanilla ViT. Addition of convolutional layers for generating the embedding significantly improved the performance of FNet, but such an increase in accuracy was not observed with MLP mixer. This shows that making a hybrid MLP mixer does not justify the increase in space and parameter count. So, we do not recommend using convolutional layers in the MLP mixer.</p><p>Undoubtedly, further experiments with hyper-parameter tuning, using different positional embedding, alternative mixing models, and image datasets and tasks can lead to more insights for improving the efficiency and accuracy in low data, low GPU RAM regime. However, our results point to several significant directions for developing alternatives to purely convolutional architectures for vision. While the use of transformer architectures for vision has produced tantalizing results in terms of accuracy, these have come at orders of magnitude higher costs in terms of training data, floating-point operations, GPU RAM, hardware costs, and power consumption . This has put training transformer-type architectures out of the reach of all researchers, except those in a select few organizations with massive budgets, while their monetization is still in question. On the other hand, in spite of the theoretical guarantee of a fully connected neural network with a single hidden layer being able to model any function <ref type="bibr" target="#b27">Kolmogorov [1957]</ref>, proposals of neural architectures that exploit domain-specific inductive biases have resulted in usable increases in accuracy as well as decreases in computational and data requirements. Our work suggests that this quest is far from over, and architectural innovation can democratize the ability to train neural architectures from scratch with state-of-the-art performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: ViT architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2: ViX architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Hybrid ViX architecture compute this matrix, we could not add the relative positional bias in LeViX. Therefore, we removed the positional bias in LeViT so that the comparison between LeViT and LeViX is justified. Subsequently, we compare the performance of ViX and hybrid ViX models by replacing 1D learnable position embedding with a rotary positional embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of classification accuracy and GPU usage by various models on CIFAR-10 for a batch size of 64</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Test accuracy and various requirements of models compared on the Tiny ImageNet dataset for a batch size of 64</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="6">Model Parameters Size (MB) Top-1 Accuracy (%) Top-5 Accuracy (%) GPU (GB)</cell></row><row><cell>LeViT</cell><cell>624,970</cell><cell>213.15</cell><cell>77.06</cell><cell>98.46</cell><cell>13.9</cell></row><row><cell>LeViP</cell><cell>662,410</cell><cell>77.14</cell><cell>79.50</cell><cell>98.91</cell><cell>4.9</cell></row><row><cell>LeViL</cell><cell>506,186</cell><cell>69.69</cell><cell>63.17</cell><cell>96.13</cell><cell>2.3</cell></row><row><cell>LeViN</cell><cell>621,402</cell><cell>77.13</cell><cell>77.81</cell><cell>98.61</cell><cell>4.3</cell></row><row><cell>CCT</cell><cell>905,547</cell><cell>225.47</cell><cell>82.23</cell><cell>99.04</cell><cell>14.7</cell></row><row><cell>CCP</cell><cell>907,083</cell><cell>101.48</cell><cell>82.48</cell><cell>99.06</cell><cell>4.3</cell></row><row><cell>CCL</cell><cell>790,859</cell><cell>94.04</cell><cell>80.05</cell><cell>98.92</cell><cell>3.5</cell></row><row><cell>CCN</cell><cell>906,075</cell><cell>101.48</cell><cell>83.36</cell><cell>99.07</cell><cell>4.2</cell></row><row><cell>CvT</cell><cell>1,099,786</cell><cell>173.21</cell><cell>79.93</cell><cell>99.02</cell><cell>12.8</cell></row><row><cell>CvP</cell><cell>827,914</cell><cell>100.17</cell><cell>83.19</cell><cell>99.20</cell><cell>4.8</cell></row><row><cell>CvL</cell><cell>711,690</cell><cell>92.73</cell><cell>72.58</cell><cell>97.81</cell><cell>2.6</cell></row><row><cell>CvN</cell><cell>826,906</cell><cell>100.17</cell><cell>83.26</cell><cell>99.14</cell><cell>4.3</cell></row></table><note>for classification of Tiny ImageNet dataset where LeViP outperforms LeViT by ? 8%, CCN and CCP outperform CCT by ? 3%, and CvN and CvP outperform CvT by ? 12%. This shows that replacing quadratic attention with Performer and Nystr?mformer attention will improve the</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of different convolutional transformers without and with linear attention mechanisms on CIFAR-10 dataset</figDesc><table><row><cell cols="6">Model Parameters Size (MB) Top-1 Accuracy (%) Top-5 Accuracy (%) GPU (GB)</cell></row><row><cell>LeViT</cell><cell>649,480</cell><cell>213.28</cell><cell>38.29</cell><cell>65.23</cell><cell>14</cell></row><row><cell>LeViP</cell><cell>646,920</cell><cell>77.27</cell><cell>41.49</cell><cell>68.39</cell><cell>4.9</cell></row><row><cell>LeViL</cell><cell>530,696</cell><cell>69.89</cell><cell>26.52</cell><cell>51.85</cell><cell>3.3</cell></row><row><cell>LeViN</cell><cell>645,912</cell><cell>77.26</cell><cell>38.28</cell><cell>64.94</cell><cell>4.3</cell></row><row><cell>CCT</cell><cell>930,057</cell><cell>225.61</cell><cell>39.05</cell><cell>65.74</cell><cell>14.2</cell></row><row><cell>CCP</cell><cell>931,593</cell><cell>101.61</cell><cell>40.11</cell><cell>66.93</cell><cell>4.1</cell></row><row><cell>CCL</cell><cell>815,369</cell><cell>94.17</cell><cell>37.24</cell><cell>62.97</cell><cell>2.3</cell></row><row><cell>CCN</cell><cell>930,585</cell><cell>101.61</cell><cell>40.80</cell><cell>67.81</cell><cell>3.9</cell></row><row><cell>CvT</cell><cell>1,124,296</cell><cell>173.34</cell><cell>40.69</cell><cell>68.19</cell><cell>12.3</cell></row><row><cell>CvP</cell><cell>852,424</cell><cell>100.3</cell><cell>45.26</cell><cell>71.21</cell><cell>4.4</cell></row><row><cell>CvL</cell><cell>736,200</cell><cell>92.86</cell><cell>33.95</cell><cell>61.29</cell><cell>2.3</cell></row><row><cell>CvN</cell><cell>851,416</cell><cell>100.3</cell><cell>45.47</cell><cell>71.35</cell><cell>4.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison of different convolutional transformers without and with linear attention mechanisms on Tiny ImageNet dataset performance in long sequence image classification using lesser memory and GPU RAM. These results indicate the possibility of training transformer models in vision with limited resources and less data using X-formers.</figDesc><table><row><cell>Model</cell><cell cols="5">Parameters Size (MB) Top-1 Accuracy (%) Top-5 Accuracy (%) GPU (GB)</cell></row><row><cell>PiT</cell><cell>897,034</cell><cell>129.74</cell><cell>63.49</cell><cell>96.95</cell><cell>14.7</cell></row><row><cell>PiN</cell><cell>897,562</cell><cell>65.81</cell><cell>69.41</cell><cell>97.68</cell><cell>5.7</cell></row><row><cell>PiP</cell><cell>898,570</cell><cell>64.09</cell><cell>65.83</cell><cell>97.28</cell><cell>5.2</cell></row><row><cell>Hybrid PiT</cell><cell>989,770</cell><cell>131.80</cell><cell>73.34</cell><cell>98.11</cell><cell>14.8</cell></row><row><cell>Hybrid PiN</cell><cell>990,298</cell><cell>67.87</cell><cell>74.00</cell><cell>98.03</cell><cell>5.7</cell></row><row><cell>Hybrid PiP</cell><cell>991,306</cell><cell>66.15</cell><cell>71.37</cell><cell>97.88</cell><cell>5.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison of performance by replacing attention with X-formers in PiT for classification of CIFAR-10 dataset</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The codes for the implementation of all models were adapted from https://github.com/lucidrains/vit-pytorch</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and &lt;0.5mb model size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah. Transformers in vision: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Nystr?mformer: A nystr?m-based algorithm for approximating self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudrasis</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfeng</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Early convolutions help transformers see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Efficient transformers: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Nikolaos Pappas, and Fran?ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Long range arena: A benchmark for efficient transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Levit: a vision transformer in convnet&apos;s clothing for faster inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Escaping the big data paradigm with compact transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abulikemu</forename><surname>Abuduweili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Rethinking spatial dimensions of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Joon</forename><surname>Oh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Do large scale molecular language representations capture important structural information?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerret</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Belgodere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijil</forename><surname>Chenthamarakshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkit</forename><surname>Padhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payel</forename><surname>Das</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Fnet: Mixing tokens with fourier transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Lee-Thorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Ya Le and X. Yang. Tiny imagenet visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Learning multiple layers of features from tiny images</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Train big, then compress: Rethinking model size for efficient training and inference of transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5958" to="5968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On the representation of continuous functions of many variables by superposition of continuous functions of one variable and addition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Nikolaevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kolmogorov</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Doklady Akademii Nauk</title>
		<imprint>
			<date type="published" when="1957" />
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="953" to="956" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
