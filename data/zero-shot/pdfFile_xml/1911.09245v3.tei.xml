<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Consensus-based Optimization for 3D Human Pose Estimation in Camera Coordinates</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diogo</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
							<email>diogo.luvizon@ensea.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">ETIS UMR 8051</orgName>
								<orgName type="institution" key="instit1">Paris Seine University</orgName>
								<orgName type="institution" key="instit2">ENSEA</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<postCode>F-95000</postCode>
									<settlement>Cergy</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Advanced Technologies</orgName>
								<orgName type="institution">Samsung Research Institute</orgName>
								<address>
									<settlement>Campinas</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Tabia</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">ETIS UMR 8051</orgName>
								<orgName type="institution" key="instit1">Paris Seine University</orgName>
								<orgName type="institution" key="instit2">ENSEA</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<postCode>F-95000</postCode>
									<settlement>Cergy</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">IBISC</orgName>
								<orgName type="institution" key="instit2">Univ. d?Evry Val d?Essonne</orgName>
								<orgName type="institution" key="instit3">Universit? Paris Saclay</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Picard</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">ETIS UMR 8051</orgName>
								<orgName type="institution" key="instit1">Paris Seine University</orgName>
								<orgName type="institution" key="instit2">ENSEA</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<postCode>F-95000</postCode>
									<settlement>Cergy</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory" key="lab1">LIGM</orgName>
								<orgName type="laboratory" key="lab2">UMR 8049</orgName>
								<orgName type="institution">UPE</orgName>
								<address>
									<addrLine>?cole des Ponts</addrLine>
									<settlement>Champs-sur-Marne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Consensus-based Optimization for 3D Human Pose Estimation in Camera Coordinates</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D human pose estimation is frequently seen as the task of estimating 3D poses relative to the root body joint. Alternatively, we propose a 3D human pose estimation method in camera coordinates, which allows effective combination of 2D annotated data and 3D poses and a straightforward multi-view generalization. To that end, we cast the problem as a view frustum space pose estimation, where absolute depth prediction and joint relative depth estimations are disentangled. Final 3D predictions are obtained in camera coordinates by the inverse camera projection. Based on this, we also present a consensus-based optimization algorithm for multi-view predictions from uncalibrated images, which requires a single monocular training procedure. Although our method is indirectly tied to the training camera intrinsics, it still converges for cameras with different intrinsic parameters, resulting in coherent estimations up to a scale factor. Our method improves the state of the art on well known 3D human pose datasets, reducing the prediction error by 32% in the most common benchmark. We also reported our results in absolute pose position error, achieving 80 mm for monocular estimations and 51 mm for multiview, on average. Source code is available at https:// github.com/dluvizon/3d-pose-consensus.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D human pose estimation is a very active research topic, mainly due to the several applications that benefit from precise human poses, such as sports performance analysis, 3D model fitting, human behavior understanding, among many others. Despite the recent works on 3D human pose estimation, most of the methods in the literature are limited to the problem of relative pose prediction <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b1">2]</ref>, where the root body joint is centered at the origin and the remaining joints are estimated relative to the center. This limitation hinders the generalization for multi-view scenarios since predictions are not in the camera coordinates. Contrarily, when estimations are relative to a static referential, predictions can be easily projected from one view to another, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. The methods in the state of the art frequently handle 3D human pose estimation as a regression task, directly converting the input images to predicted poses in millimeters <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b23">24]</ref>. However, this is a depth learning problem, because identical distances in pixels can result in different distances in millimeters. For example, a person close to the camera with the hand next to the head has a distance (head to hand in mm) much shorter than a person far from the camera with her arm extended, although both result in the same distance in pixels. Consequently, those methods have to learn the intrinsic parameters indirectly. Moreover, by predicting 3D poses directly in millimeters, the abundant images with annotated 2D poses in pixels cannot be easily exploited, since this data has no associated 3D information, and relative poses predicted from one camera cannot be easily projected into a different view, making it more difficult to handle occlusion cases in multi-view scenarios.</p><p>In our method, we tackle these limitations by casting the problem of 3D human pose estimation into a different perspective: instead of directly predicting pose in millimeters relative to the root joint, we predict 3D poses in the view frustum space, i.e., we predict (u, v) coordinates in the image plane, in pixels, and the absolute depth in millimeters. We further split depth estimation as a global absolute depth and joint relative depth estimations. Both 2D human pose and absolute depth estimation are well known problems in the literature <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22]</ref>, including absolute depth estimation benchmarks <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b52">53]</ref>, but are usually not correlated. In our method, we train a feed-forward neural network by effectively merging in-the-wild 2D data and precise 3D poses, making the best use of each. Even though our network is trained only with monocular images, the predictions from individual views can be merged by the proposed consensusbased optimization in order to produce multi-view estimations, resulting in an effective way to handle the challenging cases of occlusions, as demonstrated by a significant improvement in accuracy in our experiments. Although our training scheme is indirectly tied to the camera intrinsics, our method has demonstrated a generalization capability to predict 3D poses up to a scale from a completely different camera setup, including different intrinsic parameters. This was evidenced by qualitative and quantitative evaluations. Considering the exposed limitations of relative 3D human pose estimation, we aim to fill the gap of current methods by addressing the more complex problem of absolute 3D human pose estimation, where predictions are performed with respect to a static referential i.e., the camera position, and not to the person's root joint. In that direction, we present our contributions: First, we propose an absolute 3D human pose estimation method from monocular cameras which achieves results in the state of the art when considering similar camera intrinsics at training and inference time. Second, we propose a consensus-based optimization for multi-view absolute 3D human pose estimation from uncalibrated images, which requires a single monocular training procedure. The multi-view estimation approach is capable of generalizing for different camera setups, resulting in coherent 3D absolute predictions up to a scale factor. Our method sets the new state-of-the-art results on the challenging test set from Human3.6M, improving previous results by 10% with monocular predictions and by 32% considering multiple views.</p><p>The remaining of this paper is divided as follows. In section 2 we present the related work. Our method for 3D human pose estimation is explained in section 3 and our algorithm for consensus-based optimization is detailed in section 4. The experiments are presented in section 5 and in section 6 we conclude this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In this section, we review the methods most related to our work, giving special attention to monocular (relative and absolute) and multi-view 3D human pose estimation. We recommend the survey in <ref type="bibr" target="#b43">[44]</ref> for readers seeking for a more detailed review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Monocular relative 3D human pose estimation</head><p>In the last decade, monocular 3D human pose estimation has been a very active research topic in the community <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b14">15]</ref>. Many recent works have proposed to directly predict relative 3D poses from images <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b36">37]</ref>, which requires the model to learn a complex projection from 2D pixels to millimeters in three dimensions. Another drawback is their limitation to benefit from the abundant 2D data, since manually annotated images have no associated 3D information.</p><p>A common approach to directly use 2D data during training is to first learn a 2D pose estimator, than lift 3D poses from 2D estimations <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b7">8]</ref>. However, lifting 3D from 2D points only is an ill-defined problem since no visual cues are available, frequently resulting in ambiguity and, consequently, limited precision. Other methods assume that the absolute location of the root joint is provided during inference <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b24">25]</ref>, so the inverse projection from pixels to millimeters can be performed. In our approach, this assumptions is not made, since we estimate the 3D pose in absolute coordinates. The only additional information we need are the intrinsic parameters for monocular prediction, which is often given by the manufacturer or could be estimated by standard tools. In addition and differently from <ref type="bibr" target="#b24">[25]</ref>, our approach allows combining predictions from multiple views of the scene, resulting in more precise estimations.</p><p>Contrarily to the previous work, we are able to train our method simultaneously with 3D and 2D annotated data in an effective way, since one part of our prediction is performed in the image plane and completely independent from 3D information. Moreover, estimating the first two coordinates in pixels in the image plane is a better defined problem than estimating floating 3D positions directly in millimeters. These advantages translate into higher accuracy for our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Monocular absolute 3D human pose estimation</head><p>Contrarily to relative estimation, in absolute pose prediction the 3D coordinates of the human body are predicted with respect to the camera or in the view frustum space. A simple approach is to infer the distance to the camera considering a normalized or constant body size <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b29">30]</ref>, which is an information that may not be available and difficult to be estimated <ref type="bibr" target="#b11">[12]</ref>. Inspired by the many works on depth estimation, Nie et al. <ref type="bibr" target="#b34">[35]</ref> predict the depth of body joints individually. The drawback of this method is that it suffers to capture the human body structure, since errors in the estimated depth for individual joints can degenerate the final pose.</p><p>More recently, multi-person absolute pose estimation methods were proposed <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b28">29]</ref>. In <ref type="bibr" target="#b31">[32]</ref>, the absolute distance from the person to the camera is predicted based on the area of the cropped 2D bounding box. However, it is known from the literature on absolute depth estimation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9]</ref> that not only the size of objects are important, but also their positions in the image is an informative cue to predict its depth. For example, a person in the bottom of an image is more likely to be closer to the camera than a person on the top of the same image. Differently, in <ref type="bibr" target="#b28">[29]</ref>, the authors optimized the person absolute distance based on the initial bone lengths, estimated from the first 10 frames of a video sequence, and on the re-projection of the 3D pose into the 2D body joint locations. Besides this approach relies on video sequences, it also requires the camera parameters.</p><p>In our approach, we combine three different information to predict the distance of the root joint w.r.t. the camera position: the size of the bounding box (including its ratio), the target position in the image, and deep convolutional features that provide additional visual cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Multi-view pose estimation and camera calibration</head><p>For the challenging cases of occlusion or clutter background, multiple views can be decisive to disambiguate uncertain positions of body joints (see <ref type="figure" target="#fig_0">Fig. 1</ref>). To handle this, several works have proposed multi-view solutions for 3D human pose estimation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14]</ref>, mostly exploring the classical concept of pictorial structures from multi-view images. Deep neural networks have been used to estimate relative 3D poses from a set of 2D predictions from different views <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b35">36]</ref>. As an example, Pavlakos et al. <ref type="bibr" target="#b37">[38]</ref> proposed to collect 3D poses from 2D multi-view image, which are used to learn a second model to perform 3D estimations. Since these methods estimate 3D from multiple 2D images, they often require both intrinsic and extrinsic parameters.</p><p>In order to estimate the full camera calibration parameters, Micusik and Pajdla <ref type="bibr" target="#b30">[31]</ref> proposed to use a human body seen at different positions in the image. The main limitation of this approach is the fact that it assumes that all poses as nearly vertical and parallel to each other. Considering multiple views of the same person, Rhodin et al. <ref type="bibr" target="#b41">[42]</ref> propose to estimate 3D poses from each individual view and to estimate the extrinsic camera calibration, assuming that the intrinsic parameters are provided as input. More recently, Iskakov et al. <ref type="bibr" target="#b16">[17]</ref> proposed a learnable triangulation of 3D poses, considering multiple fully calibrated views during training. Despite the impressive results achieved in this work, the network model is training considering a predefined camera positioning, which could result in a strong overfiting in the experimental setup. Differently, our model is trained without priors about the camera positions and the proposed multi-view optimization algorithm is not directly tied to a specific camera setup.</p><p>From the recent literature, we can notice that current multi-view approaches are still completely dependent on the camera intrinsic parameters and often require a complete calibration setup, which can be prohibitive in some circumstances. Available methods are also limited to the inference of 3D from multiple 2D predictions, requiring multi-view datasets for training. Alternatively, we propose to predict absolute 3D poses from each individual view, which has two important advantages over previous methods. First, it allows us to easily combine predictions from multiple calibrated cameras, while requiring a single monocular training procedure. Second, we are able to estimate camera calibration, both intrinsic and extrinsic, from multi-view images, by a consensus-based optimization without retraining the model. The strength of our approach is evidenced by its strong results, even when considering unknown and uncalibrated cameras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed method for 3D human pose estimation</head><p>One of the goals of our method is to predict 3D human poses in absolute coordinates with respect to the camera position. For this, we believe that the most effective approach is to predict each body joint in image pixel coordinates and in absolute depth, orthogonal to the image plane, in millimeters. Then, the predicted pixel coordinates and depth can be projected to the world coordinates, considering a pinhole camera model.</p><p>We further split the problem as relative 3D pose estimation and absolute depth estimation. The motivation for this comes from the idea that a well cropped bounding box around the person is better for predicting its pose than the full image frame, since a better resolution could be attained and the person scale is automatically handled by the image crop, although small variations in the bounding boxes during training result in better robustness. Additionally, by providing a separated loss on relative depth for each joint helps the network to learn the human body structure, which would be more difficult to learn directly from absolute coordinates due to position shift.</p><p>Recent works on depth estimation have demonstrated that neural networks rely on both pictorial cues and geome-try information to predict depth <ref type="bibr" target="#b9">[10]</ref>. For the specific problem of 3D human pose estimation, the structure of the human body is also an important domain knowledge to be explored. Considering our motivations and the exposed challenges, we propose to predict 3D poses relative to a cropped region centered at the person, which eases the network to encode the human body structure, and absolute depth from combined local pictorial cues and global position and size of the cropped region.  Specifically, given an image I ? R W ?H?3 and a person bounding box region ? ? R 4?1 , we define the problem as learning a function F:</p><formula xml:id="formula_0">{I, ?} F ? ? {p,?,? a }, wherep ? R 3?J is the estimated relative pose, composed of J body joints in the format (? i ,v i ,d i ) T , with i = {1, .</formula><p>. . , J},? ? R 1?J contains the body joint confidence score, which is an additional information that represents a level of confidence for each predicted body joint coordinates, and? a ? R ?0 is the estimated absolute depth for the person's root joint. The person bounding box ? is defined by its central position (x ? , y ? ) and size (w ? , h ? ), and can be obtained using a standard person detector <ref type="bibr" target="#b40">[41]</ref>. The parametrized regression function F is implemented as a deep convolutional neural network (CNN), detailed as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network architecture</head><p>U-Nets are widely used for human pose estimation due to their multi-scale processing capability <ref type="bibr" target="#b33">[34]</ref>, while classic residual networks <ref type="bibr" target="#b12">[13]</ref> are often preferable to produce CNN features. Since we want precise pose predictions and informative visual features for absolute depth estimation, our network combines a ResNet-50 model cut at block 4f as backbone and U-blocks, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. This architecture is called ResNet-U and, in addition to a few fully connected layers to regress the absolute depth? a and the confidence scores?, implements the function F. The details about each part of our method is discussed as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">3D human pose estimation</head><p>As previously stated, we first want to estimate the 3D human pose relative to the cropped bounding box. To this end, we first predict the pixel coordinates (? i ,v i ) in the image plane, given the information about the cropped image in ?. Since it is difficult to predict the absolute depth from an arbitrarily cropped region, at this stage we predict the relative depth of each body joint with respect to the location of the person. Therefore, the human pose estimation problem can be naturally split into two parts: relative pose estimation and absolute body joints depth estimation, as detailed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Relative human pose estimation</head><p>For the pose prediction in the image plane (u, v), we use the soft-argmax operation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b54">55]</ref>, which induces the U-Nets to generate one probability distribution per body joint. This probability distribution is defined as a feature map h i ? R w f ?h f (positive and unitary sum) for the ith body joint. The third dimension of the posep is composed of the depth per body joint, with respect to the location of the person. This prediction could be integrated in the softargmax by extending the feature map h to a volumetric representation <ref type="bibr" target="#b46">[47]</ref>. However, depending on the resolution and the number of body joints, this approach can be costly. Instead, we predict a normalized depth map d ? R w f ?h f per body joint, corresponding to an interval of 2 meters, which is a common reference size in other methods <ref type="bibr" target="#b36">[37]</ref>. By restricting the estimated depth to this range, we ensure that the bounding box prediction is well defined inside an small region, corresponding to the enclosure of an average person. The regressed depth inside the bounding box is defined by:</p><formula xml:id="formula_1">d i = ? (w f ,h f ) y=(1,1) d i (y)h i (y) ,<label>(1)</label></formula><p>where ? is a normalization function, d i (y) and h i (y) are the values of the depth and the joint probability distribution for the ith joint at pixel y. Note that in Equation 1 the regions in the depth maps are pooled accordingly to the high probability locations of the body joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Absolute depth estimation</head><p>Once we have estimated the body joint coordinates in pixels and the depth with respect to the location of the person, we then predict the absolute depth of the person with respect to the camera. For this, we use two complementary sources of information: the bounding box information (position and size), and deep visual features. The position and size of the bounding box provide a rough global information about the scale and position of the person in the image. Additionally, the visual features extracted from the bounding box region by means of ResNet features provide informative visual cues that are used to refine the absolute person distance estimation. Both extracted features are then fed to a fully connected network with 256 neurons at each level and a single neuron as output, represented by ? z , which is activated by a smoothed sigmoid function, defined as:</p><formula xml:id="formula_2">z a = ? 1 1 + e ??z ,<label>(2)</label></formula><p>where ? is the maximum depth, set as 10 meters in our experiments, and ? is a smoothing factor, set to 0.5 in our experiments. The output? a is then supervised with the absolute depth of the root joint z a . This process is illustrates in <ref type="figure" target="#fig_2">Fig. 2</ref> in the bottom left part. We demonstrated in our experiments that the two different types of information, visual and bounding box information, are complementary for the task of absolute depth prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Absolute 3D human pose reconstruction</head><p>In order to accomplish our objective of estimating the absolute 3D pose represented asx = (x i ,? i ,? i ) T , we combine the estimated pose in the bounding box with the predicted absolute depth. Considering (? i ,v i ,d i ) T as the first and? a as the last, the absolute z coordinate for each body joint is defined by? i =d i +? a . Note that? i is the absolute distance in millimeters from each body joint to the camera and it results from the combination of two distinct predictions, which are individually supervised. The other two absolute coordinates required to build the final absolute 3D pose,x i and? i , can then be computed using the pinhole camera model by the following equation:</p><formula xml:id="formula_3">x ? y i = 1/f x 0 ?C x /f x 0 1/f y ?C y /f y ? ?? ? v i 1 ? ?? i ,<label>(3)</label></formula><p>where f and C are the camera focal length and the camera center, both in pixels, considering the x and y axis. Note that these parameters are camera intrinsics and can be easily obtained. The camera focal length is often given by the manufacturer and the center of the image frequently corresponds to the image center in pixels or, even, both values could be estimated with standard tools. Nevertheless, in what follows we present a method to estimate the camera parameters, both intrinsic and extrinsic, without any prior information, directly from the predictions of our method, considering a multi-view scenario with uncalibrated cameras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Consensus-based optimization</head><p>One of the main advantages of estimating absolute instead of relative 3D poses is the possibility to project the predictions from one camera to another, simply by applying a rotation and a translation. This advantage has important consequences in multi-view. For example, when the camera calibration is known, the predictions of different monocular cameras can be combined with respect to a common reference, resulting in more precise predictions. For the cases where no information is known about the camera calibration, we propose a consensus-based algorithm that can be applied to estimate both intrinsic and extrinsic parameters, resulting in a completely uncalibrated multi-view approach. This algorithm is explained as follows.</p><p>Let us define the predictions of the proposed method from two distinct cameras as:</p><formula xml:id="formula_4">p c1 = (? i ,v i ,? i ) T c1 and p c2 = (? i ,v i ,? i ) T c2</formula><p>, and their poses in absolute camera coordinates:</p><formula xml:id="formula_5">x c1 = (x i ,? i ,? i ) T c1 andx c2 = (x i ,? i ,? i ) T c2</formula><p>, respectively for cameras 1 and 2. Then, we define the projection ofx c2 into camera 1 as:</p><formula xml:id="formula_6">x c2?c1 = R 2,1 (x c2 ? T 2,1 ),<label>(4)</label></formula><p>where R 2,1 ? R 3?3 and T 2,1 ? R 3?1 are a rotation matrix and a translation vector from camera 2 to camera 1. Our goal is to minimize the projection error from camera 2 to camera 1 (and vice-versa) by optimizing a set of camera parameters:</p><formula xml:id="formula_7">K 2,1 ? {f x1 , f y 1 , C x1 , C y 1 , f x2 , f y 2 , C x2 , C y 2 , R 2,1 , T 2,1 },</formula><p>which includes the intrinsics from both cameras and the extrinsic parameters between them. Specifically, let us define the optimization problem as:</p><formula xml:id="formula_8">K * 2,1 = arg min K2,1 x c1 ?x c2?c1 2 F .<label>(5)</label></formula><p>We find a solution for Equation 5 by using an optimization approach that sequentially considers the individual variables by alternating gradient with steepest descent. This process is detailed as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Camera parameters optimization</head><p>In order to obtain the translation vector from camera 2 to camera 1 that minimizes Equation 5, we define:</p><formula xml:id="formula_9">T * 2,1 = arg min T2,1 x c1 ? R 2,1 (x c2 ? T 2,1 ) 2 F .<label>(6)</label></formula><p>By replacing the squared Frobenius norm M 2 F by Tr (M T M) and by optimizing for ? T2,1 = 0, we obtain:</p><formula xml:id="formula_10">?x c2 + T 2,1 + R T 2,1xc1 = 0.<label>(7)</label></formula><p>In Equation 7, we are considering T 2,1 with shape R 3?N to simplify notation. Therefore, for a set of N points, we assume a single translation as the average of the individual solutions for all points, which results in:</p><formula xml:id="formula_11">T * 2,1 = 1 N N i=1 (x c2 ? R T 2,1xc1 ).<label>(8)</label></formula><p>Once the poses from both cameras are aligned, the rotation matrix R 2,1 can be updated with rigid Procrustes alignment. For the camera intrinsic parameters, we can also re-write Equation <ref type="bibr" target="#b4">5</ref> for the focal length and camera center only, considering the camera projection (Equation 3), resulting in:</p><formula xml:id="formula_12">f x * 1 , C x * 1 = arg min fx1,Cx1 (? i1 ? C x1 ) f x1? i1 ?x x c2?c1 2 F .<label>(9)</label></formula><p>To obtain the focal length and the camera center that minimize Equation 9, we can re-write the individual optimizations as:</p><formula xml:id="formula_13">f x * 1 = arg min f x1 f x1 A x1 ?x x c2?c1 2 F ,<label>(10)</label></formula><formula xml:id="formula_14">C x * 1 = arg min Cx1 (? i1 ? C x1 )B x1 ?x x c2?c1 2 F ,<label>(11)</label></formula><formula xml:id="formula_15">where f x1 = 1/f x1 , A x1 = (? i1 ? C x1 )? i1 , and B x1 = z ic1 /f x1 .</formula><p>By solving the Equations 10 and 11 respectively for ? f x 1 = 0 and ? Cx1 = 0, we finally obtain:</p><formula xml:id="formula_16">f x * 1 = 1 x x c2?c1 A T x1 (A x1 A T x1 ) ?1 ,<label>(12)</label></formula><formula xml:id="formula_17">C x * 1 = (? ic1 B x1 ?x x c2?c1 )B T x1 (B x1 B T x1 ) ?1 .<label>(13)</label></formula><p>Note that, for Equations 12 and 13, the intrinsic parameters for? follow a similar form, replacing thex components b? y and? i byv i . For the intrinsics from camera 2, the same equations are used, except by swapping the camera indexes in each variable. Additionally, the reverse projection (R 1,2 and T 1,2 ), from camera 1 to camera 2, is given by isolatin? x c2 from Equation 4.</p><p>In the video case, given two static cameras, we can use a sequence of frames to estimate the camera calibration, from where we can obtain more points than from a single frame and from a single pose. Finally, we can solve the global optimization problem by alternating the optimization of camera extrinsic and intrinsic parameters. This process is detailed in Algorithm 1. Note that we initialize the camera rotation R with the identity (I 3 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Body joint confidence scores</head><p>Since the proposed consensus-based optimization algorithm relies on estimated poses, it can be affect by the precision of predicted joint positions. Despite the average error of our method being very low compared to previous approaches, we also propose a confidence score that indicates if mod(Iter, 4) = 0 then 9:</p><p>Update f x1 , f y 1 using Equation <ref type="formula" target="#formula_1">12</ref> 10:</p><p>else if mod(Iter, 4) = 1 then 11:</p><p>Update f x2 , f y 2 using Equation 12 <ref type="bibr">12:</ref> else if mod(Iter, 4) = 2 then 13:</p><p>Update C x1 , C y 1 using Equation 13 <ref type="bibr">14:</ref> else if mod(Iter, 4) = 3 then 15:</p><p>Update C x2 , C y 2 using Equation 13 <ref type="bibr">16:</ref> end if 17:</p><p>Updatex c1 andx c2 from Equation <ref type="formula" target="#formula_3">3</ref> 18:</p><formula xml:id="formula_18">Iter ? Iter + 1 19: until (Iter &lt; M axIter) 20: return f x1 , f y 1 , C x1 , C y 1 , f x2 , f y 2 , C x2 , C y 2 , R 2,1 , T 2,1</formula><p>whether the network is "confident" or not for each predicted body joint. This score varies from 0 to 1, and is implemented by a DNN that takes estimated poses as input (see <ref type="figure" target="#fig_2">Fig.2</ref> -bottom right) and is trained by comparing predictions to the pose ground truth. The ground truth for the ith joint is defined as follows:</p><formula xml:id="formula_19">c i = 1 1 + e (d?di)/? d ,<label>(14)</label></formula><p>where d i is the distance error between the predicted and ground truth joint position, d is the average prediction error, and ? d is the error standard deviation. By estimating Equation 14, we can remove predicted joints with error higher than the average simply by discarding points with? &lt; 0.5 (binary decision). The predicted confidence score? is useful in Algorithm 1, providing a way to filter wrong predictions. In addition, we also take into account the confidence scores when predicting poses in multi-view scenario by weighting each body joint from each view by its corresponding predicted confidence score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we present the results of our method on two well known datasets, as well as a sequence of ablation studies to provide insights about our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>Human3.6M <ref type="bibr" target="#b15">[16]</ref> is a large-scale dataset with 3D human poses collected by a motion capture system (MoCap) and RGB images captured by 4 synchronized cameras. A total of 15 activities are performed by 11 actors, 5 females and 6 males, resulting in 3.6 million images. Poses are composed of 23 body joints, from which 17 are used for evaluation as in the previous work <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b55">56]</ref>. MPI-INF-3DHP <ref type="bibr" target="#b27">[28]</ref> is a dataset for 3D human pose estimation captured with a marker-less MoCap system, which allows outdoor video recording, e.g., TS5 and TS6 from testing. A total of 8 activities are performed by 8 different actors in two distinct sequences. Human poses are composed of 28 body joints, from which 17 are used for evaluation. The activities involve complex exercising poses, which makes this dataset more challenging than Hu-man3.6M. However, the precision of marker-less motion capture is visually less precise than ground truth poses from <ref type="bibr" target="#b15">[16]</ref>. Despite having a training set captured by 8 different cameras, test samples are captured by a single monocular camera. PennAction <ref type="bibr" target="#b57">[58]</ref> is a dataset composed by 2,326 videos in the wild with annotated 2D poses of people performing 15 different actions. This dataset does not provide 3D pose annotations, but it is usefull to access the generability of our method in a qualitative evaluation, since the images are very challenging for pose estimation. KTH Multiview Football Dataset II <ref type="bibr" target="#b18">[19]</ref> consists of images from football players with ground truth 2D and 3D poses centered in the root joint. Partial camera parameters are given for projecting the 3D poses into the three different views, however, explicit intrinsic and extrinsic parameters are not available. This dataset is challenging since the camera setup is very different from the training scenario on both Human3.6M and MPI-INF-3DHP. Therefore, we used KTH for zero-shot evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation protocols and metrics</head><p>Three evaluation protocols are widely used for Human-3.6M. In protocol 1, six subjects are used for training and only one is used for evaluation. Since this protocol uses a Procrustes alignment between prediction and ground truth, we do not consider it in our work. In protocol 2, five subjects (S1, S5, S6, S7, S8) are dedicated for training and S9 and S11 for evaluation, and evaluation videos are subsampled every 64th frames. The third protocol is the official test set (S2, S3, S4), of which ground truth poses are withheld by the authors and evaluation is performed over all test frames (almost 1 million images) through a server. In our experiments, we report our scores in the most challenging official test set. Additionally, we consider protocol 2 for the ablation studies and for comparison with multi-view approaches.</p><p>The standard metric for Human3.6M is the mean per joint position error (MPJPE), which measures the average joint error after centering both predictions and ground truth poses to the origin. We also evaluated our method considering the mean of the root joint position error (MRPE) <ref type="bibr" target="#b31">[32]</ref>, which measures the average error related to the absolute pose estimation. This metric is considered only for validation, since the server does not support this protocol.</p><p>For MPI-INF-3DHP, evaluation is performed on a test set composed of 6 videos/subjects, of which 2 are recorded in outdoor scenes, resulting in almost 25K frames. The authors of <ref type="bibr" target="#b27">[28]</ref> proposed three evaluation metrics: the mean per joint position error, in millimeters, the 3D Percentage of Correct Keypoints (PCK), and the Area Under the Curve (AUC) for different thresholds on PCK. The standard threshold for PCK is 150mm <ref type="bibr" target="#b27">[28]</ref>, which corresponds nearly to half of the head size. Differently from previous work <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b58">59]</ref>, we use the real 3D poses to compute the error instead of the normalized 3D poses, since the last is not compatible with a constant camera projection. Since evaluation is performed on monocular images, we use the available intrinsic camera parameters to recover absolute poses in millimeters. Finally, we also evaluated our method on KTH considering the PCP metric from <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Implementation details</head><p>During training, we use the elastic net loss (L1+L2) <ref type="bibr" target="#b61">[62]</ref> for both absolute z and relative 3D pose predictions, respectively defined by: </p><formula xml:id="formula_20">L z = 1 N s Ns i=1 z ai ?? ai 1 + z ai ?? ai</formula><formula xml:id="formula_21">L p = 1 N s Ns i=1 p i ?p i 1 + p i ?p i 2 2 ,<label>(16)</label></formula><p>where z ai and? ai are the ground truth and the estimated absolute z values, and p i andp i are the ground truth and the estimated 3D poses. The final loss is then represented by L = L z + L p . Once the first part of our network is trained, we compute the average prediction error d on training, which is used to train the confidence score network using the mean average error (MAE). RMSprop and Adam are used for optimization, respectively for the first and second training processes, starting with a learning rate of 0.001 and decreased by 0.2 after 150K and 170K iterations. Batches of 24 images are used. The full training process takes less then two days with a GTX 1080 Ti GPU. We augmented the training data with common techniques, such as random rotations (?30 ? ), rescaling (from 0.7 to 1.3), horizontal flipping, color gains (from 0.9 to 1.1), and artificial occlusions with rectangular black boxes. We also added some randomness in the cropped bounding boxes, on both position and size, in order to make the model more robust against variations in human detection. Additionally, we augmented the training data in a 50/50 ratio with 2D images from MPII <ref type="bibr" target="#b2">[3]</ref>, which becomes an standard data augmentation technique for 3D human pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison with the state of the art</head><p>Human3.6M. In <ref type="table" target="#tab_0">Table 1</ref>, we show our results on the test set from Human3.6M. We provide results of our method considering monocular predictions and multi-view predictions, for estimated and ground truth camera calibration. In all the cases our method obtains state-of-the-art results by a fair merging, reducing the prediction error by more than 10% in monocular scenario. In the multi-view setup, our method achieves 39mm error, reducing errors by more than 32% on average. In the most challenging activity (Sitting Down), our method performs better than all previous approaches reporting results in the official test set. These results demonstrate the effectiveness of our method, considering that the test set from Human3.6M is very challenging and labels are withheld by the authors.</p><p>For a fairer comparison, we also consider results only from multi-view approaches in <ref type="table" target="#tab_2">Table 2</ref>. We present our scores considering ground truth and estimated camera calibration, while all previous methods use the available calibration from the dataset. Still, our method obtains 36.9mm error, which is a strong results, specially considering that the methods from <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b49">50]</ref> require multi-view training with a known calibration setup, while our network is trained with monocular images. In this comparison, we are not considering methods that make use of the ground truth absolute position of the root joint, since in our method we estimate this information. MPI-INF-3DHP. Our results on MPI-INF-3DHP are shown in <ref type="table">Table 3</ref>. We do not report results considering multiple views in this dataset, since the testing samples were captured by a single camera. Contrarily to what is more common in this dataset, we evaluated our method using non-normalized 3D poses, otherwise it will not be possible to perform the inverse camera projection. Nevertheless, our method achieves results comparable to the state of the art, even considering other methods using normalized 3D poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Qualitative results</head><p>In <ref type="figure" target="#fig_4">Fig. 3</ref> we present some qualitative results of predicted absolute 3D poses by our method. Not that the distance from predictions to the images are proportional to the absolute distance in z. In <ref type="figure" target="#fig_7">Fig. 7</ref> we show monocular predictions by our method on the MPI-INF-3DHP dataset, including challenging outdoor scenes, which are not present in the training set. Finally, in <ref type="figure" target="#fig_8">Fig. 8</ref>, we show the results from our consensus-based optimization approach, from multiview predictions on Human3.6M. Finally, in <ref type="figure" target="#fig_9">Fig. 9</ref>, we show some generalization results from our method trained on Human3.6, considering predictions on challenging images from Penn Action dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Ablation studies</head><p>In this part, we present additional experiments to provide insights about our method and our design choices. Network architecture. We evaluated three different network architectures as presented in <ref type="table">Table 4</ref>. An off-the-shelf ResNet performed 62.2mm and 53.7mm, respectively when cut at blocks 4 and 5. The proposed ResNet-U improves on ResNet block 5 by 3.2mm while requiring 2.7M less parameters. Absolute depth estimation. In <ref type="table">Table 5</ref>, we evaluate the influence of visual features and bounding box position for the absolute depth estimation, considering the mean root position error in mm (MRPE). As can be observed, using only bounding box features is insufficient to precisely predict the absolute? a , but when combined with visual features it further improves by 20mm, which evidences the need of a global bounding box information for that task. The effect of multiple camera views. Since the proposed method predicts 3D poses in absolute camera coordinates and is also capable of estimating the extrinsic camera parameters, we can use multiple cameras to predict the same pose at inference time. When considering multi-view scenarios, we can either use camera calibration, when provided, or we can use our consensus-based optimization algorithm.</p><p>In <ref type="table">Table 6</ref> we present our results considering both 3D pose estimation and absolute root position errors, as well as estimated and ground truth camera parameters. We use multiple combinations of cameras, in order to show the influence of different number of views. As we can see, each camera lowers the error by about 5mm, which is significant on Human3.6M. We can also notice that our consensus optimization approach is capable of providing highly precise estimation, even under uncalibrated conditions.</p><p>Zero-shot on new camera setup. We evaluated our method in a new camera setup considering a zero-shot sce-   nario, where we used our model trained on Human3.6M and and evaluated it on KTH. Due to the high disparity in the camera intrinsics between both datasets, the absolute depth predictions stayed in the range observed in Human3.6M, with an average of 4.040 meters. The consensus-based algorithm still converged in the multi-view scenario, even though the final 3D poses are shifted to a smaller size due to the absolute depth predicted by our method (see <ref type="figure">Fig. 5</ref>). To correct the scale and shift, we rescaled the predicted 3D poses using the torso size from KTH (the length from the neck to the hip center) and shifted our predictions to the KTH poses in the hip center. After this, we computed the PCP metrics of our predictions, which results in 0.812 and .929 for lower and upper legs, and in 0.620 and 0.804 for lower and upper arms. Additional qualitative results on KTH are shown in <ref type="figure" target="#fig_6">Fig. 6</ref>, where the final 3D estimations are also projected to the source images.</p><p>Finally, <ref type="figure" target="#fig_5">Fig. 4</ref> shows an example of highly occluded body parts where multiple camera predictions results in a significantly better reconstruction. Note that in this case we are projecting the estimated absolute 3D pose to a new point of view, not used during inference. Despite the highly occluded joints in some views, the resulting absolute pose is Torso size on H36M Torso size on K TH <ref type="figure">Figure 5</ref>. Torso size in mm of our estimated 3D poses on Hu-man3.6M and on KTH, considering a zero-shot scenario.</p><p>very coherent and has a reduced shift when our consensusbased algorithm is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Discussion</head><p>In the proposed approach, we have the advantage of predicting 3D human poses in absolute coordinates, which enables performing multi-view estimations while training the neural network model with monocular images. This as- pect allows our method to be easily adapted to a variable number of cameras, without any additional training cost, as demonstrated in <ref type="table">Table 6</ref>. On the other hand, our method requires absolute 3D poses during training, which is a limiting factor, specially for training on datasets that provides only normalised human poses. The trained model can also be limited by the low variability of camera intrinsics during training, which may result in shift and scale deviations during inference on different cameras. As a future work, the proposed consensus-based optimization could be further integrated in the training pipeline, in order to allow a training process based on multiple views of the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we have proposed a new method for the problem of predicting 3D human poses in absolute coordinates and a new algorithm for multi-view predictions optimization. We show that, by casting the problem into a new perspective, we can benefit from training with 2D and 3D data indistinguishably, while performing 3D predictions in a more effective way. These improvements boost monocular 3D pose estimation significantly. As another consequence of the absolute prediction, we show that multi-view   estimations can be easily performed from multiple absolute monocular estimations, resulting in much higher precision than previous methods in the literature, even when considering multiple uncalibrated images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Absolute 3D human pose estimated from a single image (top-left) with occlusion and projected into a different view (topright). Our multi-view consensus-based approach (bottom) results in a more precise absolute pose estimation and effectively handles cases of occlusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Proposed ResNet-U architecture. A given input image and the corresponding bounding box parameters are fed into a neural network that predicts absolute depth?a, human posep, and confidence scores?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 2 ,</head><label>2</label><figDesc>and (15)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Absolute 3D pose predictions from monocular single images by our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>On top, the absolute prediction from camera 1 is projected into camera 2 with considerable errors in occluded joints. At the bottom, predictions from cameras 1, 3, 4 are projected into camera 2 and merged, improving the prediction significantly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative results on KTH: projections of estimated 3D poses by our model trained on H36M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>3D pose predictions from monocular single images on MPI-INF-3DHP dataset, including indoor and outdoor scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>3D pose predictions from our consensus-based optimization algorithm, considering multi-view on Human3.6M. Final 3D poses are projected into the different views (a,b,c,d) and shown in perspective (e,f).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Generalization of our method for 3D pose estimation on unseen dataset (PennAction), including outdoor scenes in different contexts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1</head><label>1</label><figDesc>Camera parameters optimization. Require:p c1 ,p c2 , M axIter 1: Initialize f x1 , f y 1 , C x1 , C y 1 , f x2 , f y 2 , C x2 , C y 2 2: Computex c1 andx c2 from Equation 3 3: Initialize T 2,1 using Equation 8 (assume R 2,1 = I 3 )</figDesc><table><row><cell cols="2">4: Iter ? 0</cell></row><row><cell cols="2">5: repeat</cell></row><row><cell>6:</cell><cell>Update R 2,1 using rigid Procrustes alignment</cell></row><row><cell>7:</cell><cell>Update T 2,1 using Equation 8</cell></row><row><cell>8:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison with results from related methods on Human3.6M test set using MPJPE (millimeters error) evaluation. Training data: Human3.6M and MPII.</figDesc><table><row><cell>Methods</cell><cell cols="4">Directions Discussion Eating Greeting</cell><cell>Phoning</cell><cell>Posing</cell><cell>Purchases</cell><cell>Sitting</cell></row><row><cell>Ionescu et al. [16]</cell><cell>152</cell><cell>153</cell><cell>125</cell><cell>171</cell><cell>135</cell><cell>180</cell><cell>162</cell><cell>168</cell></row><row><cell>Popa et al. [39]</cell><cell>60</cell><cell>56</cell><cell>68</cell><cell>64</cell><cell>78</cell><cell>67</cell><cell>68</cell><cell>106</cell></row><row><cell>Zanfir et al. [56]</cell><cell>54</cell><cell>54</cell><cell>63</cell><cell>59</cell><cell>72</cell><cell>61</cell><cell>68</cell><cell>101</cell></row><row><cell>Zanfir et al. [57]</cell><cell>49</cell><cell>47</cell><cell>51</cell><cell>52</cell><cell>60</cell><cell>56</cell><cell>56</cell><cell>82</cell></row><row><cell>Shi et al. [45]</cell><cell>51</cell><cell>50</cell><cell>54</cell><cell>54</cell><cell>62</cell><cell>57</cell><cell>54</cell><cell>72</cell></row><row><cell>Ours monocular</cell><cell>42</cell><cell>44</cell><cell>52</cell><cell>47</cell><cell>54</cell><cell>48</cell><cell>49</cell><cell>66</cell></row><row><cell>Ours multi-view est. calib.</cell><cell>40</cell><cell>36</cell><cell>44</cell><cell>39</cell><cell>44</cell><cell>42</cell><cell>41</cell><cell>66</cell></row><row><cell>Ours multi-view GT calib.</cell><cell>31</cell><cell>33</cell><cell>41</cell><cell>34</cell><cell>41</cell><cell>37</cell><cell>37</cell><cell>51</cell></row><row><cell>Methods</cell><cell>Sit. Down</cell><cell>Smoking</cell><cell>Photo</cell><cell>Waiting</cell><cell cols="2">Walking Walk.Dog</cell><cell>Walk.Pair</cell><cell>Average</cell></row><row><cell>Popa et al. [39]</cell><cell>119</cell><cell>77</cell><cell>85</cell><cell>64</cell><cell>57</cell><cell>78</cell><cell>62</cell><cell>73</cell></row><row><cell>Zanfir et al. [56]</cell><cell>109</cell><cell>74</cell><cell>81</cell><cell>62</cell><cell>55</cell><cell>75</cell><cell>60</cell><cell>69</cell></row><row><cell>Zanfir et al. [57]</cell><cell>94</cell><cell>64</cell><cell>69</cell><cell>61</cell><cell>48</cell><cell>66</cell><cell>49</cell><cell>60</cell></row><row><cell>Shi et al. [45]</cell><cell>76</cell><cell>62</cell><cell>65</cell><cell>59</cell><cell>49</cell><cell>61</cell><cell>54</cell><cell>58</cell></row><row><cell>Ours monocular</cell><cell>76</cell><cell>54</cell><cell>61</cell><cell>47</cell><cell>44</cell><cell>55</cell><cell>44</cell><cell>52</cell></row><row><cell>Ours multi-view est. calib.</cell><cell>70</cell><cell>46</cell><cell>49</cell><cell>43</cell><cell>34</cell><cell>46</cell><cell>34</cell><cell>45</cell></row><row><cell>Ours multi-view GT calib.</cell><cell>56</cell><cell>43</cell><cell>44</cell><cell>37</cell><cell>33</cell><cell>42</cell><cell>32</cell><cell>39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison with related multi-view methods on Human3.6M validation set, protocol 2. We report our scores in mm error (MPJPE), considering ground truth and estimated camera calibration. Note that all previous methods use ground truth camera calibration. Training data: Human3.6M and MPII.</figDesc><table><row><cell>Methods</cell><cell>Cam. calib.</cell><cell>Dir.</cell><cell cols="3">Discussion Eating Greeting</cell><cell>Phoning</cell><cell>Posing</cell><cell>Purchases</cell><cell>Sit.</cell></row><row><cell>PVH-TSP [52]</cell><cell>GT</cell><cell>92.7</cell><cell>85.9</cell><cell>72.3</cell><cell>93.2</cell><cell>86.2</cell><cell>101.2</cell><cell>75.1</cell><cell>78.0</cell></row><row><cell>Trumble et al. [51]</cell><cell>GT</cell><cell>41.7</cell><cell>43.2</cell><cell>52.9</cell><cell>70.0</cell><cell>64.9</cell><cell>83.0</cell><cell>57.3</cell><cell>63.5</cell></row><row><cell>Pavlakos et al. [38]</cell><cell>GT</cell><cell>41.1</cell><cell>49.1</cell><cell>42.7</cell><cell>43.4</cell><cell>55.6</cell><cell>46.9</cell><cell>40.3</cell><cell>63.6</cell></row><row><cell>Tome et al. [50]</cell><cell>GT</cell><cell>43.3</cell><cell>49.6</cell><cell>42.0</cell><cell>48.8</cell><cell>51.1</cell><cell>40.3</cell><cell>43.3</cell><cell>66.0</cell></row><row><cell>Kadkho. et al. [18]</cell><cell>GT</cell><cell>39.4</cell><cell>46.9</cell><cell>41.0</cell><cell>42.7</cell><cell>53.6</cell><cell>41.4</cell><cell>50.0</cell><cell>59.9</cell></row><row><cell>Iskakov et al. [17]</cell><cell>GT</cell><cell>19.9</cell><cell>20.0</cell><cell>18.9</cell><cell>18.5</cell><cell>20.5</cell><cell>18.4</cell><cell>22.1</cell><cell>22.5</cell></row><row><cell>Ours</cell><cell>Estimated</cell><cell>59.3</cell><cell>40.7</cell><cell>38.7</cell><cell>39.1</cell><cell>41.7</cell><cell>39.5</cell><cell>40.6</cell><cell>64.1</cell></row><row><cell>Ours</cell><cell>GT</cell><cell>31.0</cell><cell>33.7</cell><cell>33.8</cell><cell>33.4</cell><cell>38.6</cell><cell>32.2</cell><cell>36.3</cell><cell>48.2</cell></row><row><cell>Methods</cell><cell cols="2">Cam. calib. Sit. D.</cell><cell>Smoking</cell><cell>Photo</cell><cell>Waiting</cell><cell cols="2">Walking Walk.Dog</cell><cell>Walk.Pair</cell><cell>Avg</cell></row><row><cell>PVH-TSP [52]</cell><cell>GT</cell><cell>83.5</cell><cell>94.8</cell><cell>85.8</cell><cell>82.0</cell><cell>114.6</cell><cell>94.9</cell><cell>79.7</cell><cell>87.3</cell></row><row><cell>Trumble et al. [51]</cell><cell>GT</cell><cell>61.0</cell><cell>95.0</cell><cell>70.0</cell><cell>62.3</cell><cell>66.2</cell><cell>53.7</cell><cell>52.4</cell><cell>62.5</cell></row><row><cell>Pavlakos et al. [38]</cell><cell>GT</cell><cell>97.5</cell><cell>119.9</cell><cell>52.1</cell><cell>42.6</cell><cell>51.9</cell><cell>41.7</cell><cell>39.3</cell><cell>56.8</cell></row><row><cell>Tome et al. [50]</cell><cell>GT</cell><cell>95.2</cell><cell>50.2</cell><cell>64.3</cell><cell>52.2</cell><cell>43.9</cell><cell>51.1</cell><cell>45.3</cell><cell>52.8</cell></row><row><cell>Kadkho. et al. [18]</cell><cell>GT</cell><cell>78.8</cell><cell>49.8</cell><cell>54.8</cell><cell>46.2</cell><cell>51.1</cell><cell>40.5</cell><cell>41.0</cell><cell>49.1</cell></row><row><cell>Iskakov et al. [17]</cell><cell>GT</cell><cell>28.7</cell><cell>21.2</cell><cell>19.4</cell><cell>20.8</cell><cell>22.1</cell><cell>19.7</cell><cell>20.2</cell><cell>20.8</cell></row><row><cell>Ours</cell><cell>Estimated</cell><cell>69.5</cell><cell>42.0</cell><cell>44.6</cell><cell>39.6</cell><cell>31.0</cell><cell>40.2</cell><cell>35.3</cell><cell>44.7</cell></row><row><cell>Ours</cell><cell>GT</cell><cell>51.5</cell><cell>39.2</cell><cell>38.8</cell><cell>32.4</cell><cell>29.6</cell><cell>38.9</cell><cell>33.2</cell><cell>36.9</cell></row><row><cell>Cam1</cell><cell></cell><cell></cell><cell>Cam2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>X</cell><cell>?750 ?500 ?250</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cam3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cam4</cell><cell></cell><cell></cell><cell>Cam2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .Table 4 .Table 6 .</head><label>346</label><figDesc>Results on MPI-INF-3DHP compared to the state of the art. Training data: MPI-INF-3DHP, Human3.6M, and MPII Methods using normalized 3D human poses for evaluation. Evaluation of the network architecture, considering the backbone only (ResNet) cut at block 4 and block 5, and the refinement network (ResNet-U). Results of our method on 3D human pose estimation and on root joint absolute error (MPJPE / MRPE) considering single and multi-view with different camera combinations.</figDesc><table><row><cell cols="2">Method</cell><cell></cell><cell cols="2">Stand Exercise PCK PCK</cell><cell cols="2">Sit PCK</cell><cell cols="5">Crouch On the Floor Sports Misc. PCK PCK PCK PCK PCK AUC MPJPE Total</cell></row><row><cell cols="2">Rogez et al. [43]</cell><cell></cell><cell>70.5</cell><cell>56.3</cell><cell>58.5</cell><cell></cell><cell>69.4</cell><cell>39.6</cell><cell>57.7</cell><cell>57.6</cell><cell>59.7</cell><cell>27.6</cell><cell>158.4</cell></row><row><cell cols="2">Zhou et al. [59]</cell><cell></cell><cell>85.4</cell><cell>71.0</cell><cell>60.7</cell><cell></cell><cell>71.4</cell><cell>37.8</cell><cell>70.9</cell><cell>74.4</cell><cell>69.2</cell><cell>32.5</cell><cell>137.1</cell></row><row><cell cols="2">Mehta et al. [28]</cell><cell></cell><cell>86.6</cell><cell>75.3</cell><cell>74.8</cell><cell></cell><cell>73.7</cell><cell>52.2</cell><cell>82.1</cell><cell>77.5</cell><cell>75.7</cell><cell>39.3</cell><cell>117.6</cell></row><row><cell cols="3">Kocabas et al. [20]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>77.5</cell><cell>-</cell><cell>108.99</cell></row><row><cell cols="3">Kolotouros et al. [21]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>76.4</cell><cell>37.1</cell><cell>105.2</cell></row><row><cell cols="2">Ours monocular</cell><cell></cell><cell>83.8</cell><cell>79.6</cell><cell>79.4</cell><cell></cell><cell>78.2</cell><cell>73.0</cell><cell>88.5</cell><cell>81.6</cell><cell>80.6</cell><cell>42.1</cell><cell>112.1</cell></row><row><cell cols="2">ResNet block 4</cell><cell cols="2">ResNet block 5</cell><cell cols="3">ResNet-U</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MPJPE</cell><cell>#Par.</cell><cell cols="4">MPJPE #Par. MPJPE</cell><cell cols="2">#Par.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>62.2</cell><cell>10.5M</cell><cell>53.7</cell><cell>26M</cell><cell>50.5</cell><cell cols="3">23.3M</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Table 5. Absolute root joint position error in mm based on differ-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ent features combinations.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Features Bounding box CNN features Combined</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MRPE</cell><cell cols="2">375.4</cell><cell cols="2">100.1</cell><cell cols="2">80.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell cols="6">GT camera MPJPE MRPE MPJPE Estimated camera MRPE</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Monocular</cell><cell>50.5</cell><cell>80.1</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Monocular + h.flip</cell><cell>49.2</cell><cell>79.9</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Cameras 1,2</cell><cell>45.7</cell><cell>73.3</cell><cell>52.2</cell><cell cols="2">167.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Cameras 1,4</cell><cell>46.2</cell><cell>74.9</cell><cell>59.0</cell><cell cols="2">171.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Cameras 1,2,3</cell><cell>41.8</cell><cell>57.4</cell><cell>47.9</cell><cell cols="2">143.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Cameras 1,2,3,4</cell><cell>36.9</cell><cell>51.0</cell><cell>44.7</cell><cell cols="2">130.7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>200</cell><cell>300</cell><cell>400</cell><cell>500</cell><cell>600</cell><cell cols="2">700</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recovering 3D human pose from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="58" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiview pictorial structures for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2D Human Pose Estimation: New Benchmark and State of the Art Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3D pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3d pictorial structures revisited: Multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bernt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1929" to="1942" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3D pictorial structures for multiple view articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3618" to="3625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3d pictorial structures for multiple view articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3618" to="3625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3D Human Pose Estimation = 2D Pose Estimation + Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Single-Image Depth Perception in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How Do Neural Networks See Depth in Single Images?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Dijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Croon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth Map Prediction from a Single Image using a Multi-Scale Deep Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">What face and body shapes can tell us about height</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gunel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-view 3D Human Pose Estimation in Complex Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="124" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Latent structured models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="2220" to="2227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hu-man3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learnable triangulation of human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Malkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A generalizable approach for multi-view 3d human pose regression. Machine Vision and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadkhodamohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multiview body part recognition with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th British Machine Vision Conference, BMVC 2013; Bristol; United Kingdom; 9 September 2013 through 13 September 2013. British Machine Vision Association</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-supervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via modelfitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Propagating LSTM: 3D Pose Estimation based on Joint Interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Maximum-Margin Structured Learning With Deep Networks for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">2D/3D Pose Estimation and Action Recognition Using Multitask Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Human pose regression by combining indirect part detection and contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="15" to="22" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Monocular 3D Human Pose Estimation In The Wild Using Improved CNN Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">XNect: Real-time multi-person 3D motion capture with a single RGB camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simultaneous surveillance camera calibration and foot-head homology estimation from human detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Micusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Camera Distance-Aware Top-Down Approach for 3D Multi-Person Pose Estimation From a Single RGB Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Indoor Segmentation and Support Inference from RGBD Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Nathan Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stacked Hourglass Networks for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Monocular 3D human pose estimation by predicting depth on joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3467" to="3475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multiview 3D human pose estimation using improved least-squares and LSTM networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>N??ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cabido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>V?lez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Montemayor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Pantrigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="335" to="343" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Coarse-to-Fine Volumetric Prediction for Single-Image 3D Human Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Harvesting Multiple Views for Marker-less 3D Human Pose Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep Multitask Architecture for Integrated 2D and 3D Human Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4714" to="4723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rayat Imtiaz Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">YOLO9000: Better, Faster, Stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>2017. 4</idno>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning monocular 3d human pose estimation from multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sp?rri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8437" to="8446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">LCR-Net: Localization-Classification-Regression for Human Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">3D Human pose estimation: A review of the literature and analysis of covariates. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boteanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Fbi-pose: Towards bridging the gap between 2d images and 3d human poses using forward-or-backward information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Compositional Human Pose Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Integral Human Pose Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Structured Prediction of 3D Human Pose with Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Lifting From the Deep: Convolutional 3D Pose Estimation From a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Rethinking pose in 3d: Multi-stage refinement and recovery for markerless motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep autoencoder for combined human pose estimation and body model upscaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="784" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Total Capture: 3D Human Pose Estimation Fusing Video and Inertial Sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">DIODE: A Dense Indoor and Outdoor DEpth Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vasiljevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Daniele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<idno>abs/1908.00463</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">3D Human Pose Estimation in the Wild by Adversarial Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<title level="m">LIFT: Learned Invariant Feature Transform. The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Monocular 3D Pose and Shape Estimation of Multiple People in Natural Scenes -The Importance of Multiple Scene Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep Network for the Integrated 3D Sensing of Multiple People in Natural Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-I</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8410" to="8419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">From actemes to action: A strongly-supervised representation for detailed action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2248" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Towards 3D Human Pose Estimation in the Wild: A Weakly-Supervised Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep Kinematic Pose Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Workshops</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Sparseness Meets Deepness: 3D Human Pose Estimation From Monocular Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the Elastic Net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="301" to="320" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
