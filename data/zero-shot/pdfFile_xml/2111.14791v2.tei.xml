<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Pre-Training of Swin Transformers for 3D Medical Image Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Tangv</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bennett</forename><surname>Landman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daguang</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishwesh</forename><surname>Nath</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hatamizadeh</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">anderbilt University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Vanderbilt University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Pre-Training of Swin Transformers for 3D Medical Image Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision Transformers (ViT)s have shown great performance in self-supervised learning of global and local representations that can be transferred to downstream applications. Inspired by these results, we introduce a novel self-supervised learning framework with tailored proxy tasks for medical image analysis. Specifically, we propose: (i) a new 3D transformer-based model, dubbed Swin UNEt TRansformers (Swin UNETR), with a hierarchical encoder for self-supervised pre-training; (ii) tailored proxy tasks for learning the underlying pattern of human anatomy. We demonstrate successful pre-training of the proposed model on 5,050 publicly available computed tomography (CT) images from various body organs. The effectiveness of our approach is validated by fine-tuning the pre-trained models on the Beyond the Cranial Vault (BTCV) Segmentation Challenge with 13 abdominal organs and segmentation tasks from the Medical Segmentation Decathlon (MSD) dataset. Our model is currently the state-of-the-art on the public test leaderboards of both MSD 1 and BTCV 2 datasets. Code: https://monai.io/research/swin-unetr.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Vision Transformers (ViT)s <ref type="bibr" target="#b19">[20]</ref> have started a revolutionary trend in computer vision <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b62">62]</ref> and medical image analysis <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24]</ref>. Transformers demonstrate exceptional capability in learning pre-text tasks, are effective in learning of global and local information across layers, and provide </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Supervised Heads</head><p>Sub-Volume Input CT <ref type="figure">Figure 1</ref>. Overview of our proposed pre-training framework. Input CT images are randomly cropped into sub-volumes and augmented with random inner cutout and rotation, then fed to the Swin UNETR encoder as input. We use masked volume inpainting, contrastive learning and rotation prediction as proxy tasks for learning contextual representations of input images. scalability for large-scale training <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b60">60]</ref>. As opposed to convolutional neural networks (CNNs) with limited receptive fields, ViTs encode visual representations from a sequence of patches and leverage self-attention blocks for modeling long-range global information <ref type="bibr" target="#b43">[44]</ref>. Recently, Shifted windows (Swin) Transformers <ref type="bibr" target="#b35">[36]</ref> proposed a hierarchical ViT that allows for local computing of self-attention with non-overlapping windows. This architecture achieves linear complexity as opposed to quadratic complexity of self-attention layers in ViT, hence making it more efficient. In addition, due to the hierarchical nature of Swin Transformers, they are well-suited for tasks requiring multi-scale modeling.</p><p>In comparison to CNN-based counterparts, transformerbased models learn stronger features representations during pre-training, and as a result perform favorably on fine-tuning downstream tasks <ref type="bibr" target="#b43">[44]</ref>. Several recent efforts on ViTs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b56">56]</ref> have achieved new state-of-the-art results by self-supervised pre-training on large-scale datasets such as ImageNet <ref type="bibr" target="#b16">[17]</ref>.</p><p>In addition, medical image analysis has not benefited from these advances in general computer vision due to: (1) large domain gap between natural images and medical imaging modalities, like computed tomography (CT) and magnetic resonance imaging (MRI); (2) lack of cross-plane contextual information when applied to volumetric (3D) images (such as CT or MRI). The latter is a limitation of 2D transformer models for various medical imaging tasks such as segmentation. Prior studies have demonstrated the effectiveness of supervised pre-training in medical imaging for different applications <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b44">45]</ref>. But creating expert-annotated 3D medical datasets at scale is a non-trivial and time-consuming effort.</p><p>To tackle these limitations, we propose a novel selfsupervised learning framework for 3D medical image analysis. First, we propose a new architecture dubbed Swin UNEt TRansformers (Swin UNETR) with a Swin Transformer encoder that directly utilizes 3D input patches. Subsequently, the transformer encoder is pre-trained with tailored, self-supervised tasks by leveraging various proxy tasks such as image inpainting, 3D rotation prediction, and contrastive learning (See <ref type="figure">Fig. 1</ref> for an overview). Specifically, the human body presents naturally consistent contextual information in radiographic images such as CT due to its depicted anatomical structure <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b58">58]</ref>. Hence, proxy tasks are utilized for learning the underlying patterns of the human anatomy. For this purpose, we extracted numerous patch queries from different body compositions such as head, neck, lung, abdomen, and pelvis to learn robust feature representations from various anatomical contexts, organs, tissues, and shapes.</p><p>Our framework utilizes contrastive learning <ref type="bibr" target="#b40">[41]</ref>, masked volume inpainting <ref type="bibr" target="#b42">[43]</ref>, and 3D rotation prediction <ref type="bibr" target="#b20">[21]</ref> as pre-training proxy tasks. The contrastive learning is used to differentiate various ROIs of different body compositions, whereas the inpainting allows for learning the texture, structure and correspondence of masked regions to their surrounding context. The rotation task serves as a mechanism to learn the structural content of images and generates various sub-volumes that can be used for contrastive learning. We utilize these proxy tasks to pre-train our proposed framework on a collection of 5,050 CT images that are acquired from various publicly available datasets.</p><p>Furthermore, to validate the effectiveness of pre-training, we use 3D medical image segmentation as a downstream application and reformulate it as a 1D sequence-to-sequence prediction task. For this purpose, we leverage the Swin UNETR encoder with hierarchical feature encoding and shifted windows to extract feature representations at four different resolutions. The extracted representations are then connected to a CNN-based decoder. A segmentation head is attached at the end of the decoder for computing the final segmentation output. We fine-tune Swin UNETR with pre-trained weights on two publicly available benchmarks of Medical Segmentation Decathlon (MSD) and the Beyond the Cranial Vault (BTCV). Our model is currently the state-of-the-art on their respective public test leaderboards. Our main contributions in this work are summarized as follows:</p><p>? We introduce a novel self-supervised learning framework with tailored proxy tasks for pre-training on CT image datasets. To this end, we propose a novel 3D transformer-based architecture, dubbed as Swin UNETR, consisting of an encoder that extracts feature representations at multiple resolutions and is utilized for pre-training.</p><p>? We demonstrate successful pre-training on a cohort of 5,050 publicly available CT images from various applications using the proposed encoder and proxy tasks. This results in a powerful pre-trained model with robust feature representation that could be utilized for various medical image analysis downstream tasks.</p><p>? We validate the effectiveness of proposed framework by fine-tuning the pre-trained Swin UNETR on two public benchmarks of MSD and BTCV and achieve state-of-the-art on the test leaderboards of both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Medical Segmentation with Transformers Vision transformers are first used in classification tasks and are adopted from sequence-to-sequence modeling in natural language processing. Self-attention mechanisms that aggregate information from the entire input sequence are first achieving comparable, then better performance against prior arts of convolutional architectures such as ResNet <ref type="bibr" target="#b25">[26]</ref> or U-Net <ref type="bibr" target="#b14">[15]</ref>. Recently, transformer-based networks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b63">63]</ref> are proposed for medical image segmentation. In these pioneering works, the transformer blocks are used either as a bottleneck feature encoder or as additional modules after convolutional layers, resulting in limited exploitation of the spatial context advantages of transformers. Comparing to prior works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b55">55]</ref>, which are using transformers as secondary encoder, we propose to utilize transformers to embed high-dimensional volumetric medical images, which allow for a more direct encoding of 3D patches and positional embeddings. </p><formula xml:id="formula_0">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 48 ? Input ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 48</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Swin Transformer Block</head><p>Linear Embed</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Patch Partition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Res-Block</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Res-Block Res-Block</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Res-Block Res-Block</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Res-Block Res-Block</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Res-Block Res-Block</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Res-Block</head><p>Res-Block</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Res-Block</head><p>Residual Block <ref type="figure">Figure 2</ref>. Overview of the Swin UNETR architecture.</p><p>Most medical image analysis tasks such as segmentation requires dense inference from multi-scale features. Skip connection-based architectures such as UNet <ref type="bibr" target="#b14">[15]</ref> and pyramid networks <ref type="bibr" target="#b45">[46]</ref> are widely adopted to leverage hierarchical features. However, vision transformers with a single patch size, while successful in natural image applications, are intractable for high-resolution and high-dimensional volumetric images. To avoid quadratic overflow of computing self-attention at scales <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b48">49]</ref>, Swin Transformer <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> is proposed to construct hierarchical encoding by a shiftedwindow mechanism. Recent works such as Swin UNet <ref type="bibr" target="#b4">[5]</ref> and DS-TransUNet <ref type="bibr" target="#b33">[34]</ref> utilize the merits of Swin Transformers for 2D segmentation and achieve promising performance. Augmenting the above-mentioned methods, we learn from 3D anatomy in broader medical image segmentation scenarios by incorporating hierarchically volumetric context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training in Medical Image Analysis</head><p>In medical image analysis, previous studies of pre-training on labeled data demonstrate improved performance by transfer learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b44">45]</ref>. However, generating annotation for medical images is expensive and time-consuming. Recent advances in self-supervised learning offer the promise of utilizing unlabeled data. Self-supervised representation learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b32">33]</ref> constructs feature embedding spaces by designing pre-text tasks, such as solving jigsaw puzzles <ref type="bibr" target="#b39">[40]</ref>. Another commonly used pre-text task is to memorize spatial context from medical images, which is motivated by image restoration. This idea is generalized to inpainting tasks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b65">65]</ref> to learn visual representations <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b54">54]</ref> by predicting the original image patches. Similar efforts for reconstructing spatial context have been formulated as solving Rubik's cube problem <ref type="bibr" target="#b66">[66]</ref>, random rotation prediction <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b49">50]</ref> and contrastive coding <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b40">41]</ref>. Different from these efforts, our pre-training framework is simulta-neously trained with a combination of pre-text tasks, tailored for 3D medical imaging data, and leverages a transformerbased encoder as a powerful feature extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Swin UNETR</head><p>Swin UNETR comprises a Swin Transformer <ref type="bibr" target="#b35">[36]</ref> encoder that directly utilizes 3D patches and is connected to a CNNbased decoder via skip connections at different resolutions. <ref type="figure">Fig. 2</ref> illustrates the overall architecture of Swin UNETR. We describe the details of encoder and decoder in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Swin Transformer Encoder</head><p>Assuming that the input to the encoder is a sub-volume X P R H?W?D?S , a 3D token with a patch resolution of pH 1 ,W 1 ,D 1 q has a dimension of H 1?W 1?D1?S . The patch partitioning layer creates a sequence of 3D tokens with size H H 1?W W 1?D D 1 that are projected into a C-dimensional space via an embedding layer. Following <ref type="bibr" target="#b35">[36]</ref>, for efficient modeling of token interactions, we partition the input volumes into non-overlapping windows and compute local self-attention within each region. Specifically, at layer l, we use a window of size M?M?M to evenly divide a 3D token into</p><formula xml:id="formula_1">Q H 1 M UQ W 1 M U?Q D 1 M U</formula><p>windows. In the subsequent layer l`1, we shift the partitioned windows by`</p><formula xml:id="formula_2">X M 2 \ , X M 2 \ , X M 2 \?v</formula><p>oxels. The shifted windowing mechanism is illustrated in <ref type="figure">Fig. 3</ref>. The outputs of encoder blocks in layers l and l`1 are computed as in</p><formula xml:id="formula_3">z l " W-MSApLNpz l?1 qq`z l?1 z l " MLPpLNp? l qq`? l z l`1 " SW-MSApLNpz l qq`z l z l`1 " MLPpLNp? l`1 qq`? l`1 ,<label>(1)</label></formula><p>where W-MSA and SW-MSA denote regular and window partitioning multi-head self-attention modules, respectively, ? l and? l are the outputs of W-MSA and SW-MSA; LN and MLP denote layer normalization and Multi-Layer Perceptron (see <ref type="figure">Fig. 2</ref>). Following <ref type="bibr" target="#b35">[36]</ref>, we adopt a 3D cyclic-shifting for efficient batch computation of shifted windowing. Furthermore, we calculate the self-attention according to</p><formula xml:id="formula_4">AttentionpQ,K,V q " Softmax?Q K J ? d?V .<label>(2)</label></formula><p>where Q,K,V represent queries, keys and values respectively, d is the size of the query and key. Our encoder uses a patch size of 2?2?2 with a feature dimension of 2?2?2?1 " 8 (i.e. single input channel CT images) and a C " 48-dimensional embedding space. Furthermore, the overall architecture of the encoder consists of 4 stages comprising of 2 transformer blocks at each stage (i.e. L " 8 total layers). In between every stage, a patch merging layer is used to reduce the resolution by a factor of 2. Stage 1 consists of a linear embedding layer and transformer blocks that maintain the number of tokens as H 2?W 2?D 2 . Furthermore, a patch merging layer groups patches with resolution 2?2?2 and concatenates them, resulting in a 4C-dimensional feature embedding. A linear layer is then used to downsample the resolution by reducing the dimension to 2C. The same procedure continues in stage 2, stage 3 and stage 4 with resolutions of H 4?W 4?D 4 , H 8?W 8?D 8 and H 16?W 16?D 16 respectively. The hierarchical representations of the encoder at different stages are used in downstream applications such as segmentation for multi-scale feature extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Decoder</head><p>The encoder of Swin UNETR is connected to a CNN-based decoder at each resolution via skip connections to create a "U-shaped" network for downstream applications such as segmentation. Specifically, we extract the output sequence representations of each stage i (i P t0,1,2,3,4uq in the encoder as well as the bottleneck (i " 5) and reshape them into features with size H 2 i?W 2 i?D 2 i . The extracted representations at each stage are then fed into a residual block consisting of two postnormalized 3?3?3 convolutional layers with instance normalization <ref type="bibr" target="#b53">[53]</ref>. The processed features from each stage are then upsampled by using a deconvolutional layer and concatenated with processed features of the preceding stage. The concatenated features are fed into a residual block with aforementioned descriptions. For segmentation, we concatenate the output of the encoder (i.e. Swin Transformer) with processed features of the input volume and feed them into a residual block followed by a final 1?1?1 convolutional layer with a proper activation function (i.e. softmax ) for computing the segmentation probabilities (see <ref type="figure">Fig. 2</ref> for details of the architecture).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Pre-training</head><p>We pre-train the Swin UNETR encoder with multiple proxy tasks and formulate it with a multi-objective loss func-  <ref type="figure">Figure 3</ref>. Shifted windowing mechanism for efficient self-attention computation of 3D tokens with 8?8?8 tokens and 4?4?4 window size. tion <ref type="figure">(Fig. 1)</ref>. The objective of self-supervised representation learning is to encode region of interests (ROI)-aware information of the human body. Inspired by previous works on context reconstruction <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b65">65]</ref> and contrastive encoding <ref type="bibr" target="#b24">[25]</ref>, we exploit three proxy tasks for medical image representation learning. Three additional projection heads are attached to the encoder during pre-training. Furthermore, the downstream task, e.g. segmentation, fine-tunes the full Swin UNETR model with the projection heads removed. In training, sub-volumes are cropped random regions of the volumetric data. Then, stochastic data augmentations with random rotation and cutout are applied twice to each sub-volume within a mini-batch, resulting in two views of each data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Masked Volume Inpainting</head><p>The cutout augmentation masks out ROIs in the subvolume X P R H?W?D?C randomly with volume ratio of s. We attach a transpose convolution layer to the encoder as the reconstruction head and denote its output asX M . The reconstruction objective is defined by an L1 loss between X</p><formula xml:id="formula_5">andX M L inpaint " }X?X M } 1 ,<label>(3)</label></formula><p>The masked volume inpainting is motivated by prior work which focused on 2D images <ref type="bibr" target="#b42">[43]</ref>. We extend it to 3D domain to showcase its effectiveness on representation learning of volumetric medical images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image Rotation</head><p>The rotation prediction task predicts the angle categories by which the input sub-volume is rotated. For simplicity, we employ R classes of 0?, 90?, 180?, 270?rotations along the z-axis. An MLP classification head is used for predicting the softmax probabilities? r of rotation categories. Given the ground truth y r , a cross-entropy loss is used for rotation prediction task:</p><formula xml:id="formula_6">L rot "?R ? r"1 y r logp? r q,<label>(4)</label></formula><p>The 3D rotation and cutout also serves simultaneously as an augmentation transformation for contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Contrastive Coding</head><p>The self-supervised contrastive coding presents promising performance on visual representation learning when transferred to downstream tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b41">42]</ref>. Given a batch of augmented sub-volumes, the contrastive coding allows for a better representation learning by maximizing the mutual information between positive pairs (augmented samples from same sub-volume), while minimizing that between negative pairs (views from different sub-volumes). The contrastive coding is obtained by attaching a linear layer to the Swin UNETR encoder, which maps each augmented sub-volume to a latent representation v. We use cosine similarity as the distance measurement of the encoded representations as defined in <ref type="bibr" target="#b11">[12]</ref>. Formally, the 3D contrastive coding loss between a pair v i and v j is defined as:</p><formula xml:id="formula_7">L contrast "?log exppsimpv i ,v j q{tq ? 2N k 1 k?i exppsimpv i ,v k q{tq ,<label>(5)</label></formula><p>where t is the measurement of normalized temperature scale. 1 is the indicator function evaluating to 1 iff k ? i. sim denotes the dot product between normalized embeddings. The contrastive learning loss function strengthens the intra-class compactness as well as the inter-class separability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Loss Function</head><p>Formally, we minimize the total loss function by training Swin UNETR's encoder with multiple pre-training objectives of masked volume inpainting, 3D image rotation &amp; contrastive coding as follows:</p><formula xml:id="formula_8">L tot " ? 1 L inpaint`?2 L contrast`?3 L rot .<label>(6)</label></formula><p>A grid-search hyper-parameter optimization was performed which estimated the optimal values of ? 1 " ? 2 " ? 3 " 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>Pre-training Datasets : A total of 5 public CT datasets, consisting of 5,050 subjects, are used to construct our pre-training dataset. The corresponding number of 3D volumes for chest, abdomen and head/neck are 2,018, 1,520 and 1,223 respectively. The collection and source details are presented in the supplementary materials. Existing annotations or labels are not utilized from these datasets during the pre-training stage.</p><p>BTCV : The Beyond the Cranial Vault (BTCV) abdomen challenge dataset <ref type="bibr" target="#b31">[32]</ref> contains 30 subjects with abdominal CT scans where 13 organs are annotated by interpreters under supervision of radiologists at Vanderbilt University Medical Center. Each CT scan is acquired with contrast enhancement phase at portal venous consists of 80 to 225 slices with 512?512 pixels and slice thickness ranging from 1 to 6 mm. The multi-organ segmentation problem is formulated as a 13 classes segmentation task (see <ref type="table">Table 1</ref> for details). The preprocessing pipeline is detailed in supplementary materials.</p><p>MSD: Medical Segmentation Decathlon (MSD) dataset <ref type="bibr" target="#b0">[1]</ref> comprises of 10 segmentation tasks from different organs and image modalities. These tasks are designed to feature difficulties across medical images, such as small training sets, unbalanced classes, multi-modality data and small objects. Therefore, the MSD challenge can serve as a comprehensive benchmark to evaluate the generalizability of medical image segmentation methods. The pre-processing pipeline for this dataset is outlined in supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>For pre-training tasks, (1) masked volume inpainting: the ROI dropping rate is set to 30% (as also used in <ref type="bibr" target="#b2">[3]</ref>); the dropped regions are randomly generated and they sum up to reach overall number of voxels; (2) 3D contrastive coding: a feature size of 512 is used as the embedding size; (3) rotation prediction: the rotation degree is configured to 0?, 90?, 180?, and 270?. We train the model using the AdamW <ref type="bibr" target="#b37">[38]</ref> optimizer with a warm-up cosine scheduler of 500 iterations. The pre-training experiments use a batch-size of 4 per GPU (with 96?96?96 patch), and initial learning rate of 4e?4, momentum of 0.9 and decay of 1e?5 for 450K iterations. Our model is implemented in PyTorch and MONAI <ref type="bibr" target="#b3">4</ref> . A five-fold cross validation strategy is used to train models for all BTCV and MSD experiments. We select the best model in each fold and ensemble their outputs for final segmentation predictions. Detailed training hyperparameters for fine-tuning BTCV and MSD tasks can be found in the supplementary materials. All models are trained on a NVIDIA DGX-1 server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluation Metrics</head><p>The Dice similarity coefficient (Dice) and Hausdorff Distance 95% (HD95) are used as measurements for experiment results. HD95 calculates 95 th percentile of surface distances between ground truth and prediction point sets. Metric formulations are as follows:</p><formula xml:id="formula_9">Dice " 2 ? I i"1 Y i?i ? I i"1 Y i`? I i"1? i ,<label>(7)</label></formula><p>HD " maxtmax y 1 PY 1 min</p><formula xml:id="formula_10">y 1 P? 1 }y 1??1 },max y 1 P? 1 min y 1 PY 1 }? 1?y1 }u. (8)</formula><p>where Y and? denote the ground truth and prediction of voxel values. Y 1 and? 1 denote ground truth and prediction  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Rank</head><p>Average Accuracy </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">BTCV Multi-organ Segmentation Challenge</head><p>We extensively compare the benchmarks of our model with baselines. The published leaderboard evaluation is shown in <ref type="table">Table 1</ref>. Compared with other top submissions, the proposed Swin UNETR achieves the best performance. We obtain the state-of-the-art Dice of 0.908, outperforming the second, third and fourth top-ranked baselines by 1.6%, 2.0% and 2.4% on average of 13 organs, respectively. Distinct improvements can be specifically observed for organs that are smaller in size, such as splenic and portal veins of 3.6% against prior stateof-the-art method, pancreas of 1.6%, and adrenal glands of 3.8%. Moderate improvements are observed in other organs. The representative samples in <ref type="figure" target="#fig_2">Fig. 4</ref> demonstrate the success of identifying organ details by Swin UNETR. Our method detects the pancreas tail (row 1), and branches in the portal vein (row 2) in <ref type="figure" target="#fig_2">Fig. 4</ref>, where other methods under segment parts of each tissue. In addition, our method demonstrates distinct improvement in segmentation of adrenal glands (row 3).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Segmentation Results on MSD</head><p>The overall MSD results per task and ranking from the challenge leaderboard are shown in <ref type="table">Table.</ref>   <ref type="table">Table 4</ref>. Ablation study of the effectiveness of each objective function in the proposed pre-training loss. HD denotes Hausdorff Distance. Experiments on fine-tuning the BTCV dataset.   labeled data, experiments with pre-training weights achieve approximately 10% improvement comparing to training from scratch. On employing all labeled data, the self-supervised pre-training shows 1.3% higher average Dice. The Dice number 83.13 of learning from scratch with entire dataset can be achieved by using pre-trained Swin UNETR with 60% data. <ref type="figure" target="#fig_3">Fig. 7</ref> indicates that our approach can reduce the annotation effort by at least 40% for BTCV task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Reduce Manual Labeling Efforts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.3">Size of Pre-training Dataset</head><p>We perform organ-wise study on BTCV dataset by using pre-trained weights of smaller unlabeled data. In <ref type="figure">Fig. 8</ref>, the fine-tuning results are obtained from pre-training 100, 3,000, and 5,000 scans. We observe that Swin UNETR is robust with respect to the total number of CT scans trained. <ref type="figure">Fig. 8</ref> demonstrates the proposed model can benefit from larger pre-training datasets with increasing size of unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.4">Efficacy of Self-Supervised Objectives</head><p>We perform empirical study on pre-training with different combinations of self-supervised objectives. As shown in Table 4, on BTCV test set, using pre-trained weights by inpainting achieves the highest improvement at single task modeling. On pairing tasks, inpainting and contrastive learning show Dice of 84.45% and Hausdorff Distance (HD) of 24.37. Overall, employing all proxy tasks achieves best Dice of 84.72%. <ref type="figure">Figure 8</ref>. Pre-trained weights using 100, 3000 and 5000 scans are compared for fine-tuning on the BTCV dataset for each organ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and Limitations</head><p>Our state-of-the-art results on the test leaderboards of MSD and BTCV datasets validate the effectiveness of the proposed self-supervised learning framework in taking the advantage of large number of available medical images without the need of annotation effort. Subsequently, fine-tuning the pretrained Swin UNETR model achieves higher accuracy, improves the convergence speed, and reduces the annotation effort in comparison to training with randomly initialized weights from scratch. Our framework is scalable and can be easily extended with more proxy tasks and augmentation transformations. Meanwhile, the pre-trained encoder can benefit the transfer learning of various medical imaging analysis tasks, such as classification and detection. In MSD pancreas segmentation task, Swin UNETR with pre-trained weights outperforms AutoML algorithms such as DiNTS <ref type="bibr" target="#b26">[27]</ref> and C2FNAS <ref type="bibr" target="#b59">[59]</ref> that are specifically designed for searching the optimal network architectures on the same segmentation task. Currently, Swin UNETR has only been pre-trained using CT images, and our experiments have not demonstrated enough transferability when applied directly to other medical imaging modalities such as MRI. This is mainly due to obvious domain gaps and different number of input channels that are specific to each modality. As a result, this is a potential direction that should be studied in future efforts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>In this work, we present a novel framework for selfsupervised pre-training of 3D medical images. Inspired by merging feature maps at scales, we built the Swin UNETR by exploiting transformer-encoded spatial representations into convolution-based decoders. By proposing the first transformer-based 3D medical image pre-training, we leverage the power of Swin Transformer encoder for fine-tuning segmentation tasks. Swin UNETR with self-supervised pre-training achieves the state-of-the-art performance on the BTCV multi-organ segmentation challenge and MSD challenge. Particularly, we presented the large-scale CT pre-training with 5,050 volumes, by combining multiple publicly available datasets and diversities of anatomical ROIs.</p><p>We provide the supplementary materials in the following. In Sec. A, we describe the details of datasets that are used for pre-training from public sources. In Sec. B, we illustrate the preprocessing and implementation details of fine-tuning tasks using BTCV and MSD datasets. In Sec. C, we present qualitative and quantitative comparisons of segmentation tasks in MRI modality from MSD dataset. The presented results include benchmarks from all top-ranking methods using the MSD test leaderboard. In Sec. D, the model complexity analysis is presented. Finally, we provide pseudocode of Swin UNETR self-supervised pre-training in Sec. E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pre-training Datasets</head><p>In this section, we provide additional information for our pre-training datasets. The proposed Swin UNETR is pre-trained using five collected datasets. The total data cohort contains 5,050 CT scans of various body region of interests (ROI) such as head, neck, chest, abdomen, and pelvis. LUNA16 <ref type="bibr" target="#b46">[47]</ref>, TCIA Covid19 <ref type="bibr" target="#b17">[18]</ref> and LiDC <ref type="bibr" target="#b1">[2]</ref> contain 888, 761 and 475 CT scans which composes the chest CT cohort. The HNSCC <ref type="bibr" target="#b21">[22]</ref> has 1,287 CT scans from head and neck squamour cell carcinoma patients. The TCIA Colon dataset <ref type="bibr" target="#b28">[29]</ref> comprises the abdomen and pelvis cohort with 1,599 scans. We split 5% of each dataset for validation in the pre-training stage. <ref type="table" target="#tab_7">Table S</ref>.1 summarizes sources of each collected dataset. Overall, the number of training and validation volumes are 4, 761 and 249, respectively. The Swin UNETR encoder is pre-trained using only unlabeled images, annotations were not utilized from any of theses datasets. We first clip CT image intensities from?1000 to 1000, then normalize to 0 and 1. To obtain informative patches of covering anatomies, we crop sub-volumes of 96?96?96 voxels at foregrounds, and exclude full air (voxel = 0) patches. In summary, Swin UNETR is pre-trained via a diverse set of human body compositions, and learn a general-purpose representation from different institutes' data that can be leveraged for wide range of fine-tuning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Preprocessing Pipelines</head><p>We report fine-tuning results on two public benchmarks: BTCV <ref type="bibr" target="#b31">[32]</ref> and MSD challenge <ref type="bibr" target="#b47">[48]</ref>. BTCV contains 30 CT scans with 13 annotated anatomies and can be formulated as a single multi-organ segmentation task. The MSD contains 10 tasks for multiple organs, from different sources and using different modalities. Details regarding preprocessing these datasets are provided in the subsequent sub-sections of 2.1 and 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. BTCV Dataset</head><p>All CT scans are interpolated into the isotropic voxel spacing of r1.5?1.5?2.0s mm. The multi-organ segmen-tation problem is formulated as a 13 class segmentation, which includes large organs such as liver, spleen, kidneys and stomach; vascular tissues of esophagus, aorta, IVC, splenic and portal veins; small anatomies of gallbladder, pancreas and adrenal glands. Soft tissue window is used for clipping the CT intensities, then normalized to 0 and 1 followed by random sampling of 96?96?96 voxels. Data augmentation of random flip, rotation and intensities shifting are used for training, with probabilities of 0.1, 0.1, and 0.5, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. MSD Dataset</head><p>The MSD challenge contains 6 CT and 4 MRI datasets. We provide additional parameters of pre-processing and augmentation details for each task as follows: Task01 BrainTumour: The four modalities MRI images for each subject are formed into 4 channels input. We convert labels to multiple channels based on tumor classes. which label 1 is the peritumoral edema, label 2 is the GD-enhancing tumor, and label 3 is the necrotic and non-enhancing tumor core. Label 2 and 3 are merged to construct tumor core (TC), label 1, 2 and 3 are merged to construct whole tumor (WT), and label 2 is the enhancing tumor (ET). We crop the sub-volume of 128?128?128 voxels and use channel-wise nonzero normalization for MRI images. Data augmentation probabilities of 0.5, 0.1 and 0.1 are set for random flips at each axis, intensities scaling and shifting, respectively. Task02 Heart: The heart MRI images are interpolated to the isotropic voxel spacing of 1.0 mm. Channel-wise nonzero normalization is applied to each scan. We sample the training sub-volumes of 96?96?96 voxels by ratio of positive and negative as 2:1. Augmentation probabilities for random flip, rotation, intensities scaling and shifting are set to 0.5, 0.1, 0.2, 0.5, respectively. Task03 Liver: Each CT scan is interpolated to the isotropic voxel spacing of 1.0 mm. Intensities are scaled to r?21,189s, then normalized to r0,1s. 3D patches of 96?96?96 voxels are obtained by sampling positive and negative ratio of 1 : 1. Data augmentation of random flip, rotation, intensities scaling and shifting are used, for which the probabilities are set to 0.2, 0.2, 0.1, 0.1, respectively. Task04 Hippocampus: Each hippocampus MRI image is interpolated by voxel spacing of 0.2?0.2?0.2, then applied spatial padding to 96?96?96 as the input size of Swin UNETR model. Same as other MRI datasets, channel-wise nonzero normalization is used for intensities. Probability of 0.1 is used for random flip, rotation, intensity scaling &amp; shifting.</p><p>Task05 Prostate: We utilize both given modalities for prostate MRI images for each subject as two channels input. Channel-wise nonzero normalization is used. Voxel spacing of 0.5 and spatial padding of each axis are employed to construct the input size of 96?96?96. We use random flip, rotation, intensity scaling and shifting with probabilities  Task07 Pancreas: We clip the intensities to a range of 87 to 199. Patch size of 96?96?96 is used to sample training data with positive and negative ratio of 1 : 1. We set augmentation of random flip, rotation and intensity scaling to probabilities of 0.5, 0.25 and 0.5, respectively.</p><p>Task08 HepaticVessel: To fit the optimal tissue window for hepatic vessel and tumor, we clip each CT image intensities to r0, 230s HU. We apply data augmentation same with Task07 Pancreas for training. Task09 Spleen: Spleen CT scans are pre-process with interpolation isotropic voxel spacing of 1.0 mm on each axis. Soft tissue window of r?125,275s HU is used for the portal venous phase contrast enhanced CT images. We use the training data augmentation of random flip, intensity scaling &amp; shifting with probabilities of 0.15, 0.1, and 0.1, respectively. Task10 Colon: We use HU range of r?57,175s for the colon tumor segmentation task and normalized to 0 and 1. Next, we sample training sub-volumes by positive and negative ratio of 1 : 1. Same as Task07 and Task08, we use random flip, rotation, intensity scaling as augmentation transforms with probabilities of 0.5, 0.25 and 0.5, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. MSD Qualitative Comparisons</head><p>In this section, we provide extensive segmentation visualization from MSD dataset. In particular, we compare two cases randomly selected from Swin UNETR and DiNTS for each MSD task. As shown in <ref type="figure">Fig S.1</ref>  <ref type="figure">Fig S.1)</ref>. Overall, Swin UNETR achieves better segmentation results and solves the under-and over-segmentation outliers as observed in segmentation via DiNTS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. MSD Quantitative Comparisons</head><p>In this section, we provide the quantitative benchmarks of MRI segmentation tasks from MSD dataset. In addition to Task01 BrainTumour, we implement experiment on three remaining MRI dataset including Heart, Hippocampus and Prostate (see <ref type="table">Table.</ref> S.2). The results are directly obtained from the MSD 6 leaderboard. Regarding MRI benchmark, we achieve much better performance on brain tumor segmentation presented in the paper, with average Dice improvement of 2% against second best performance. Comparing to models genesis <ref type="bibr" target="#b65">[65]</ref>, nnUNet <ref type="bibr" target="#b27">[28]</ref>, the Swin UNETR shows comparable results on Heart, Hippocampus and Prostate. Overall, we achieve the best average results (Dice of 82.14% and NSD of 94.66%) across four MRI datasets, showing Swin UNETR's superiority of medical image segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Model Complexity and Pre-training Time</head><p>In this section, we examine the model complexity along with inference time. In <ref type="table">Table.</ref> S.3, the number of network paramerts, FLOPs, and averaged inference time of Swin UNETR and baselines on BTCV dataset are presented. We calculate the FLOPs and inference time based on input size of 96?96?96 used in the BTCV experiments with sliding window approach. Swin UNETR shows moderate size of parameter with 61.98M, less than transformer-based methods such as TransUNet <ref type="bibr" target="#b6">[7]</ref> of 96.07M, SETR <ref type="bibr" target="#b62">[62]</ref> of 86.03M, and UNETR <ref type="bibr" target="#b23">[24]</ref> of 92.58M, but larger than 3DUNet (nnUNet) <ref type="bibr" target="#b27">[28]</ref> of 19.07M, ASPP [10] 47.92M. Our model also shows comparable FLOPs and inference time in terms of 3D approaches such as nnUNet <ref type="bibr" target="#b27">[28]</ref> and CoTr <ref type="bibr" target="#b55">[55]</ref>. Overall, Swin UNETR outperforms CNN-based and other transformerbased methods while perserves moderate model complexity.</p><p>Regarding self-supervised pre-training time of Swin UNETR encoder, our approach takes only approximately 6 GPU days. We evaluate pre-training on the 5 collected public datasets with totally 5,050 scans for training and validation, and set maximum training iterations to 45K steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Pre-Training Algorithm Details</head><p>In this section, we illustrate the Swin UNETR pre-training details. The Pytorch-like pseudo-code implementation is shown in Algorithm S.1. The Swin UNETR is trained in self-supervised learning paradigm, where we design masked volume inpainting, rotation prediction and contrastive coding as proxy tasks. The self-training aims at improving the quality of representations learnt by large unlabeled data and propagating to smaller fine-tuning dataset. To this end, we leverage multiple transformations for input 3D data, which can exploit inherent context by a mechanism akin to autoencoding and similarity identification. In particular, given an input mini batch data, the transform of random rotation is implemented on each image in the mini batch iteratively. To simultaneously utilize augmentation transformations for contrastive learning, the random rotation of 0?, 90?, 180?, 270?is applied twice on the same input to generate randomly augmented image pairs of the same image patch. Subsequently, the mini batch data pairs are constructed with the cutout transforms. The drop size of voxels are set to 30% of input sub-volumes. We randomly generate masked ROIs inside image, until the total masked voxels are larger than scheduled number of dropping voxels. Unlike canonical pre-training rules of masked tokens in BERT <ref type="bibr" target="#b18">[19]</ref>, our local transformations to the CT sub-volumes are then arranged to neighbouring tokens. This scheme can construct semantic targets across partitioned tokens, which is critical in medical spatial context. By analogy to Models Genesis <ref type="bibr" target="#b65">[65]</ref>, which is CNN-based model consisting expensive convolutional, transposed convolution layers and skip connection between encoder and decoder, our pre-training approach is trained to reconstruct input sub-volumes from the output tokens of the Swin Transformer. Overall, the intuition of modeling inpainting, rotation prediction and contrastive coding is to generalize better representations from aspects of images context, geometry and similarity, respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Work completed during internship at NVIDIA : Co-senior advising ; Corresponding author: ahatamizadeh@nvidia.com 1 https : / / decathlon -10 . grand -challenge . org / evaluation/challenge/leaderboard/ 2 https://www.synapse.org/#!Synapse:syn3193805/ wiki/217785/ +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative visualizations of the proposed Swin UNETR and baseline methods. Three representative subjects are demonstrated. Regions of evident improvements are enlarged to show better details of pancreas (blue), portal vein (light green), and adrenal gland (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7</head><label>7</label><figDesc>demonstrates the comparison results of fine-tuning using a subset of BTCV dataset. We show using 10% of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>The indication of Dice gap between using pre-training (Green) and scratch model (Blue) on MSD CT tasks validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Data-efficient performance on BTCV test dataset. Significance under Wilcoxon Signed Rank test,?: p ? 0.001.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Overall performance of top-ranking methods on all 10 segmentation tasks in the MSD public test leaderboard. NSD denotes Normalized Surface Distance. surface point sets. Surface Dice<ref type="bibr" target="#b38">[39]</ref> is also used, which is referred as Normalized Surface Distance (NSD) in MSD challenge evaluation. The metric measures the overlap of ground truth and prediction surfaces (with a fixed tolerance) instead of the overlap of two volumes. This provides a measure of agreement between the surfaces of two structures.</figDesc><table><row><cell></cell><cell></cell><cell>Dice ?</cell><cell>NSD ?</cell></row><row><cell>Swin UNETR</cell><cell>1</cell><cell>78.68</cell><cell>89.28</cell></row><row><cell>DiNTS [27]</cell><cell>2</cell><cell>77.93</cell><cell>88.68</cell></row><row><cell>nnUNet [28]</cell><cell>3</cell><cell>77.89</cell><cell>88.09</cell></row><row><cell>Models Gen. [65]</cell><cell>4</cell><cell>76.97</cell><cell>87.19</cell></row><row><cell>Trans VW [23]</cell><cell>5</cell><cell>76.96</cell><cell>87.64</cell></row></table><note>5 https : / / decathlon -10 . grand -challenge . org / evaluation/challenge/leaderboard/ 5.4. Results</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>45.75 68.26 60.47 86.65 72.03 90.28 82.99 94.25 72.96 83.61 96.76 88.58 92.67 63.10 62.51 Trans VW [23] 68.03 46.98 68.40 61.14 87.52 72.42 90.91 83.62 95.18 76.90 86.04 97.86 92.03 94.95 74.54 76.22 C2FNAS [59] 67.62 48.60 69.72 61.98 87.61 72.87 91.16 83.88 94.98 72.89 83.94 98.38 89.15 93.77 70.44 72.22 Models Gen. [65] 68.03 46.98 68.40 61.14 87.52 72.42 90.91 83.62 95.72 77.50 86.61 98.48 91.92 95.20 74.54 76.22 nnUNet [28] 68.04 46.81 68.46 61.10 87.51 72.47 90.78 83.59 95.75 75.97 85.86 98.55 90.65 94.60 73.97 76.02 DiNTS [27] 69.28 48.65 69.75 62.56 89.33 73.16 91.69 84.73 95.35 74.62 84.99 98.69 91.02 94.86 74.75 77.02 Swin UNETR 70.02 52.52 70.51 64.35 89.07 80.30 93.46 87.61 95.35 75.68 85.52 98.34 91.59 94.97 76.60 77.40 51.75 66.18 95.83 73.09 84.46 62.34 68.63 65.49 83.22 78.43 80.83 91.92 94.83 49.32 62.21 Trans VW [23] 81.42 51.08 66.25 96.07 70.13 83.10 65.80 71.44 68.62 84.01 80.15 82.08 97.35 99.87 51.47 60.53 C2FNAS [59] 80.76 54.41 67.59 96.16 75.58 85.87 64.30 71.00 67.65 83.78 80.66 82.22 96.28 97.66 58.90 72.56 Models Gen. [65] 81.36 50.36 65.86 96.16 70.02 83.09 65.80 71.44 68.62 84.01 80.15 82.08 97.35 99.87 51.47 60.53 nnUNet [28] 81.64 52.78 67.21 96.14 71.47 83.81 66.46 71.78 69.12 84.43 80.72 82.58 97.43 99.89 58.33 68.43 DiNTS [27] 81.02 55.35 68.19 96.26 75.90 86.08 64.50 71.76 68.13 83.98 81.03 82.51 96.98 99.83 59.21 70.34 Swin UNETR 81.85 58.21 70.71 96.57 79.10 87.84 65.69 72.20 68.95 84.83 81.62 83.23 96.99 99.84 59.45 70.89</figDesc><table><row><cell>Organ</cell><cell></cell><cell cols="2">Task01 Brain Tumour</cell><cell></cell><cell></cell><cell>Task03 Liver</cell><cell>Task06 Lung</cell></row><row><cell>Metric</cell><cell cols="2">Dice1 Dice2 Dice3</cell><cell cols="2">Avg. NSD1 NSD2 NSD3</cell><cell cols="2">Avg. Dice1 Dice2</cell><cell>Avg. NSD1 NSD2</cell><cell>Avg. Dice1 NSD1</cell></row><row><cell cols="2">Kim et al [31] 67.40 Organ</cell><cell cols="2">Task07 Pancreas</cell><cell></cell><cell cols="2">Task08 Hepatic Vessel</cell><cell>Task09 Spleen</cell><cell>Task10 Colon</cell></row><row><cell>Metric</cell><cell>Dice1 Dice2</cell><cell cols="2">Avg. NSD1 NSD2</cell><cell cols="2">Avg. Dice1 Dice2</cell><cell>Avg. NSD1 NSD2</cell><cell>Avg. Dice1 NSD1 Dice1 NSD1</cell></row><row><cell>Kim et al [31]</cell><cell>80.61</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>MSD test dataset performance comparison of Dice and NSD. Benchmarks obtained from MSD test leaderboard 5 .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Due to space limitations, we present the MSD test benchmarks for the remaining three MRI tasks in the supplementary materials. Qualitative results of representative MSD CT tasks.Average Dice values are illustrated on top of each image. Our model demonstrates more accurate performance in comparison to DiNTS for both organ and tumor segmentation across different tasks.</figDesc><table><row><cell></cell><cell>Figure 5. Loss Function</cell><cell cols="2">Average Accuracy</cell></row><row><cell></cell><cell></cell><cell cols="2">Dice ? HD ?</cell></row><row><cell></cell><cell>Scratch</cell><cell>83.43</cell><cell>42.36</cell></row><row><cell></cell><cell>L rot</cell><cell>83.56</cell><cell>36.19</cell></row><row><cell></cell><cell>L contrast</cell><cell>83.67</cell><cell>38.81</cell></row><row><cell>5.5. Ablation Study</cell><cell>L inpaint L inpaint`Lrot</cell><cell>83.85 84.01</cell><cell>28.94 26.06</cell></row><row><cell>5.5.1 Efficacy of Pre-training</cell><cell cols="3">L inpaint`Lcontrast L inpaint`Lcontrast`Lrot 84.72 20.03 84.45 24.37</cell></row><row><cell>A comparison of all MSD CT tasks using pre-trained model</cell><cell></cell><cell></cell><cell></cell></row><row><cell>against training from scratch can be observed in Fig. 6.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Distinct improvement can be observed for Task03 Liver,</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dice of 77.77% comparing to 75.27%. Task08 Hepatic</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Vessel achieves 68.52% against 64.63%. Task10 Colon</cell><cell></cell><cell></cell><cell></cell></row><row><cell>shows the largest improvement, from 34.83% to 43.38%.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Task07 Pancreas and Task09 Spleen both achieve significant</cell><cell></cell><cell></cell><cell></cell></row><row><cell>improvement from 67.12% to 67.82%, and 96.05% to 97.32%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>respectively.</cell><cell></cell><cell></cell><cell></cell></row></table><note>2. The proposed Swin UNETR achieves state-of-the-art performance in Task01 BrainTumour, Task06 Lung, Task07 Pancreas, and Task10 Colon. The results are comparable for Task02 Heart, Task03 Liver, Task04 Hippocampus, Task05 Prostate, Task08 HepaticVessel and Task09 Spleen. Overall, Swin UNETR presents the best average Dice of 78.68% across all ten tasks and achieves the top ranking in the MSD leaderboard. The detail number of multiple tasks are shown in Table 3. Qualitative visualization can be observed in Fig. 5. Swin UNETR with self-supervised pre-training demonstrates visually better segmentation results in the CT tasks. The pre-trained weights are only used for fine-tuning CT tasks including Liver, Lung, Pancreas, HepaticVessel, Spleen, and Colon. For MRI tasks: Brain Tumour, Heart, Hippocampus, Prostate, experiments are trained from scratch because of the domain gap between CT and MRI images.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table S .</head><label>S</label><figDesc>1. Summary of datasets for pre-training, the use of cohorts identifies diversified regions of interest.of 0.5 as data augmentations. Random affine is applied as additional transformation with scale factor of r0.3,0.3,0.0s and rotation range of r0,0,pis at each axis.</figDesc><table><row><cell>Task06 Lung: We interpolate each image to isotropic voxel</cell></row><row><cell>spacing of 1.0. Houndsfield unit (HU) range of [-1000, 1000]</cell></row></table><note>is used and normalized to r0,1s. Subsequently, training sam- ple are cropped to 96?96?96 with positive and negative ratio of 2 : 1. Augmentation probabilities of 0.5, 0.3, 0.1, 0.1 are used for random flip, rotation, intensities scaling and shifting.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>96.44 90.11 88.72 89.42 97.77 97.73 97.75 72.64 89.02 80.83 95.05 98.03 96.54 80.96 93.43 Trans VW [23] 93.33 96.51 90.29 88.77 89.53 97.87 97.67 97.77 73.69 88.88 81.29 95.42 98.52 96.97 81.32 93.72 C2FNAS [59] 92.49 95.81 89.37 87.96 88.67 97.27 97.35 97.31 74.88 88.75 81.82 98.79 95.12 96.96 81.24 93.49 Models Gen [65] 93.33 96.51 90.29 88.77 89.53 97.87 97.67 97.77 73.69 88.88 81.29 95.42 98.52 96.97 81.32 93.72 nnUNet [28] 93.30 96.74 90.23 88.69 89.46 97.79 97.53 97.75 76.59 89.62 83.11 96.27 98.85 97.56 81.74 93.91 DiNTS [27] 92.99 96.35 89.91 88.41 89.16 97.76 97.56 97.66 75.37 89.25 82.31 95.96 98.82 97.39 81.76 94.03 SwinUNETR 92.62 96.23 89.95 88.42 89.19 97.63 97.32 97.48 75.65 89.15 82.40 95.89 98.70 97.30 82.14 94.66 Table S.2. Additional MSD MRI test dataset performance comparison of Dice and NSD. Benchmarks obtained from MSD test leaderboard. Task01 BrainTumuor results are shown in the paper. Note: The results reported for TransVW<ref type="bibr" target="#b22">[23]</ref> and Models Genesis<ref type="bibr" target="#b65">[65]</ref> are from the official leaderboard for MRI tasks.</figDesc><table><row><cell>Organ</cell><cell>Task02 Heart</cell><cell cols="2">Task04 Hippocampus</cell><cell></cell><cell>Task05 Prostate</cell><cell></cell><cell cols="2">MRI tasks Avg</cell></row><row><cell>Metric</cell><cell cols="2">DSC1 NSD1 DSC1 DSC2</cell><cell>Avg. NSD1 NSD2</cell><cell>Avg. DSC1 DSC2</cell><cell>Avg. NSD1 NSD2</cell><cell>Avg.</cell><cell>DSC</cell><cell>NSD</cell></row><row><cell cols="4">Kim et al [31] 93.11 Models #Params (M) FLOPs (G) Inference Time (s)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>nnUNet [28]</cell><cell>19.07</cell><cell>412.65</cell><cell>10.28</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CoTr [55]</cell><cell>46.51</cell><cell>399.21</cell><cell>19.21</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TransUNet [7]</cell><cell>96.07</cell><cell>48.34</cell><cell>26.97</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ASPP [10]</cell><cell>47.92</cell><cell>44.87</cell><cell>25.47</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SETR [61]</cell><cell>86.03</cell><cell>43.49</cell><cell>24.86</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UNETR</cell><cell>92.58</cell><cell>41.19</cell><cell>12.08</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SwinUNETR</cell><cell>61.98</cell><cell>394.84</cell><cell>13.84</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Table S.3. Comparison of number of parameters, FLOPs and</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">averaged inference time for various models in BTCV experiments.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>,</head><label></label><figDesc>DiNTS includes the under-segmentation due to lack of parts of labels (Heart, Hippocampus). The missing parts result in a lower Dice score. On BrainTumour, Liver, Pancreas, HepaticVessel and Colon tasks, the comparison indicate that our method achieves better segmentation where the under-segmentation of tumors are observed in DiNTS. For Lung task, the over-segmentation is observed with DiNTS where surrounding tissues are included with label of the lung cancer, while Swin UNETR clearly delineate the boundary. In Heart and Spleen, DiNTS and Swin UNETR have comparable Dice score, yet Swin UNETR performs better segmentation on tissue corner (See</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.synapse.org/#!Synapse:syn3193805/ wiki/217785/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://monai.io/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https : / / decathlon -10 . grand -challenge . org / evaluation/challenge/leaderboard/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The medical segmentation decathlon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michela</forename><surname>Antonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annika</forename><surname>Reinke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyridon</forename><surname>Bakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyvan</forename><surname>Farahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geert</forename><surname>Bennett A Landman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bram</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Ginneken</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05735</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The lung image database consortium (lidc) and image database resource initiative (idri): a completed reference database of lung nodules on ct scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Samuel G Armato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Mclennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Bidaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcnitt-Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><forename type="middle">P</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binsheng</forename><surname>Reeves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><forename type="middle">I</forename><surname>Denise R Aberle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">A</forename><surname>Henschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical physics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Sit: Self-supervised vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Atito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Kittler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.03602</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Big self-supervised models advance medical image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shekoofeh</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fiona</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Beaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Freyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Deaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Karthikesalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Swin-unet: Unet-like pure transformer for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueyue</forename><surname>Hu Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joy</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manning</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05537</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Transunet: Transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieneng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangde</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Self-supervised learning for medical image analysis using image context restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensaku</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazunari</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michitaka</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">101539</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02611</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Med3d: Transfer learning for 3d medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yefeng</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00625</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06278</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>?zg?n ? I?ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soeren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on medical image computing and computer-assisted intervention</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>3d u-</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Up-detr: Unsupervised pre-training for object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yugeng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junying</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Chest imaging representing a covid-19 positive rural us population</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivang</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Baghal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thidathip</forename><surname>Wongsurawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piroon</forename><surname>Jenjaroenpun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaymaa</forename><surname>Al-Shukri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Gates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Farmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geri</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Imaging and clinical data archive for head and neck squamous cell carcinoma patients treated with radiotherapy. Scientific data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grossberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Abdallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hesham</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elhalawani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirk</forename><forename type="middle">E</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tracy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowman</forename><surname>Nolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasikarn</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jolien</forename><surname>Chamchod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Heukelom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kantor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transferable visual words: Exploiting the semantics of anatomical patterns for self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatemeh</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad Reza Hosseinzadeh</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Gotway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unetr: Transformers for 3d medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hatamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishwesh</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Myronenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bennett</forename><surname>Landman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Holger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daguang</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dints: Differentiable neural network topology search for 3d medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daguang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">nnu-net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><forename type="middle">H</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Accuracy of ct colonography for detection of large adenomas and cancers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Daniel</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei-Hsiu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alicia</forename><forename type="middle">Y</forename><surname>Toledano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><forename type="middle">P</forename><surname>Heiken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Dachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><forename type="middle">O</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Betina</forename><surname>Menias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Siewert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jugesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">G</forename><surname>Cheema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Obregon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New England Journal of Medicine</title>
		<imprint>
			<biblScope unit="volume">359</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Medical transformer: gated axial-attention for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scalable neural architecture search for 3d medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbin</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woonhyuk</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiheon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyungjoo</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boogeon</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Miccai multi-atlas labeling beyond the cranial vaultworkshop and challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>B Landman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Igelsias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Styner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Langerak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICCAI Multi-Atlas Labeling Beyond Cranial Vault-Workshop Challenge</title>
		<meeting>MICCAI Multi-Atlas Labeling Beyond Cranial Vault-Workshop Challenge</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Swinir: Image restoration using swin transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Ds-transunet: Dual swin transformer u-net for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingzhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangming</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06716</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision</title>
		<meeting>the IEEE/CVF Interna-tional Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13230</idno>
		<title level="m">Video swin transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep learning to achieve clinically applicable segmentation of head and neck anatomy for radiotherapy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Zverovitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruheena</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Livne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yojan</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harry</forename><surname>Askham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04430</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Contrastive learning for unpaired image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Do vision transformers see like convolutional neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.08810</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Transfusion: Understanding transfer learning for medical imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A multi-scale pyramid of 3d fully convolutional networks for abdominal multi-organ segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Holger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirohisa</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masahiro</forename><surname>Sugino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichiro</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazunari</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensaku</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International conference on medical image computing and computer-assisted intervention</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: the luna16 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud Arindra Adiyoso</forename><surname>Setio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Traverso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">De</forename><surname>Bel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Moira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cas</forename><surname>Berens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piergiorgio</forename><surname>Bogaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cerello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">Evelina</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bram</forename><surname>Fantacci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geurts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">A large annotated medical image dataset for the development and evaluation of segmentation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Amber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michela</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyridon</forename><surname>Antonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Bakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyvan</forename><surname>Bilello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bram</forename><surname>Farahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annette</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kopp-Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geert</forename><surname>Bennett A Landman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Menze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09063</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Sniper: Efficient multi-scale training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="9310" to="9320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">3d self-supervised methods for medical imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiham</forename><surname>Taleb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winfried</forename><surname>Loetzsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Danz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julius</forename><surname>Severin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gaertner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Bergner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lippert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Body part regression with self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riqiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dashan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishwesh</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camilo</forename><surname>Bermudez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunxing</forename><surname>Savona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilwoo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1499" to="1507" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riqiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhong</forename><surname>Ho Hin Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqiang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dashan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishwesh</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camilo</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bermudez</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Highresolution 3d abdominal segmentation with random patch network fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">G</forename><surname>Michael R Savona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abramson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">101894</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Self-supervised image-text pre-training with mixed data in chest x-rays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyue</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daguang</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16022</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Cotr: Efficiently bridging cnn and transformer for 3d medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International conference on medical image computing and computer-assisted intervention</title>
		<imprint>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Self-supervised learning with swin transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04553</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Levit-unet: Make faster encoders with transformer for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwei</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08623</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Self-supervised learning of pixel-wise anatomical embeddings in radiological images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinzheng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dakai</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhou</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youbao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.02383</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">C2fnas: Coarseto-fine neural architecture search for 3d medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daguang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04560</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Scaling vision transformers. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15840</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">nnformer: Interleaved transformer for volumetric segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiansen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lequan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liansheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.03201</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Prior-aware neural network for partially-supervised multi-organ segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliot</forename><surname>Fishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Models genesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vatsal</forename><surname>Sodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Gotway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Rubik&apos;s cube+: A self-supervised feature learning framework for 3d medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiuwen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yefeng</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">101746</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
