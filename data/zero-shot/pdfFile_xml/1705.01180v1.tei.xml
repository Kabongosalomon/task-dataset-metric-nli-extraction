<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cascaded Boundary Regression for Temporal Action Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
							<email>jiyangga@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Robotics and Intelligent Systems</orgName>
								<orgName type="institution">University of Southern California Los Angeles</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
							<email>zhenheny@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Robotics and Intelligent Systems</orgName>
								<orgName type="institution">University of Southern California Los Angeles</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
							<email>nevatia@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Robotics and Intelligent Systems</orgName>
								<orgName type="institution">University of Southern California Los Angeles</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cascaded Boundary Regression for Temporal Action Detection</title>
					</analytic>
					<monogr>
						<title level="j" type="main">CASCADED BOUNDARY REGRESSION FOR TEMPORAL ACTION DETECTION</title>
						<imprint>
							<biblScope unit="volume">1</biblScope>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal action detection in long videos is an important problem. State-of-the-art methods address this problem by applying action classifiers on sliding windows. Although sliding windows may contain an identifiable portion of the actions, they may not necessarily cover the entire action instance, which would lead to inferior performance. We adapt a two-stage temporal action detection pipeline with Cascaded Boundary Regression (CBR) model. Class-agnostic proposals and specific actions are detected respectively in the first and the second stage. CBR uses temporal coordinate regression to refine the temporal boundaries of the sliding windows. The salient aspect of the refinement process is that, inside each stage, the temporal boundaries are adjusted in a cascaded way by feeding the refined windows back to the system for further boundary refinement. We test CBR on THUMOS-14 and TVSeries, and achieve state-of-the-art performance on both datasets. The performance gain is especially remarkable under high IoU thresholds, e.g. map@tIoU=0.5 on THUMOS-14 is improved from 19.0% to 31.0%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Temporal action detection in long videos is an important and challenging problem, which has been receiving increasing attention recently. Given a long video, the task of action detection is to localize intervals where actions of interest take place and also predict the action categories.</p><p>Good progress has been achieved in action classification <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24]</ref>, where the task is to predict action classes in "trimmed" videos. Current state-of-the-art methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28]</ref> on action detection extend classification methods to detection by applying action classifiers on dense sliding windows. However, while sliding windows may contain an identifiable portion of the action, they do not necessarily cover the entire action instance or they could contain extraneous background frames, which may lead to inferior performance. Similar observations have also been made for use of sliding windows in object detection <ref type="bibr" target="#b18">[19]</ref>. Inspired by object detection, Shou et al. <ref type="bibr" target="#b19">[20]</ref> proposed a two-stage pipeline for action detection, called SCNN. In the first stage, it produces actionness scores for multi-scale sliding windows and outputs the windows with high scores as class-agnostic temporal proposals; in the second stage, SCNN categorizes the proposals to specific actions. However, SCNN still suffers from the imprecision of sliding window intervals.</p><p>To improve temporal localization accuracy, recently a method called TURN <ref type="bibr" target="#b4">[5]</ref> proposes to use temporal boundary regression. TURN takes sliding windows and their surrounding context as input and refine their temporal boundaries by learning a boundary regressor. We propose that the process of boundary estimation can be improved by a cascade, where a regressed clip is fed back to the system for further refinement, as the system could observe different content in each round of refinement, and refine the boundary gradually.</p><p>We adapt a two-stage action detection model with temporal coordinate regression. In the first stage, our model takes sliding windows as input, and outputs class-agnostic temporal proposals. In the second stage, our model detects actions based on the proposals. The salient aspect in our model is that, inside each stage, we propose to use Cascaded Boundary Regression (CBR) to adjust temporal boundaries in a regression cascade, where regressed clips are fed back to the system for further boundary refinement. We evaluate CBR on two challenging datasets: THUMOS-14 and TVSeries <ref type="bibr" target="#b1">[2]</ref>. CBR outperforms state-of-theart methods on both temporal action proposal generation and action detection tasks by a large margin. The performance gain is especially remarkable under high IoU thresholds, e.g map@tIoU=0.5 on THUMOS-14 is improved from 19.0% to 31.0%.</p><p>Our contributions are two-fold:</p><p>(1) We propose a Cascaded Boundary Regression method for temporal boundary estimation, which is shown to be effective on both proposal generation and action detection.</p><p>(2) We evaluate CBR on both proposal generation and action detection, and achieve state-of-the-art performance on both THUMOS-14 and TVSeries <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Temporal action detection, temporal proposal generation and object detection are related to our work, we will introduce these three topics in this section.</p><p>Temporal Action Detection Temporal action localization has been received much attentions recently. S-CNN <ref type="bibr" target="#b19">[20]</ref> presented action detection framework which involves two stages: the first stage uses proposal network to generate temporal action proposals; the second stage classifies the proposals with localization network, which is trained using classification and localization loss. Singh et al. <ref type="bibr" target="#b21">[22]</ref> extended two-stream <ref type="bibr" target="#b20">[21]</ref> framework to multi-stream network and use bi-directional LSTM networks to encode temporal information, they achieved state-of-the-art performance on MPII-Cooking dataset <ref type="bibr" target="#b17">[18]</ref>. Ma et al. <ref type="bibr" target="#b13">[14]</ref> addressed the problem of early action detection. They proposed to train a LSTM network with ranking loss and merge the detection spans based on the frame-wise prediction scores generated by the LSTM. Sun et al. <ref type="bibr" target="#b22">[23]</ref> proposed to transfer knowledge from web images to address temporal detection in untrimmed web videos.</p><p>Temporal Action Proposal. Similar to object proposal generation, temporal proposal generation aims to produce class-agnostic proposals efficiently and accurately. Sparse-prop <ref type="bibr" target="#b0">[1]</ref> presented a method that use STIPs <ref type="bibr" target="#b11">[12]</ref> and dictionary learning for class-independent proposal generation. SCNN-prop <ref type="bibr" target="#b19">[20]</ref> presented a method that fine-tunes 3D convolutional network <ref type="bibr" target="#b23">[24]</ref> for binary proposal classification. DAPs <ref type="bibr" target="#b2">[3]</ref> used LSTM networks to encode a video stream and produce proposals inside the video stream. Gao et al. <ref type="bibr" target="#b4">[5]</ref> proposed a method, called TURN, to use unit-level temporal coordinate regression to refine the temporal action boundary. Object Proposals and Object Detection. Recent successful object detection frameworks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17]</ref> are built on high quality object proposals. SelectiveSearch <ref type="bibr" target="#b24">[25]</ref> and Edgebox <ref type="bibr" target="#b28">[29]</ref> rely on hand-crafted low-level visual features. R-CNN <ref type="bibr" target="#b6">[7]</ref> and Fast R-CNN <ref type="bibr" target="#b5">[6]</ref> use this type of object proposals as input. RPNs <ref type="bibr" target="#b16">[17]</ref> proposed to use anchor boxes and spatial regression for object proposal generation, which is based on ConvNet's conv-5 featmap. YOLO <ref type="bibr" target="#b15">[16]</ref> proposed to divide the input image into grid cells and estimate the object bounding box by coordinate regression. SSD <ref type="bibr" target="#b12">[13]</ref> further adopted multi-scale grid cells to predict bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In this section, we describe the two-stage Cascaded Boundary Regression (CBR) network and the training procedure, its architecture is shown in <ref type="figure" target="#fig_0">Figure 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Video Unit Feature Extraction</head><formula xml:id="formula_0">A video V containing T frames, V = { f i } T 1 , is divided into T /u f consecutive video units , where u f is the number of frames in a unit. A video unit can be represented as u = {t i } f s +u f ?1 f s ,</formula><p>where f s is the starting frame, f s + u f ? 1 is the ending frame f e . Units do not overlap with each other. Each unit is processed by a visual encoder E v to get a unit-level representation f u = E v (u). In our experiments, C3D <ref type="bibr" target="#b23">[24]</ref> and two-stream CNN models <ref type="bibr" target="#b20">[21]</ref> are investigated. Details are given in Section 4. are the surrounding units before and after c respectively, n ctx is the number of units we consider. The surrounding units provide temporal context for clips, which are important for temporal boundary inferring. Internal feature and context features are pooled from unit-level features separately by mean pooling operation P. The final feature f c for a clip is the concatenation of context features and the internal feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Video Clip Modeling</head><formula xml:id="formula_1">f c = P({u j } s u s u ?n ctx ) P({u j } e u s u ) P({u j } e u +n ctx e u )<label>(1)</label></formula><p>where represents vector concatenation. We scan a video by multi-scale temporal sliding windows. The temporal sliding windows are modeled by two parameters: window length l i and window overlap o i . Note that, although multi-scale clips would have temporal overlaps, the clip-level features are computed from unit-level features, which are only calculated once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Temporal Coordinate Regression</head><p>We first introduce temporal coordinate regression and then introduce the two-stage proposal and detection pipeline. Our goal is to design a method which is able to estimate the temporal boundaries of actions. For spatial boundary regression, previous work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17]</ref> uses parameterized coordinate offsets, that is, the boundary coordinates are first parameterized by the central coordinates and the size (i.e. length and width) of the bounding box, the offsets are calculated based on these parameterized coordinates. In the temporal coordinate settings, the parameterization offsets could be represented as,</p><formula xml:id="formula_2">o x = (x gt ? x clip )/l clip , o l = log(l gt /l clip )<label>(2)</label></formula><p>where x and l denote the clip's center coordinate and clip length respectively. Variables x gt , x clip are for ground truth clip and test clip (like wise for l). Instead of using parameterization, non-parameterized offset is to use the start end end coordinates directly. Specifically, there are two levels of coordinates: frame-level and unitlevel. The frame-level coordinate is the index of the frame f i ; the unit-level coordinate is the index of the unit u j . For an action instance, the ground truth start and end coordinates t gt s and t gt e are usually annotated in seconds, which could be always transferred to frame-level (multiplied by FPS) f gt s and f gt e , the unit-level ground truth coordinates are calculated by rounding:</p><formula xml:id="formula_3">u gt s =&lt; f gt s /u f &gt;, u gt e =&lt; f gt e /u f &gt;<label>(3)</label></formula><p>where &lt; ? &gt; represents rounding, u f is the frame number in a unit. The non-parameterized regression offsets are</p><formula xml:id="formula_4">o s = s clip ? s gt , o e = e clip ? e gt<label>(4)</label></formula><p>where s clip , e clip are the start and end coordinates of the input clip, which could be at framelevel or unit-level. s gt , e gt are the coordinates for the matched ground truth action instance. The intuition behind unit-level coordinate regression is that, as the basic unit-level features are extracted to encode n u frames, the feature may not be discriminative enough to regress the coordinates at frame-level. Comparing with frame-level regression, unit-level coordinate regression is easier to learn, though with coarser boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Two-Stage Proposal and Detection Pipeline</head><p>Inspired by the proposal and detection pipeline in object detection, we design a two-stage pipeline for temporal action detection, in which class-agnostic proposals and class-specific detections are generated respectively, shown in <ref type="figure" target="#fig_0">Figure 1</ref>. In both stages, temporal coordinate regression is used to infer temporal action boundaries. Sepcifically, given a clip c =&lt; s, e &gt;, it is first processed by the proposal network, which outputs two boundary regression offsets &lt; o s , o e &gt; and an actionness score p c indicating whether c is an action instance. If the output p c is higher than a threshold ? , the detection network takes c =&lt; s , e &gt; (new temporal boundaries) as input and generates n + 1 softmax scores p z , x ? [1, n + 1] and n pairs of boundary offsets &lt; o z s , o z e &gt;, x ? [1, n], where n is the number of action categories. In each stage (i.e. the proposal and detection stages), boundary regression is applied in a cascaded manner-the output boundaries are fedback as input to the network for further refinement, as shown in <ref type="figure" target="#fig_3">Figure 2</ref>. For proposal network, given a input clip c =&lt; s, e &gt;, the output clip c 1 =&lt; s 1 , e 1 &gt; is fedback as input to do a second round of refinement, and the second output is c 2 =&lt; s 2 , e 2 &gt;. The iteration process takes K p c steps, the final boundaries and the actionness score for c are</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Cascaded Boundary Regression</head><formula xml:id="formula_5">c K p c =&lt; s K p c , e K p c &gt;, p = K p c ? i=1 p i<label>(5)</label></formula><p>The cascade process of detection network is similar to that of proposal network. For detection network, it outputs n pairs of temporal boundary offsets and n + 1 category scores. Among the n non-background categories, we take the category with the highest score as the prediction x, and pick the corresponding boundary offsets &lt; o s,x , o e,x &gt;. The refined clip c 1 =&lt; s 1</p><p>x , e 1 x &gt; is fed back into the network. After K d c steps, the final boundaries and score for the predicted category x are</p><formula xml:id="formula_6">c K d c =&lt; s z K d c , e z K d c &gt;, p = K d c ? i=1 p z i<label>(6)</label></formula><p>The proposal network and the detection network are trained separately, details could be found in the next section. In either stage (proposal or detection), each cascade step could be trained separately, but here we have chosen to use the same network parameters in each step for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Loss Function</head><p>To train CBR, we collect training samples from sliding windows, whose parameters (overlap and window length) will be introduced in Section 4. A class label is assigned to a sliding window if: (1) the window clip with the highest temporal Intersection over Union (tIoU) overlaps with a ground truth clip; or (2) the window clip has tIoU larger than 0.5 with any of the ground truth clips. Note that, a single ground truth clip may assign its label to multiple window clips. Negative labels are assigned to non-positive clips whose tIoU is equal to 0.0 (i.e. no overlap) for all ground truth clips. We design a multi-task loss L to jointly train classification and coordinate regression.</p><formula xml:id="formula_7">L = L cls + ? L reg<label>(7)</label></formula><p>where L cls is the loss for classification, which is a standard cross-entropy loss. For proposal network, L cls is a binary classification cross-entropy loss; for detection network, L cls is a stardard multi-class cross-entropy loss. L reg is for temporal coordinate regression and ? is a hyper-parameter, which is set empirically. The regression loss is</p><formula xml:id="formula_8">L reg = 1 N N ? i=1 n ? z=1 l z i [R(? z s,i ? o z s,i ) + R(? z e,i ? o z e,i )]<label>(8)</label></formula><p>where R is L1 distance, N is batch size and n is the total number of categories, l z i is the label, when the ith sample is from category z, l z i = 1, otherwise, l z i = 0.? is the regression estimate offset, and o is the ground truth offset. For parameterized offsets, o z s,i and o z e,i are replaced by o z</p><p>x,i and o z l,i . The learning rate and batch size are set as 0.005 and 128 respectively. We use the Adam <ref type="bibr" target="#b10">[11]</ref> optimizer to train CBR. The ratio of sample numbers of background to non-background in a mini-batch is set to be 10 for training proposal network. For training detection network, the number background samples are equal to the average sample numbers of all categories. ? is set to 2 for both proposal and detection network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We evaluate the effectiveness of the proposed Cascaded Boundary Regression (CBR) on standard benchmarks THUMOS-14 and TVSeries for both temporal action proposal generation and action detection.</p><p>Unit-level Feature Extraction. C3D unit-level features: The C3D model is pre-trained on Sports1M <ref type="bibr" target="#b9">[10]</ref>, we uniformly sample 16 frames in a unit and then input them into C3D; the output of f c6 layer is used as unit-level feature. Two-stream features: We use the twostream model <ref type="bibr" target="#b25">[26]</ref> that is pre-trained on ActivityNet v1.3 training set. In each unit, the central frame is sampled to calculate the appearance CNN feature, which is the output of "Flatten_673" layer in ResNet <ref type="bibr" target="#b7">[8]</ref>. For the motion feature, we sample 6 consecutive frames at the center of a unit and calculate optical flows <ref type="bibr" target="#b3">[4]</ref> between them; these flows are then fed into the pretrained BN-Inception model <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26]</ref> and the output of "global pool" layer is extracted. The motion features and the appearance features are concatenated into 4096dimensional vectors, which are used as unit-level features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiments on THUMOS-14</head><p>We first introduce the datasets and the experiment setup, then discuss the experimental results on THUMOS-14.</p><p>Dataset. The temporal action localization part of THUMOS-14 contains over 20 hours of videos from 20 sport classes. There are 200 untrimmed videos in validation set and 213 untrimmed videos in test set. The training set of THUMOS-14 contains only trimmed videos. We train our model on the validation set and test it on the test set.</p><p>Experimental setup. We perform the following experiments on THUMOS-14: (1) explore components in the proposed framework: (a) parametrized offsets vs non-parameterized unit-level regression vs non-parameterized frame-level regression, (b) cascaded steps for boundary regression; (2) comparison with state-of-the-art approaches. The unit size u f is 16, the surrounding unit number n ctx is set to 4. The sliding window lengths and overlaps are {16(16), 32(16), 64 <ref type="bibr" target="#b15">(16)</ref>, 128(32), 256(64), 512(128)}, where the numbers out of brackets are lengths of sliding windows, and the numbers in brackets are the corresponding overlaps of the sliding windows.</p><p>Temporal coordinate regression. To explore which type of coordinate offsets is most effective for boundary regression in temporal action detection, we test three types: (a) parameterized coordinate offsets, which are similar to the ones in object detection <ref type="bibr" target="#b16">[17]</ref>, (b) non-parameterized frame-level coordinate offsets and (c) non-parameterized unit-level coordinate offsets. The results of temporal action detection are listed in <ref type="table" target="#tab_0">Table 1</ref>. The cascade step K c is set to be 1 for both proposal stage and detection stage. Both C3D feature and Two-stream CNN feature are tested. We can see that all three regression offsets provide improvement over "no regression". Unit-level offsets are more effective than frame-level offsets; we think the reason is that, the features are extracted at unit level, frame-level coordinates contain redundant information, which may make the regression task more difficult. The performance of parameterized coordinate offsets is lower than that of non-parameterized unit-level offsets. We think that the reason is that unlike objects which can be re-scaled in images with camera projection, actions' time spans can not be easily re-scaled in videos, although the time spans of the same action can be varied in different videos. Therefore, "time" itself work as a standard scale for action instances.</p><p>Cascaded boundary regression. We explore the effects of boundary regression cascade. Cascade step K p c and K d c are the number of boundary regression conducted in proposal stage and detection stage respectively. The results are shown in <ref type="table" target="#tab_1">Table 2 and Table 3</ref>. We investigate the cascade step with C3D feature and two-stream CNN feature. Non-parameterized unitlevel coordinate offset is adopted.   For the proposal network (shown in <ref type="table" target="#tab_1">Table 2</ref>), we can see that cascaded boundary regres-sion increase the performance from 42.7 to 45.0 for two-stream features, and from 38.6 to 39.6 for C3D features. When K p c = 3, two-stream CBR achieves the best performance, and after the performance peak, the performance drops slightly. To test the effects of cascaded boundary regression for action detection, we fix K p c = 3. As shown in <ref type="table" target="#tab_3">Table 3</ref>, we observe a similar trend as in proposal generation: when K d c = 2, CBR increases the performance from 28.4 to 31.0 for two-stream features, and from 21.5 to 22.7 for C3D features. After K d c = 2, the performance becomes saturated. Comparison with state-of-the-art on temporal proposal generation. We compare CBR-P with state-of-the-art methods on temporal action proposal generation, including SCNNprop <ref type="bibr" target="#b19">[20]</ref>, DAPs <ref type="bibr" target="#b2">[3]</ref> and Sparse-prop <ref type="bibr" target="#b0">[1]</ref> and TURN <ref type="bibr" target="#b4">[5]</ref>. The results are shown in <ref type="table" target="#tab_4">Table 4</ref>. To fairly compare with TURN, we also provide the performance using only optical flow CNN features, which is the same for TURN-FL. We can see that CBR-FL outperforms state-ofthe-art (TURN-FL) and CBR-TS provides further improvement over CBF-FL.</p><p>Comparison with state-of-the-art on temporal action detection. We compare our method with other state-of-the-art temporal action localization methods on THUMOS-14, the results are shown in <ref type="table" target="#tab_5">Table 5</ref>. We compare with the challenge results <ref type="bibr" target="#b14">[15]</ref>, and recent methods including based on segment window C3D <ref type="bibr" target="#b19">[20]</ref>, score pyramids <ref type="bibr" target="#b27">[28]</ref> and deep recurrent reinforcement learning <ref type="bibr" target="#b26">[27]</ref>. Both SCNN and CBR-C3D are based on C3D features, we can see that CBR-C3D outperforms SCNN at all tIoU thresholds, especially at high tIoU, which shows the effectiveness of CBR. If two-stream features are adopted, CBR outperforms state-of-the-art methods by 12% at tIoU=0.5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments on TVSeries</head><p>We first introduce the datasets and the experiment setup, then discuss the experimental results on TVSeries <ref type="bibr" target="#b1">[2]</ref>. Dataset. The TVSeries Dataset <ref type="bibr" target="#b1">[2]</ref> is a realistic, large-scale dataset for temporal action detection, which contains 16 hours of videos (27 episodes) from six recent popular TV series. 30 daily life action categories are defined in TVSeries, such as "close door", "drive car", "wave". There are totally 6231 action instances annotated with start and end times and action categories in the dataset. The train/validation/test sets contain 13/7/7 episodes respectively.</p><p>Experimental setup. We test the cascaded steps of boundary regression for both proposal generation and action detection, and then compare with state-of-the-art performance on action detection. The unit size u f is 6, the surrounding unit number n ctx is set to 4. The sliding window lengths and overlaps are {12(6), 24(6), 48 <ref type="bibr" target="#b11">(12)</ref>, 72 <ref type="bibr" target="#b17">(18)</ref>, 96 <ref type="bibr" target="#b23">(24)</ref>, 192(48), 384(96)}, where the numbers out of brackets are lengths of sliding window, and the numbers in brackets are the corresponding overlaps of the sliding window.</p><p>Cascaded boundary regression. We explore the effects of cascaded boundary regression on TVSeries. Note that K c is the cascaded step. We investigate the cascaded step with two-stream CNN features. <ref type="table">Table 6</ref>: Comparison of cascaded step K c = 0, 1, 2, 3, 4 for temporal action detection (% mAP@tIoU=0.5) on TVSeries. K c = 0 means that the system only do classification, no boundary regression. As shown in <ref type="table">Table 6</ref>, comparing with K c = 0 and K c = 1, we can see that temporal coordinate regression brings a big improvement, which shows its effectiveness. We can also see that when K c = 3, CBR achieves the best performance for proposal network. To test detection network, we fix K p c = 3, the results show that when K d c = 2 CBR achieves the best performance for action detection. After the peak, the performance starts to decrease. The performance distribution of cascaded step is consistent with THUMOS-14.</p><formula xml:id="formula_9">K c = 0 K c = 1 K c = 2 K c = 3 K c =</formula><p>Comparison with state-of-the-art on action detection. We compare CBR with stateof-the-art performance on TVSeires in <ref type="table" target="#tab_7">Table 7</ref>. Overall, we can see that TVSeries is a more challenging dataset than THUMOS-14. To provide another comparison, we train SVM classifiers based on two-stream features, which is shown in <ref type="table" target="#tab_7">Table 7</ref> as SVM-TS. The SVM classifiers are trained and tested using the same samples as CBR, which are described in Section 3.6; clip-level features are mean-pooled from unit-level features. We can see that with the same features, CBR outperforms the SVM-based classifiers by 3.5% at tIoU=0.2. At tIoU = 0.2, CBR achieves 9.5, while the state-of-the-art method FV <ref type="bibr" target="#b1">[2]</ref> only achieves 4.9. We also report mAP performance at tIoU = 0.1 and tIoU = 0.3, which are 11.0 and 7.9 respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present a novel two-stage action detection pipeline with Cascaded Boundary Regression (CBR), which achieves state-of-the-art performance on standard benchmarks. In the first stage, temporal proposals are generated; based on the proposals, actions are detected in the second stage. Cascaded boundary regression are conducted in both stages. Detailed experiments and analysis on cascaded steps are conducted, which show the effectiveness of CBR for both temporal proposal generation and action detection. Different temporal regression offset settings are also investigated and discussed. State-of-the-art performance has been achieved on both THUMOS-14 and TVSeires dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architecture of two-stage action detection pipeline with Cascaded Boundary Regression (CBR)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>A</head><label></label><figDesc>clip c is composed of units, c = {u j } u s +c u ?1 u s , where u s is the index of starting unit and c u is the number of units inside c. u e = u s + c u ? 1 is the index of ending unit u e , and {u j } u e u s are called internal units of c. Besides the internal units, surrounding units for c are also modeled. {u j } u s ?1 u s ?n ctx and {u j } u e +n ctx u e +1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Unrolled model of Cascaded Boundary Regression (CBR), the parameters of the MLPs are shared.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of different coordinate offsets on action localization (% mAP@tIoU=0.5): parameterized, non-parameterized frame-level, non-parameterized unitlevel. The performance with no boundary regression is also listed.</figDesc><table><row><cell></cell><cell cols="4">no regression parameterized non-para, frame-level non-para, unit-level</cell></row><row><cell>CBR-C3D</cell><cell>16.7</cell><cell>19.4</cell><cell>18.8</cell><cell>20.5</cell></row><row><cell>CBR-TS</cell><cell>22.3</cell><cell>26.1</cell><cell>25.3</cell><cell>27.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Comparison of cascaded step K p c = 1, 2, 3, 4 for temporal proposal generation (%</cell></row><row><cell>AR@F=1.0) on THUMOS-14.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of cascaded step K d c = 1, 2, 3, 4 for temporal action detection (% mAP@tIoU=0.5) on THUMOS-14.</figDesc><table><row><cell></cell><cell cols="4">K d c = 1 K d c = 2 K d c = 3 K d c = 4</cell></row><row><cell>CBR-C3D</cell><cell>21.5</cell><cell>22.7</cell><cell>22.4</cell><cell>22.2</cell></row><row><cell>CBR-TS</cell><cell>28.4</cell><cell>31.0</cell><cell>30.5</cell><cell>30.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison with state-of-the-art on temporal action proposal generation. Average Recall at Proposal Frequency (AR@F=1.0) performance are reported.</figDesc><table><row><cell>Method</cell><cell>Sparse-prop[1]</cell><cell cols="2">DAPs[3] SCNN-prop[20]</cell><cell>TURN-FL [5]</cell><cell cols="2">CBR-FL CBR-TS</cell></row><row><cell>AR@AN=200</cell><cell>32.3</cell><cell>34.1</cell><cell>37.2</cell><cell>42.8</cell><cell>43.5</cell><cell>44.2</cell></row><row><cell>AR@F=1.0</cell><cell>33.3</cell><cell>35.7</cell><cell>38.3</cell><cell>43.5</cell><cell>44.4</cell><cell>45.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Temporal action detection performance (mAP %) comparison at different tIoU thresholds on THUMOS-14.</figDesc><table><row><cell>tIoU</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell></row><row><cell cols="6">Oneata et al.[15] 36.6 33.6 27.0 20.8 14.4</cell><cell>8.5</cell><cell>3.2</cell></row><row><cell>Yeung et al.[27]</cell><cell cols="5">48.9 44.0 36.0 26.4 17.1</cell><cell>-</cell><cell>-</cell></row><row><cell>Yuan et al. [28]</cell><cell cols="5">51.4 42.6 33.6 26.1 18.8</cell><cell>-</cell><cell>-</cell></row><row><cell>S-CNN [20]</cell><cell cols="7">47.7 43.5 36.3 28.7 19.0 10.3 5.3</cell></row><row><cell>CBR-C3D</cell><cell cols="7">48.2 44.3 37.7 30.1 22.7 13.8 7.9</cell></row><row><cell>CBR-TS</cell><cell cols="7">60.1 56.7 50.1 41.3 31.0 19.1 9.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Temporal action detection performance (mAP %) comparison at different tIoU thresholds on TVSeries.</figDesc><table><row><cell cols="6">tIoU CNN [2] LSTM [2] FV [2] SVM-TS CBR-TS</cell></row><row><cell>0.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>7.3</cell><cell>11.0</cell></row><row><cell>0.2</cell><cell>1.1</cell><cell>2.7</cell><cell>4.9</cell><cell>6.0</cell><cell>9.5</cell></row><row><cell>0.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>4.6</cell><cell>7.9</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast temporal activity proposals for efficient detection of human actions in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cees Snoek, and Tinne Tuytelaars. Online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Roeland De Geest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyang</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Daps: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Two-frame motion estimation based on polynomial expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Farneb?ck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scandinavian conference on Image analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06189</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning activity progression in lstms for activity detection and early detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shugao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The lear submission at thumos 2014</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A multistream bi-directional recurrent neural network for fine-grained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Temporal localization of fine-grained actions in videos by domain transfer from web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Cuhk &amp; ethz &amp; siat submission to activitynet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00797</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporal action localization with pyramid of score distribution features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashraf</forename><forename type="middle">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
