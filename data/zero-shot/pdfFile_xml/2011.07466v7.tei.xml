<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Continuous Conditional Generative Adversarial Networks: Novel Empirical Losses and Label Input Mechanisms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Xin</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Yongwei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuheng</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Welch</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Z</forename><forename type="middle">Jane</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">Continuous Conditional Generative Adversarial Networks: Novel Empirical Losses and Label Input Mechanisms</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms CcGAN</term>
					<term>conditional generative modeling</term>
					<term>conditional generative adversarial networks</term>
					<term>continuous and scalar conditions !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper focuses on conditional generative modeling for image data with continuous, scalar conditions (termed regression labels). We propose the first model for this task which is called continuous conditional generative adversarial network (CcGAN). Existing conditional GANs (cGANs) are mainly designed for categorical conditions (e.g., class labels). Conditioning on regression labels is mathematically distinct and raises two fundamental problems: (P1) since there may be very few (even zero) real images for some regression labels, minimizing existing empirical versions of cGAN losses (a.k.a. empirical cGAN losses) often fails in practice; and (P2) since regression labels are scalar and infinitely many, conventional label input mechanisms (e.g., combining a hidden map of the generator/discriminator with a one-hot encoded label) are not applicable. We solve these problems by: (S1) reformulating existing empirical cGAN losses to be appropriate for the continuous scenario; and (S2) proposing a naive label input (NLI) mechanism and an improved label input (ILI) mechanism to incorporate regression labels into the generator and the discriminator. The reformulation in (S1) leads to two novel empirical discriminator losses, termed the hard vicinal discriminator loss (HVDL) and the soft vicinal discriminator loss (SVDL) respectively, and a novel empirical generator loss. Hence, we propose four versions of CcGAN employing different proposed losses and label input mechanisms. The error bounds of the discriminator trained with HVDL and SVDL, respectively, are derived under mild assumptions. To evaluate the performance of CcGANs, two new benchmark datasets (RC-49 and Cell-200) are created. A novel evaluation metric (Sliding Fr?chet Inception Distance) is also proposed to replace Intra-FID when Intra-FID is not applicable. Our extensive experiments on several benchmark datasets (i.e., RC-49, UTKFace, Cell-200, and Steering Angle with both low and high resolutions) support the following findings: the proposed CcGAN is able to generate diverse, high-quality samples from the image distribution conditional on a given regression label; and CcGAN substantially outperforms cGAN both visually and quantitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index Terms</head><p>CcGAN, conditional generative modeling, conditional generative adversarial networks, continuous and scalar conditions. !</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Conditional generative adversarial networks (cGANs), first proposed in [1], aim to estimate the distribution of images conditioning on some auxiliary information, especially class labels. Subsequent studies [2]-[5] confirm the feasibility of generating diverse, high-quality (even photo-realistic), and class-label consistent fake images from class-conditional GANs. Unfortunately, these cGANs are not applicable for conditional generative modeling with continuous, scalar conditions, termed regression labels, due to two problems: (P1) cGANs are often trained to minimize the empirical versions of their losses (a.k.a. empirical cGAN losses) on some training data, a principle also known as empirical risk minimization (ERM) [6]- <ref type="bibr" target="#b7">[8]</ref>. The success of ERM relies on a large sample size for each distinct condition. Unfortunately, we usually have only a few real images for some regression labels. Moreover, since regression labels are continuous, some values may not even appear in the training set. Consequently, a cGAN cannot accurately estimate the image distribution conditional on such missing labels. (P2) In the class-conditional generative modeling, class labels are often encoded by one-hot vectors or label embedding and then fed into the generator and discriminator by hidden concatenation <ref type="bibr" target="#b0">[1]</ref>, an auxiliary classifier [2] or label projection <ref type="bibr" target="#b2">[3]</ref>. A precondition for such label encoding is that the number of distinct labels (e.g., the number of classes) is finite and known. Unfortunately, in the continuous scenario, we may have infinitely many distinct regression labels.</p><p>A naive approach denoted by cGAN (K classes) to solve (P1)-(P2) is to "bin" the regression labels into K disjoint intervals and still train a cGAN in the class-conditional manner (these intervals are treated as independent classes) <ref type="bibr" target="#b8">[9]</ref>. Another naive approach denoted by cGAN (concat) for solving (P2) directly combines a regression label with the input or a hidden map of the generator and discriminator. However, both approaches fails in our empirical studies in Sections 5 and</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>6, representing two types of failures of conventional cGANs: (1) cGAN (K classes) cannot generate visually realistic and diverse images; (2) cGAN (concat) fails to control the image generation by setting regression labels.</p><p>In machine learning, vicinal risk minimization (VRM) <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref> is an alternative rule to ERM. VRM assumes that a sample point shares the same label with other samples in its vicinity. Motivated by VRM, in generative modeling conditional on regression labels where we estimate a conditional distribution p(x|y) (x is an image and y is a regression label), it is natural to assume that a small perturbation to y results in a negligible change to p(x|y). This assumption is consistent with our perception of the world. For example, the image distribution of facial features for a population of <ref type="bibr">15-</ref>year-old teenagers should be close to that of 16-year olds.</p><p>We therefore introduce the continuous conditional GAN (CcGAN) to tackle (P1) and (P2). To our best knowledge, this is the first generative model for image data conditional on regression labels. It is noted that <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b11">[12]</ref> train GANs in an unsupervised manner and synthesize unlabeled fake images for a subsequent image regression task. <ref type="bibr" target="#b12">[13]</ref> proposes a semi-supervised GAN for dense crowd counting. <ref type="bibr" target="#b13">[14]</ref> uses a convolutional neural network (CNN) to generate images of objects in terms of some high-level parameters such as object style, viewpoint, color, brightness, etc. <ref type="bibr" target="#b14">[15]</ref> proposes InfoGAN, which can control some continuous/discrete attributes of generated images. Some text-to-image generation methods <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref> train generative models conditional on high-dimensional attribute vectors with continuous or discrete elements. The objectives of these works are entirely different from ours since they do not aim to estimate the image distribution conditional on regression labels. Moreover, some recent works <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b21">[22]</ref> propose several novel schemes to train GANs when training data are limited, which seems to be relevant to (P1). However, they are also fundamentally different from CcGAN, since they are designed for unconditional and class-conditional scenarios rather than continuous ones. Our contributions can be summarized as follows:</p><p>? We propose in Section 2.1 a solution to address (P1), which consists of two novel empirical discriminator losses, termed the hard vicinal discriminator loss (HVDL) and the soft vicinal discriminator loss (SVDL), and a novel empirical generator loss. We take the vanilla cGAN loss as an example to show how to derive HVDL, SVDL, and the novel empirical generator loss by reformulating existing empirical cGAN losses. ? In Section 2.2, we propose two novel label input mechanisms, consisting of a naive label input (NLI) mechanism and an improved label input (ILI) mechanism, as solutions to address (P2). <ref type="bibr">?</ref> We derive in Section 3 the error bounds of a discriminator trained with HVDL and SVDL. These error bounds not only help us understand how HVDL and SVDL influence the discriminator training but also guide our implementation in practice (especially the selection of hyper-parameters). <ref type="bibr">?</ref> We propose in Section 4 a novel evaluation metric, termed Sliding Fr?chet Inception Distance (SFID), to evaluate the generative image modeling conditional on regression labels when there are insufficient real images to compute Intra-FID <ref type="bibr" target="#b2">[3]</ref>. ? In Sections 5 and 6, we propose two new benchmark datasets, RC-49 and Cell-200, for generative image modeling conditional on regression labels, since very few benchmark datasets are suitable for the studied continuous scenario. We conduct extensive experiments on four benchmark datasets with various resolutions (from 64 ? 64 to 256 ? 256) to demonstrate that CcGAN not only generates diverse, high-quality, and label consistent images, but also substantially outperforms cGAN both visually and quantitatively. The effectiveness of SFID is also studied on the RC-49 dataset at the end of Section 5. A preliminary version of this paper has been presented in International Conference on Learning Representations <ref type="bibr" target="#b22">[23]</ref>. This paper improves the initial version from several aspects to strengthen our method. <ref type="bibr" target="#b0">(1)</ref> We propose an improved label input (ILI) mechanism to better incorporate regression labels into CcGAN. Our experiments in this paper demonstrate the superiority of ILI to the naive label input method in <ref type="bibr" target="#b22">[23]</ref>. <ref type="bibr" target="#b1">(2)</ref> We introduce Lemmas 1 and 2, which are used to derive the error bounds of HVDL and SVDL but are omitted in <ref type="bibr" target="#b22">[23]</ref>. The motivation of deriving these error bounds are also better illustrated, and an improved proof for the derivation is provided in Appendix. <ref type="bibr" target="#b2">(3)</ref> We propose SFID to replace Intra-FID when Intra-FID is not applicable. <ref type="bibr" target="#b3">(4)</ref> We create a new benchmark dataset called Cell-200 for generaive modeling conditional on regression labels. <ref type="bibr" target="#b4">(5)</ref> We conduct a more extensive empirical study to demonstrate the effectiveness of CcGAN. This extensive study includes more datasets (e.g., Cell-200 and Steering Angle) and more complicated settings (e.g., various image resolutions). We also add a new baseline, cGAN (concat), to the comparison to better demonstrate that conventional cGANs are inapplicable in our task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">FROM CGAN TO CCGAN</head><p>In this section, we introduce the continuous conditional GAN (CcGAN), consisting of solutions to (P1) and (P2). The combinations of two vicinal discriminator losses (HVDL and SVDL) proposed in Section 2.1 and two novel label input mechanisms (NLI and ILI) proposed in Section 2.2 result in four CcGAN methods denoted by HVDL+NLI, SVDL+NLI, HVDL+ILI, and SVDL+ILI, respectively. The overall workflow of CcGAN is visualized in <ref type="figure" target="#fig_8">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Solution to (P1): Reformulated Empirical Losses</head><p>Theoretically, cGAN losses (e.g., the vanilla cGAN loss <ref type="bibr" target="#b0">[1]</ref>, the Wasserstein loss <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, and the hinge loss <ref type="bibr" target="#b23">[24]</ref>) are suitable for both class labels and regression labels; however, their empirical versions fail in the continuous scenario (i.e., (P1)). Our <ref type="figure" target="#fig_8">Fig. 1</ref>: A typical workflow of the proposed CcGAN framework. A regression label y is input into the generator (G) and the discriminator (D) by novel label input mechanisms proposed in Section 2.2. Novel empirical losses proposed in Section 2.1 are adopted to optimize G and D, respectively. The CcGAN framework is compatible with modern GAN architectures (e.g., SNGAN <ref type="bibr" target="#b23">[24]</ref>, SAGAN <ref type="bibr" target="#b4">[5]</ref>) and advanced GAN training techniques (e.g., DiffAugment <ref type="bibr" target="#b18">[19]</ref>).</p><p>first solution (S1) focuses on reformulating these empirical cGAN losses for continuous labels. Without loss of generality, we only take the vanilla cGAN loss as an example to show such reformulation (the empirical versions of the Wasserstein loss and the hinge loss can be reformulated similarly).</p><p>The vanilla discriminator loss and generator loss <ref type="bibr" target="#b0">[1]</ref> are defined as: = ? log(D(G(z, y), y))q(z)p g (y)dzdy,</p><formula xml:id="formula_0">L(D) = ? E</formula><p>where x ? X is an image, y ? Y is a label, p r (y) and p g (y) are respectively the true and fake label marginal distributions, p r (x|y) and p g (x|y) are respectively the true and fake image distributions conditional on y, p r (x, y) and p g (x, y) are respectively the true and fake joint distributions of x and y, and q(z) is the probability density function of N (0, I).</p><p>Since the distributions in the losses of Eqs. <ref type="bibr" target="#b0">(1)</ref> and <ref type="bibr" target="#b1">(2)</ref> are unknown, for class-conditional generative modeling, <ref type="bibr" target="#b0">[1]</ref> follows ERM and minimizes the empirical losses: </p><p>where C is the number of classes, N r and N g are respectively the number of real and fake images, N r c and N g c are respectively the number of real and fake images with label c, x r c,j and x g c,j are respectively the j-th real image and the j-th fake image with label c, and the z c,j are independently and identically sampled from q(z). Eq. (3) implies we estimate p r (x, y) and p g (x, y) by their empirical probability density functions as follows: </p><p>where ?(?) is a Dirac delta function (Appendix A of <ref type="bibr" target="#b26">[27]</ref>) centered at 0. However,p ? r (x, y) andp ? g (x, y) in Eq. <ref type="bibr" target="#b4">(5)</ref> are not good estimates in the continuous scenario because of (P1).</p><p>To overcome (P1), we propose a novel estimate for each of p r (x, y) and p g (x, y), termed the hard vicinal estimate (HVE). We also provide an intuitive alternative to HVE, named the soft vicinal estimate (SVE). The HVEs of p r (x, y) and p g (x, y) are:</p><formula xml:id="formula_4">p HVE r (x, y) =C 1 ? ? ? 1 N r N r j=1 exp ? (y ? y r j ) 2 2? 2 ? ? ? 1 N r y,? N r i=1 1 {|y?y r i |??} ?(x ? x r i ) ,<label>(6)</label></formula><formula xml:id="formula_5">p HVE g (x, y) =C 2 ? ? ? 1 N g N g j=1 exp ? (y ? y g j ) 2 2? 2 ? ? ? 1 N g y,? N g i=1 1 {|y?y g i |??} ?(x ? x g i ) ,<label>(7)</label></formula><p>where x r i and x g i are respectively real image i and fake image i, y r i and y g i are respectively the labels of x r i and x g i , ? and ? are two positive hyper-parameters, C 1 and C 2 are two constants making these two estimates valid probability density functions, N r y,? is the number of the y r i satisfying |y ? y r i | ? ?, N g y,? is the number of the y g i satisfying |y ? y g i | ? ?, and 1</p><p>is an indicator function with support in the subscript. The terms in the first square brackets ofp HVE r andp HVE g imply we estimate the marginal label distributions p r (y) and p g (y) by kernel density estimates (KDEs) <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b30">[31]</ref>. The terms in the second square brackets are designed based on the assumption that a small perturbation to y results in negligible changes to p r (x|y) and p g (x|y). If this assumption holds, we can use images with labels in a small vicinity of y to estimate p r (x|y) and p g (x|y). The SVEs of p r (x, y) and p g (x, y) are:</p><formula xml:id="formula_6">p SVE r (x, y) =C 3 ? ? ? 1 N r N r j=1 exp ? (y ? y r j ) 2 2? 2 ? ? ? N r i=1 w r (y r i , y)?(x ? x r i ) N r i=1 w r (y r i , y) ,<label>(8)</label></formula><formula xml:id="formula_7">p SVE g (x, y) =C 4 ? ? ? 1 N g N g j=1 exp ? (y ? y g j ) 2 2? 2 ? ? ? N g i=1 w g (y g i , y)?(x ? x g i ) N g i=1 w g (y g i , y) ,<label>(9)</label></formula><p>where C 3 and C 4 are two constants making these two estimates valid probability density functions,</p><formula xml:id="formula_8">w r (y r i , y) = e ??(y r i ?y) 2 and w g (y g i , y) = e ??(y g i ?y) 2 ,<label>(10)</label></formula><p>and the hyper-parameter ? &gt; 0. In Eqs. <ref type="bibr" target="#b7">(8)</ref> and <ref type="formula" target="#formula_7">(9)</ref>, similar to the HVEs, we estimate p r (y) and p g (y) by KDEs. Instead of using samples in a hard vicinity, the SVEs use all respective samples to estimate p r (x|y) and p g (x|y) but each sample is assigned with a weight based on the distance of its label from y. Two diagrams in <ref type="figure" target="#fig_3">Fig. 2</ref> visualize the process of using hard/soft vicinal samples to estimate the Gaussian distribution p(x|y) conditional on y, for univariate x.  <ref type="formula" target="#formula_5">(7)</ref>) and SVE (Eqs. <ref type="bibr" target="#b7">(8)</ref> and <ref type="formula" target="#formula_7">(9)</ref>) estimate p(x|y) (a univariate Gaussian conditional on y) by using two samples in hard and soft vicinities, respectively, of y. To estimate p(x|y) (the red Gaussian curve) only from samples drawn from p(x|y1) and p(x|y2) (the blue Gaussian curves), estimation is based on the samples (red dots) in a hard vicinity (defined by y ? ?) or a soft vicinity (defined by the weight decay curve) around y. The histograms in blue are samples in the hard or soft vicinity. The labels y1, y, and y2 on the x-axis denote the means of x conditional on y1, y, and y2, respectively.</p><p>By plugging Eq. (6), <ref type="bibr" target="#b6">(7)</ref>, <ref type="bibr" target="#b7">(8)</ref>, and (9) into Eq. (1), we derive the hard vicinal discriminator loss (HVDL) and the soft vicinal discriminator loss (SVDL) as follows:</p><formula xml:id="formula_9">L HVDL (D) = ? C 5 N r N r j=1 N r i=1 E r ?N (0,? 2 ) W 1 log(D(x r i , y r j + r )) ? C 6 N g N g j=1 N g i=1 E g ?N (0,? 2 ) W 2 log(1 ? D(x g i , y g j + g )) ,<label>(11)</label></formula><formula xml:id="formula_10">L SVDL (D) = ? C 7 N r N r j=1 N r i=1 E r ?N (0,? 2 ) W 3 log(D(x r i , y r j + r )) ? C 8 N g N g j=1 N g i=1 E g ?N (0,? 2 ) W 4 log(1 ? D(x g i , y g j + g )) ,<label>(12)</label></formula><p>where r y ? y r j , g y ? y g j ,</p><formula xml:id="formula_11">W 1 = 1 {|y r j + r ?y r i |??} N r y r j + r ,? , W 2 = 1 {|y g j + g ?y g i |??} N g y g j + g ,? W 3 = w r (y r i , y r j + r ) N r i=1 w r (y r i , y r j + r ) , W 4 = w g (y g i , y g j + g ) N g i=1 w g (y g i , y g j + g )</formula><p>, and C 5 , C 6 , C 7 , and C 8 are some constants.</p><p>Generator training: The generator of CcGAN is trained by minimizing Eq. <ref type="formula" target="#formula_12">(13)</ref>,</p><formula xml:id="formula_12">L (G) = ? 1 N g N g i=1 E g ?N (0,? 2 ) log(D(G(z i , y g i + g ), y g i + g )).<label>(13)</label></formula><p>How do HVDL, SVDL, and Eq. (13) overcome (P1)? (i) Given a label y as the condition, we use images in a hard/soft vicinity of y to train the discriminator instead of just using images with label y. It enables us to estimate p r (x|y) when there are not enough real images with label y. (ii) From Eqs. <ref type="bibr" target="#b10">(11)</ref> and <ref type="formula" target="#formula_1">(12)</ref>, we can see that the KDEs in Eqs. (6), <ref type="bibr" target="#b6">(7)</ref>, <ref type="bibr" target="#b7">(8)</ref>, and (9) are adjusted by adding Gaussian noise to the labels. Moreover, in Eq. (13), we add Gaussian noise to seen labels (assume the y g i are seen) to train the generator to generate images at unseen labels. This enables estimation of p r (x|y ) when y is not in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 1.</head><p>Based on the kernel density estimation <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b30">[31]</ref> and the property of the Dirac delta function (Appendix A of <ref type="bibr" target="#b26">[27]</ref>), ?(x ? x r i )dx = ?(x ? x g i )dx = 1 and C 1 = C 2 = C 3 = C 4 = 1/?. Therefore, C 5 = C 6 and C 7 = C 8 , which implies these constants C 1 , . . . , C 8 can be ignored when minimizing L HVDL (D) and L SVDL (D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 2.</head><p>An algorithm is proposed in Supp. S.2 for training CcGAN in practice. Moreover, CcGAN does not require any specific network architecture, so it can use modern GAN architectures such as SNGAN <ref type="bibr" target="#b23">[24]</ref>, SAGAN <ref type="bibr" target="#b4">[5]</ref> and BigGAN <ref type="bibr" target="#b3">[4]</ref>. CcGAN is also compatible with modern GAN training techniques such as DiffAugment <ref type="bibr" target="#b18">[19]</ref>.</p><p>Remark 3 (A rule of thumb for hyper-parameter selection). In our experiments, we normalize labels to real numbers in [0, 1] and the hyper-parameter selection is conducted based on the normalized labels. To be more specific, the hyper-parameter ? is computed based on a rule of thumb formula for the bandwidth selection of KDE <ref type="bibr" target="#b29">[30]</ref>, i.e., ? = 4? 5 y r /3N r 1/5 , where? y r is the sample standard deviation of normalized labels in the training set. Let</p><formula xml:id="formula_13">? base = max y r [2] ? y r [1] , y r [3] ? y r [2] , . . . , y r [N r uy ] ? y r [N r uy ?1] , where y r [l]</formula><p>is the l-th smallest normalized distinct real label and N r uy is the number of normalized distinct labels in the training set. The ? is set as a multiple of ? base (i.e., ? = m ? ? base ) where the multiplier m ? stands for 50% of the minimum number of neighboring labels used for estimating p r (x|y) given a label y. For example, m ? = 1 implies using 2 neighboring labels (one on the left while the other one on the right). In our experiments, m ? is generally set as 1 or 2. In some extreme case when many distinct labels have too few real samples, we may consider increasing m ? . We also found ? = 1/? 2 works well in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Solutions to (P2): Novel Label Input Mechanisms</head><p>In this section, we propose two solutions, consisting of a naive and an improved label input mechanism, to solve (P2). A naive label input (NLI) mechanism: We first propose a naive approach to incorporate the regression labels into the cGANs. For G, we add the label y element-wisely to the output of its first linear layer. For D, the label y is first projected to the latent space learned by an extra linear layer. Then, we incorporate the embedded label into the discriminator by label projection <ref type="bibr" target="#b2">[3]</ref>. <ref type="figure" target="#fig_4">Fig. 3</ref> visualizes the naive label input mechanism. An improved label input (ILI) mechanism: Empirical studies in Section 5 show that CcGAN with the naive label input mechanism already substantially outperforms cGAN. Nevertheless, it still suffers from severe label inconsistency on some datasets (e.g., Cell-200 and Steering Angle). To improve the label consistency of CcGAN, we propose an improved label input (ILI) mechanism. The ILI approach consists of a pre-trained CNN and a label embedding network. The pre-trained CNN, as shown in <ref type="figure" target="#fig_5">Fig. 4</ref>, includes two subnetworks, T 1 and T 2 , where T 1 maps an image x to a feature space and T 2 maps the extracted feature h to a regression label y. The dimension of the feature space is set to 128 in our experiments. The label embedding network T 3 , as shown in <ref type="figure" target="#fig_6">Fig. 5</ref>, is a multilayer perceptron (MLP) <ref type="bibr" target="#b30">[31]</ref> mapping a regression label y back to its hidden representation h in the feature space defined by T 1 . Assume that there are m distinct regression labels in the training set, i.e., y u 1 , y u 2 , . . . , y u m , then the label embedding network T 3 is trained by:</p><formula xml:id="formula_14">min T3 1 m E ??N (0,? 2 ? ) (T 2 (T 3 (y u i + ?)) ? (y u i + ?)) 2 ,<label>(14)</label></formula><p>where ? ? is often a small value and is set at 0.2 in this paper. Then, given a regression label y, we can evaluate T 3 (y) to get a unique hidden representation of y which will be incorporated into CcGAN as the condition (visualized in <ref type="figure" target="#fig_7">Fig. 6</ref>). Specifically, for G, we input the embedded label by using conditional batch normalization <ref type="bibr" target="#b31">[32]</ref>. For D, similar to the naive approach, we input the embedded label into D by label projection <ref type="bibr" target="#b2">[3]</ref>. The workflow of the naive label input (NLI) mechanism. NLI inputs a regression label y into G by adding y element-wisely to the output of the first linear layer. NLI inputs y into D by label projection <ref type="bibr" target="#b2">[3]</ref>.   The workflow of the improved label input (ILI) mechanism. ILI first uses an embedding network to convert y into its high-dimensional representation h. Then, h is input into G and D by conditional batch normalization <ref type="bibr" target="#b31">[32]</ref> and label projection <ref type="bibr" target="#b2">[3]</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ERROR BOUNDS OF D TRAINED WITH HVDL AND SVDL</head><p>In this section, we derive the error bounds of a discriminator D trained with L HVDL (D) and L SVDL (D) under the theoretical discriminator loss L(D). Denote by D * the optimal discriminator <ref type="bibr" target="#b32">[33]</ref> which minimizes L(D). Let D HVDL arg min D?D L HVDL (D); similarly, we define D SVDL . We are interested in an reasonable bound (i.e, error bound) of the distance of D HVDL and D SVDL from D * under L(D), i.e., L( D HVDL ) ? L(D * ) and L( D SVDL ) ? L(D * ). These error bounds theoretically illustrate how HVDL and SVDL influence the discriminator training, which can guide our implementation of HVDL and SVDL in practice such as the selection of ? and ?. Before we move to the derivation, without loss of generality, we first assume y ? [0, 1]. Then, we introduce some notations. Let D stand for the Hypothesis Space of D. Please note that D may not cover D * . Letp KDE r (y) andp KDE g (y) stand for the KDEs of p r (y) and p g (y) respectively. Let p r w (y |y)</p><formula xml:id="formula_15">w r (y ,y)p r (y ) W r (y)</formula><p>, p g w (y |y)</p><formula xml:id="formula_16">w g (y ,y)p g (y ) W g (y)</formula><p>, W r (y) w r (y , y)p r (y )dy and W g (y)</p><p>w g (y , y)p g (y )dy . </p><formula xml:id="formula_17">?(L) p : ?t 1 , t 2 ? Y, ?L &gt; 0, s.t. |p (t 1 ) ? p (t 2 )| |t 1 ? t 2 | ? L .<label>(15)</label></formula><p>Please see Supp. S.3 for more details of these notations. Moreover, we will also work with the following assumptions: (A1) All D's in D are measurable and uniformly bounded.</p><formula xml:id="formula_18">Let U max{sup D?D [? log D] , sup D?D [? log(1 ? D)]} and U &lt; ?; (A2) For ?x ? X and y, y ? Y, ?g r (x) &gt; 0 and M r &gt; 0, s.t. |p r (x|y ) ? p r (x|y)| ? g r (x)|y ? y| with g r (x)dx = M r ; (A3) For ?x ? X and y, y ? Y, ?g g (x) &gt; 0 and M g &gt; 0, s.t. |p g (x|y ) ? p g (x|y)| ? g g (x)|y ? y| with g g (x)dx = M g ; (A4) p r (y) ? ?(L r ) and p g (y) ? ?(L g ).</formula><p>With these definitions and assumptions, we derive two lemmas based on which we derive the error bounds of a discriminator trained by using HVDL and SVDL in Theorems 1 and 2.</p><p>Lemma 1 (Lemma for HVDL). Suppose that (A1)-(A2) and (A4) hold, then ?? ? (0, 1), with probability at least 1 ? ?,</p><formula xml:id="formula_19">sup D?D 1 N y,? N i=1 1 {|y?yi|??} [? log D(x i , y)] ? E x?p(x|y) [? log D(x, y)] ? U 1 2N y,? log 2 ? + ?U M,<label>(16)</label></formula><p>for a fixed y. If image-label pairs are real, then N = N r , N y,? = N r y,? , p = p r , and M = M r . Similarly, we have N = N g , N y,? = N g y,? , p = p g , and M = M g for fake image-label pairs. Lemma 2 (Lemma for SVDL). Suppose that (A1), (A2) and (A4) hold, then ?? ? (0, 1), with probability at least 1 ? ?,</p><formula xml:id="formula_20">sup D?D 1 N N i=1 w(y i , y) [? log D(x i , y)] 1 N N i=1 w(y i , y) ? E x?p(x|y) [? log D(x, y)] ? 2U W (y) 1 2N log 4 ? + U M E y ?pw(y |y) [|y ? y|] ,<label>(17)</label></formula><p>for a fixed y. If image-label pairs are real, then N = N r , N y,? = N r y,? , p = p r , p w = p r w , w = w r , W = W r , and M = M r . Similarly, we have N = N g , N y,? = N g y,? , p = p g , p w = p g w , w = w g , W = W g , and M = M g for fake image-label pairs. Theorem 1 (Error Bound of D trained with HVDL). Assume that (A1)-(A4) hold, then ?? ? (0, 1), with probability at least 1 ? ?,</p><formula xml:id="formula_21">L( D HVDL ) ? L(D * ) ? 2U ? ? C KDE 1,? log N r N r ? + L r ? 2 ? ? + 2U ? ? C KDE 2,? log N g N g ? + L g ? 2 ? ? + 2?U (M r + M g ) + 2U 1 2 log 8 ? E y?p KDE r (y) 1 N r y,? +E y?p KDE g (y) 1 N g y,? + L( D) ? L(D * ),<label>(18)</label></formula><p>for some constants C KDE 1,? , C KDE 2,? depending on ?.</p><p>Theorem 2 (Error Bound of D trained with SVDL). Assume that (A1)-(A4) hold, then ?? ? (0, 1), with probability at least 1 ? ?,</p><formula xml:id="formula_22">L( D SVDL ) ? L(D * ) ? 2U ? ? C KDE 1,? log N r N r ? + L r ? 2 ? ? + 2U ? ? C KDE 2,? log N g N g ? + L g ? 2 ? ? + 4U 1 2 log 16 ? 1 ? N r E y?p KDE r (y) 1 W r (y) + 1 ? N g E y?p KDE g (y) 1 W g (y) + 2U M r E y?p KDE r (y) E y ?p r w (y |y) |y ? y| +M g E y?p KDE g (y) E y ?p g w (y |y) |y ? y| + L( D) ? L(D * ),<label>(19)</label></formula><p>for some constant C KDE 1,? , C KDE 2,? depending on ?. Remark 4. Please see Supp. S.3 for the proofs to these lemmas and theorems.</p><p>Remark 5 (Illustration of Theorems 1 and 2). Both theorems imply HVDL and SVDL perform well if the output of D is not too close to 0 or 1 (i.e., favor small U ). The first two terms in both upper bounds control the quality of KDE, which implies KDE works better if we have a large N r and a large N g but a small ?. The rest terms of the two bounds are different. In the HVDL case, we favor small ?, M r , and M g . However, we should avoid setting ? for a too small value because we prefer large N r y,? and N g y,? . In the SVDL case, we prefer small M r and M g but large W r (y) and W g (y). Large W r (y) and W g (y) imply that the weight function decays slowly (i.e., small ?). However, we should avoid setting ? too small because a small ? leads to large E y ?p r w (y |y) |y ? y| and E y ?p g w (y |y) |y ? y| (i.e., large weights for y 's which are far away from y). The rule of thumb formulae to select ? and ? in Remark 3 are consistent with our analysis here. Besides the rule of thumb formulae, as a future work, a more refined hyper-parameter selection method should be proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SLIDING FR?CHET INCEPTION DISTANCE</head><p>A conditional GAN (no matter the type of the condition) needs to be evaluated from three perspectives <ref type="bibr" target="#b33">[34]</ref>: (1) the visual quality, (2) the intra-label diversity (the diversity of fake images with the same label), and (3) the label consistency (whether assigned labels of fake images are consistent with their true labels). Measuring the performance of cGANs from these three perspectives is often conducted by using a popular overall metric, termed the Intra-FID <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b33">[34]</ref>. Intra-FID computes the Fr?chet inception distance (FID) <ref type="bibr" target="#b34">[35]</ref> separately at each of the distinct labels and reports the average FID score. Intra-FID is also used in our experiments on RC-49, UTKFace, and Cell-200 in Section 5; however, Intra-FID is not reliable or is even inapplicable when we have very few (even zero) real images for some distinct regression labels, e.g., the experiment on the Steering Angle dataset in Section 5.4. We therefore propose a novel metric, termed the Sliding Fr?chet Inception Distance (SFID), to replace Intra-FID in this scenario. SFID computes FID within an interval sliding on the range of the regression label y, and then reports the average of these FIDs. Specifically, we first prespecify a finite set of SFID centers c SFID evenly over the range of y and a constant SFID radius r SFID . Then, based on the c SFID and r SFID , we can define many joint SFID intervals of the form [c SFID ? r SFID , c SFID + r SFID ]. For each SFID interval, we compute FID between real and fake images with labels within this interval. Finally, SFID reports the average of these FIDs. Usually, we also report the standard deviation of these FIDs. We visualize the procedure for computing SFID in <ref type="figure">Fig. 7</ref>. A pseudo code for computing SFID is shown in Alg. 1. Similar to Intra-FID, a small SFID is preferred. <ref type="figure">Fig. 7</ref>: Sliding Fr?chet Inception Distance (SFID). We preset finite centers (blue dots) on y's range evenly and a radius r SFID . Given an interval [c SFID ? r SFID , c SFID + r SFID ], we compute FID between fake and real images with labels in this interval. SFID is the average of these FIDs.</p><p>Algorithm 1: An algorithm to compute the Sliding Fr?chet Inception Distance (SFID). Data: Real image-label pairs D r = {(x r 1 , y r 1 ), ? ? ? , (x r N r , y r N r )}; fake image-label pairs D g = {(x g 1 , y g 1 ), ? ? ? , (x g N g , y g N g )}; preset SFID centers {y c 1 , ? ? ? , y c N c }; preset window radius r SF ID . Result: SF ID. <ref type="bibr" target="#b0">1</ref> Initialize real and fake image sets in l-th sliding-window, i.e., D r l = ? and D g l = ?, l = 1, 2, ? ? ? , N c ; 2 Initialize a FID array F ID(l) = ?, l = 1, 2, ? ? ? , N c ; 3 for l = 1 to N c do <ref type="bibr" target="#b3">4</ref> Update real image set at y c l : D r l = |y r i ?y c l |?r SF ID {x r i }, i = 1, 2, ? ? ? , N r ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5</head><p>Update fake image set at y c l : D g l = |y g i ?y c l |?r SF ID {x g i }, i = 1, 2, ? ? ? , N g ;</p><formula xml:id="formula_23">6 Compute current FID: F ID(l) = F ID(D r l , D g l ) ; 7 end 8 Compute SF ID = 1 N c N c l=1 F ID(l).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">LOW-RESOLUTION EXPERIMENTS</head><p>In this section, we study the effectiveness of CcGAN on four image datasets with resolution 64 ? 64 where cGAN (K classes) and cGAN (concat) cannot generate realistic samples. For a fair comparison, all candidate methods use the same network architecture (a customized DCGAN <ref type="bibr" target="#b35">[36]</ref> architecture for Cell-200 and the SNGAN <ref type="bibr" target="#b23">[24]</ref> architecture for the remaining datasets) except for the label input modules. The four CcGAN methods (i.e., HVDL+NLI, SVDL+NLI, HVDL+ILI, and SVDL+ILI) are tested in all our experiments below. For stability, regression labels in all datasets are normalized to [0, 1] during training.</p><p>The proposed SFID (see Section 4) is only used in the experiment conducted on the Steering Angle dataset in Section 5.4, where there are not enough real images to compute Intra-FID. The effectiveness of SFID is studied on the RC-49 dataset in Section 5.5, where we can control the sample size of real images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">RC-49</head><p>Since most benchmark datasets in the GAN literature do not have continuous, scalar regression labels, we propose a new benchmark dataset-RC-49, a synthetic dataset created by rendering 49 3-D chair models at different yaw angles. Each of 49 chair models is rendered at 899 yaw angles ranging from 0.1 ? to 89.9 ? with step size 0. When training cGAN (K classes), we divide [0.1 ? , 89.9 ? ] into 150 equal intervals where each interval is treated as a class. When training CcGAN, we use the rule of thumb formulae in Remark 3 to select the three hyper-parameters of HVDL and SVDL, i.e., ? ? 0.047, ? ? 0.004 and ? = 50625. The two novel label input mechanisms for CcGAN (NLI and ILI) are implemented in this experiment. For ILI, we pre-train a modified ResNet-34 <ref type="bibr" target="#b36">[37]</ref> with 3 linear layers after the average pooling layer and we only keep the last linear layer for label embedding (i.e., the T 1 + T 2 in <ref type="figure" target="#fig_5">Fig. 4</ref>). We use a five-layer MLP with 128 nodes in each layer to convert an angle into its hidden representation (i.e., the T 3 in <ref type="figure" target="#fig_6">Fig. 5</ref>). All candidates are trained for 30,000 iterations with batch size 256. Afterwards, we evaluate the trained GANs on all 899 angles by generating 200 fake images for each angle. Please see Supp. S.4 for the network architectures and more details about the training/testing setup. Quantitative and visual results: To evaluate (1) the visual quality, (2) the intra-label diversity, and (3) the label consistency of fake images, we study an overall metric and three separate metrics here. (i) Intra-FID <ref type="bibr" target="#b2">[3]</ref> is utilized as the overall metric. It computes FID <ref type="bibr" target="#b34">[35]</ref> separately at each of the 899 evaluation angles and reports the average FID score along with the standard deviation of these 899 FIDs. (ii) Naturalness Image Quality Evaluator (NIQE) <ref type="bibr" target="#b37">[38]</ref> measures the visual quality only. (iii) Diversity is the average entropy of predicted chair types of fake images over evaluation angles. (iv) Label Score is the average absolute error between assigned angles and predicted angles. Please see Supp. S.4.5 for details of these metrics.</p><p>We report in <ref type="table" target="#tab_0">Table 1</ref>   <ref type="figure" target="#fig_11">Fig. 8</ref> support the quantitative results. cGAN (150 classes) often generates unrealistic, identical images for a target angle (i.e., low visual quality and low intra-label diversity). "Binning" [0.1 ? , 89.9 ? ] into other number of classes (e.g., 90 classes and 210 classes) is also tried but does not improve cGAN's performance. cGAN (concat) has good visual quality and high intra-label diversity but terrible label consistency. In contrast, four CcGAN methods performs well in all three perspectives, i.e., good visual quality, high intra-label diversity, and high label consistency. Moreover, both ILI-based CcGANs outperform the two NLI-based CcGANs in terms of all four metrics, especially the label score. Extra experimental results: To test cGAN and CcGAN under more challenging scenarios, we vary the sample size for each distinct angle in the training set from 45 to 5. We visualize the line graphs of Intra-FID versus the sample size for each distinct training angle in <ref type="figure" target="#fig_12">Fig. 9</ref>. From this figure, we can see the four CcGAN methods substantially outperform two cGANs and ILI performs better than NLI no matter what is the sample size for each distinct angle in the training set. The overall trend in this figure also shows that smaller sample size reduces the performance of both cGAN and CcGAN.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">UTKFace</head><p>In this section, we compare CcGAN and cGAN on UTKFace <ref type="bibr" target="#b38">[39]</ref>, a dataset consisting of RGB images of human faces which are labeled by age. Experimental setup: In this experiment, we only use images with age in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">60]</ref>. Some images with bad visual quality and watermarks are also discarded. After the preprocessing, 14,760 images are left. The number of images for each age ranges from 50 to 1051. We resize all selected images to 64 ? 64. Some example UTKFace images are shown in the first image array in <ref type="figure">Fig. S</ref>.5.7. When implementing cGAN (K classes), each age is treated as a class. For CcGAN we use the rule of thumb formulae in Remark 3 to select the three hyper-parameters of HVDL and SVDL, i.e., ? ? 0.041, ? ? 0.017 and ? = 3600. Similar to the RC-49 experiment, we use NLI and ILI to incorporate ages into CcGAN. All GANs are trained for 40,000 iterations with batch size 512. In testing, we generate 1,000 fake images from each trained GAN for each age. Please see Supp. S.5 for more details of the data preprocessing, network architectures and training/testing setup. Quantitative and visual results: Similar to the RC-49 experiment, we evaluate the quality of fake images by Intra-FID, NIQE, Diversity (entropy of predicted races), and Label Score. We report in <ref type="table" target="#tab_1">Table 2</ref> the average quality of 60,000 fake images. From this table, we can see the four CcGAN methods substantially outperform both cGANs and ILI performs better than NLI. Notably, although cGAN (concat) has the highest Diversity score, the huge label score reveals that cGAN (concat) cannot control the image generation with respective to ages. Thus, cGAN (concat) fails in this experiment. We also show in <ref type="figure">Fig.</ref> S.5.7 some example fake images from candidate models and line graphs of FID/NIQE/Diversity/Lable Score versus Age in <ref type="figure" target="#fig_8">Fig. 10</ref>. Analogous to the quantitative comparisons, we can see that CcGAN performs much better than cGAN. Note that, we do not restrict the maximum sample size in the main study. Since we have a much smaller sample size, we reduce the number of iterations for the GAN training from 40,000 to 20,000 and slightly increase m ? in Remark 3 from 1 to 2 (we therefore use a wider hard/soft vicinity). We visualize the line graphs of Intra-FID versus the maximum sample size for each age of cGAN and CcGAN in <ref type="figure" target="#fig_8">Fig. 11</ref>. From the figure, we can clearly see that a smaller sample size worsens the performance of both cGAN and CcGAN. Moreover,  <ref type="figure">Fig. (d)</ref> also shows that the ILI-based CcGANs have higher label consistency than the NLI-based CcGANs. <ref type="figure" target="#fig_8">Fig. 11</ref>: Line graphs of Intra-FID versus the maximum sample size for each distinct age in the training set of UTKFace. <ref type="figure" target="#fig_8">Fig. 11</ref> shows that four CcGAN methods perform much better than cGAN and ILI is better than NLI. Moreover, a smaller sample size deteriorates the performance of both cGAN and CcGAN.</p><p>the Intra-FID scores of two cGANs often stay at a high level and are larger than those of the four CcGAN methods. The ILI-based CcGANs are also better than the NLI-based CcGANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Cell-200</head><p>In addition to RC-49, we propose another benchmark dataset-Cell-200, a dataset of synthetic fluorescence microscopy images with cell populations generated by SIMCEP <ref type="bibr" target="#b39">[40]</ref>. Please see Supp. S.6.1 for more details about the data generation. Some example images are shown in <ref type="figure">Fig</ref> When training cGAN (K classes), we divide <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">200]</ref> into 100 equal intervals where each interval is treated as a class (i.e., K = 100). We use the rule of thumb formulae in Remark 3 to select the three hyperparameters of HVDL and SVDL, i.e., ? ? 0.077, ? ? 0.020 and ? = 2500. Both cGAN and CcGAN are trained for 5,000 iterations. Afterwards, we evaluate the trained GANs on all 200 cell counts by generating 1,000 fake images for each count. Please see Supp. S.6 for the network architectures and more details about the training/testing setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative and visual results:</head><p>We evaluate the quality of fake images by Intra-FID, NIQE, and Label Score. Please note that the Diversity score is not available in this experiment because there is no class label in Cell-200. We report in <ref type="table" target="#tab_2">Table 3</ref> the average quality of 200,000 fake images from cGAN and CcGAN. We also show in <ref type="figure">Fig</ref>   <ref type="figure">Fig. (c)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Steering Angle</head><p>In this section, we demonstrate the effectiveness of the proposed CcGAN on the Steering Angle dataset, a subset of an autonomous driving dataset <ref type="bibr" target="#b40">[41]</ref>. The complete autonomous driving dataset consists of 109,231 RGB images. Each image is taken by using a dash camera mounted on a car and, at the same moment, the angle of the steering wheel rotation of the same car (i.e., steering angle) is recorded by a device attached to the steering wheel. Thus, each image in this autonomous driving dataset is paired with a steering angle ranging from ?338.82 ? to 501.78 ? . Experimental setup: To make the training and evaluation easier, we remove many images in this autonomous driving dataset where an image is removed due to at least one of the following reasons:</p><p>? The image is incorrectly labeled (e.g., some images show that the car was turning left/right but the corresponding steering angles are zero). ? The image has very bad visual quality due to overexposure or underexposure. ? There is no reference object (e.g., double yellow lines or side roads) in the image to let a human visually determine whether the car was turning left/right.</p><formula xml:id="formula_24">? The corresponding steering angle is outside [?80 ? , 80 ? ].</formula><p>Eventually, there are 12,271 images left with 1,904 distinct steering angles in [?80 ? , 80 ? ]. These images are then resized to 64 ? 64 and they form a subset of the autonomous driving dataset <ref type="bibr" target="#b40">[41]</ref>, termed Steering Angle in this paper. Please note that the Steering Angle dataset is highly imbalanced and a histogram of steering angles is shown in <ref type="figure">Fig. S.7</ref>.14.</p><p>When training cGAN (K classes), we divide [?80 ? , 80 ? ] into 210 equal intervals where each interval is taken as a class (i.e., K = 210). When implementing CcGAN, the three hyper-parameters of HVDL and SVDL are selected by the rule of thumb formulae in Remark 3, i.e., ? ? 0.029, ? ? 0.032 and ? ? 1000.438. All GANs are trained for 20,000 iterations. To evaluate the candidate models, we choose 2,000 evenly spaced angles in [?80 ? , 80 ? ] and generate 50 images from each candidate GAN model for each of these angles. Please see Supp. S.7 for the network architectures and more details about the training/testing setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative and visual results:</head><p>To evaluate the quality of fake images, we use the proposed Sliding Fr?chet Inception Distance (SFID) as the overall metric instead of Intra-FID, since we have very few real images for many angles (e.g., angles close to the two end points of [?80 ? , 80 ? ]). We preset 1,000 SFID centers in [?80 ? , 80 ? ] and let the SFID radius be 2 ? . NIQE, Diversity (entropy of predicted types of scenes), and Label Score are also reported. Please see Supp. S.7.5 for more details of these performance measures.</p><p>We report in <ref type="table" target="#tab_3">Table 4</ref> the average quality of 100,000 fake images from each candidate method. Some example fake images are also shown in <ref type="figure">Fig. S.7</ref>.16 in Appendix. We also compute FID, NIQE, Diversity, and Label Score in each SFID interval and plot the line graphs of FID/NIQE/Diversity/Label Score versus SFID Center in <ref type="figure" target="#fig_4">Fig. 13</ref>. Based on these quantitative and visual results, we can conclude:</p><p>? The two ILI-based CcGAN methods are better than cGAN (210 classes) in terms of all four metrics; however, the two NLI-based CcGAN methods have lower label consistency than cGAN (210 classes). Although cGAN (concat) has the highest Diversity score, four CcGAN methods outperform cGAN (concat) in terms of other three metrics. ? Although the NIQE score and Label Score of cGAN (210 classes) are not grossly uncompetitive, cGAN (210 classes) has a very low Diversity score and <ref type="figure" target="#fig_4">Fig. 13</ref>(c) shows that the Diversity scores are almost zero at some angles. Example fake images in <ref type="figure">Fig. S.7</ref>.16 also show that cGAN (210 classes) has the mode collapse problem <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref> (i.e., it always generates the same image for some angles). ? Although cGAN (concat) has the highest Diversity score, its NIQE score and Label Score are terrible, implying bad visual quality and low label consistency. Example fake images in <ref type="figure">Fig. S.7</ref>.16 support these quantitative results. ? The line graphs in <ref type="figure" target="#fig_4">Fig. 13</ref> show that the performance of cGAN (210 classes) is very unstable across all SFID intervals.</p><p>Differently, cGAN (concat) has very smooth graphs but its graph for Label Score is above all other graphs. ? The two ILI-based CcGANs perform better than the two NLI-based CcGANs in terms of all metrics except NIQE. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Effectiveness of SFID</head><p>In this section, we study the effectiveness of SFID on RC-49. Since RC-49 has a large enough sample size of real images, we can get a reliable Intra-FID. At the same time, we can also deliberately reduce the sample size of real images to mimic the scenario where a reliable Intra-FID is not applicable but SFID still works well. The experiment in this section can also be conducted on Cell-200 but is omitted in this paper. We study 11 combinations of the SFID radius (r SFID ) and the number of SFID centers (# c SFID ) in this experiment where we use SFID to evaluate cGAN and two CcGAN methods (i.e., HVDL+NLI and HVDL+ILI) pre-trained in Section 5.1. In Setting 1, we let r SFID = 0 so SFID degenerates to Intra-FID. In the same setting, we evaluate the three GANs on all 899 distinct angles and all real images in RC-49 are used to compute Intra-FID, so Setting 1 is taken as the oracle in this experiment. In Setting 2, we also let r SFID = 0 so SFID degenerates to Intra-FID again. In Setting 2, however, we only evaluate GANs on the 450 distinct angles which are seen in the training set of the experiment in Section 5. reduce the number of real images at each distinct angle from 49 to 10. Therefore, in Setting 2, there are 10 real images for each angle seen in the training set and 0 real image for each angle unseen in the training set. Setting 2 is treated as the baseline in this experiment. Settings 3 to 11 are designed to show the effectiveness of SFID so we let r SFID &gt; 0. Similar to Setting 2, from Settings 3 to 11, real images are available only for those 450 distinct angles seen in the training set and only 10 real images are available for each angle. We consider three values for r SFID (i.e., 0.5, 1, and 2) and three values for the number of c SFID 's (i.e., 400, 600, and 800). For all settings, we compute one FID in each SFID interval (in Settings 1 and 2, the SFID interval degenerates to an angle) and report in <ref type="table">Table 5</ref> the mean of these FIDs along with their standard deviation after the "?" symbol. Setting 1 is the oracle setting whose evaluation results can be seen as the ground truth, and we hope the evaluation results of SFID are close to Setting 1. In Setting 2, when we have very few real images (even zero) for each angle, Intra-FID overestimates the average FID of each GAN (e.g., from 1.7201 to 1.9664 for cGAN) and underestimates the quantitative difference between cGAN and CcGANs (e.g., from (1.7201 ? 0.6119)/1.7201 ? 64.4% to (1.9664 ? 1.2102)/1.9664 ? 38.5% for cGAN and HVDL+NLI). However, the performance of our proposed SFID in Settings 3 to 11 is very close to the oracle setting. If we compare within Settings 3 to 11, we can see r SFID is inversely proportional to the SFID score while # c SF ID does not have obvious influence on SFID. From <ref type="table">Table 5</ref>, we may conclude that as long as r SFID is set at a moderate level, SFID is a valid proxy to the oracle Intra-FID when there are insufficient real images to compute Intra-FID. 5: Evaluation results of SFID on RC-49 under different setups of r SFID and number of c SFID 's. In the first two settings, SFID degenerates to Intra-FID since r SFID = 0. In Setting 1, we evaluate GANs on all 899 distinct angles and all real images are used to compute Intra-FID, so Setting 1 is the oracle setting. In Setting 2, we evaluate GANs on the 450 angles seen in the training set and, for each angle, 10 real images are used to compute Intra-FID, so Setting 2 is treated as the baseline. The performance of our proposed SFID (Settings 3 to 11) is close to the oracle setting while Intra-FID (Setting 2) tends to overestimate the average FID and underestimate the quantitative difference between cGAN and CcGANs. Both visual and quantitative results demonstrate that the high-resolution fake images generated from CcGAN are visually realistic, diverse, and label consistent. These results also show that CcGAN is compatible with state-of-the-art GAN architectures and training techniques. Furthermore, the failure patterns of cGAN (K classes) and cGAN (concat) in this experiment are consistent with those in Section 5. cGAN (K classes) tends to have high label consistency but bad visual quality and low diversity. Oppositely, cGAN (concat) often has high diversity but bad/fair visual quality and terrible label consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">High-resolution RC-49</head><p>In this experiment, we test three candidate methods on RC-49 with two resolutions, i.e., 128 ? 128 and 256 ? 256. Most training setups are consistent with Section 5.1 and please see Supp. S.9.0.2 for details. The quantitative and visual results are shown in <ref type="table" target="#tab_7">Table 6</ref> and <ref type="figure" target="#fig_5">Fig. 14.</ref> We can see CcGAN can generate high-quality images and the example fake images <ref type="figure" target="#fig_5">(Fig. 14)</ref> are indistinguishable from real images. However, cGAN (150 classes) and cGAN (concat) fail again. Fake images generated from cGAN (150 classes) are visually unrealistic and less diverse. Differently, cGAN (concat) can generate images with fair visual quality and high diversity, but it cannot control the image generation via conditioning angles.   <ref type="table" target="#tab_8">Table 7</ref> and <ref type="figure" target="#fig_6">Fig. 15</ref>. We can see CcGAN substantially outperforms two cGANs. Fake images generated from cGAN (150 classes) are visually unrealistic and less diverse. cGAN (concat) cannot control the image generation via conditioning ages. 1.207 ? 0.260 7.885 ? 6.272</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">High-resolution Steering Angle</head><p>Although the low-resolution Steering Angle experiment is already challenging enough due to the highly imbalanced problem, We further increase the image resolution to 128 ? 128, making the generative modeling more difficult. Most training setups are consistent with Section 5.4 and please see Supp. S.9.0.4 for details. The quantitative and visual results are shown in <ref type="table" target="#tab_10">Table 8</ref> and <ref type="figure" target="#fig_7">Fig. 16</ref>. Both visual and quantitative results of cGAN (210 classes) imply severe mode collapse problem <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>. Similar to previous experiments, cGAN (concat) has a very high Diversity score, but its label consistency is terrible. On the contrary, the proposed CcGAN performs well in all three evaluation perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We propose the CcGAN in this paper for generative image modeling conditional on regression labels. In CcGAN, two novel empirical discriminator losses (HVDL and SVDL), a novel empirical generator loss and two novel label input mechanisms (NLI and ILI) are proposed to overcome the two problems of existing cGANs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPLEMENTARY MATERIAL S.1 GITHUB REPOSITORY</head><p>Please find the codes for this paper at https://github.com/UBCDingXin/improved CcGAN Train G; <ref type="bibr" target="#b11">12</ref> Draw m g labels Y g with replacement from ?;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.2 ALGORITHMS FOR CCGAN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13</head><p>Create another set of target labels Y g, = {y i + |y i ? Y g , ? N (0, ? 2 ), i = 1, . . . , m g } (G is conditional on these labels) ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14</head><p>Generate m g fake images conditional on Y g, and put these image-label pairs in ? f g ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>15</head><p>Update G with samples in Train G; <ref type="bibr" target="#b13">14</ref> Draw m g labels Y g with replacement from ?; <ref type="bibr" target="#b14">15</ref> Create another set of target labels Y g, = {y i + |y i ? Y g , ? N (0, ? 2 ), i = 1, . . . , m g } (G is conditional on these labels) ; <ref type="bibr" target="#b15">16</ref> Generate m g fake images conditional on Y g, and put these image-label pairs in ? f g ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17</head><p>Update G with samples in ? f g via gradient-based optimizers based on Eq.(13) ; 18 end Remark S.1. If should be noted that, for computational efficiency, the normalizing constants N r y r j + r ,? , N g y g j + g ,? , N r i=1 w r (y r i , y r j + r ), and N g i=1 w g (y g i , y g j + g ) in Eq. <ref type="formula" target="#formula_9">(11)</ref> and <ref type="formula" target="#formula_1">(12)</ref> are excluded from the training and only used for theoretical analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.3 THEORETICAL ANALYSIS FOR HVDL AND SVDL</head><p>In this section, we provide a self-contained theoretical analysis of HVDL and SVDL. To make the derivation clearer, we use some notations and definitions slightly different from those in the main content of the paper. Some necessary assumptions, lemmas, and theorems are also introduced or derived in Sections S.3.2 and S.3.3. The main theorems on the error bounds of D are derived in Section S.3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.3.1 Some Necessary Definitions and Notations</head><p>This section summarizes some necessary definitions and notations used in the derivation. Please note that all these definitions and notations are valid in Supp. S.3 only.</p><p>? Unlike other contents of this paper, we use different symbols to denote random variables/vectors and the fixed values that random variables/vectors may take. Specifically, a random image and a random label are represented respectively by a bold capital X and a capital Y . A sequence of N random image-label pairs are represented by (X 1 , Y 1 ), . . . , (X N , Y N ). Please note that some subscripts or superscripts may apply to X and Y to provide some extra information. An observed (fixed) image and an observed (fixed) label are denoted respectively by a bold lowercase x and a lowercase y. Moreover, without loss of generality, we assume Y, y ? [0, 1], i.e., Y = [0, 1]. ? Let p(x|Y = y) denote the conditional probability density function (PDF) of X given the occurrence of the value y of Y . p may have superscripts or subscripts to provide some extra information. ? Let p(y |Y = y) denote the conditional PDF of Y given the occurrence of the value y of Y . p may have superscripts or subscripts to provide some extra information. ? Let D stand for the Hypothesis Space of D. D is a set of functions that can be represented by D (a neural network with determined architecture but undetermined weights). We also let p r w (y |Y = y) w r (y , y)p r (y ) W r (y) and p g w (y |Y = y) w g (y , y)p g (y ) W g (y) .</p><p>? The H?lder Class defined below is a set of functions with bouned second derivatives, which controls the variation of the function when parameter changes. Definition S.1. (H?lder Class) Define the H?lder class of functions as:</p><formula xml:id="formula_25">?(L) p : ?t 1 , t 2 ? Y, ?L &gt; 0, s.t. |p (t 1 ) ? p (t 2 )| |t 1 ? t 2 | ? L . (S.1)</formula><p>? With some new notations above, we restate the theoretical discriminator losses L(D) as follows:</p><formula xml:id="formula_26">L(D) = ? E Y ?pr(y) E X?pr(x|Y ) [log (D(X, Y ))] ? E Y ?pg(y) E X?pg(x|Y ) [log (1 ? D(X, Y ))] ,<label>(S.</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2)</head><p>? Recall that, given a G, the optimal discriminator which minimizes L(D) is in the form of D * (x, y) = p r (x, y) p r (x, y) + p g (x, y) .</p><p>However, D * may not be covered by the hypothesis space D. Define D, D HVDL , and D SVDL as follows D arg min D?D L(D), D HVDL arg min D?D L HVDL (D),</p><formula xml:id="formula_27">D SVDL arg min D?D L SVDL (D).</formula><p>Note that L( D) ? L(D * ) should be a non-negative constant. In CcGAN, we minimize L HVDL (D) or L SVDL (D) with respect to D ? D, so we are interested in the distance of D HVDL and D SVDL from D * , i.e., L( D HVDL ) ? L(D * ) and L( D SVDL ) ? L(D * ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.3.2 Some Necessary Assumptions</head><p>In this theoretical analysis, we work with the following assumptions: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.3.3 Some Necessary Lemmas and Theorems</head><p>In this section, we first introduce the Hoeffding's inequality that are widely used later to derive some lemmas. </p><formula xml:id="formula_28">P r 1 m m i=1 Z i ? ? &gt; ? 2 exp ? 2m 2 (b ? a) 2 .</formula><p>Proof. Please see <ref type="bibr" target="#b7">[8,</ref><ref type="bibr">Lemma B.6]</ref> for the proof. Remark S.2. Let ? = 2 exp ? 2m 2 (b?a) 2 , then = 1 2m log 2 ? . Thus, we can get another form of the Hoeffding's inequality. For ?? ? (0, 1), with probability at least 1 ? ?, we have</p><formula xml:id="formula_29">1 m m i=1 Z i ? ? ? 1 2m log 2 ? .</formula><p>Lemma S.1 (Lemma for HVDL). Suppose that (A1)-(A2) and (A4) hold and let (X 1 , Y 1 ), . . . , (X N , Y N ) be a sequence of i.i.d. random image-label pairs, then ?? ? (0, 1), with probability at least 1 ? ?,</p><formula xml:id="formula_30">sup D?D 1 N y,? N i=1 1 {|y?Yi|??} [? log D(X i , y)] ? E X?p(x|Y =y) [? log D(X, y)] ? U 1 2N y,? log 2 ? + ?U M, (S.3)</formula><p>for a fixed y. If image-label pairs are real, then N = N r , N y,? = N r y,? , p = p r , and M = M r . Similarly, we have N = N g , N y,? = N g y,? , p = p g , and M = M g for fake image-label pairs. Proof. Triangle inequality yields</p><formula xml:id="formula_31">sup D?D 1 N y,? N i=1 1 {|y?Yi|??} [? log D(X i , y)] ? E X?pr(x|Y =y) [? log D(X, y)] ? sup D?D 1 N y,? N i=1 1 {|y?Yi|??} [? log D(X i , y)] ? E X?p y,? (x) [? log D(X, y)] + sup D?D E X?p y,? (x) [? log D(X, y)] ? E X?p(x|Y =y) [? log D(X, y)]</formula><p>We then bound the two terms of the RHS separately as follows:</p><p>1) Real images with labels in [y ? ?, y + ?] can be seen as independent samples from p y,? (x). Then the first term can be bounded by applying Hoeffding's inequality as follows: ?? ? (0, 1), with at least probability 1 ? ?, </p><formula xml:id="formula_32">sup D?D 1 N y,? N i=1 1 {|y?Yi|??} U ? log D(X i , y) U ? E X?p y,? (x) U ? log D(X, y) U ? U 1 2N y,? log 2 ? .</formula><formula xml:id="formula_33">sup D?D 1 N N i=1 w(Y i , y) [? log D(X i , y)] 1 N N i=1 w(Y i , y) ? E X?p(x|Y =y) [? log D(X, y)] ? 2U W (y) 1 2N log 4 ? + U M E Y ?pw(Y |Y =y) [|Y ? y|] , (S.7)</formula><p>for a fixed y. If image-label pairs are real, then N = N r , N y,? = N r y,? , p = p r , p w = p r w , w = w r , W = W r , and M = M r . Similarly, we have N = N g , N y,? = N g y,? , p = p g , p w = p g w , w = w g , W = W g , and M = M g for fake image-label pairs. Proof. Triangle inequality yields</p><formula xml:id="formula_34">sup D?D 1 N N i=1 w(Y i , y) [? log D(X i , y)] 1 N N i=1 w(Y i , y) ? E X?pr(x|Y =y) [? log D(X, y)] (Recall f (x, y) = ? log D(x, y) and F = ? log D.) = sup f ?F 1 N N i=1 w(Y i , y)f (X i , y) 1 N N i=1 w(Y i , y) ? E X?pr(x|Y =y) [f (X, y)] ? sup f ?F 1 N N i=1 w(Y i , y)f (X i , y) 1 N N i=1 w(Y i , y) ? E X?p y,w (x) [f (X, y)] + sup f ?F E X?p y,w (x) [f (X, y)] ? E X?pr(x|Y =y) [f (X, y)] .</formula><p>(S.8) (p y,w = p y,w r r for real images and p y,w = p y,w g g for fake images)</p><p>We then derive bounds for both terms on the RHS of the inequality. 1) For the first term, we can further split it into two parts,</p><formula xml:id="formula_35">1 N N i=1 w(Y i , y)f (X i , y) 1 N N i=1 w(Y i , y) ? E X?p y,w (x) [f (X, y)] ? 1 N N i=1 w(Y i , y)f (X i , y) 1 N N i=1 w(Y i , y) ? 1 N N i=1 w(Y i , y)f (X i , y) W (y) + 1 N N i=1 w(Y i , y)f (X i , y) W (y) ? E X?p y,w (x) [f (X, y)] (S.9)</formula><p>Focusing on the first part of RHS of Eq.(S.9). By (A1),</p><formula xml:id="formula_36">1 N N i=1 w(Y i , y)f (X i , y) 1 N N i=1 w(Y i , y) ? 1 N N i=1 w(Y i , y)f (X i , y) W (y) ?U 1 N N i=1 w(Y i , y) ? W (y) W (y)</formula><p>Note that ?y, y , w(y , y) = e ??|y?y | 2 ? 1 (since ? &gt; 0) and hence given y, w(Y , y) is a random variable bounded by 1. Moreover, given y, W (y) is the expectation of w(Y , y). Then, apply Hoeffding's inequality to the numerator of above, yielding that with probability at least 1 ? ? ,</p><formula xml:id="formula_37">1 N N i=1 w(Y i , y) ? W (y) ? 1 2N log 2 ? .</formula><p>Thus, by the boundedness of f , with probability at least 1 ? ? ,</p><formula xml:id="formula_38">1 N N i=1 w(Y i , y)f (X i , y) 1 N N i=1 w(Y i , y) ? 1 N N i=1 w(Y i , y)f (X i , y) W (y) ? U W (y) 1 2N log 2 ? . (S.10)</formula><p>Then, consider the second part of RHS of Eq.(S.9). Recall that p y,w (x) p(x|Y = y ) w(y ,y)p(y )</p><formula xml:id="formula_39">W (y) dy . Thus, 1 N N i=1 w(Y i , y)f (X i , y) W (y) ? E X?p y,w (x) [f (X, y)] = 1 W (y) 1 N N i=1 w(Y i , y)f (X i , y) ? E (X,Y )?p(x,y ) [w r (Y , y)f (X i , y)] ,</formula><p>where p(x, y ) = p(x|Y = y )p(y ) denotes PDF of the joint distribution of real image and its label. Again, since w(Y , y)f (X i , y) is uniformly bounded by U under (A1), we can apply Hoeffding's inequality. This implies that with probability at least 1 ? ? , the above can be upper bounded by</p><formula xml:id="formula_40">U W (y) 1 2N log 2 ? . (S.11)</formula><p>Combining Eq. (S.10) and (S.11) and by setting ? = ? 2 , we have with probability at least 1 ? ?,</p><formula xml:id="formula_41">1 N N i=1 w(Y i , y)f (X i , y) 1 N N i=1 w(Y i , y) ? E X?p y,w (x) [f (X, y)] ? 2U W (y) 1 2N log 4 ? .</formula><p>Since this holds for ?f ? F, taking supremum over f , we have</p><formula xml:id="formula_42">sup f ?F 1 N N i=1 w(Y i , y)f (X i , y) 1 N N i=1 w(Y i , y) ? E X?p y,w (x) [f (X, y)] ? 2U W (y) 1 2N log 4 ? . (S.12)</formula><p>2) For the second term on the RHS of Eq. (S.8). By (A1) that |f | ? U ,</p><formula xml:id="formula_43">sup f ?F E X?p y,w (x) [f (X, y)] ? E X?p(x|Y =y) [f (X, y)] (See Eq. (S.5)) ?U |p y,w (x) ? p(x|Y = y)|dx.</formula><p>Note that by the definition of By (A.2) and y ? [0, 1], the above is upper bounded by</p><formula xml:id="formula_44">g(x)E Y ?pw(y |Y =y) [|y ? Y |] .</formula><p>Thus,</p><formula xml:id="formula_45">sup f ?F E X?p y,w (x) [f (X, y)] ? E X?p(x|Y =y) [f (X, y)] ?U g(x)E Y ?pw(y |Y =y) [|Y ? y|] dx =U M E Y ?pw(y |Y =y) [|Y ? y|] . (S.13)</formula><p>Therefore, combining both Eq.(S.12) and (S.13), with probability at least 1 ? ?,</p><formula xml:id="formula_46">sup D?D 1 N N i=1 w(Y i , y) [? log D(X i , y)] 1 N N i=1 w(Y i , y) ? E X?p(x|Y =y) [? log D(X, y)] ? 2U W (y) 1 2N log 4 ? + U M E Y ?pw(y |Y =y) [|Y ? y|] .</formula><p>This finishes the proof.</p><p>As introduced in Section 2, we use KDE for the density of the marginal label distribution with Gaussian kernel. The next theorem characterizes the difference between a p r (y), p g (y) and their KDE using N i.i.d. samples. for some constants C KDE 1,? , C KDE 2,? depending on ?. Proof. By ( <ref type="bibr" target="#b44">[45]</ref>; P.12), for any p(t) ? ?(L) (the H?lder Class, see Definition S.1), with probability at least 1 ? ?,</p><formula xml:id="formula_47">sup t p KDE (t) ? p(t) ? C KDE ? log N N ? + c?,</formula><p>for some constants C KDE ? and c, where C depends on ? and c = L K(s)|s| 2 ds. Since in this work, K is chosen as Gaussian kernel, c = L K(s)|s| 2 ds = L.</p><p>Based on above lemmas and theorems, we derive the following two theorems, which will be used in the derivation of the error bounds of D trained with HVDL and SVDL in Section S.3.4. Theorem S.3. Assume that (A1)-(A4) hold, then ?? ? (0, 1), with probability at least 1 ? ?,</p><formula xml:id="formula_48">sup D?D L HVDL (D) ? L(D) ? U ? ? C KDE 1,? log N r N r ? + L r ? 2 ? ? + U ? ? C KDE 2,? log N g N g ? + L g ? 2 ? ? + ?U (M r + M g ) + U 1 2 log 8 ? E Y ?p KDE r (y) 1 N r Y,? + E Y ?p KDE g (y) 1 N g Y,? , (S.16) for some constants C KDE 1,? , C KDE 2,? depending on ?. Proof. Let (X r 1 , Y r 1 ), . . . , (X r N r , Y r N r ) and (X g 1 , Y g 1 ), . . . , (X g N g , Y g N g )</formula><p>denote respectively real and fake i.i.d. random imagelabel pairs.</p><p>We first decompose sup D?D L HVDL (D) ? L(D) as follows for some constants C KDE 1,?1 depending on ? 1 . 2) Similarly, for the second term, ?? 2 ? (0, 1), with at least probability 1 ? ? 2 ,</p><formula xml:id="formula_49">sup D?D L HVDL (D) ? L(D) ? sup D?D [? log D(x, y)] p r (x|Y = y)dx (p r (y) ?p KDE r (y))dy + sup D?D [? log(1 ? D(x, y))] p g (x|Y = y)dx (p g (y) ?p KDE g (y))dy + sup D?D 1 N r y,? N r i=1 1 {|y?Y r i |??} [? log D(X r i , y)] ? E X?pr(x|Y =y) [? log D(X, y)] p KDE r (y)dy + sup D?D 1 N g y,? N r i=1 1 {|y?Y g i |??} [? log(1 ? D(X g i , y))] ? E</formula><formula xml:id="formula_50">sup D?D [? log(1 ? D(x, y))] p g (x|Y = y)dx (p g (y) ?p KDE g (y))dy ?U ? ? C KDE 2,?2 log N g N r ? + L g ? 2 ? ? , (S.18)</formula><p>for some constants C KDE 2,?2 depending on ? 2 . 3) The third term can be bounded by using Lemma S.1. For the third term, ?? 3 ? (0, 1), with at least probability 1 ? ? 3 , Note that N r y,? = N r i=1 1 {|y?Y r i |} , which is a random variable of Y i 's. The above can be expressed as</p><formula xml:id="formula_51">sup D?D 1 N r y,? N r i=1 1 {|y?Y r i |??} [? log D(X r i , y)] ? E X?pr(x|Y =y) [? log D(X, y)] p KDE r (y)dy ? ?U M r + U 1 2 log 2 ? 3 E Y ?p KDE r (y) 1 N r Y,? . (S.19) 4)</formula><p>Similarly, for the fourth term, ?? 4 ? (0, 1), with at least probability 1 ? ? 4 , </p><formula xml:id="formula_52">sup D?D 1 N g y,? N r i=1 1 {|y?Y g i |??} [? log(1 ? D(X g i , y))] ? E X?pg(x|Y =y) [? log(1 ? D(X, y))] dx p KDE g (y)dy ??U M g + U 1 2 log 2 ? 4 E Y ?p KDE g (y) 1 N g Y,</formula><formula xml:id="formula_53">sup D?D L SVDL (D) ? L(D) ? U ? ? C KDE 1,? log N r N r ? + L r ? 2 ? ? + U ? ? C KDE 2,? log N g N g ? + L g ? 2 ? ? + 4U 1 2 log 16 ? 1 ? N r E Y ?p KDE r (y) 1 W r (Y ) + 1 ? N g E Y ?p KDE g (y) 1 W g (Y ) + 2U M r E Y ?p KDE r (y) E Y ?p r w (y |Y ) |Y ? Y | + M g E Y ?p KDE g (y) E Y ?p g w (y |Y ) |Y ? Y | (S.21)</formula><p>for some constant C KDE 1,? , C KDE 2,? depending on ?.</p><p>Proof. Similar to the decomposition for Theorem S.3, we can decompose sup D?D | L SVDL (D) ? L(D)| into four terms which can be bounded by using Theorem S.2, the boundness of D, and Lemma S.2. The detail is omitted because it is almost identical to the one of Theorem S.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.3.4 Error Bounds of D Trained with HVDL and SVDL</head><p>Based on above theorems and lemmas, we derive the error bounds of D that is trained with HVDL and SVDL respectively. The error bound is characterized by the distance of D HVDL and D SVDL from the optimal D * under the theoretical discriminator loss L(D), i.e., L( D HVDL ) ? L(D * ) and L( D SVDL ) ? L(D * ) respectively. Please see Theorem S.5 and S.6 for details. An illustrative diagram to visualize the theoretical analysis is shown in <ref type="figure" target="#fig_4">Fig. S.3.1</ref>.</p><p>Theorem S.5 (Error bound of D trained with HVDL). Assume that (A1)-(A4) hold, then ?? ? (0, 1), with probability at least 1 ? ?,</p><formula xml:id="formula_54">L( D HVDL ) ? L(D * ) (S.22) ?2U ? ? C KDE 1,? log N r N r ? + L r ? 2 ? ? + 2U ? ? C KDE 2,? log N g N g ? + L g ? 2 ? ? + 2?U (M r + M g ) + 2U 1 2 log 8 ? E Y ?p KDE r (y) 1 N r Y,? + E Y ?p KDE g (y) 1 N g Y,? + L( D) ? L(D * ), (S.23)</formula><p>for some constants C KDE 1,? , C KDE 2,? depending on ?. Proof. We first decompose L( D HVDL ) ? L(D * ) as follows</p><formula xml:id="formula_55">L( D HVDL ) ? L(D * ) =L( D HVDL ) ? L( D HVDL ) + L( D HVDL ) ? L( D) + L( D) ? L( D) + L( D) ? L(D * ) (by L( D HVDL ) ? L( D) ? 0) ?2 sup D?D L HVDL (D) ? L(D) + L( D) ? L(D * ) (by Theorem S.3) ?2U ? ? C KDE 1,? log N r N r ? + L r ? 2 ? ? + 2U ? ? C KDE 2,? log N g N g ? + L g ? 2 ? ? + 2?U (M r + M g ) + 2U 1 2 log 8 ? E Y ?p KDE r (y) 1 N r Y,? + E Y ?p KDE g (y) 1 N g Y,? + L( D) ? L(D * ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(S.24)</head><p>Theorem S.6 (Error bound of D trained with SVDL). Assume that (A1)-(A4) hold, then ?? ? (0, 1), with probability at least 1 ? ?,</p><formula xml:id="formula_56">L( D SVDL ) ? L(D * ) ?2U ? ? C KDE 1,? log N r N r ? + L r ? 2 ? ? + 2U ? ? C KDE 2,? log N g N g ? + L g ? 2 ? ? + 4U 1 2 log 16 ? 1 ? N r E Y ?p KDE r (y) 1 W r (Y ) + 1 ? N g E Y ?p KDE g (y) 1 W g (Y ) + 2U M r E Y ?p KDE r (y) E Y ?p r w (y |Y ) |Y ? Y | + M g E Y ?p KDE g (y) E Y ?p g w (y |Y ) |Y ? Y | + L( D) ? L(D * ), (S.25)</formula><p>for some constant C KDE 1,? , C KDE 2,? depending on ?. Proof. Smilarly, based on Theorem S.4, we can derive Theorem S.6. The detailed proof is omitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.4 MORE DETAILS OF THE EXPERIMENT ON LOW-RESOLUTION RC-49 IN SECTION 5.1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.4.1 Description of RC-49</head><p>To generate RC-49, firstly we randomly select 49 3-D chair object models from the "Chair" category provided by ShapeNet <ref type="bibr" target="#b45">[46]</ref>. Then we use Blender v2.79 1 to render these 3-D models. Specifically, during the rendering, we rotate each chair model 1. https://www.blender.org/download/releases/2-79/ along with the yaw axis for a degree between 0.1 ? and 89.9 ? (angle resolution as 0.1 ? ) where we use the scene image mode to compose our dataset. The rendered images are converted from the RGBA to RGB color model. In total, the RC-49 dataset consists of 44051 images of image size 64?64 in the PNG format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.4.2 Network architectures</head><p>The RC-49 dataset is a more sophisticated dataset compared with the simulation, thus it requires networks with deeper layers. We employ the SNGAN architecture <ref type="bibr" target="#b23">[24]</ref> in both cGAN and CcGAN consisting of residual blocks for the generator and the discriminator. Moreover, for the generator in cGAN, the regression labels are input into the network by the label embedding <ref type="bibr" target="#b46">[47]</ref> and the conditional batch normalization <ref type="bibr" target="#b31">[32]</ref>. For the discriminator in cGAN, the regression labels are fed into the network by the label embedding and the label projection <ref type="bibr" target="#b2">[3]</ref>. For CcGAN, the regression labels are fed into networks by the two proposed label input methods (NLI and ILI) in Section 2.2. The pre-trained CNN T 1 + T 2 for ILI is a modified ResNet-34 with two extra linear layers before the final linear layer. The label embedding network T 3 is a 5-layer MLP with 128 nodes in each layer. The dimension of the noise z is 128 for NLI-based CcGANs and 256 for ILI-based CcGANs. Please refer to our codes for more details about the network specifications of cGAN and CcGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.4.3 Training setups</head><p>The cGAN and CcGAN are trained for 30,000 iterations on the training set with the Adam <ref type="bibr" target="#b47">[48]</ref> optimizer (with ? 1 = 0.5 and ? 2 = 0.999), a constant learning rate 10 ?4 and batch size 256.</p><p>The rule of thumb formulae in Rmk 3 are used to select the hyper-parameters for HVDL and SVDL, where we let m ? = 2. Thus, the three hyper-parameters in this experiments are set as follows: ? = 0.0473, ? = 0.004, ? = 50625.</p><p>The modified ResNet-34 (i.e., the T 1 + T 2 in <ref type="figure" target="#fig_5">Fig. 4)</ref> for ILI is trained for 200 epochs with the SGD optimizer, initial learning rate 0.1 (decay at epoch 60, 120, and 160 with factor 0.2), weight decay 10 ?4 , and batch size 256. The 5-layer MLP for the label embedding in ILI is trained for 500 epochs with the SGD optimizer, initial learning rate 0.1 (decay at epoch 100, 200, and 400 with factor 0.2), weight decay 10 ?4 , and batch size 256.</p><p>Please see our codes for more details of the training setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.4.4 Testing setups</head><p>The RC-49 dataset consists of 899 distinct yaw angles and at each angle there are 49 images (corresponding to 49 types of chairs). At the test stage, we ask the trained cGAN or CcGAN to generate 200 fake images at each of these 899 yaw angles. Please note that, among these 899 yaw angles, only 450 of them are seen at the training stage so real images at the rest 449 angles are not used in the training.</p><p>We evaluate the quality of the fake images from three perspectives, i.e., visual quality, intra-label diversity, and label consistency. One overall metric (Intra-FID) and three separate metrics (NIQE, Diversity, and Label Score) are used. Their details are shown in Supp. S.4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.4.5 Performance measures</head><p>Before we conduct the evaluation in terms of the four metrics, we first train an autoencoder (AE) , a regression-oriented ResNet-34 <ref type="bibr" target="#b36">[37]</ref> and a classification-oriented ResNet-34 <ref type="bibr" target="#b36">[37]</ref> on all real images of RC-49. The bottleneck dimension of the AE is 512 and the AE is trained to reconstruct the real images in RC-49 with the MSE loss. The regression-oriented ResNet-34 is trained to predict the yaw angle of a given image. The classification-oriented ResNet-34 is trained to predict the chair type of a given image. The autoencoder and both two ResNets are trained for 200 epochs with a batch size of 256.</p><p>? Intra-FID <ref type="bibr" target="#b2">[3]</ref>: We take Intra-FID as the overall score to evaluate the quality of fake images and we prefer the small Intra-FID score. At each evaluation angle, we compute the FID <ref type="bibr" target="#b34">[35]</ref> between 49 real images and 200 fake images in terms of the bottleneck feature of the pre-trained AE. The Intra-FID score is the average FID over all 899 evaluation angles. Please note that we also try to use the classification-oriented ResNet-34 to compute the Intra-FID but the Intra-FID scores vary in a very wide range and sometimes obviously contradict with the three separate metrics. ? NIQE <ref type="bibr" target="#b37">[38]</ref>: NIQE is used to evaluate the visual quality of fake images with the real images as the reference and we prefer the small NIQE score. We train one NIQE model with the 49 real images at each of the 899 angles so we have 899 NIQE models. During evaluation, an average NIQE score is computed for each evaluation angle based on the NIQE model at that angle. Finally, we report the average and standard deviations of the 899 average NIQE scores over the 899 yaw angels (i.e., "the mean/standard deviation of 899 means"). Note that the NIQE is implemented by the NIQE library in MATLAB.</p><p>The block size and the sharpness threshold are set to 8 and 0.1 respectively in this and rest experiments. ? Diversity: Diversity is used to evaluate the intra-label diversity and the larger the better. In RC-49, there are 49 chair types.</p><p>At each evaluation angle, we ask a pre-trained classification -oriented ResNet-34 to predict the chair types of the 200 fake images and an entropy is computed based on these predicted chair types. The diversity reported in <ref type="table" target="#tab_0">Table 1</ref> is the average of the 899 entropies over all evaluation angles. ? Label Score: Label Score is used to evaluate the label consistency and the smaller the better. We ask the pre-trained regressionoriented ResNet-34 to predict the yaw angles of all fake images and the predicted angles are then compared with the assigned angles. The Label Score is defined as the average absolute distance between the predicted angles and assigned angles over all fake images, which is equivalent to the Mean Absolute Error (MAP). Note that, to plot the line graphs, we compute Label Score at each of the 899 evaluation angles.   <ref type="figure" target="#fig_5">Fig. S.4.4</ref>. Since, at each angle, the degenerated CcGAN only uses the images at this angle, it leads to the mode collapse problem (e.g, the row in the yellow rectangle) and bad visual quality (e.g., images in the red rectangle) at some angles. Note that the degenerated CcGAN is still different from cGAN, since we still treat y as a continuous scalar instead of a class label here and we use the proposed label input method (e.g., NLI) to incorporate y into the generator and the discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.4.7.3 cGAN: different number of classes</head><p>In this experiment, we show that cGAN still fails even though we bin [0.1, 89.9] into other number of classes. We experimented with three different bin setting -grouping labels into 90, 150, and 210 classes, respectively. Experimental results are shown in <ref type="figure" target="#fig_5">Fig. S.4</ref>.5 and we observe all three cGANs completely fail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.5 MORE DETAILS OF THE EXPERIMENT ON THE LOW-RESOLUTION UTKFACE DATASET IN SECTION 5.2 S.5.1 Description of the UTKFace dataset</head><p>The UTKFace dataset is an age regression dataset <ref type="bibr" target="#b38">[39]</ref>, with human face images collected in the wild. We use a preprocessed version (cropped and aligned), with ages spanning from 1 to 60. After the data cleaning (i.e., removing images of very low quality or with clearly wrong labels), the number of images left is 14760. These images are then resized to 64 ? 64. The histogram of the UTKFace dataset after data cleaning is shown in S.5.6.</p><p>From <ref type="figure">Fig. S</ref>.5.6, we can see UTKFace dataset is very imbalanced so the samples from the minority age groups are unlikely to be chosen at each iteration during the GAN training. Consequently, cGAN and CcGAN may not be well-trained at these minority age groups. To increase the chance of drawing these minority samples during training, we randomly replicate samples in the minority age groups to ensure that the sample size of each age is more than 200.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.5.2 Network architectures</head><p>The network architectures used in this experiment is similar to those in the RC-49 experiment. Please refer to our codes for more details about the network specifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.5.3 Training setups</head><p>The cGAN and CcGAN are trained for 40,000 iterations on the training set with the Adam <ref type="bibr" target="#b47">[48]</ref> optimizer (with ? 1 = 0.5 and ? 2 = 0.999), a constant learning rate 10 ?4 and batch size 512. The rule of thumb formulae in Section ?? are used to select the hyper-parameters for HVDL and SVDL, where we let m ? = 1.</p><p>Please see our codes for more details of the training setups. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.5.4 Performance measures</head><p>Similar to the RC-49 experiment, we evaluate the quality of fake images by Intra-FID, NIQE, Diversity, and Label Score. We also train an AE (bottleneck dimension is 512), a classification-oriented ResNet-34, and a regression-oriented ResNet-34 on the UTKFace dataset. Please note that, the UTKFace dataset consists of face images from 5 races based on which we train the classification-oriented ResNet-34. The AE and both two ResNets are trained for 200 epochs with a batch size 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.5.5 Example UTKFace images</head><p>Example UTKFace images are shown in <ref type="figure">Fig. S</ref>.5.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.5.6 Extra experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.5.6.1 Interpolation</head><p>To perform label interpolation experiments, we keep the noise vector z fixed and vary label from age 3 to age 57 for the four CcGANs. The interpolation results are illustrated in S.5.8. As age y increases, we observe the synthetic face gradually becomes older in appearance. This observation convincingly shows that all four CcGANs do not simply memorize or overfit to the training set. Indeed, our CcGANs demonstrate continuous control over synthetic images with respect to ages. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.5.6.2 Degenerated CcGAN</head><p>We consider the extreme cases of the proposed CcGANs on the UTKFace dataset. As shown in <ref type="figure" target="#fig_6">Fig. S.5</ref>.9, the degenerated NLI-based CcGANs fails to generate facial images at some ages (e.g., 51 and 57) because of too small sample sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.5.6.3 cGAN: different number of classes</head><p>In the last experiment, we bin samples into different number of classes based on ground-truth labels, in order to increase the number of training samples at each class. Then we train cGAN using samples from the binned classes. We experimented with two different bin setting, i.e., binning image samples into 60 classes and 40 classes, respectively. The results are reported in <ref type="figure" target="#fig_6">Fig.S.5.10</ref>. The results demonstrate cGANs consistently fail to generate diverse synthetic images with labels aligned with their conditional information. Moreover, the image quality is much worse than those from the proposed CcGANs. In conclusion, compared with existing cGANs, our proposed CcGANs have substantially better performance in terms of the image quality and diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.6 MORE DETAILS OF THE EXPERIMENT ON THE LOW-RESOLUTION CELL-200 DATASET IN SECTION 5.3</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.6.1 Description of Cell-200</head><p>Cell-200 is a synthetic image dataset, emulating the colonies of bacterial cells in the view of fluorescence microscope. This dataset contains cell populations with overall number varying between 1 and 200, generated with <ref type="bibr" target="#b48">[49]</ref>. For each cell population (e.g., 1 to 200), we generate 1000 different synthetic fluorescence microscopic images, with diverse cell variations (e.g., shapes, locations, overlaps and blurring effects). As in <ref type="bibr" target="#b49">[50]</ref>, we set nucleus radius as 5, and image size as 256 ? 256. To alleviate computational burden, images in the Cell-200 dataset are then resized to 64 ? 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.6.2 Network architectures</head><p>The network architectures for cGAN and CcGAN in this experiment are adapted from the famous DCGAN <ref type="bibr" target="#b35">[36]</ref> architecture. The dimension of the noise z is 128 for NLI-based CcGANs and 256 for ILI-based CcGANs. Please refer to our codes for more details about the network specifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.6.3 Training setups</head><p>The cGAN and CcGAN are trained for 5000 iterations on the training set with the Adam <ref type="bibr" target="#b47">[48]</ref> optimizer (with ? 1 = 0.5 and ? 2 = 0.999) and a constant learning rate 10 ?4 . The rule of thumb formulae in Section ?? are used to select the hyperparameters for HVDL and SVDL, where we let m ? = 2.</p><p>For cGAN training, the cell count range <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">200]</ref> is split into 100 disjoint intervals, i.e., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3)</ref>, <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5)</ref>, . . . , <ref type="bibr">[197,</ref><ref type="bibr">199)</ref>, <ref type="bibr">[199,</ref><ref type="bibr">200]</ref>. In this case, cGAN estimates image distribution conditional on these intervals. In Supp. S.6.7.2, we also compare the performance of cGAN under different splitting schemes. Please note that we use different batch sizes for cGAN and CcGAN in this experiment. The batch size for cGAN is 512. Differently, for all CcGAN methods in this experiment, the batch size is 512 for the generator but 32 for the discriminator. The reason that we use different batch sizes for the generator and discriminator in CcGAN is based on some observations we got during training. In this experiment, if the generator and the discriminator in CcGAN have the same batch size, the discriminator loss often decreases to almost zero very quickly while the generator loss still maintains at a high level.</p><p>Consequently, at each iteration, the discriminator can easily distinguish the real and fake images while the generator cannot  fool the discriminator and won't improve in the next iteration, which implies a high imbalance between the generator update and the discriminator update. To balance the training of the generator and the discriminator, we deliberately decrease the number of images seen by the discriminator at each iteration to slow down the update of the discriminator so that the generator can catch up.</p><p>Please see our codes for more details of the training setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.6.4 Testing setups</head><p>We evaluate the trained cGAN and four CcGAN methods on all 200 cell counts (half of them are blinded during training). Each method generates 1,000 images for each cell count, so there are 200,000 fake images from each method. When evaluating the trained cGAN, if a test label y is unseen in the training set, we just need to find which interval (recall we split <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">200]</ref> is split into 100 disjoint intervals) covers this label. Then, we generate samples from the trained cGAN conditional on this interval instead of y .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.6.5 Performance measures</head><p>Similar to the previous two experiments, we evaluate the quality of fake images by Intra-FID, NIQE, and Label Score but Diversity. The Diversity score is not available here because we don't have any class label in Cell-200. An AE with a bottleneck dimension of 512 and a regression-oriented ResNet-34 are pre-trained on the complete Cell-200 dataset (i.e., 200,000 images) to compute the Intra-FID and Label Score respectively. The possibility of lacking class labels in regression-oriented datasets is another reason that we propose to use an AE to compute Intra-FID instead of a classification-oriented CNN. The AE is trained for 50 epochs with a batch size of 256. The regression-oriented ResNet-34 is trained for 200 epochs with a batch size of 256.  resolution for each of 10 cell counts absent in the training data: real images and example fake images from cGAN and four proposed CcGANs, respectively. cGAN has severe mode collapse problem in this experiment. Two NLI-based CcGANs do not perform well enough but two ILI-based CcGANs produce images with higher visual quality, more diversity, and higher label consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.6.6 Example UTKFace images</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.6.7 Extra experiments</head><p>S.6.7.1 Interpolation To perform the label interpolation, we keep the noise vector z fixed and vary label from 10 to 200 for the four CcGANs. The interpolation results are illustrated in S.6.12. As cell count y increases, we observe the cells in images become more crowded. This observation convincingly shows that all four CcGANs do not simply memorize or overfit to the training set. Indeed, our CcGANs demonstrate continuous control over synthetic images with respect to cell counts. S.6.7.2 cGAN: different number of classes In this experiment, we experimented with two different bin setting -grouping labels into 100 classes and 50 classes, respectively. Experimental results are shown in <ref type="figure" target="#fig_7">Fig. S.6</ref>.13. We observe both cGANs fail in this experiment. First of all, cGANs still suffer from the mode collapse problems. Besides, cell counts of generated images do not match those of their given labels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.7.1 Description of Steering Angle</head><p>The Steering Angle dataset is a subset of an autonomous driving dataset <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b50">[51]</ref>. Steering Angle consists of 12,271 RGB images with 1,904 distinct steering angles ranging from ?80 ? to 80 ? . We resize all images to 64 ? 64. The histogram of the steering angles in this dataset is show in <ref type="figure">Fig. S.7</ref>.14. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.7.2 Network architectures</head><p>The network architectures used in this experiment is similar to those in the RC-49 and UTKFace experiments. Please refer to our codes for more details about the network specifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.7.3 Training setups</head><p>The cGAN and CcGAN are trained for 20,000 iterations on the training set with the Adam <ref type="bibr" target="#b47">[48]</ref> optimizer (with ? 1 = 0.5 and ? 2 = 0.999) and a constant learning rate 10 ?4 . The rule of thumb formalue in Section ?? are used to select the hyperparameters for HVDL and SVDL, where we let m ? = 5.</p><p>Please note that, similar to the Cell-200 experiment, we use different batch sizes for the generator and discriminator in four CcGAN methods. The batch size is set to 64 and 512 respectively for the discriminator and generator in CcGAN. Please refer to Supp. S.6.3 for the reason.</p><p>Please see our codes for more details of the training setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.7.4 Testing setups</head><p>At the testing stage, we first set 2,000 evenly spaced evaluation labels in [?80 ? , 80 ? ] and we ask each GAN model to generate 50 images conditional on each evaluation label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.7.5 Performance measures</head><p>The quality of generated images from each GAN is evaluated by SFID, NIQE, Diversity, and Label Score.</p><p>? SFID: To computing SFID, we preset 1,000 SFID centers [?80 ? , 80 ? ] and let r SFID = 2 ? . These SFID centers and r SFID = 2 define 1,000 joint SFID intervals. We compute one FID between real and fake images with labels in this interval. We report the mean (i.e., SFID) and standard deviation of these FIDs for each GAN. ? NIQE <ref type="bibr" target="#b37">[38]</ref>: Different from previous three experiments, we train only one NIQE model by using all real images in the Steering Angle dataset since this dataset is highly imbalanced. In the evaluation, we compute one NIQE score for each SFID interval. The reported NIQE score in <ref type="table" target="#tab_3">Table 4</ref> is the mean of these NIQE scores. ? Diversity: The original autonomous driving dataset <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b50">[51]</ref> does not have class labels. To compute Diversity, we manually group the images in Steering Angle into five categories according to their background objects or the types of the road in the images. The five groups are labeled respectively by tree, tree+barrier, bush, bush+barrier, and winding mountain road. Some example images for these five groups are show in <ref type="figure" target="#fig_6">Fig. S.7.15</ref>. Images in the tree group all have trees in the background. Images in the tree+barrier group all have trees and barriers in the background. Images in the bush group all have bushes in the background. Images in the bush+barrier group all have bushes and barriers in the background. Images in the winding mountain road group all correspond to the scenes of winding mountain roads. A classification-oriented ResNet-34 is trained to classify images from these five groups, and then the Diversity score can be computed based on the entropies of predicted scenes in each SFID interval. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.8 EVALUATION RESULTS OF LOW-RESOLUTION EXPERIMENTS UNDER FID AND IS</head><p>Inception Score (IS) <ref type="bibr" target="#b51">[52]</ref> and Fr?chet Inception Distance <ref type="bibr" target="#b34">[35]</ref>, originally proposed for unconditional image generation, are not appropriate overall metrics for our experiment. Consistent with the evaluation of cGANs in <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b33">[34]</ref>, a conditional generative model in our task needs to be evaluated from three perspectives: (1) visual quality, (2) intra-label diversity (the diversity of images with the same label), and (3) label consistency (whether the labels used as conditions are consistent with the true labels of fake images). (The labels used as conditions are also called assigned labels in our paper.) A conditional generative model's performance in these three perspective partially reflects its conditional density estimation quality. Since IS and FID are developed initially to evaluate images generated from unconditional GANs, they aim to assess the visual quality and marginal diversity of fake images, partially reflecting the marginal density estimation quality. Because computing IS and FID does not need the true and assigned labels of fake images, IS and FID cannot measure intra-label diversity and label consistency. For example, assume we have some fake images with high intra-label diversity and label consistency. We may dramatically degenerate these fake images' intra-label diversity and label consistency by shuffling their assigned labels. However, the IS and FID scores of these fake images won't change because assigned labels are not used in computing IS and FID scores. Furthermore, in our task, a model with high IS and low FID scores may still fail in our task since it may have low intra-label diversity or low label consistency. For example, although cGAN (concat) have better diversity and higher visual quality than cGAN (K classes) does, their label consistency scores are terrible, implying their failure in our task. Therefore, IS and FID are not appropriate overall metrics for our task. To comprehensively evaluate cGAN-generated images, <ref type="bibr" target="#b2">[3]</ref> proposes Intra-FID, which computes FID separately at each of the distinct labels and reports the average FID score. We further extend Intra-FID by SFID to the scenario with insufficient real images. Besides Intra-FID and SFID, we also use three separate metrics (NIQE, Diversity, and Label Score) to evaluate the visual quality, intra-label diversity, and label consistency, respectively. The evaluation in terms of these three individual metrics is often consistent with that based on the overall metric (i.e., Intra-FID or SFID) in our experiments. Therefore, Intra-FID and SFID are taken as the overall metric in our task. Although FID and IS may not correctly evaluate the quality of a conditional generative model in our task, we still report them in this appendix. Since our datasets are quite different from ImageNet <ref type="bibr" target="#b52">[53]</ref>, we train a classification ResNet-34 and an autoencoder (the one used for computing Intra-FID) from scratch on our datasets to compute IS and FID respectively. Table S.8.1 summarizes the evaluation results of all candidate methods on four low-resolution datasets in terms of IS and FID. We can see CcGAN still outperforms two cGANs, even though IS and FID are not appropriate overall metrics for our task.   S.9.0.1 Reformulated hinge loss Our CcGAN (SVDL+ILI) in the high-resolution experiments is trained with a reformulated hinge loss shown as follows.</p><formula xml:id="formula_57">L SVDL (D) = ? C 7 N r N r j=1 N r i=1</formula><p>E r ?N (0,? 2 ) W 3 ? min(0, ?1 + D(x r i , y r j + r ))</p><formula xml:id="formula_58">? C 8 N g N g j=1 N g i=1</formula><p>E g ?N (0,? 2 ) W 4 ? min(0, ?1 ? D(x g i , y g j + g )) ,</p><p>(S. <ref type="bibr" target="#b25">26)</ref> where r y ? y r j , g y ? y g j ,</p><formula xml:id="formula_59">W 3 = w r (y r i , y r j + r ) N r i=1 w r (y r i , y r j + r )</formula><p>, W 4 = w g (y g i , y g j + g ) N g i=1 w g (y g i , y g j + g )</p><p>, and C 7 and C 8 are some constants.</p><p>S.9.0.2 High-resolution RC-49</p><p>In the high-resolution experiment, we test CcGAN (SVDL+ILI), cGAN (150 classes), and cGAN (concat) on RC-49 with two resolutions, i.e., 128 ? 128 and 256 ? 256. We use SAGAN <ref type="bibr" target="#b4">[5]</ref> as the backbone for all candidates. We also use hinge loss <ref type="bibr" target="#b43">[44]</ref> to train cGAN (150 classes) and cGAN (concat), and Eq. (S.26) to train CcGAN (SVDL+ILI). DiffAugment <ref type="bibr" target="#b18">[19]</ref> with the strongest transformation combination (Color + Translation + Cutout) is also used in all GAN training. DiffAugment substantially alleviates the mode collapse problem of cGAN (K classes) on RC-49. When training each candidate GAN, at each iteration, we update the discriminator twice while update the generator once. For 128 ? 128 experiment, the batch size is 256. For 256 ? 256 experiment, the batch size is set 128. The rest experimental setups are consistent with the low-resolution experiment. Some example images in the 128 ? 128 resolution for this experiment are shown in Figs. S.9.19 and S.9.20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.9.0.3 High-resolution UTKFace</head><p>In the high-resolution experiment, we test CcGAN (SVDL+ILI), cGAN (60 classes), and cGAN (concat) on UTKFace with two resolutions, i.e., 128 ? 128 and 192 ? 192. We use SAGAN <ref type="bibr" target="#b4">[5]</ref> as the backbone for all candidates. We also use hinge loss <ref type="bibr" target="#b43">[44]</ref> to train cGAN (150 classes) and cGAN (concat), and Eq. (S.26) to train CcGAN (SVDL+ILI). DiffAugment <ref type="bibr" target="#b18">[19]</ref> with the strongest transformation combination (Color + Translation + Cutout) is also used in all GAN training. DiffAugment substantially alleviates the mode collapse problem of cGAN (K classes) on UTKFace. When training each candidate GAN, at each iteration, we update the discriminator four times while update the generator once. For 128 ? 128 experiment, the batch size is 256. For 192 ? 192 experiment, the batch size is set 96. We also use ? = 900 for the CcGAN training. The rest experimental setups are consistent with the low-resolution experiment. Some example images in the 192 ? 192 resolution for this experiment are shown in Figs. S.9.21 and S.9.22.</p><p>S.9.0.4 High-resolution Steering Angle In the high-resolution experiment, we test CcGAN (SVDL+ILI), cGAN (210 classes), and cGAN (concat) on Steering Angle in 128 ? 128 resolution. We use SAGAN <ref type="bibr" target="#b4">[5]</ref> as the backbone for all candidates. We also use hinge loss <ref type="bibr" target="#b43">[44]</ref> to train cGAN (150 classes) and cGAN (concat), and Eq. (S.26) to train CcGAN (SVDL+ILI). DiffAugment <ref type="bibr" target="#b18">[19]</ref> with the strongest transformation combination (Color + Translation + Cutout) is also used in all GAN training. In this experiment, even with DiffAugment, cGAN (K classes) still has the mode collapse problem. When training each candidate GAN, at each iteration, we update the discriminator twice while update the generator once. The batch size is set 256. The rest experimental setups are consistent with the low-resolution experiment. Some example images in the 128 ? 128 resolution for this experiment are shown in Figs. S.9.23 and S.9.24. and low intra-label diversity (e.g., second row only has boys). cGAN (concat) has high intra-label diversity and moderate visual quality but low label consistency (e.g., first row has many adults). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>y?pr(y) E x?pr(x|y) [log (D(x, y))] ? E y?pg(y) E x?pg(x|y) [log (1 ? D(x, y))] = ? log(D(x, y))p r (x, y)dxdy ? log(1 ? D(x, y))p g (x, y)dxdy,(1)L(G) = ? E y?pg(y) E z?q(z) [log (D(G(z, y), y))]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(G(z c,j , c), c)),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>? x g c,j )?(y ? c),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 :</head><label>2</label><figDesc>HVE (Eqs.<ref type="bibr" target="#b5">(6)</ref> and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: The workflow of the naive label input (NLI) mechanism. NLI inputs a regression label y into G by adding y element-wisely to the output of the first linear layer. NLI inputs y into D by label projection [3].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>The pre-trained CNN T1 + T2 for label embedding. The first subnetwork T1 consists of some convolutional layers (Conv.) and some linear layers. The second subnetwork T2 includes one linear layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>The label embedding network is a multilayer perceptron (MLP).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: The workflow of the improved label input (ILI) mechanism. ILI first uses an embedding network to convert y into its high-dimensional representation h. Then, h is input into G and D by conditional batch normalization [32] and label projection [3], respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Definition 1 .</head><label>1</label><figDesc>(H?lder Class) Define the H?lder class of functions as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>1 ?</head><label>1</label><figDesc>. Therefore, RC-49 consists of 44,051 64 ? 64 rendered RGB images and 899 distinct angles. Please see Supp. S.4 for more details of the data generation. Example images are shown in Fig. S.4.2 in Appendix. Experimental setup: Not all images are used for the GAN training. A yaw angle is selected for training if its last digit is odd. Moreover, at each selected angle, only 25 images are randomly chosen for training. Thus, the training set includes 11250 images and 450 distinct angles. The remaining images are held out for evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>the performances of each GAN. The example fake images in Fig. S.4.2 in Appendix and line graphs in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 8 :</head><label>8</label><figDesc>Line graphs of FID/NIQE/Diversity/Lable Score versus yaw angle for RC-49. Figs. (a) to (c) show that four CcGAN methods consistently outperform cGAN (150 classes) across all angles. All graphs of CcGANs appear much smoother than those of cGAN (150 classes) because of HVDL and SVDL. Figs. (a) and (d) show that four CcGAN methods consistently outperform cGAN (concat) across all angles. Moreover, in most graphs, we can clearly see ILI-based CcGANs perform better than NLI-based CcGANs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 9 :</head><label>9</label><figDesc>Line graphs of Intra-FID versus the sample size for each distinct training angle of RC-49. The grey vertical dashed line stands for the sample size used in the main study of the RC-49 experiment. Four CcGAN methods substantially outperform two cGANs and ILI performs better than NLI no matter what the sample size for each distinct angle in the training set. The overall trend in this figure shows that a smaller sample size deteriorates the performance of both cGAN and CcGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 10 :</head><label>10</label><figDesc>Line graphs of FID/NIQE/Diversity/Lable Score versus Age for UTKFace. The four CcGAN methods significantly outperform cGAN (60 classes) in Figs. (a) to (c). All graphs of CcGANs appear much smoother than those of cGAN (60 classes) because of HVDL and SVDL. Figs. (a) and (d) show that four CcGAN methods consistently outperform cGAN (concat) across all ages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>. S.6.11. Experimental setup: The Cell-200 dataset consists of 200,000 64 ? 64 grayscale images. The number of cells per image ranges from 1 to 200 and there are 1,000 images for each cell count. However, only a subset of Cell-200 with only odd cell counts and 10 images per count (1,000 training images in total) is used for the GAN training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>. S.6.11 some example fake images from cGAN and CcGAN and line graphs of FID/NIQE/Label Score versus Cell Count inFig. 12. Unlike the experimental results on RC-49 and UTKFace, although NLI-based CcGANs outperform cGAN (100 classes) in terms of Intra-FID and NIQE, their label scores are very high, which implies low label consistency. Fortunately, two ILI-based CcGANs still perform very well and substantially outperform two cGANs and two NLI-based CcGANs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 14 :</head><label>14</label><figDesc>Six example RC-49 images in 128 ? 128 resolution for each of 3 rotation angles: real images and example fake images from CcGAN (SVDL+ILI), cGAN (150 classes), and cGAN (concat), respectively. The fake images from CcGAN are almost indistinguishable from real images. Oppositely, cGAN (150 classes) has bad visual quality and low diversity. cGAN (concat) has fair visual quality and terrible label consistency. see Supp. S.9.0.3 for details. The quantitative and visual results are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>TRAINING Algorithm S. 1 : 2 Train D; 3 4 5 Initialize ? r d = ?, ? f d = ?; 6 for i = 1 to m d do 7 8 9 end 10 Update</head><label>12345678910</label><figDesc>An algorithm for CcGAN training with the proposed HVDL.Data: N r real image-label pairs ? r = {(x r 1 , y r 1 ), . . . , (x r N r , y r N r )}, N r uy ordered distinct labels ? = {y r [1] , . . . , y r [N ruy ] } in the dataset, preset ? and ?, number of iterations K, the discriminator batch size m d , and the generator batch size m g . Result: Trained generator G.1 for k = 1 to K do Draw m d labels Y d with replacement from ?;Create a set of target labels Y d, = {y i + |y i ? Y d , ? N (0, ? 2 ), i = 1, . . . , m d } (D is conditional on these labels) ;Randomly choose an image-label pair (x, y) ? ? r satisfying |y ? y i ? | ? ? where y i + ? Y d, and let ? r d = ? r d ? (x, y i + ). ;Randomly draw a label y from U (y i + ? ?, y i + + ?) and generate a fake image x by evaluating G(z, y ), where z ? N (0, I).Let? f d = ? f d ? (x , y i + ). ; D with samples in set ? r d and ? f d via gradient-based optimizers based on Eq. (11); 11</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Algorithm S. 2 : 2 Train D; 3 4 6 for i = 1 to m d do 7 8 9 3 ?) 11 end 12 Update</head><label>2234678931112</label><figDesc>? f g via gradient-based optimizers based on Eq.(13) ; 16 end An algorithm for CcGAN training with the proposed SVDL.Data: N r real image-label pairs ? r = {(x r 1 , y r 1 ), . . . , (x r N r , y r N r )}, N r uy ordered distinct labels ? = {y r [1] , . . . , y r [N ruy ] } in the dataset, preset ? and ?, number of iterations K, the discriminator batch size m d , and the generator batch size m g . Result: Trained generator G.1 for k = 1 to K do Draw m d labels Y d with replacement from ?;Create a set of target labels Y d, = {y i + |y i ? Y d , ? N (0, ? 2 ), i = 1, . . . , m d } (D is conditional on these labels) ;5Initialize? r d = ?, ? f d = ?;Randomly choose an image-label pair (x, y) ? ? r satisfying e ??(y?y i ? ) 2 &gt; 10 ?3 where y i + ? Y d, and let ? r d = ? r d ? (x, y i + ). This step is used to exclude real images with too small weights. ;Compute w r i (y, y i + ) = e ??(y i + ?y) 2 ; Randomly draw a label y from U (y i + ? ? log 10 ?3 ? , y i + + ? log 10 ?and generate a fake image x by evaluating G(z, y ), where z ? N (0, I). Let ? f d = ? f d ? (x , y i + ). ; 10 Compute w g i (y , y i + ) = e ??(y i + ?y ) 2 ; D with samples in set ? r d and ? f d via gradient-based optimizers based on Eq. (12); 13</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>?</head><label></label><figDesc>Let f (x, y) = ? log D(x, y) and F = ? log D. ? Letp KDE r (y) andp KDE g (y) stand for the KDEs of p r (y) and p g (y) respectively. ? For HVDL, denote respectively by p y,? r (x) p r (x|Y = y ) 1 {|y ?y|??} p r (y ) 1 {|y ?y|??} p r (y )dy dy and p y,? g (x) p g (x|Y = y ) 1 {|y ?y|??} p g (y ) 1 {|y ?y|??} p g (y )dy dy the PDFs of the marginal distributions for real and fake images with labels in [y ? ?, y + ?]. ? For SVDL, given y and the weight functions, if the number of real and fake images are infinite, the real and fake empirical densities converges to p y,w r r (x) p r (x|Y = y ) w r (y , y)p r (y ) W r (y) dy and p y,w g g (x) p g (x|Y = y ) w g (y , y)p g (y ) W g (y) dy respectively, where W r (y) w r (y , y)p r (y )dy , W g (y) w g (y , y)p g (y )dy , and w r and w g are the weight functions defined as follows w r (y , y) = e ??(y ?y) 2 and w g (y , y) = e ??(y ?y) 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>( A1 )</head><label>A1</label><figDesc>All D's in D are measurable and uniformly bounded. Let U max{ sup D?D [? log D] , sup D?D [? log(1 ? D)]} and U &lt; ?; (A2) For ?x ? X and y, y ? Y, ?g r (x) &gt; 0 and M r &gt; 0, s.t. |p r (x|Y = y ) ? p r (x|Y = y)| ? g r (x)|y ? y| with g r (x)dx = M r ; (A3) For ?x ? X and y, y ? Y, ?g g (x) &gt; 0 and M g &gt; 0, s.t. |p g (x|Y = y ) ? p g (x|Y = y)| ? g g (x)|y ? y| with g g (x)dx = M g ; (A4) p r (y) ? ?(L r ) and p g (y) ? ?(L g ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Theorem S. 1 (</head><label>1</label><figDesc>Hoeffding's inequality<ref type="bibr" target="#b7">[8]</ref>). Let Z 1 , . . . , Z m be a sequence of i.i.d. random variables and letZ = 1 m m i=1 Z i . Assume that E[Z] = ? and P r(a ? Z i ? b) = 1 for every i. Then, for any &gt; 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>(S. 4 ) 2 )EELemma S. 2 (</head><label>422</label><figDesc>For the second term, we have sup D?D X?p y,? (x) [? log D(X, y)] ? E X?p(x|Y =y) [? log D(X, y)]= sup D?D [? log D(x, y)] ? [p y,? (x) ? p(x|Y = y)] dx ? sup D?D |? log D(x, y)| ? |p y,? (x) ? p(x|Y = y)| dx ?U |p y,? (x) ? p(x|Y = y)| dx. (S.5)Then, we focus on |p y,? (x) ? p(x|Y = y)|. By the definition of p y,? (x) and defining p ? (y ) =1 {|y ?y|??} p(y ) 1 {|y ?y|??} p(y )dy , we have |p y,? (x) ? p(x|Y = y)| = p(x|Y = y )p ? (y )dy ? p(x|Y = y) ? |p(x|Y = y ) ? p(x|Y = y)| p ? (y )dy(by (A2), and let g = g r for real images and g = g g for fake images)? g(x)|y ? y|p ? (y )dy ??g(x). Thus, Eq. (S.5) is upper bounded as follows, sup D?D X?p y,? (x) [? log D(X, y)] ? E X?p(x|Y =y) [? log D(X, y)] ?U ?g(x)dx (by (A2)) =?U M. (S.6) By combining Eq. (S.4) and (S.6), we can get Eq. (S.3), which finishes the proof. Lemma for SVDL). Suppose that (A1), (A2) and (A4) hold and let (X 1 , Y 1 ), . . . , (X N , Y N ) be a sequence of i.i.d. random image-label pairs, then ?? ? (0, 1), with probability at least 1 ? ?,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>p</head><label></label><figDesc>y,w (x) p(x|Y = y ) w(y , y)p(y ) W (y) dy and p w (y |Y = y) w (y , y) p r (y ) W r (y) , we have |p y,w (x) ? p(x|Y = y)| = p(x|Y = y )p w (y |Y = y) dy ? p(x|Y = y) ? |p(x|Y = y ) ? p(x|Y = y)| p w (y |Y = y) dy .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Theorem S. 2 .</head><label>2</label><figDesc>Letp KDE r (y) andp KDE g (y) stand for the KDE of p r (y) and p g (y) respectively. Under condition (A4), if the KDEs are based on N i.i.d. samples from p r /p g and a bandwidth ?, for all ? ? (0, 1), with probability at least 1 ? ?, g (y) ? p g (y) ? C KDE 2,? log N N ? + L g ?, (S.15)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head></head><label></label><figDesc>X?pg(x|Y =y) [? log(1 ? D(X, y))] p KDE g (y)dy . These four terms in the RHS can be bounded separately as follows 1) The first term can be bounded by using Theorem S.2 and the boundness of D. For the first term, ?? 1 ? (0, 1), with at least probability 1 ? ? 1 , sup D?D [? log D(x, y)] p r (x|Y = y)dx (p r (y) ?p KDE r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>11</head><label></label><figDesc>{|y?Y r i |??} [? log D(X r i , y)] ? E X?pr(x|Y =y) [? log D(X, y)] p KDE r {|y?Y r i |??} [? log D(X r i , y)] ? E X?pr(x|Y =y) [? log D(X, y)] p KDE r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>With ? 1 = ? 2 = ? 3 = ? 4 = ? 4 ,</head><label>1234</label><figDesc>combining Eq. (S.17) -(S.20) leads to the upper bound in Theorem S.3. Theorem S.4. Assume that (A1)-(A4) hold, then ?? ? (0, 1), with probability at least 1 ? ?,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Fig. S. 3 . 1 :</head><label>31</label><figDesc>An illustrative diagram for error bounds of HVDL and SVDL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>S. 4 . 6</head><label>46</label><figDesc>Example 64 ? 64 RC-49 images Example RC-49 images are shown in Fig. S.4.2. Fig. S.4.2: Three RC-49 example images in 64 ? 64 resolution for each of 10 angles: real images and example fake images from cGAN and four proposed CcGANs, respectively. CcGANs produce chair images with higher visual quality and more diversity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Fig. S. 4 . 4 :Fig. S. 5 . 6 :</head><label>4456</label><figDesc>Some example RC-49 fake images from a degenerated NLI-based CcGAN.Fig. S.4.5: Example RC-49 fake images from cGAN when we bin the yaw angle range into different number of classes. The histogram of the UTKFace dataset with ages varying from 1 to 60.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Fig. S. 5 . 7 :</head><label>57</label><figDesc>Three UTKFace example images in 64 ? 64 resolution for each of 10 ages: real images and example fake images from cGAN and four proposed CcGANs, respectively. CcGANs produce face images with higher visual quality and more diversity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Fig. S. 5 . 8 :</head><label>58</label><figDesc>Some examples of generated UTKFace images from the four CcGAN methods. We fix the noise z but vary the label y from 3 to 57.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Fig. S. 5 . 9 :</head><label>59</label><figDesc>Some example UTKFace fake images from a degenerated NLI-based CcGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>Fig. S. 5 . 10 :</head><label>510</label><figDesc>Example UTKFace fake images from cGAN when we bin the age range into different number of classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head></head><label></label><figDesc>Example Cell-200 images are shown in Fig. S.6.11.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>Fig. S. 6 . 11 :</head><label>611</label><figDesc>Three Cell-200 images in 64 ? 64</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head>Fig. S. 6 . 12 :</head><label>612</label><figDesc>Some examples of generated Cell-200 images from the four CcGAN methods. We fix the noise z but vary the label y from 10 to 200.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head>Fig. S. 6 . 13 :</head><label>613</label><figDesc>Example Cell-200 fake images from cGAN when we bin the range of cell count into different number of classes. S.7 MORE DETAILS OF THE EXPERIMENT ON THE LOW-RESOLUTION STEERING ANGLE DATASET IN SECTION 5.4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head>Fig. S. 7 . 14 :</head><label>714</label><figDesc>The histogram of the Steering Angle dataset with steering angles varying from ?80 ? to 80 ? . At many angles, we only have 1 or 2 images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><head>Fig. S. 7 . 16 :</head><label>716</label><figDesc>Three Steering Angle images in 64 ? 64 resolution for each of 10 angles: real images and example fake images from cGAN and four proposed CcGANs, respectively. cGAN has severe mode collapse problem in this experiment. Two NLI-based CcGANs do not work well but two ILI-baesd CcGANs produce images with higher visual quality and more diversity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_41"><head>Fig. S. 7 . 17 :</head><label>717</label><figDesc>Some examples of generated Steering Angle images from the four CcGAN methods. We fix the noise z but vary the label y from ?71.8 ? to 72 ? .Fig. S.7.18: Example Steering Angle fake images from cGAN when we bin the range of steering angles into different number of classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_42"><head>Fig. S. 9 . 19 :</head><label>919</label><figDesc>Some example real RC-49 images and fake RC-49 images from CcGAN (SVDL+ILI) in the 128 ? 128 resolution. We can see CcGAN can generate visually realistic, diverse and label consistent images.Fig. S.9.20: Some example fake RC-49 images from cGAN (150 classes) and cGAN (concat) in the 128 ? 128 resolution. They show two types of failures of conventional cGANs. cGAN (150 classes) has high label consistency but low visual quality and low intra-label diversity. cGAN (concat) has high intra-label diversity and fair visual quality but low label consistency.Fig. S.9.21: Some example real UTKFace images and fake UTKFace images from CcGAN (SVDL+ILI) in the 192 ? 192 resolution. We can see CcGAN can generate visually realistic, diverse and label consistent images.Fig. S.9.22: Some example fake UTKFace images from cGAN (60 classes) and cGAN (concat) in the 192 ? 192 resolution. They show two types of failures of conventional cGANs. cGAN (60 classes) has high label consistency but low visual quality (e.g., last row)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_43"><head>Fig. S. 9 . 23 :</head><label>923</label><figDesc>Some example real Steering Angle images and fake Steering Angle images from CcGAN (SVDL+ILI) in the 128 ? 128 resolution. We can see CcGAN can generate visually realistic, diverse and label consistent images.Fig. S.9.24: Some example fake Steering Angle images from cGAN (210 classes) and cGAN (concat) in the 128 ? 128 resolution. They show two types of failures of conventional cGANs. cGAN (150 classes) has high label consistency but fair visual quality and low intra-label diversity. cGAN (concat) has high intra-label diversity and moderate visual quality but low label consistency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 :</head><label>1</label><figDesc>Average quality of 179,800 fake RC-49 images from cGAN and CcGAN with standard deviations after the "?" symbol. "?" ("?") indicates lower (higher) values are preferred.</figDesc><table><row><cell>Method</cell><cell>Intra-FID ?</cell><cell>NIQE ?</cell><cell>Diversity ?</cell><cell>Label Score ?</cell></row><row><cell>cGAN (150 classes)</cell><cell>1.720 ? 0.384</cell><cell>2.731 ? 0.162</cell><cell>0.779 ? 0.199</cell><cell>4.815 ? 5.152</cell></row><row><cell>cGAN (concat)</cell><cell>1.141 ? 0.108</cell><cell>1.819 ? 0.111</cell><cell>2.459 ? 0.049</cell><cell>30.212 ? 21.391</cell></row><row><cell>CcGAN (HVDL+NLI)</cell><cell>0.612 ? 0.145</cell><cell>1.869 ? 0.181</cell><cell>2.353 ? 0.121</cell><cell>5.617 ? 4.452</cell></row><row><cell>CcGAN (SVDL+NLI)</cell><cell>0.515 ? 0.181</cell><cell>1.853 ? 0.159</cell><cell>2.610 ? 0.113</cell><cell>4.982 ? 4.439</cell></row><row><cell>CcGAN (HVDL+ILI)</cell><cell cols="3">0.424 ? 0.081 1.805 ? 0.179 2.814 ? 0.052</cell><cell>1.816 ? 1.481</cell></row><row><cell>CcGAN (SVDL+ILI)</cell><cell cols="3">0.389 ? 0.095 1.783 ? 0.173 2.949 ? 0.069</cell><cell>1.940 ? 1.489</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 :</head><label>2</label><figDesc>Average quality of 60,000 fake UTKFace images from cGAN and CcGAN with standard deviations after the "?" symbol. "?" ("?") indicates lower (higher) values are preferred.</figDesc><table><row><cell>Method</cell><cell>Intra-FID ?</cell><cell>NIQE ?</cell><cell>Diversity ?</cell><cell>Label Score ?</cell></row><row><cell>cGAN (60 classes)</cell><cell>4.516 ? 0.965</cell><cell>2.315 ? 0.306</cell><cell>0.254 ? 0.353</cell><cell>11.087 ? 8.119</cell></row><row><cell>cGAN (concat)</cell><cell>0.834 ? 0.199</cell><cell>2.051 ? 0.227</cell><cell cols="2">1.394 ? 0.026 17.291 ? 11.717</cell></row><row><cell>CcGAN (HVDL+NLI)</cell><cell>0.572 ? 0.167</cell><cell>1.739 ? 0.145</cell><cell>1.338 ? 0.178</cell><cell>9.782 ? 7.166</cell></row><row><cell>CcGAN (SVDL+NLI)</cell><cell>0.547 ? 0.181</cell><cell>1.753 ? 0.196</cell><cell>1.326 ? 0.198</cell><cell>10.739 ? 8.340</cell></row><row><cell>CcGAN (HVDL+ILI)</cell><cell cols="2">0.480 ? 0.145 1.709 ? 0.169</cell><cell>1.280 ? 0.203</cell><cell>7.505 ? 5.857</cell></row><row><cell>CcGAN (SVDL+ILI)</cell><cell cols="2">0.425 ? 0.157 1.725 ? 0.171</cell><cell>1.298 ? 0.176</cell><cell>7.452 ? 6.022</cell></row></table><note>Extra experimental results: The histogram in Fig. S.5.6 shows that the UTKFace dataset is highly imbalanced. To balance the training data and also test the performance of cGAN and CcGAN under smaller sample sizes, we vary the maximum sample size for each distinct age in the training from 200 to 50.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3 :</head><label>3</label><figDesc>Average quality of 200,000 fake Cell-200 images from cGAN and CcGAN with standard deviations after the "?" symbol. "?" ("?") indicates lower (higher) values are preferred.Fig. 12: Line graphs of FID/NIQE/Lable Score versus Cell Count for Cell-200.Figs. (a)to (c) show that, although the NLI-based CcGANs do not perform well, the ILI-based CcGANs outperform both cGANs across most cell counts. All graphs of CcGANs also appear much smoother than those of cGAN (100 classes) because of HVDL and SVDL. Moreover, in all figures, we can see ILI-based CcGANs perform better than NLI-based CcGANs especially in</figDesc><table><row><cell>Method</cell><cell>Intra-FID ?</cell><cell>NIQE ?</cell><cell>Label Score ?</cell></row><row><cell>cGAN (100 classes)</cell><cell>90.255 ? 64.595</cell><cell>2.130 ? 2.440</cell><cell>66.748 ? 51.711</cell></row><row><cell>cGAN (concat)</cell><cell>41.599 ? 21.430</cell><cell>3.250 ? 0.646</cell><cell>73.187 ? 51.133</cell></row><row><cell cols="2">CcGAN (HVDL+NLI) 50.052 ? 20.584</cell><cell>1.488 ? 0.153</cell><cell>72.599 ? 37.425</cell></row><row><cell>CcGAN (SVDL+NLI)</cell><cell>56.078 ? 19.334</cell><cell>1.829 ? 0.386</cell><cell>83.367 ? 49.577</cell></row><row><cell>CcGAN (HVDL+ILI)</cell><cell>8.759 ? 6.652</cell><cell>1.283 ? 0.534</cell><cell>5.861 ? 4.900</cell></row><row><cell>CcGAN (SVDL+ILI)</cell><cell>7.266 ? 2.305</cell><cell>1.220 ? 0.515</cell><cell>5.905 ? 5.020</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4 :</head><label>4</label><figDesc>Average quality of 100,000 fake Steering Angle images from cGAN and CcGAN with standard deviations after the "?" symbol. "?" ("?") indicates lower (higher) values are preferred.</figDesc><table><row><cell>Method</cell><cell>SFID ?</cell><cell>NIQE ?</cell><cell>Diversity ?</cell><cell>Label Score ?</cell></row><row><cell>cGAN (210 classes)</cell><cell>3.285 ? 0.647</cell><cell>1.296 ? 0.095</cell><cell>0.603 ? 0.396</cell><cell>14.596 ? 15.402</cell></row><row><cell>cGAN (concat)</cell><cell>2.446 ? 1.122</cell><cell>1.717 ? 0.003</cell><cell>1.255 ? 0.015</cell><cell>41.686 ? 25.864</cell></row><row><cell>CcGAN (HVDL+NLI)</cell><cell>1.969 ? 0.676</cell><cell>1.093 ? 0.024</cell><cell>0.991 ? 0.361</cell><cell>22.322 ? 18.758</cell></row><row><cell>CcGAN (SVDL+NLI)</cell><cell>1.866 ? 0.649</cell><cell>1.098 ? 0.038</cell><cell>1.007 ? 0.248</cell><cell>19.678 ? 18.281</cell></row><row><cell>CcGAN (HVDL+ILI)</cell><cell>1.635 ? 0.699</cell><cell>1.152 ? 0.047</cell><cell>1.153 ? 0.153</cell><cell>10.868 ? 9.644</cell></row><row><cell>CcGAN (SVDL+ILI)</cell><cell>1.546 ? 0.626</cell><cell>1.130 ? 0.078</cell><cell>1.156 ? 0.189</cell><cell>10.933 ? 8.978</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>To plot these line graphs, we compute these metrics within each SFID interval defined by the corresponding SFID center. Figs. (a) to (d) show that, although the NLI-based CcGANs do not have good label consistency, the ILI-based CcGANs substantially outperform cGAN (210 classes) in most SFID intervals. All graphs of CcGANs also appear much smoother than those of cGAN (210 classes) because of HVDL and SVDL.Figs. (b)to (c) show that athough cGAN (concat) has the highest Diversity score, it also has the worst NIQE score and Label Score.</figDesc><table /><note>1. Moreover, to simulate the scenario where we have very few real images to compute Intra-FID, we deliberately Fig. 13: Line graphs of FID/NIQE/Diversity/Lable Score versus SFID Center for the Steering Angle dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6 :</head><label>6</label><figDesc>Average quality of 179,800 fake high-resolution RC-49 images from cGAN (150 classes), cGAN (concat), and CcGAN with standard deviations after the "?" symbol. "?" ("?") indicates lower (higher) values are preferred.</figDesc><table><row><cell>Resolution</cell><cell>Method</cell><cell>Intra-FID ?</cell><cell>NIQE ?</cell><cell>Diversity ?</cell><cell>Label Score ?</cell></row><row><cell></cell><cell>cGAN (150 classes)</cell><cell>1.250 ? 0.492</cell><cell>2.293 ? 0.133</cell><cell>2.341 ? 0.224</cell><cell>2.032 ? 1.653</cell></row><row><cell>128 ? 128</cell><cell>cGAN (concat)</cell><cell>1.128 ? 0.166</cell><cell>2.104 ? 0.104</cell><cell>3.431 ? 0.039</cell><cell>29.414 ? 7.052</cell></row><row><cell></cell><cell cols="4">CcGAN (SVDL+ILI) 0.111 ? 0.033 1.775 ? 0.051 3.552 ? 0.047</cell><cell>2.643 ? 2.077</cell></row><row><cell></cell><cell>cGAN (150 classes)</cell><cell>1.224 ? 0.336</cell><cell>2.147 ? 0.085</cell><cell>2.462 ? 0.095</cell><cell>2.790 ? 2.852</cell></row><row><cell>256 ? 256</cell><cell>cGAN (concat)</cell><cell>1.325 ? 0.227</cell><cell>3.153 ? 0.122</cell><cell>3.120 ? 0.043</cell><cell></cell></row></table><note>28.776 ? 20.273 CcGAN (SVDL+ILI) 0.495 ? 0.139 1.655 ? 0.070 2.844 ? 0.101 3.260 ? 2.641 6.2 High-resolution UTKFace In this experiment, we test three candidate methods on UTKFace with two resolutions, i.e., 128 ? 128 and 192 ? 192. Most training setups are consistent with Section 5.2 except that we let ? = 900 when implementing CcGAN (SVDL+ILI). Please</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7 :</head><label>7</label><figDesc>CcGAN (SVDL+ILI) 0.499 ? 0.186 1.661 ? 0.047</figDesc><table><row><cell>Resolution</cell><cell>Method</cell><cell>Intra-FID ?</cell><cell>NIQE ?</cell><cell>Diversity ?</cell><cell>Label Score ?</cell></row><row><cell></cell><cell>cGAN (60 classes)</cell><cell>1.195 ? 0.356</cell><cell>1.381 ? 0.208</cell><cell>0.788 ? 0.425</cell><cell>6.150 ? 5.268</cell></row><row><cell>128 ? 128</cell><cell>cGAN (concat)</cell><cell>0.408 ? 0.144</cell><cell>1.377 ? 0.079</cell><cell>1.332 ? 0.026</cell><cell>18.064 ? 12.550</cell></row><row><cell></cell><cell cols="3">CcGAN (SVDL+ILI) 0.367 ? 0.123 1.113 ? 0.033</cell><cell>1.199 ? 0.232</cell><cell>7.747 ? 6.580</cell></row><row><cell></cell><cell>cGAN (60 classes)</cell><cell>0.908 ? 0.327</cell><cell>1.755 ? 0.215</cell><cell>1.047 ? 0.381</cell><cell>6.639 ? 5.686</cell></row><row><cell>192 ? 192</cell><cell>cGAN (concat)</cell><cell>0.591 ? 0.214</cell><cell>2.352 ? 0.126</cell><cell>1.358 ? 0.019</cell><cell>17.116 ? 11.652</cell></row></table><note>Average quality of 60,000 fake high-resolution UTKFace images from cGAN (60 classes), cGAN (concat), and CcGAN with standard deviations after the "?" symbol. "?" ("?") indicates lower (higher) values are preferred.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 8 :</head><label>8</label><figDesc></figDesc><table><row><cell>Method</cell><cell>SFID ?</cell><cell>NIQE ?</cell><cell>Diversity ?</cell><cell>Label Score ?</cell></row><row><cell>cGAN (210 classes)</cell><cell>4.963 ? 0.916</cell><cell>2.520 ? 0.282</cell><cell>0.564 ? 0.401</cell><cell>15.903 ? 11.522</cell></row><row><cell>cGAN (concat)</cell><cell>2.140 ? 0.821</cell><cell>2.542 ? 0.006</cell><cell cols="2">1.292 ? 0.014 21.383 ? 13.697</cell></row><row><cell cols="3">CcGAN (SVDL+ILI) 1.689 ? 0.443 2.411 ? 0.100</cell><cell>1.088 ? 0.243</cell><cell>9.193 ? 8.005</cell></row></table><note>Average quality of 100,000 fake 128 ? 128 Steering Angle images from cGAN (210 classes), cGAN (concat), and CcGAN with standard deviations after the "?" symbol. "?" ("?") indicates lower (higher) values are preferred.ACKNOWLEDGMENTS This work was supported by UBC ARC Sockeye, Compute Canada, and the Natural Sciences and Engineering Research Council of Canada (NSERC) under Grants CRDPJ 476594-14, RGPIN-2019-05019, and RGPAS2017-507965. Fig. 16: Six example Steering Angle images in 128 ? 128 resolution for each of 3 steering angles: real images and example fake images from CcGAN (SVDL+ILI), cGAN (150 classes), and cGAN (concat), respectively. Obviously, CcGAN outperforms both cGANs. Notably, cGAN (210 classes) has the mode collapse problem [25], [42], [43] on this dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>InterpolationInFig. S.4.3, we present some interpolation results of the four CcGAN methods (i.e., HVDL+NLI, SVDL+NLI, HVDL+ILI, and SVDL+ILI). For an input pair (z, y), we fix the noise z but perform label-wise interpolations, i.e., varying label y from 4.5 to 85.5. Clearly, all generated images are visually realistic and we can see the chair distribution smoothly changes over continuous angles. Please note that,Fig. S.4.3 is meant to show the smooth change of the chair distribution instead of one single chair so the chair type may change over angles. This confirms CcGAN is capable of capturing the underlying conditional image distribution rather than simply memorizing training data. Fig. S.4.3: Some example RC-49 fake images from the four CcGAN methods. We fix the noise z but vary the label y. S.4.7.2 Degenerated CcGAN In this experiment, we consider the extreme case of the proposed CcGAN (degenerated CcGAN), i.e., ? ? 0 and ? ? 0 or ? ? +?. Some examples from a degenerated NLI-based CcGAN are shown in</figDesc><table><row><cell>S.4.7 Extra experiments</cell></row><row><cell>S.4.7.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Fig. S.7.15: Example Steering Angle images from 5 scenes, i.e., tree, tree+barrier, bush, bush+barrier, and winding mountain road (from left to right). Similar to previous experiments, we pre-train a regression-oriented ResNet-34 to predict the angle for each fake image, and then computes Label Score. Please note that, when plotting the line graph of Label Score versus SFID Center inFig 13,one Label Score is computed for each SFID interval. Interpolation In this section, for each CcGAN method, we fix the noise vector z but vary the regression label y from ?71.8 ? to 72 ? . We can see the road in the image gradually changes from a left turn to a right turn.S.7.7.2 cGAN: different number of classesIn this experiment, we experimented with three different bin setting -grouping labels into 90 classes, 150 classes, and 210 classes, respectively. Experimental results are shown inFig. S.7.18. We observe that different bin settings cannot improve cGAN's performance.</figDesc><table><row><cell>S.7.7 Extra experiments</cell></row><row><cell>S.7.7.1</cell></row></table><note>? Label Score:S.7.6 Example Steering Angle images Example Steering Angle images are shown in Fig. S.7.16.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE S .</head><label>S</label><figDesc>8.1: IS and FID scores of all candidates methods in 64 ? 64 experiments. CcGAN outperforms two cGANs, even though IS and FID may not correctly evaluate the quality of a model in our task.</figDesc><table><row><cell></cell><cell>RC-49</cell><cell></cell><cell>UTKFace</cell><cell></cell><cell cols="2">Cell-200</cell><cell cols="2">Steering Angle</cell></row><row><cell>Method</cell><cell>IS ?</cell><cell>FID ?</cell><cell>IS ?</cell><cell>FID ?</cell><cell>IS ?</cell><cell>FID ?</cell><cell>IS ?</cell><cell>FID ?</cell></row><row><cell>cGAN (K classes)</cell><cell>2.382 ? 0.004</cell><cell>1.066</cell><cell>2.636 ? 0.019</cell><cell>0.963</cell><cell>-</cell><cell>30.086</cell><cell>2.572 ? 0.029</cell><cell>0.976</cell></row><row><cell>cGAN (concat)</cell><cell>11.440 ? 0.056</cell><cell>0.295</cell><cell>3.103 ? 0.027</cell><cell>0.465</cell><cell>-</cell><cell>37.689</cell><cell>3.251 ? 0.020</cell><cell>0.255</cell></row><row><cell>CcGAN (HVDL+NLI)</cell><cell>14.730 ? 0.050</cell><cell>0.285</cell><cell>3.328 ? 0.027</cell><cell>0.114</cell><cell>-</cell><cell>40.279</cell><cell>3.587 ? 0.014</cell><cell>0.316</cell></row><row><cell>CcGAN (SVDL+NLI)</cell><cell>19.425 ? 0.087</cell><cell>0.207</cell><cell>3.307 ? 0.023</cell><cell>0.087</cell><cell>-</cell><cell>51.318</cell><cell>3.968 ? 0.014</cell><cell>0.212</cell></row><row><cell>CcGAN (HVDL+ILI)</cell><cell>17.992 ? 0.064</cell><cell>0.213</cell><cell>3.256 ? 0.028</cell><cell>0.056</cell><cell>-</cell><cell>3.263</cell><cell>4.592 ? 0.017</cell><cell>0.327</cell></row><row><cell>CcGAN (SVDL+ILI)</cell><cell>20.173 ? 0.072</cell><cell>0.197</cell><cell>3.382 ? 0.035</cell><cell>0.142</cell><cell>-</cell><cell>1.684</cell><cell>4.439 ? 0.013</cell><cell>0.331</cell></row></table><note>S.9 MORE DETAILS OF THE HIGH-RESOLUTION EXPERIMENTS IN SECTION 6</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">cGANs with projection discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The nature of statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Foundations of machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Understanding machine learning: from theory to algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semi-supervised regression with generative adversarial networks using minimal labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Olmschenk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Vicinal risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="416" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semi-supervised regression with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rezagholizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Haidar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>uS Patent App. 15/789,518</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reg-GAN: Semi-supervised learning based on generative adversarial networks for regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rezagholiradeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Haidar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2806" to="2810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dense crowd counting convolutional neural networks with minimal data using semi-supervised dual-goal generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Olmschenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition: Learning with Imperfect Data Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to generate chairs, tables and cars with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="692" to="705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attribute2image: Conditional image generation from visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="776" to="791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Controllable text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">DF-GAN: Deep fusion generative adversarial networks for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05865</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Differentiable augmentation for data-efficient GAN training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">On data augmentation for GAN training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-H</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-B</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-M</forename><surname>Cheung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Image augmentations for GAN training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">CcGAN: Continuous conditional generative adversarial networks for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ser. Proceedings of Machine Learning Research</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved training of Wasserstein GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Quantum Physics of Light and Matter: A Modern Introduction to Photons, Atoms and Many-Body Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Salasnich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Remarks on some nonparametric estimates of a density function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-S</forename><surname>Lii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Politis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Selected Works of Murray Rosenblatt</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="95" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On estimation of a probability density function and mode</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Parzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The annals of mathematical statistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1065" to="1076" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Density estimation for statistics and data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Silverman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>CRC press</publisher>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The elements of statistical learning: data mining, inference, and prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6594" to="6604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">On the evaluation of conditional GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pineda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.08175</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local Nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Age progression/regression by conditional adversarial autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5810" to="5818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Computational framework for simulating fluorescence microscope images with cell populations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lehmussola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ruusuvuori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Selinummi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huttunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yli-Harja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1010" to="1016" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">The Steering Angle dataset @ONLINE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://github.com/SullyChen/driving-datasets" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">VEEGAN: Reducing mode collapse in GANs using implicit variational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Valkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">U</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3308" to="3318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Escaping from collapsing modes in a constrained space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hubert Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="204" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Geometric GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Density estimation @ONLINE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wasserman</surname></persName>
		</author>
		<ptr target="http://www.stat.cmu.edu/?larry/=sml/densityestimation.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">Shapenet: An information-rich 3D model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Label-embedding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1425" to="1438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio and Y. LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Computational framework for simulating fluorescence microscope images with cell populations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lehmussola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ruusuvuori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Selinummi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huttunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yli-Harja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1010" to="1016" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to count objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1324" to="1332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">How a high school junior made a self-driving car? @ONLINE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://towardsdatascience.com/how-a-high-school-junior-made-a-self-driving-car-705fa9b6e860" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems, ser. NIPS&apos;16</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems, ser. NIPS&apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
