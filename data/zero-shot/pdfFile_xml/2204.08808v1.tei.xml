<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SUBMITED TO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 SePiCo: Semantic-Guided Pixel Contrast for Domain Adaptive Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binhui</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjia</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Chi</forename><forename type="middle">Harold</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoren</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">SUBMITED TO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 SePiCo: Semantic-Guided Pixel Contrast for Domain Adaptive Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Domain adaptation</term>
					<term>semantic segmentation</term>
					<term>semantic variations</term>
					<term>representation learning</term>
					<term>self-training !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain adaptive semantic segmentation attempts to make satisfactory dense predictions on an unlabeled target domain by utilizing the supervised model trained on a labeled source domain. One popular solution is self-training, which retrains the model with pseudo labels on target instances. Plenty of methods tend to alleviate noisy pseudo labels, however, they ignore intrinsic connections among cross-domain pixels with similar semantic concepts. In consequence, they would struggle to deal with the semantic variations across domains and to build a category-discriminative embedding space, leading to less discrimination and poor generalization. In this work, we propose Semantic-Guided Pixel Contrast (SePiCo), a novel one-stage adaptation framework that highlights the semantic concepts of individual pixel to promote learning of class-discriminative and class-balanced pixel embedding space across domains, eventually boosting the performance of self-training methods. Specifically, to explore proper semantic concepts, we first investigate a centroid-aware pixel contrast that employs the category centroids of the entire source domain or a single source image to guide the learning of discriminative features. Considering the possible lack of category diversity in semantic concepts, we then blaze a trail of distributional perspective to involve a sufficient quantity of instances, namely distribution-aware pixel contrast, in which we approximate the true distribution of each semantic category from the statistics of labeled source data. Moreover, such an optimization objective can derive a closed-form upper bound by implicitly involving an infinite number of (dis)similar pairs, making it computationally efficient. Extensive experiments show that SePiCo not only helps stabilize training but also yields discriminative features, making significant progress on both synthetic-to-real and daytime-to-nighttime scenarios. Most notably, SePiCo establishes state-of-the-art performance on tasks of GTAV ? Cityscapes, SYNTHIA ? Cityscapes and Cityscapes ? Dark Zurich, improving by 12.8% mIoU, 8.8% mIoU, and 9.2% mIoU compared to the previous best method, respectively. The code will be available at https://github.com/BIT-DA/SePiCo.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>G ENERALIZING deep neural networks to unseen domain is pivotal to a broad range of critical applications such as autonomous driving <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> and medical analysis <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. For example, autonomous cars are required to operate smoothly in diverse weather and illumination conditions, e.g., foggy, rainy, snowy, dusty, and nighttime. While humans excel at such scene understanding problems, it is struggling for machines to forecast. Semantic segmentation is a fundamental task relevant that assigns a unique label to every single pixel in the image. Recently, deep Convolution Neural Networks (CNNs) have made rapid progress with the remarkable generalization ability <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. CNNs, however, are quite data-hungry and the pixel-level labeling process is expensive and labor-intensive, thereby restricting their real-world utility. As a trade-off, training with freelyavailable synthetic data rendered from game engines <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> turns into a promising alternative. This is not the case, unfortunately, deep models trained on simulated data often drops largely in realistic scenarios due to domain shift <ref type="bibr" target="#b10">[11]</ref>.</p><p>Recent trends of domain adaptation (DA) inspire the emergence of extensive works to transfer knowledge from a label-rich source (synthetic) domain to a label-scarce target <ref type="figure">Fig. 1</ref>: Results preview on two popular synthetic-to-real semantic segmentation tasks. Our method is shown in bold.</p><p>(real) domain, which enjoys tremendous success <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. Most previous works develop adversarial training algorithms to diminish the domain shift existing in input <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, feature <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b19">[20]</ref> or output <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> space. Despite the fact that above methods can draw the two domains closer globally, it does not guarantee those feature representations from different classes in the target domain are well-separated. Utilizing category information can refine such alignment <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. <ref type="bibr">But</ref>  <ref type="figure">Fig. 2</ref>: Illustration of the main idea. By contrastively matching a pixel query q to distinct semantics, features with the same semantic concepts are drawn closer while those with different ones are pushed apart across domains. We first explore (a) Centroid-aware pixel contrast including (a.1) global category prototypes simply computed on the entire source domain, which render the overall appearance of each category and (a.2) local category centroids of each class in a single source image, which are stored into a memory bank. Further, we develop (b) Distribution-aware pixel contrast: the distributions of each category on source features are depicted as class-specific holistic concepts to guide the semantic alignment.</p><p>their visual characteristics, such as color, scale, illumination, etc. could be quite different, which is deleterious to the continual learning of pixel features across the two domains. Another line of work harnesses self-training to promote the segmentation performance <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. By adopting confidence estimation <ref type="bibr" target="#b33">[34]</ref>, consistency regularization <ref type="bibr" target="#b34">[35]</ref>, or label denoising <ref type="bibr" target="#b35">[36]</ref>, the bias in pseudo labels could be relieved to some extent. While many works are already capable of establishing milestone performance, there is still much room for improvement beyond the current state-of-the-art. We find that most approaches do not explicitly address the domain gap across domains, and the learned target representations are still dispersed. In addition, many works opt for a stage-wise training mechanism to avoid training error amplification in a single-stage model, which heavily relies on a warm-up stage to increase the reliability of the generated pseudo labels. Hereafter, several methods combine adversarial training and self-training <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> or train with auxiliary tasks <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref> to learn discriminative representations from unlabeled target data. Contrastive learning is a relevant topic, which learns proper visual representations by comparing different unlabeled data <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>. Without any supervision, models are capable of finding patterns like similarity and dissimilarity.</p><p>The huge success of contrastive learning and the aforementioned drawbacks in prior arts together motivate us to rethink the current de facto training paradigm in semantic segmentation under a domain shift. Basically, the power of contrastive learning roots from instance discrimination, which takes advantage of semantic concepts within data. With this insight, we find a new path to build models that are robust to distribution shifts by exploring crossdomain pixel contrast under the guidance of semantics, which attracts similar pixels and dispels dissimilar ones in a latent space as illustrated in <ref type="figure">Fig. 2</ref>.</p><p>In this work, we present a novel end-to-end framework, SePiCo, for domain adaptive semantic segmentation. Not only does SePiCo outperform previous works ( <ref type="figure">Fig. 1)</ref>, but it is also simple, effective, remaining one-stage training complexity. To be precise, we build upon a self-training method <ref type="bibr" target="#b30">[31]</ref> and introduce several dense contrastive learning mechanisms. The core is to explore suitable semantic concepts to guide the learning of class-discriminative and class-balanced pixel embedding space across domains. First, a straightforward way is to adopt the centroids of categorywise features in the entire source domain as global category prototypes, and then bring the pixel features with the same semantic to the prototypes closer and push the others farther away. Albeit the prototype can render the overall appearance of a category, it might omit variations in some attributes (e.g., shape, color, illumination) of the category, impairing the discriminability of the learned features. Then, in order to enhance the diversity, a natural extension is to enlarge the number of contrastive pairs. We investigate a memory bank mechanism, in which the category centroids of current source features are enqueued into a dictionary and the oldest features are dequeued. However, class biases may exist in this mechanism since those under-represented classes (e.g., truck, bus, rider) update more slowly. Meanwhile, it is computationally expensive. Finally, if we assume that every dimension in the embedding space follows a distribution, we can observe that pixel representations from similar semantic class have a similar distribution. Thereby, we seek the distribution of each category in the source domain as a richer and more comprehensive semantic description and the real distribution can be properly estimated with sufficient supervision of source data. This formulation enables a wide variety of samples from the estimated distribution, which is tailored for representation learning in dense prediction tasks like semantic segmentation.</p><p>Furthermore, we conduct an analysis with Pixel-wise Discrimination Distance (PDD) to certify the validity of our method regarding pixel-wise category alignment. The results demonstrate that contrastively driving the source and target pixel representations towards semantic concepts can lead to more effective domain alignment and significantly improve the generalization capability on the target domain. This work makes the following contributions:</p><p>? We provide a new perspective to reduce domain shift via explicitly enhancing the similarity of pixel features with corresponding semantic concepts and increasing the discrimination power on mismatched pairs, no matter the source or target domain. SePiCo is an one-stage adaptation framework robust to both daytime and nighttime segmentation situations.</p><p>? To further facilitate the efficiency, a closed-form upper bound of the expected contrastive loss is derived with the moments of each category.</p><p>? Extensive experiments on popular semantic segmentation benchmarks show that SePiCo achieves the superior performance. Particularly, we obtain mIoUs of 61.0%, 58.1%, and 45.4% on benchmarks GTAV ? Cityscapes, SYNTHIA ? Cityscapes and Cityscapes ? Dark Zurich respectively. Equipped with the latest Transformer, SePiCo further improves by mIoUs of 9.3%, 5.6% and 8.0% respectively. Ablation study verifies the effectiveness of each component in SePiCo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our work draws upon existing literature in semantic image segmentation, domain adaptation, and representation learning. For brevity, we only discuss the most relevant works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semantic Segmentation</head><p>The recent renaissance in semantic segmentation began with the fully convolutional networks <ref type="bibr" target="#b5">[6]</ref>. Mainstream methods strive to enlarge receptive fields and capture context information <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>. Among them, the family of DeepLab enjoys remarkable popularity because of its effectiveness. Inspired by the success of the Transformers <ref type="bibr" target="#b47">[48]</ref> in natural language processing, many works adopt it to visual tasks including image classification <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> and semantic segmentation <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, offering breakthrough performance. These studies, though impressive, require a large amount of labeled datasets and struggle to generalize to new domains.</p><p>In this work, we operate semantic segmentation under such a domain shift with the aim of learning an adequate model on the unlabeled target domain. Concretely, we map pixel representations in different semantic classes to a distinctive feature space via a pixel-level contrastive learning formulation. The learned pixel features are not only discriminative for segmentation within the source domain, but also, more critically, well-aligned for cross-domain segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Nighttime Semantic Segmentation</head><p>Nighttime Semantic Segmentation is much more challenging in safe autonomous driving due to poor illuminations and arduous human annotations. Only a handful of works have been investigated in the past few years. Dai et al. <ref type="bibr" target="#b52">[53]</ref> introduce a two-step adaptation method with the aid of an intermediate twilight domain. Sakaridis et al. <ref type="bibr" target="#b53">[54]</ref> leverage geometry information to refine predictions and transfer the style of nighttime images to that of the daytime images to reduce the domain gap. Recently, Wu et al. <ref type="bibr" target="#b54">[55]</ref> jointly train a translation model and a segmentation model in one stage, which efficiently performs on par with prior methods.</p><p>While daytime and nighttime image segmentation tasks differ only in appearances, current works focus on designing specialized methods for each task. Different from the above methods, SePiCo is able to address both daytime and nighttime image segmentation tasks in a universal framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Domain Adaptation in Semantic Segmentation</head><p>Domain Adaptation (DA) has been investigated for decades in theory <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref> and in various tasks <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>. Given the power of DCNNs, deep DA methods have been gaining momentum to significantly boost the transfer performance of a segmentation model. A multitude of works generally fall into two categories: adversarial training <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref> and self-training <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref>.</p><p>Adversarial training methods diminish the distribution shift of two domains at image <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, feature <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b63">[64]</ref>, or output <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref> level in an adversarial manner. To name a few, Hoffman et al. <ref type="bibr" target="#b13">[14]</ref> bring DA to segmentation by building generative images for alignment. On the other hand, Tsai et al. <ref type="bibr" target="#b19">[20]</ref> suggest that performing alignment in the output space is more practical. A few works also leverage different techniques via entropy <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> and information bottleneck <ref type="bibr" target="#b22">[23]</ref>. To enforce local semantic consistency, Luo et al. <ref type="bibr" target="#b64">[65]</ref> propose to adaptively weigh the adversarial loss while Wang et al. <ref type="bibr" target="#b25">[26]</ref> directly incorporate class information into the discriminator and encourage it to align features in a category-level manner.</p><p>Due to the absence of holistic information about each semantic class, the adversarial training is usually less stable. To solve this issue, some methods instead adopt category anchors <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b37">[38]</ref> computed on source data to advance the alignment. A new related work <ref type="bibr" target="#b65">[66]</ref> presents a category contrast method to learn discriminative representation. By contrast, we endeavor to explore semantic concepts from multiple perspectives. More importantly, we set forth a generic semantic-guided pixel contrast to emphasize pixelwise discriminative learning, which allows us to minimize the intra-class discrepancy and maximize the inter-class margin of pixel representations across the two domains.</p><p>Self-training methods exploit unlabeled target data via training with pseudo labels <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b67">[68]</ref>. In an example, Zou et al. <ref type="bibr" target="#b68">[69]</ref> propose an iterative learning strategy with class balance and spatial prior for target instances. In <ref type="bibr" target="#b30">[31]</ref>, Tranheden et al. propose a domain-mixed self-training pipeline to avoid training instabilities, which mixes images from two domains along with source groundtruth labels and target pseudo labels. Later on, Wang et al. <ref type="bibr" target="#b39">[40]</ref> enhance self-training via leveraging the auxiliary supervision from depth estimation to diminish the domain gap. Lately, Zhang et al. <ref type="bibr" target="#b35">[36]</ref> utilize the feature distribution from prototypes to refine target pseudo labels and distill knowledge from a strongly pre-trained model. However, most existing methods always encounter an obstacle that target representations are dispersed due to the discrepancy across domains. In addition, most of them utilize a warmup model to generate initial pseudo labels, which is hard to tune. Differently, our framework performs a one-stage endto-end adaptation produce without using any separate preprocessing stages. In addition, SePiCo can largely improve self-training and easily optimize pixel embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Representation Learning</head><p>To date, unsupervised representation learning has been extensively investigated due to its promising ability to learn representations in the absence of human supervision, especially for contrastive learning <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b70">[71]</ref>. Let f be an embedding function that transforms a sample x to an embedding vector q = f (x) , q ? R d and let (x , x + ) be similar pairs and (x , x ? ) be dissimilar pairs. Then, normalize q onto a unit sphere and a popular contrastive loss such as InfoNCE <ref type="bibr" target="#b70">[71]</ref> is formulated as:</p><formula xml:id="formula_0">E q ,q + ,{q ? n } N n=1 ? log e q q + /? e q q + /? + N n=1 e q q ? n /? .</formula><p>In practice, the expectation is replaced by the empirical estimate. As shown above, the contrastive loss is essentially based on the softmax formulation with a temperature ? . Intuitively, the above methods encourage the instance discrimination. Recent works <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b73">[74]</ref> also extend contrastive learning to dense prediction tasks. These methods either engage in better visual pre-training for dense prediction tasks <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b71">[72]</ref> or explore dense representation learning in the fully supervised setting <ref type="bibr" target="#b72">[73]</ref> or semi-supervised setting <ref type="bibr" target="#b73">[74]</ref>. Thereby, they generally tend to learn the pixel correspondence on the category of objects that appear in different views of an image rather than learning the semantic concepts across datasets, so the learned representation embeddings cannot directly deploy under a domain shift. On the contrary, we draw inspiration from contrastive learning and construct contrastive pairs according to different ways of semantic information to bridge the domain shift, which has received limited consideration in the existing literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In this section, we first briefly introduce the background and illustrate the overall idea in Section 3.1. Then the details of semantic statistics and our framework are elaborated in Section 3.2 and Section 3.3, respectively. Finally, we present training procedure and the SePiCo algorithm in Section 3.4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Problem formulation</head><p>For domain adaptive semantic segmentation, we have a collection of labeled source data I s with pixel-level labels Y s as well as unlabeled target data I t . The goal is to categorize each pixel of a target image into one of the predefined K categories through learning a model consisting of a feature  <ref type="figure">Fig. 3</ref>: Framework overview. First, our basic framework is based on a teacher-student architecture and the teacher model provides source feature mapF s and target pseudo labels? t . Second, we propose the class-balanced cropping to frequently crop image patches with under-represented objects that balance performance across classes. And third, except for self training losses, L ce and L ssl , we contrastively enforce the pixel representations F s , F t towards centroidaware or distribution-aware semantics using L cl and L reg . After training is completed, we throw away projection head ? p and use encoder ? e and head ? c for segmentation task.</p><p>encoder ? e , a multi-class segmentation head ? c , and an auxiliary projection head ? p . We adopt the teacher-student architecture <ref type="bibr" target="#b74">[75]</ref> (Teacher network are denoted as ? e , ? c , and ? p .) as our basic framework, shown in <ref type="figure">Fig. 3</ref>. During training, images from source and target domains I s , I t ? R H?W ?3 are randomly sampled and passed into both teacher and student networks, respectively. The hidden-layer features F s , F t ? R H ?W ?A , and final pixellevel predictions P s , P t ? R H?W ?K are generated from the student, where A is the channel dimension of intermediate features and H ( H) , W ( W ) are spatial dimensions of features. Similarly, we access corresponding source features F s ? R H ?W ?A , and target pixel-level predictionsP t from the momentum-updated teacher. Note that no gradients will be back-propagated into the teacher network <ref type="bibr" target="#b74">[75]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Self-training domain adaptation revisit</head><p>Here, we give an overview over a self-training method <ref type="bibr" target="#b30">[31]</ref> for evaluating different semantic-guided pixel contrasts. Traditional self-training methods usually consider two aspects. On the one hand, these methods train a model ? c ? ? e to minimize the categorical cross-entropy (CE) loss in the source domain, formalized as a fully supervised problem:</p><formula xml:id="formula_1">Lce = ? 1 HW i?{1 ,2 ,??? ,H?W } k 1 [Y s,i =k] log P k s,i ,<label>(1)</label></formula><p>where Y s,i is the one-hot label for pixel i in I s and 1 <ref type="bibr">[?]</ref> is an indicator function that returns 1 if the condition holds or 0 otherwise. On the other hand, to better transfer the knowledge from the source domain to the target domain, self-training usually uses a teacher network to produce more reliable pseudo label? t for a target image,</p><formula xml:id="formula_2">Yt,j = arg max kP k t,j , j ? {1 , 2 , ? ? ? , H ? W } .<label>(2)</label></formula><p>In practice, we compute the pseudo labels online during the training and avoid any additional inference step, which is simpler and more efficient. Specifically, we forward a target image and obtain the pseudo labels using Eq. (2). Besides, since the pseudo labels are usually noisy, a confidence estimation is made for generated pseudo labels. Specifically, the number of pixels with the maximum softmax probability exceeding a threshold ? is calculated first:</p><formula xml:id="formula_3">N U M conf = j?{1 ,2 ,??? ,H?W } 1 [max kP k t,j &gt;?] .<label>(3)</label></formula><p>Next, the ratio of pixels exceeding the threshold over the whole image serves as confidence weights, w = N U M conf HW and the student network is re-trained on target data,</p><formula xml:id="formula_4">L ssl = ? 1 HW j?{1 ,2 ,??? ,H?W } k w ? 1 [? t,j =k] log P k t,j . (4)</formula><p>Finally, let's go back to the teacher network. The weights of teacher network ? e , ? c , ? p are set as the exponentially moving average (EMA) of the weights of student network ? e , ? c , ? p in each iteration <ref type="bibr" target="#b74">[75]</ref>. Take ? e as an example,</p><formula xml:id="formula_5">? e ? ?? e + (1 ? ?)?e ,<label>(5)</label></formula><p>where ? is a momentum parameter. Similarly, ? c , ? p should also be updated via Eq. <ref type="bibr" target="#b4">(5)</ref>. In this work, ? is fixed to 0.999. Note that incorporating data augmentation with selftraining has been shown to be particularly efficient <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b75">[76]</ref>. Following <ref type="bibr" target="#b30">[31]</ref>, we use the teacher network to generate a set of pseudo labels? t on the weakly-augmented target data. Concurrently, the student network is trained on strongly-augmented target data. We use standard resize and random flip as the weak augmentation. Strong augmentation includes color jitter, gaussian blur, and ClassMix <ref type="bibr" target="#b76">[77]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Overall motivation</head><p>As mentioned before, however, there is a major limitation for traditional self-training methods: most of them neglect the explicit domain alignment. As a result, even under perfect pseudo labeling on target samples, negative transfer may exist, causing pixel features from different domains but of the same semantic class to be mapped farther away.</p><p>To address the above issue, we promote semantic-guided representation learning in the embedding space. A naive way is to directly adopt global category prototypes computed on the source domain to guide the alignment between source and target domains. An obstacle of this design, however, is that prototypes could only reflect the common characteristic of each category but cannot fully unlock the potential strength of semantic information, leading to erroneous representation learning. Inspired by <ref type="bibr" target="#b40">[41]</ref>, we go a step further and store the local image centroids of each source image into a memory bank so that the semantic information exploited is roughly proportional to the size of bank. But this mechanism will arise class bias as features of some classes, e.g., bicycles, pedestrians and poles, rarely appear. Meanwhile, this does consume a lot of computing resources.</p><p>Consequently, to promote the diversity in semantic concepts, we newly introduce the distribution-aware pixel constant to contrastively strengthen the connections between each pixel representation and estimated distributions. Moreover, such distribution-aware mechanism could be viewed as training on infinite data and is more computation efficient, which is intractable for a memory bank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Centroid-aware and Distribution-aware Statistics Calculation</head><p>Given the source feature mapF s ? R H ?W ?A from the teacher model, for any pixel indexed i ? {1 , 2 , ? ? ? , H ? W } inF s , we first divide its feature into the set of k th semantic class, i.e., ? k according to its mask M s,i ? R H ?W downsampled from ground truth label. Hereafter, the local centroid of the k th category in an image is calculated by</p><formula xml:id="formula_6">? k = 1 |? k | i?{1 ,2 ,??? ,H ?W } 1 [M s,i =k]Fs,i ,<label>(6)</label></formula><p>where | ? | is the cardinality of the set.</p><p>For centroid-aware semantic information, we require either global category prototypes or local category centroids. On the one side, we opt for an online fashion on the entire source domain, aggregating mean statistics one by one to build global category prototypes. Mathematically, the online estimate algorithm for mean of the k th category is given by</p><formula xml:id="formula_7">? k (t) = n k (t?1) ? k (t?1) + m k (t) ? k (t) n k (t?1) + m k (t) ,<label>(7)</label></formula><p>where n k (t?1) is the total number of pixels belonging to the k th category in previous t?1 images, and m k (t) is the number of pixels belonging to the k th category in current t th image. Thereby, we are allowed to obtain K global prototypes:</p><formula xml:id="formula_8">P = {? 1 , ? 2 , ? ? ? , ? K } .<label>(8)</label></formula><p>On the other side, we maintain local centroids of each class from the latest source images to form a dynamic categorical dictionary with K-group queue,</p><formula xml:id="formula_9">B = {B 1 , B 2 , ? ? ? , B K } ,<label>(9)</label></formula><p>where</p><formula xml:id="formula_10">B k = {? k (t?B) , ? k (t?B+1)</formula><p>, ? ? ? , ? k (t) } and B is the shared queue size for all queues. Note that the oldest centroids are dequeued and the newly computed centroid are enqueued.</p><p>Discussion: merit and demerit of centroid-aware statistics. Each global category prototype renders the overall appearance of one category, yet it might omit diversity and impair the discriminability of the learned representations. On the other side, the memory bank is able to expand the set of negative and positive samples, thus it can embrace more semantic information. More importantly, almost all semantic information could be covered when B is large enough, however, it is neither elegant nor efficient in presenting pixel embedding space. To capture and utilize rich semantic information as efficiently and comprehensively as possible, we try to build from the distributional perspective as follows.</p><p>We observe that pixel representations with respective to each class will have a similar distribution. With this in mind, we propose to build the distribution-aware semantic concepts with sufficient labeled source instances. Therefore, we need to acquire the covariance of the multidimensional feature vectorF s,i for a better representation of the variance between any pair of elements in the feature vector. The covariance matrix ? k for category k can be updated via</p><formula xml:id="formula_11">? k (t) = n k (t?1) ? k (t?1) + m k (t) ? k (t) n k (t?1) + m k (t) + n k (t?1) m k (t) ? k (t?1) ? ? k (t) ? k (t?1) ? ? k (t) n k (t?1) + m k (t) 2 ,<label>(10)</label></formula><p>where ? k (t) is the covariance matrix of the features between the k th category in the t th image. It is noteworthy that K mean vectors and K covariance matrices are initialized to zeros. During training, we dynamically update these statistics using Eq. <ref type="bibr" target="#b6">(7)</ref> and Eq. (10) with source feature mapF s from momentum-updated teacher network. The estimated distribution-aware semantic statistics are more informative to guide the pixel representation learning between domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Semantic-Guided Pixel Contrast</head><p>In the literature, a handful of methods have leveraged categorical feature centroids <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b65">[66]</ref> as anchors to remedy domain shift, yielding promising results. However, few attempts have been made in this regime to quantify the distance between features of different category. It is arduous to separate pixel representations with similar semantic information in target data as no supervision information is available, which severely limits their potential capability in dense prediction tasks. On the contrary, we design a unified framework to integrate three distinct contrastive losses that target at learning similar/dissimilar pairs at the pixel level to mitigate the domain gap via either centroid-aware pixel contrast or distribution-aware pixel contrast.</p><p>As described above, the pixel representation separation in the source domain is naturally guaranteed by source mask M s from ground truth label Y s . Similarly, for target instances, we obtain satisfactory target mask M t for each pixel via generated pseudo labels from? t (Eq. <ref type="formula" target="#formula_2">(2)</ref>). To this end, every pixel representation either in the source or in the target feature maps (defined as the pixel query q ? R A for simplicity) now needs to yield a low loss value when simultaneously forming multiple positive pairs (q , q + m ) and multiple negative pairs (q , q k? n ), where q + m indicates the m th positive example from the same category with respect to q and q k? n represents n th negative example from the k th different class. Formally, we define a new pixel contrast loss function for q:</p><formula xml:id="formula_12">cl q = ? 1 M M m=1 log e q q + m /? e q q + m /? + k?K ? 1 N N n=1 e q q k? n /? ,<label>(11)</label></formula><p>where M and N are the numbers of positive and negative pairs and K ? denotes the set containing all different classes from that of q. In the following sections, we will describe three pixel contrast losses protocl q , bankcl q and distcl q respectively to derive a better-structured embedding space, eventually boosting the performance of segmentation model.</p><p>In summary, we can enable learning discriminative pixel representations on source and target domains by using a unified contrastive loss</p><formula xml:id="formula_13">L cl = 1 |?| q?Fs?F t cl q ,<label>(12)</label></formula><p>where |?| is the total number of pixels in the union of F s and F t . Note that such contrastive loss is employed in both domains simultaneously. For one thing, when the loss is applied in the source domain, the student network is able to learn more discriminative feature representations for pixel-level predictions, which could increase the robustness of the segmentation model. Another effect is that the target pixel-wise representations are contrastively adapted, which benefits minimizing the intra-category discrepancy and maximizing the inter-category margin within the target domain, and facilitates transferring knowledge from source to target explicitly. Moreover, except for individually pixel representation learning, we introduce a regularization term to make the feature representations of input images globally diverse and smooth, which is formalized as</p><formula xml:id="formula_14">Lreg = 1 K log K K k=1 log e Q ? k /? K l=1 e Q ? l /? ,<label>(13)</label></formula><p>where Q = 1 H ?W i?{1 ,2 ,??? ,H ?W } F s/t,i is the mean feature representation of a source or target image. This objective is similar to diversity-promoting objective used in prior DA methods <ref type="bibr" target="#b77">[78]</ref>, but is employed in the embedding space. It could circumvent the trivial solution where all unlabeled target data have the same feature encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Centroid-aware Pixel Contrast</head><p>Here, we introduce two variants of centroid-aware pixel contrast, namely SePiCo (ProtoCL) and SePiCo (BankCL). CASE 1: ProtoCL (M = N = 1). Naively operate K global category prototypes to establish one positive pair and K ? 1 negative pairs. We consider this formulation as the prototype pixel contrast loss function</p><formula xml:id="formula_15">protocl q = ? log e q ? + /? e q ? + /? + k?K ? e q ? k? /? ,<label>(14)</label></formula><p>where ? + is the positive prototype belonging to the same category as the specific query q and ? k? is the prototype of the k th different category. CASE 2: BankCL (M = N = B). To involve more negative and positive samples for representation learning, we could access more contrastive pairs from a memory bank, in which local category centroids of a single source image are stored. We consider this formulation as the bank pixel contrast loss function</p><formula xml:id="formula_16">bankcl q = ?E q + ?B + log e q q + /? e q q + /? + k?K ? E q k? ?B k? e q q k? /? ,<label>(15)</label></formula><p>where B + is the queue comprised of positive samples and B k? refs to a queue containing negative ones. In Section 4.3, we will analyze the effect of bank size B.</p><p>Discussion: merit and demerit of centroid-aware pixel contrast. In a word, global prototypes or local centroids can be used as good contrastive samples to pull similar pixel representations closer and push those dissimilar pixel representations away in the embedding space. However, from Eq. <ref type="bibr" target="#b13">(14)</ref> and Eq. (15), we can theorize that the main difference between them is the number of positive and negative pairs. Because of this, if the number of contrastive pairs does matter, it is intuitively reasonable that an infinite number of such pairs would contribute to the establishment of a more robust and discriminative embedding space. We will justify this assumption from the distributional perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Distribution-aware Pixel Contrast</head><p>In this section, we derive a particular form of the contrastive loss where infinite positive/negative pixel pairs are simultaneously involved with regard to each pixel representation in the source and target domain. A naive implementation is to explicitly sample M examples from the estimated distribution that has the same latent class and N examples from each of the other distributions featuring different semantic concepts. Unfortunately, this is not computationally feasible when M and N are large, as carrying all positive/negative pixel pairs in an iteration would quickly drain the GPU memory.</p><p>To get around this issue, we take an infinity limit on the number of M and N , where the effect of M and N are hopefully absorbed in a probabilistic way. With this application of infinity limit, the statistics of the data are sufficient to achieve the same goal of multiple pairing. As M , N go to infinity, it becomes the estimation of:</p><formula xml:id="formula_17">? q = lim M ?? ,N ?? cl q = lim M ?? ? 1 M M m=1 log e q q + /? e q q + /? + k?K ? lim N ?? 1 N N n=1 e q q k? n /? = ?E q + ?p(q + ) log e q q + /? e q q + /? + k?K ? E q k? ?p(q k? ) e q q k? /? ,<label>(16)</label></formula><p>where p(q + ) is the positive semantic distribution with the same label as q and p(q k? ) is the k th negative semantic distribution with a different label from that of each query q. The analytic form of the above is intractable, but it has a rigorous closed form of upper bound, which can be derived</p><formula xml:id="formula_18">? q = ?E q + log e q q + /? e q q + /? + k?K ? E q k? e q q k? /? ? log ? ? ?E q + ? ? e q q + ? + k?K ? E q k? e q q k? ? ? ? ? ? ? ? q E q + q + ? = log ? ? E q + e q q + ? + k?K ? E q k? e q q k? ? ? ? ? q E q + q + ? (17) = distcl q ,</formula><p>where the above inequality follows from the Jensen's inequality on concave functions, i.e., E log(X) ? log E(X). Thus, distribution-aware pixel contrast loss, i.e., SePiCo (DistCL) is yielded to implicitly explore infinite samples.</p><p>To facilitate our formulation, we further need an assumption on the feature distribution. For any random variable x that follows Gaussian distribution x ? N (?, ?), we have the moment generation function <ref type="bibr" target="#b78">[79]</ref> that satisfies:</p><formula xml:id="formula_19">E e a x = e a ?+ 1 2 a ?a ,<label>(18)</label></formula><p>where ? is the expectation of x, ? is the covariance matrix of x. Therefore, we assume that q + ? N (? + , ? + ) and</p><formula xml:id="formula_20">q k? ? N (? k? , ? k? ),</formula><p>where ? + and ? + are respectively the </p><formula xml:id="formula_21">+ q ? + q 2? 2 + k?K ? e q ? k? ? + q ? k? q 2? 2 ? ? ? q ? + ? = ? log e q ? + ? + q ? + q 2? 2 e q ? + ? + q ? + q 2? 2 + k?K ? e q ? k? ? + q ? k? q 2? 2 + q ? + q 2? 2 .<label>(19)</label></formula><p>The overall loss function regarding each pixel-wise representation thereby boils down to the closed form whose gradients can be analytically solved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Procedure</head><p>In brief, the training procedure of SePiCo can be optimized in a one-stage manner, and we further introduce a classbalanced cropping in Alg. 1 to stabilize and regularize the training produce. We summarize the algorithm in Alg. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Class-balanced Cropping (CBC)</head><p>In many semantic segmentation datasets, one challenge of training a more capable model under distribution shifts is overfitting to the majority classes of the source dataset. To circumvent this issue, we follow <ref type="bibr" target="#b27">[28]</ref> and sample source images with rare classes more frequently during the training.</p><p>On the other hand, since there are no available annotations for target images, we utilize the generated pseudo labels as a substitution and introduce a class-balanced cropping (CBC) (Alg. 1) to crop image regions that jointly promote class balance in pixel number and diversity of internal categories. Therefore, classes with a smaller frequency will have a higher sampling probability for source images and image regions with multiple categories and balanced pixel count will enjoy a higher cropping probability in target images. </p><formula xml:id="formula_22">? e ? ? e , ? c ? ? c , ? p ? ? p . 5 for iter ? 0 to L do 6</formula><p>Randomly sample a source image I s with Y s and a target image I t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7</head><p>Apply class-balance cropping with (I s , Y s ) and (I t ,? t ), where? t is generated from ? c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8</head><p>Compute hidden-layer feature maps F s and F t and corresponding masks M s and M t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9</head><p>Separate pixel-wise representations of both domains in the embedding space using M s , M t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10</head><p>Update mean {? k } K k=1 via Eq. <ref type="formula" target="#formula_7">(7)</ref> and covariance matrices {? k } K k=1 via Eq. (10) or memory bank with current image-wise centroids {? k } K k=1 . if iter &gt; L w then <ref type="bibr" target="#b10">11</ref> Train ? e , ? c , ? p via Eq. <ref type="bibr" target="#b19">(20)</ref> . else <ref type="bibr" target="#b11">12</ref> Train ? e , ? c using L ce , L ssl .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13</head><p>Update ? e , ? c , ? p with ? e , ? c , ? p via Eq. (5). Return: Final network weights ? c and ? e .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Optimization Objective</head><p>The well known self-training extensively studied in previous methods <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b35">[36]</ref>, is usually achieved by iteratively generating a set of pseudo labels based on the most confident predictions on target data. Nevertheless, it primarily depends on a good initialization model and is hard to tune. Our SePiCo aims to learn a discriminative embedding space and is complementary to the selftraining. Therefore, we unify both into a one-stage, end-toend pipeline to stabilize training and yield discriminative features, which promotes the generalization ability of the model. The overall training objective is formulated as: min ?e,?c,?p</p><formula xml:id="formula_23">Lce + L ssl + ? cl L cl + ?regLreg ,<label>(20)</label></formula><p>where ? cl , ? reg are constants controlling the strength of corresponding loss. Initial tests suggest that using equal weights to combine the L cl with L reg yields better results. For simplicity, both are set to 1.0 without any tuning. By optimizing Eq. <ref type="formula" target="#formula_2">(20)</ref>, clusters of pixels belonging to the same category are pulled together in the feature space while synchronously pushed apart from other categories, which eventually establishes a discriminative embedding space. In this way, our method can simultaneously minimize the gap across domains as well as enhance the intra-class compactness and inter-class separability in a unified framework. Meanwhile, it is beneficial for the generation of reliable pseudo labels which in turn facilitates self-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT</head><p>In this section, we validate SePiCo on two popular syntheticto-real tasks and a challenging daytime-to-nighttime task.</p><p>First, we describe datasets and implementation details. Next, numerous experimental results are reported for comparison across diverse datasets and architectures. Finally, we conduct detailed analyses to obtain a complete picture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setups</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets</head><p>GTAV <ref type="bibr" target="#b8">[9]</ref> is a composite image dataset sharing 19 classes with Cityscapes. 24,966 city scene images are extracted from the physically-based rendered computer game "Grand Theft Auto V" and are used as source domain data for training. SYNTHIA <ref type="bibr" target="#b9">[10]</ref> is a synthetic urban scene dataset. Following <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b27">[28]</ref>, we select its subset, called SYNTHIA-RAND-CITYSCAPES, that has 16 common semantic annotations with Cityscapes. In total, 9,400 images with the resolution 1280?760 from SYNTHIA dataset are used as source data.</p><p>Cityscapes <ref type="bibr" target="#b80">[81]</ref> is a dataset of real urban scenes taken from 50 cities in Germany and neighboring countries. We use finely annotated images which consists of 2,975 training images, 500 validation images, and 1,525 test images, with the resolution at 2048?1024. Each pixel of the image is divided into 19 categories. For synthetic-to-real adaptation <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b35">[36]</ref>, we adopt training images as unlabeled target domain and operate evaluations on its validation set. For daytimeto-nighttime adaptation <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b81">[82]</ref>, we use all images from the training set as the source training data. Dark Zurich <ref type="bibr" target="#b81">[82]</ref> is another real-world dataset consisting of 2,416 nighttime images, 2,920 twilight images and 3,041 daytime images, with a resolution of 1920?1080. Following <ref type="bibr" target="#b54">[55]</ref>, we utilize 2,416 day-night image pairs as target training data and another finely annotated 201 nighttime images are divided into the validation (Dark-Zurich val with 50 images) and test part (Dark-Zurich test with 151 images). As there is no direct access to the ground truth of the test part, which serves as an online benchmark, we get the performance of our method by submitting to the online evaluation site 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Implementation Details</head><p>Network architecture. Our implementation is based on the mmsegmentation toolbox <ref type="bibr" target="#b82">[83]</ref>. We start from utilizing the DeepLab-V2 architecture <ref type="bibr" target="#b7">[8]</ref> with ResNet101 <ref type="bibr" target="#b83">[84]</ref> as the backbone. To further testify the potential of SePiCo on recent Transformer-based architectures, we adopt the same framework used in DAFormer <ref type="bibr" target="#b27">[28]</ref> as a strong backbone. For the segmentation head, We use the same architectures following the mainstream methods <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b34">[35]</ref>. Subsequently, a projection head is integrated into the network that maps high-dimensional pixel embedding into a 512-d 2 -normalized feature vector <ref type="bibr" target="#b72">[73]</ref>. It consists of two 1?1 convolutional layers with ReLU. For fairness, all backbones are initialized using the weights pre-trained on ImageNet <ref type="bibr" target="#b84">[85]</ref>, with the remaining layers being initialized randomly. Training. Our model is implemented in PyTorch <ref type="bibr" target="#b85">[86]</ref> and trained on a single GeForce RTX 3090 GPU. We use the AdamW <ref type="bibr" target="#b86">[87]</ref> as our optimizer with betas (0.9, 0.999) and weight decay 0.01. The learning rate is initially set to 6 ? 10 ?5 for the encoder, and 6 ? 10 ?4 for decoders. Similar to <ref type="bibr" target="#b27">[28]</ref>, linear learning rate warmup policy and rare class Evaluation. We employ per-class intersection-over-union (IoU) and mean IoU over all classes as the evaluation metric which is broadly adopted in semantic segmentation. Following the common protocols <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b54">[55]</ref>, we report the results on common 19 classes for GTAV ? Cityscapes and Cityscapes ? Dark Zurich. For SYNTHIA ? Cityscapes, we report the results on 13 and 16 common classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head><p>We comprehensively compare our SePiCo with the recently leading approaches in the two representative synthetic-toreal adaptation scenarios: GTAV ? Cityscapes in  <ref type="table" target="#tab_7">Zurich in TABLE 3</ref>. Additionally, we also give the visualization results of the segmentation maps in <ref type="figure" target="#fig_1">Fig. 4 and Fig. 5</ref>. Finally, due to great potentials of Vision Transformer variants in domain adaptive semantic segmentation, we evaluate our framework on above three benchmarks and list the results in TABLE 4. We experimentally show that, with famous segmentation models (i.e., DeepLab-V2, SegFormer) and ResNet backbone, our SePiCo attains excellent segmentation performance across daytime and nighttime conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2.1</head><p>Comparisons with the state-of-the-arts GTAV ? Cityscapes. We first present the adaptation results on the task of GTAV ? Cityscapes in TABLE 1, with comparisons to the baseline model <ref type="bibr" target="#b30">[31]</ref> as well as the state-of-the-art DA approaches <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b39">[40]</ref>, and the best results are highlighted in bold. Overall, our SePiCo (ProtoCL/BankCL/DistCL) yield a leading result among comparison methods <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b61">[62]</ref>. Particularly, we observe: (i) SePiCo (DistCL) achieves 61.0% mIoU, outperforming the baseline model trained merely on source data by a large margin of +22.4% mIoU; (ii) Adversarial training methods, e.g., AdaptSeg <ref type="bibr" target="#b19">[20]</ref>, CLAN <ref type="bibr" target="#b22">[23]</ref>, SIM <ref type="bibr" target="#b24">[25]</ref>, FADA <ref type="bibr" target="#b25">[26]</ref>, can improve the transferability, but the effect is not as obvious as using self-training methods, e.g., Seg-Uncert. <ref type="bibr" target="#b31">[32]</ref>, DACS <ref type="bibr" target="#b30">[31]</ref>, IAST <ref type="bibr" target="#b66">[67]</ref>, SAC <ref type="bibr" target="#b34">[35]</ref>; (iii) On top of that, our SePiCo (DistCL) beats the best-performing model, ProDA <ref type="bibr" target="#b35">[36]</ref>, by a considerable margin of +3.5% mIoU, while ProDA has three complex training produces including warm up, self-training, and knowledge distillation.</p><p>Comparing the three variants of our framework, SePiCo (ProtoCL) and SePiCo (BankCL) also achieve a remarkable mIoUs of 59.5% and 60.4% respectively. It is clear that BankCL and DistCL performs much better than ProtoCL, indicating features of higher quality are generated thanks to semantic concepts with greater diversity. It is worth reminding that methods built on memory banks are generally slower and demands more memory in training, while  SePiCo (DistCL) eases such burden and still manages to surpass SePiCo (BankCL) in the same time.</p><p>SYNTHIA ? Cityscapes. As revealed in TABLE 2, our SePiCo remains competitive on SYNTHIA ? Cityscapes. SePiCo (DistCL) attains 58.1% mIoU and 66.5% mIoU * , achieving a significant gain of +24.6% mIoU and +27.9% mIoU * in comparison with "Source Only" model. It is noticeable that our SePiCo (DistCL) ranks among the best in both mIoU and mIoU * , outperforming ProDA <ref type="bibr" target="#b35">[36]</ref> by +2.6% mIoU and CorDA <ref type="bibr" target="#b39">[40]</ref> by +3.7% mIoU * . The former is a multi-stage self-training framework and the latter combines auxiliary task, i.e., depth estimation, to facilitate knowledge transfer to the target domain. SePiCo (ProtoCL/BankCL) also obtain a comparable performance in terms of mIoU * compared with SePiCo (DistCL), but under-perform or tie with it in mIoU, indicating that a more class-balanced per-formance is done by SePiCo (DistCL).</p><p>Cityscapes ? Dark Zurich. TABLE 3 highlights the capability of our SePiCo on the challenging daytime-tonighttime task Cityscapes ? Dark Zurich. To show that the current daytime-trained semantic segmentation models face significant performance degradation at night, we compare with AdaptSeg <ref type="bibr" target="#b19">[20]</ref>, AdvEnt <ref type="bibr" target="#b20">[21]</ref>, and BDL <ref type="bibr" target="#b36">[37]</ref>, adopting DeepLab-V2 as backbone network. Our framework, especially SePiCo (BankCL) and SePiCo (DistCL), outperforms the comparison counterparts by a large margin. The less powerful variant, SePiCo (ProtoCL), is still able to win by a narrow margin when compared to the previous SOTA DANNet <ref type="bibr" target="#b54">[55]</ref>. Due to the huge domain divergence between daytime and nighttime scenarios, there are always two steps in prior methods. Take CDAda <ref type="bibr" target="#b88">[89]</ref> as an example, it consists of inter-domain style transfer and intra-domain gradual  <ref type="bibr" target="#b50">[51]</ref> and SegF. MiT-B5 <ref type="bibr" target="#b51">[52]</ref>. The best result is highlighted in bold.   self-training. While our SePiCo aims at ensuring pixel-wise representation consistency between daytime and nighttime images, it is complementary to models designed for the nighttime and can still be trained in one stage. It is worth noting that our methods based on DeepLab-V2 are even superior or comparable to CDAda based on RefineNet <ref type="bibr" target="#b87">[88]</ref>, which further demonstrates the efficacy of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Qualitative results</head><p>In <ref type="figure" target="#fig_1">Fig. 4</ref>, we first visualize the segmentation results on two synthetic-to-real scenarios, predicted by our SePiCo (DistCL), and compare our results to those predicted by the Source Only, DACS, and ProDA models. The results predicted by SePiCo (DistCL) are smoother and contain less spurious areas than those predicted by other models, showing that the performance has been largely improved with SePiCo (DistCL). Next, as the daytime-to-nighttime task is far more challenging than the previous two, we further show several qualitative segmentation results in <ref type="figure" target="#fig_2">Fig. 5</ref> to illustrate the advantage of SePiCo (DistCL) over the other two variants SePiCo (ProtoCL) and SePiCo (BankCL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Advanced Network Architecture</head><p>Vision Transformer-based DA methods have been actively studied not long ago <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b89">[90]</ref>. Lukas et. al <ref type="bibr" target="#b27">[28]</ref> analyze different architectures for domain adaptation and propose a new architecture, DAFormer, based on a Transformer encoder <ref type="bibr" target="#b51">[52]</ref> and a context-aware fusion decoder. Lately, Chen et al. <ref type="bibr" target="#b89">[90]</ref> introduce a momentum network and dynamic of discrepancy measurement to smooth the learning dynamics for target data. Therefore, we further adopt one of architectures such as DAFormer <ref type="bibr" target="#b27">[28]</ref>, to support our claims. Inspired by multi-level context-aware feature fusion decoder <ref type="bibr" target="#b27">[28]</ref>, we also fuse all stacked multi-level features from decoder to provide valuable concepts for representation learning. From TABLE 4, we have the following observations: (i) Approaches based on Transformer architectures perform generally better than those based on DeepLab-V2, confirming the strength of these advanced architectures; (ii) Our SePiCo is still competitive on the new architecture. All variants of SePiCo achieve an extraordinary improve of around +20% mIoU on each task when compared with the models trained merely on source data, i.e., Swin-B ViT <ref type="bibr" target="#b50">[51]</ref> and SegF. MiT-B5 <ref type="bibr" target="#b51">[52]</ref>; (iii) SePiCo (DistCL) improves the state-of-the-art DAFormer by +2.0% mIoU for GTAV ? Cityscapes, +3.4% mIoU for SYNTHIA ? Cityscapes, and +5.7% mIoU for Cityscapes ? Dark Zurich.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>We evaluate the contribution of each component present in our one-stage framework. Specifically, we testify SePiCo on the task of GTAV ? Cityscapes, and the results are reported in TABLE 5 -8. As can be seen, each of these components contributes to the ultimate success. Eventually, we achieve 49.8% and 61.0% mIoU under "w/o self-training" and "SePiCo (DistCL)" respectively, outperforming the corresponding baselines by +11.2% and +8.9%.</p><p>Effect of semantic-guided pixel contrast. As discussed in Section 3.3, centroid-aware and distribution-aware pixel contrast can build up stronger intra-/inter-category connections and minimize the domain divergence efficiently. We validate the performance increments by separately training models with and without self-training. As shown <ref type="table" target="#tab_10">Table 5</ref>, contrastive learning alone can improve the segmentation performance, but the effect is not as noticeable as using self-training (48.5% mIoU vs. 52.1% mIoU). When they are adopted properly in a unified pipeline, the full potential of the model is released, further promoting gains of +7.2% mIoU. The results imply the effect and necessity of representation learning for classical self-training paradigm.</p><p>Effect of L reg . We study the advantages of diversitypromoting regularization term L reg in TABLE 5. It is clearly shown that using L reg also brings an extra increase (+0.7% mIoU and +1.1% mIoU, respectively), verifying the effectiveness of smoothing the learned representations.</p><p>Effect of class-balanced cropping (CBC). We remove the class-balanced cropping discussed in Section 3.4.1 to verify its necessity. As shown in TABLE 5, as expected, without using CBC on target, the mIoU of the adapted model decreases moderately, supporting the importance of class imbalance cropping.   Effect of the teacher network. The teacher-student architecture is frequently adopted to introduce a strong regularization during training <ref type="bibr" target="#b74">[75]</ref>. Essentially, larger momentum value ? indicates a stronger effect from teacher net. We adjust ? to change the amount of regularization and report the results in TABLE 6. A performance gain of more than +3.0% mIoU is brought about by the teacher net, confirming its efficacy. Thus ? is fixed to 0.999 for proper regularization.</p><p>Effect of the bank size B. TABLE 7 lists the effect of bank size for SePiCo (BankCL). As we enlarge the memory bank from 10 to 200, a gradual gain can be witnessed in performance, with a slight drop when B=500. Generally, a larger bank size means more diversity in semantic concepts, leading to better performance. However, a huge bank will result in prolonged retention of outdated representations, which may exert a negative effect on pixel-level guidance. In comparison, SePiCo (DistCL) overcomes this issue by using the distribution to simulate infinite bank size on-the-fly, thus exceeding SePiCo (BankCL) by a considerable margin.</p><p>Effect of multi-level features. We provide the performance of applying SePiCo to intermediate layers. In particular, there are four residual blocks in the original ResNet-101 backbone <ref type="bibr" target="#b83">[84]</ref>. The four layers (denoted by layer 1layer 4) are taken from the output of each residual block and {1,2,3,4}-fusion means that features from all four layers are concatenated together. Interestingly, features from layer 1 also exhibit distinctive information. This is expected because earlier features provide valuable low-level concepts for semantic segmentation at a high resolution. However, if we fuse all the features for adaptation, the results are slightly degraded, which is different from the Transformer-based architecture. We conjecture that the ViTs has more similarity  <ref type="formula" target="#formula_1">21)</ref> on Cityscapes validation set. These comparison results are from 1) category adversarial learning methods, i.e, CLAN <ref type="bibr" target="#b22">[23]</ref>, FADA <ref type="bibr" target="#b25">[26]</ref>, 2) category centroid-based alignment methods i.e., CAG-UDA <ref type="bibr" target="#b26">[27]</ref>, SIM <ref type="bibr" target="#b24">[25]</ref> and 3) the adapted model using Ours (DistCL), respectively. A high PDD suggests the pixel-wise representations of same category are clustered densely while the distance between different categories is relatively large. between the representations obtained in shallow and deep layers compared to CNNs. Overall, the features from layer 4 prove to be the best choice for all three variants of SePiCo. It can be seen that SePiCo (DistCL) performs nearly equally well while adopting features from the last layer or the fusion of multiple layers, indicating that the distribution indeed increases the diversity of features and is more robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Further Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Pixel-wise Discrimination Distance</head><p>To verify whether our adaptation framework can yield a discriminative embedding space, we design a metric to take a closer look at what degree the pixel-wise representations are aligned. In the literature, CLAN <ref type="bibr" target="#b22">[23]</ref> defines a Cluster Center Distance as the ratio of intra-category distance between the initial model and the aligned model and FADA <ref type="bibr" target="#b25">[26]</ref> proposes a new Class Center Distance to consider intercategory distance. To better evaluate the effectiveness of pixel-wise representation alignment, we introduce a new Pixel-wise Discrimination Distance (PDD) by taking intraand inter-category affinities of pixel representations into account. Formally, a PDD value for category k is given by:</p><formula xml:id="formula_24">P DD(k) = 1 |? k | x?? k sim(x, ? k ) K i=1,i =k sim(x, ? i ) ,<label>(21)</label></formula><p>where sim(?, ?) is the similarity metric, and we adopt cosine similarity. ? k denotes the pixel set that contains all the pixel representations belonging to the k th semantic class.</p><p>With PDD, we could investigate the relative magnitude of inter-category and intra-category pixel feature distances. Specifically, we calculate the PDD on the whole Cityscapes validate set and compare PDD values with other state-of-the-art category alignment methods: CLAN <ref type="bibr" target="#b22">[23]</ref> and FADA <ref type="bibr" target="#b25">[26]</ref> for category-level adversarial training and SIM <ref type="bibr" target="#b24">[25]</ref> and CAG-UDA <ref type="bibr" target="#b26">[27]</ref> for category centroid based counterparts that do not tackle the distance between different category features. As shown in <ref type="figure" target="#fig_3">Fig. 6</ref>, SePiCo achieves a much higher PDD on most categories compared with other methods. Based on these quantitative results, together with the t-SNE analysis in <ref type="figure" target="#fig_4">Fig. 7</ref>, it is clear that SePiCo can achieve better pixel-wise category alignment and largely improve the pixel-wise accuracy of predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">t-SNE Visualization</head><p>To better develop intuition, we draw t-SNE visualizations <ref type="bibr" target="#b90">[91]</ref> of the learned feature representations for famous category alignment methods (CLAN <ref type="bibr" target="#b22">[23]</ref>, FADA <ref type="bibr" target="#b25">[26]</ref>, SIM <ref type="bibr" target="#b24">[25]</ref>, CAG-UDA <ref type="bibr" target="#b26">[27]</ref>) and our SePiCo (DistCl) in <ref type="figure" target="#fig_4">Fig. 7</ref>. With this in mind, we first randomly select an image from target domain and then map its high-dimensional latent feature representations to a 2D space. From the t-SNE visualizations, we can observe that (i) Existing category alignment methods could produce separated features, but it may be hard for dense prediction since the margins between different category features are not obvious and the distribution is still dispersed; (ii) When we apply distributionaware pixel contrast, features among different categories are better separated, demonstrating that the semantic distribu-   tions can provide correct supervision signal for target data;</p><p>(ii) In comparison, the representations of SePiCo (DistCL) exhibit clearer clusters compared with others, revealing the discriminative capability of the contrastive adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Parameter Sensitivity</head><p>We conduct parameter sensitivity analysis to evaluate the sensitivity of SePiCo (DistCL) on GTAV ? Cityscapes. As shown respectively in <ref type="table" target="#tab_4">TABLE 9, TABLE 10, and  TABLE 11</ref>, we select loss weights ? cl and ? reg ? {0.01 , 0.1 , 0.5 , 1.0 , 2.0}, and the iteration at which to start contrastive learning L w ? {0 , 1500 , 3000 , 5000 , 10000}. While altering ? cl and ? reg in a large range, we find that both losses are sensitive to their assigned weight, and probably their relative weight due to their resemblance. Nevertheless, our method keeps outperforming the previous SOTA in different compositions of loss weights. We also explore the sensitivity of our method on the iteration to start contrastive learning at, and observe that SePiCo (DistCL) is relatively robust to L w , peaking at L w = 3, 000. The result could be attributed to better category information learned through warm up iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we present SePiCo, a novel end-to-end adaptation framework tailored for semantic segmentation, which successfully enhances the potential of self-training paradigm in conjunction with representation learning. Our main contribution is the discovery of pixel contrast guided by different semantic concepts. Eventually, we propose a particular form of contrastive loss at pixel level, which implicitly involves the joint learning of an infinite number of similar/dissimilar pixel pairs for each pixel representation of both domains. Additionally, we derive an upper bound on this formulation and transfer the originally intractable loss function into practical implementation. Though simple yet effective, it works surprisingly well. Extensive experiments demonstrate the superiority of SePiCo on both daytime and nighttime segmentation benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>(a) Target Image (b) Ground Truth (c) Source Only (d) DACS (e) ProDA (f) Ours (DistCL) Qualitative results on Cityscapes (val). From left to right: target image, ground truth, the maps predicted by Source Only, DACS, ProDA and Ours (DistCL) are shown one by one. Our method shows a clear visual improvement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Truth (c) Ours (ProtoCL) (d) Ours (BankCL) (e) Ours (DistCL) Qualitative results on Dark Zurich (val). From left to right: target image, ground truth, the maps predicted by Ours (ProtoCL), Ours (BankCL) and Ours (DistCL).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Quantitative analysis of the discrimination of features. For each class, we show the values of pixel-wise discrimination distance (PDD) as defined in Eq. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>t-SNE analysis<ref type="bibr" target="#b90">[91]</ref> of existing category alignment methods and Ours (DistCL). The visualization of the embedded features further proves that our method can exhibit the clearest clusters compared with other comparable baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, pixels in different images might share much similar semantics while arXiv:2204.08808v1 [cs.CV] 19 Apr 2022</figDesc><table><row><cell cols="2">Embedding space</cell><cell>Class 1</cell><cell cols="2">Class 2</cell><cell>Class 3</cell><cell>Push apart</cell><cell>Draw close</cell></row><row><cell></cell><cell></cell><cell cols="2">Adapt</cell><cell></cell><cell>Adapt</cell></row><row><cell>Source domain</cell><cell cols="2">Source embedding space</cell><cell cols="2">Semantic-Guided Pixel Contrast</cell><cell cols="2">Target embedding space</cell><cell>Target domain</cell></row><row><cell></cell><cell cols="3">(a) Centroid-aware pixel contrast</cell><cell></cell><cell></cell><cell>(b) Distribution-aware pixel contrast</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">image-wise centroid</cell><cell></cell><cell>Moments</cell></row><row><cell>1</cell><cell>3</cell><cell></cell><cell>... ...</cell><cell></cell><cell></cell><cell>1 , ? 1 , ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>...</cell><cell></cell><cell></cell><cell>2 , ? 2 , ?</cell></row><row><cell>2</cell><cell></cell><cell>enqueue</cell><cell>Memory bank</cell><cell>dequeue</cell><cell></cell><cell>3 , ? 3 , ?</cell></row><row><cell cols="2">(a.1) Global category prototypes</cell><cell cols="3">(a.2) Local catgory centroids</cell><cell></cell><cell>Semantic distributions</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Source data I s , Y s and target data I t , memory bank size B, maximum/warm-up iteration L/L w , and hyperparameters ? cl , ? reg . Initialize ? e with ImageNet pre-trained parameters and randomly initialize two heads ? c and ? p . Initialize statistics {? k } K k=1 and {? k } K k=1 to zeros.</figDesc><table /><note>Algorithm 2: SePiCo algorithm.1 Input:234 Teachers init:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 1 :</head><label>1</label><figDesc>14.6 71.3 24.1 15.3 25.5 32.1 13.5 82.9 25.1 78.0 56.2 33.3 76.3 26.6 29.8 12.3 28.5 18.0 38.6 AdaptSeg [20] 86.5 36.0 79.9 23.4 23.3 23.9 35.2 14.8 83.4 33.3 75.6 58.5 27.6 73.7 32.5 35.4 3.9 30.1 28.1 42.4 CLAN [23] 88.7 35.5 80.3 27.5 25.0 29.3 36.4 28.1 84.5 37.0 76.6 58.4 29.7 81.2 38.8 40.9 5.6 32.9 28.8 45.5 CBST [69] 91.8 53.5 80.5 32.7 21.0 34.0 28.9 20.4 83.9 34.2 80.9 53.1 24.0 82.7 30.3 35.9 16.0 25.9 42.8 sampling are also applied. In all experiments, we set tradeoffs ? cl , ? reg to 1.0, 1.0 and set threshold ?, momentum ?, and bank size B to 0.968, 0.999, 200 respectively. We train the network with a batch of two 640?640 random crops for total 40k iterations in one stage. The statistics in Section 3.2 are estimated right from the beginning, but pixel contrast starts from L w (default 3k) iteration to stabilize training. Testing. At the test stage, we only resize the validation images to 1280?640 as input image. Note that there is no extra inference step inserted to the basic segmentation model, that is, the teacher network, projection head ? p , and memory bank B, are directly discarded.</figDesc><table><row><cell>Method</cell><cell>r o a d</cell><cell>s i d e .</cell><cell>b u i l .</cell><cell>w a l l</cell><cell>f e n c e</cell><cell>p o l e</cell><cell>l i g h t</cell><cell>s i g n</cell><cell>v e g .</cell><cell>t e r r .</cell><cell>s k y</cell><cell>p e r s .</cell><cell>r i d e r</cell><cell>c a r</cell><cell>t r u c k</cell><cell>b u s</cell><cell>t r a i n</cell><cell>m b i k e</cell><cell>b i k e</cell><cell>mIoU</cell></row><row><cell>Source Only</cell><cell cols="20">70.2 45.9</cell></row><row><cell>MRKLD [30]</cell><cell cols="19">91.0 55.4 80.0 33.7 21.4 37.3 32.9 24.5 85.0 34.1 80.8 57.7 24.6 84.1 27.8 30.1 26.9 26.0 42.3</cell><cell>47.1</cell></row><row><cell>PLCA [68]</cell><cell cols="16">84.0 30.4 82.4 35.3 24.8 32.2 36.8 24.5 85.5 37.2 78.6 66.9 32.8 85.5 40.4 48.0</cell><cell>8.8</cell><cell cols="2">29.8 41.8</cell><cell>47.7</cell></row><row><cell>BDL [37]</cell><cell cols="16">91.0 44.7 84.2 34.6 27.6 30.2 36.0 36.0 85.0 43.6 83.0 58.6 31.6 83.3 35.3 49.7</cell><cell>3.3</cell><cell cols="2">28.8 35.6</cell><cell>48.5</cell></row><row><cell>SIM [25]</cell><cell cols="16">90.6 44.7 84.8 34.3 28.7 31.6 35.0 37.6 84.7 43.3 85.3 57.0 31.5 83.8 42.6 48.5</cell><cell>1.9</cell><cell cols="2">30.4 39.0</cell><cell>49.2</cell></row><row><cell>CaCo [66]</cell><cell cols="19">91.9 54.3 82.7 31.7 25.0 38.1 46.7 39.2 82.6 39.7 76.2 63.5 23.6 85.1 38.6 47.8 10.3 23.4 35.1</cell><cell>49.2</cell></row><row><cell>ConDA [34]</cell><cell cols="16">93.5 56.9 85.3 38.6 26.1 34.3 36.9 29.9 85.3 40.6 88.3 58.1 30.3 85.8 39.8 51.0</cell><cell>0.0</cell><cell cols="2">28.9 37.8</cell><cell>49.9</cell></row><row><cell>FADA [26]</cell><cell cols="16">91.0 50.6 86.0 43.4 29.8 36.8 43.4 25.0 86.8 38.3 87.4 64.0 38.0 85.2 31.6 46.1</cell><cell>6.5</cell><cell cols="2">25.4 37.1</cell><cell>50.1</cell></row><row><cell>LTIR [64]</cell><cell cols="16">92.9 55.0 85.3 34.2 31.1 34.9 40.7 34.0 85.2 40.1 87.1 61.0 31.1 82.5 32.3 42.9</cell><cell>0.3</cell><cell cols="2">36.4 46.1</cell><cell>50.2</cell></row><row><cell>CAG-UDA [27]</cell><cell cols="19">90.4 51.6 83.8 34.2 27.8 38.4 25.3 48.4 85.4 38.2 78.1 58.6 34.6 84.7 21.9 42.7 41.1 29.3 37.2</cell><cell>50.2</cell></row><row><cell>PixMatch [63]</cell><cell cols="16">91.6 51.2 84.7 37.3 29.1 24.6 31.3 37.2 86.5 44.3 85.3 62.8 22.6 87.6 38.9 52.3</cell><cell>0.7</cell><cell cols="2">37.2 50.0</cell><cell>50.3</cell></row><row><cell>Seg-Uncert. [32]</cell><cell cols="16">90.4 31.2 85.1 36.9 25.6 37.5 48.8 48.5 85.3 34.8 81.1 64.4 36.8 86.3 34.9 52.2</cell><cell>1.7</cell><cell cols="2">29.0 44.6</cell><cell>50.3</cell></row><row><cell>FDA-MBT [19]</cell><cell cols="19">92.5 53.3 82.4 26.5 27.6 36.4 40.6 38.9 82.3 39.8 78.0 62.6 34.4 84.9 34.1 53.1 16.9 27.7 46.4</cell><cell>50.5</cell></row><row><cell>KATPAN [29]</cell><cell cols="19">90.8 49.8 85.1 39.5 28.4 30.5 43.1 34.7 84.9 38.9 84.7 62.6 31.6 85.1 38.7 51.8 26.2 35.4 42.6</cell><cell>51.8</cell></row><row><cell>DACS [31]</cell><cell cols="16">89.9 39.7 87.9 30.7 39.5 38.5 46.4 52.8 88.0 44.0 88.8 67.2 35.8 84.5 45.7 50.2</cell><cell>0.0</cell><cell cols="2">27.3 34.0</cell><cell>52.1</cell></row><row><cell cols="17">MetaCorrection [80] 92.8 58.1 86.2 39.7 33.1 36.3 42.0 38.6 85.5 37.8 87.6 62.8 31.7 84.8 35.7 50.3</cell><cell>2.0</cell><cell cols="2">36.8 48.0</cell><cell>52.1</cell></row><row><cell>IAST [67]</cell><cell cols="19">94.1 58.8 85.4 39.7 29.2 25.1 43.1 34.2 84.8 34.6 88.7 62.7 30.3 87.6 42.3 50.3 24.7 35.2 40.2</cell><cell>52.2</cell></row><row><cell>UPLR [33]</cell><cell cols="16">90.5 38.7 86.5 41.1 32.9 40.5 48.2 42.1 86.5 36.8 84.2 64.5 38.1 87.2 34.8 50.4</cell><cell>0.2</cell><cell cols="2">41.8 54.6</cell><cell>52.6</cell></row><row><cell>DPL-dual [62]</cell><cell cols="19">92.8 54.4 86.2 41.6 32.7 36.4 49.0 34.0 85.8 41.3 86.0 63.2 34.2 87.2 39.3 44.5 18.7 42.6 43.1</cell><cell>53.3</cell></row><row><cell>SAC [35]</cell><cell cols="16">90.4 53.9 86.6 42.4 27.3 45.1 48.5 42.7 87.4 40.1 86.1 67.5 29.7 88.5 49.1 54.6</cell><cell>9.8</cell><cell cols="2">26.6 45.3</cell><cell>53.8</cell></row><row><cell>CTF [38]</cell><cell cols="19">92.5 58.3 86.5 27.4 28.8 38.1 46.7 42.5 85.4 38.4 91.8 66.4 37.0 87.8 40.7 52.4 44.6 41.7 59.0</cell><cell>56.1</cell></row><row><cell>CorDA [40]</cell><cell cols="16">94.7 63.1 87.6 30.7 40.6 40.2 47.8 51.6 87.6 47.0 89.7 66.7 35.9 90.2 48.9 57.5</cell><cell>0.0</cell><cell cols="2">39.8 56.0</cell><cell>56.6</cell></row><row><cell>ProDA [36]</cell><cell cols="16">87.8 56.0 79.7 46.3 44.8 45.6 53.5 53.5 88.6 45.2 82.1 70.7 39.2 88.8 45.5 59.4</cell><cell>1.0</cell><cell cols="2">48.9 56.4</cell><cell>57.5</cell></row><row><cell>SePiCo (ProtoCL)</cell><cell cols="16">95.6 69.2 89.0 40.8 38.6 44.3 56.3 64.4 88.3 46.5 88.6 73.1 47.6 90.7 58.9 53.8</cell><cell>5.4</cell><cell cols="2">22.4 43.8</cell><cell>58.8</cell></row><row><cell>SePiCo (BankCL)</cell><cell cols="16">96.1 72.1 88.6 43.1 42.4 43.7 56.0 63.5 88.9 44.5 89.0 72.7 45.7 91.1 61.7 59.6</cell><cell>0.0</cell><cell cols="2">24.7 53.6</cell><cell>59.8</cell></row><row><cell>SePiCo (DistCL)</cell><cell cols="16">95.2 67.8 88.7 41.4 38.4 43.4 55.5 63.2 88.6 46.4 88.3 73.1 49.0 91.4 63.2 60.4</cell><cell>0.0</cell><cell cols="2">45.2 60.0</cell><cell>61.0</cell></row></table><note>Comparison results of GTAV ? Cityscapes. All methods are based on DeepLab-V2 with ResNet-101 for a fair comparison. The best result is highlighted in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 1 ,</head><label>1</label><figDesc></figDesc><table /><note>and SYNTHIA ? Cityscapes in TABLE 2 and a challenging daytime-to-nighttime transfer scenario: Cityscapes ? Dark</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 2 :</head><label>2</label><figDesc>Comparison results of SYNTHIA ? Cityscapes. mIoU * denotes the mean IoU of 13 classes excluding the classes with * . All methods are based on DeepLab-V2 with ResNet-101 for a fair comparison. The best result is highlighted in bold. road side. buil. wall * fence * pole * light sign veg. sky pers. rider car bus mbike bike mIoU mIoU *</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Source Only</cell><cell>55.6</cell><cell>23.8</cell><cell>74.6</cell><cell>9.2</cell><cell>0.2</cell><cell>24.4</cell><cell>6.1</cell><cell cols="2">12.1 74.8 79.0 55.3</cell><cell>19.1 39.6 23.3</cell><cell>13.7</cell><cell>25.0</cell><cell>33.5</cell><cell>38.6</cell></row><row><cell>AdaptSeg [20]</cell><cell>79.2</cell><cell>37.2</cell><cell>78.8</cell><cell>10.5</cell><cell>0.3</cell><cell>25.1</cell><cell>9.9</cell><cell cols="2">10.5 78.2 80.5 53.5</cell><cell>19.6 67.0 29.5</cell><cell>21.6</cell><cell>31.3</cell><cell>39.5</cell><cell>45.9</cell></row><row><cell>CLAN [23]</cell><cell>82.7</cell><cell>37.2</cell><cell>81.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">17.7 13.1 81.2 83.3 55.5</cell><cell>22.1 76.6 30.1</cell><cell>23.5</cell><cell>30.7</cell><cell>-</cell><cell>48.8</cell></row><row><cell>CBST [69]</cell><cell>68.0</cell><cell>29.9</cell><cell>76.3</cell><cell>10.8</cell><cell>1.4</cell><cell>33.9</cell><cell cols="3">22.8 29.5 77.6 78.3 60.6</cell><cell>28.3 81.6 23.5</cell><cell>18.8</cell><cell>39.8</cell><cell>42.6</cell><cell>48.9</cell></row><row><cell>LTIR [64]</cell><cell>92.6</cell><cell>53.2</cell><cell>79.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.6</cell><cell>7.5</cell><cell>78.6 84.4 52.6</cell><cell>20.0 82.1 34.8</cell><cell>14.6</cell><cell>39.4</cell><cell>-</cell><cell>49.3</cell></row><row><cell>MRKLD [30]</cell><cell>67.7</cell><cell>32.2</cell><cell>73.9</cell><cell>10.7</cell><cell>1.6</cell><cell>37.4</cell><cell cols="3">22.2 31.2 80.8 80.5 60.8</cell><cell>29.1 82.8 25.0</cell><cell>19.4</cell><cell>45.3</cell><cell>43.8</cell><cell>50.1</cell></row><row><cell>BDL [37]</cell><cell>86.0</cell><cell>46.7</cell><cell>80.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">14.1 11.6 79.2 81.3 54.1</cell><cell>27.9 73.7 42.2</cell><cell>25.7</cell><cell>45.3</cell><cell>-</cell><cell>51.4</cell></row><row><cell>SIM [25]</cell><cell>83.0</cell><cell>44.0</cell><cell>80.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">17.1 15.8 80.5 81.8 59.9</cell><cell>33.1 70.2 37.3</cell><cell>28.5</cell><cell>45.8</cell><cell>-</cell><cell>52.1</cell></row><row><cell>FDA-MBT [19]</cell><cell>79.3</cell><cell>35.0</cell><cell>73.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">19.9 24.0 61.7 82.6 61.4</cell><cell>31.1 83.9 40.8</cell><cell>38.4</cell><cell>51.1</cell><cell>-</cell><cell>52.5</cell></row><row><cell>CAG-UDA [27]</cell><cell>84.7</cell><cell>40.8</cell><cell>81.7</cell><cell>7.8</cell><cell>0.0</cell><cell>35.1</cell><cell cols="3">13.3 22.7 84.5 77.6 64.2</cell><cell>27.8 80.9 19.7</cell><cell>22.7</cell><cell>48.3</cell><cell>44.5</cell><cell>-</cell></row><row><cell cols="2">MetaCorrection [80] 92.6</cell><cell>52.7</cell><cell>81.3</cell><cell>8.9</cell><cell>2.4</cell><cell>28.1</cell><cell>13.0</cell><cell>7.3</cell><cell>83.5 85.0 60.1</cell><cell>19.7 84.8 37.2</cell><cell>21.5</cell><cell>43.9</cell><cell>45.1</cell><cell>52.5</cell></row><row><cell>FADA [26]</cell><cell>84.5</cell><cell>40.1</cell><cell>83.1</cell><cell>4.8</cell><cell>0.0</cell><cell>34.3</cell><cell cols="3">20.1 27.2 84.8 84.0 53.5</cell><cell>22.6 85.4 43.7</cell><cell>26.8</cell><cell>27.8</cell><cell>45.2</cell><cell>52.5</cell></row><row><cell>ConDA [34]</cell><cell>88.1</cell><cell>46.7</cell><cell>81.1</cell><cell>10.6</cell><cell>1.1</cell><cell>31.3</cell><cell cols="3">22.6 19.6 81.3 84.3 53.9</cell><cell>21.7 79.8 42.9</cell><cell>24.2</cell><cell>46.8</cell><cell>46.0</cell><cell>53.3</cell></row><row><cell>CaCo [66]</cell><cell>87.4</cell><cell>48.9</cell><cell>79.6</cell><cell>8.8</cell><cell>0.2</cell><cell>30.1</cell><cell cols="3">17.4 28.3 79.9 81.2 56.3</cell><cell>24.2 78.6 39.2</cell><cell>28.1</cell><cell>48.3</cell><cell>46.0</cell><cell>53.6</cell></row><row><cell>PixMatch [63]</cell><cell>92.5</cell><cell>54.6</cell><cell>79.8</cell><cell>4.78</cell><cell>0.08</cell><cell>24.1</cell><cell cols="3">22.8 17.8 79.4 76.5 60.8</cell><cell>24.7 85.7 33.5</cell><cell>26.4</cell><cell>54.4</cell><cell>46.1</cell><cell>54.5</cell></row><row><cell>PLCA [68]</cell><cell>82.6</cell><cell>29.0</cell><cell>81.0</cell><cell>11.2</cell><cell>0.2</cell><cell>33.6</cell><cell cols="3">24.9 18.3 82.8 82.3 62.1</cell><cell>26.5 85.6 48.9</cell><cell>26.8</cell><cell>52.2</cell><cell>46.8</cell><cell>54.0</cell></row><row><cell>DPL-Dual [62]</cell><cell>87.5</cell><cell>45.7</cell><cell>82.8</cell><cell>13.3</cell><cell>0.6</cell><cell>33.2</cell><cell cols="3">22.0 20.1 83.1 86.0 56.6</cell><cell>21.9 83.1 40.3</cell><cell>29.8</cell><cell>45.7</cell><cell>47.0</cell><cell>54.2</cell></row><row><cell>Seg-Uncert. [32]</cell><cell>87.6</cell><cell>41.9</cell><cell>83.1</cell><cell>14.7</cell><cell>1.7</cell><cell>36.2</cell><cell cols="3">31.3 19.9 81.6 80.6 63.0</cell><cell>21.8 86.2 40.7</cell><cell>23.6</cell><cell>53.1</cell><cell>47.9</cell><cell>54.9</cell></row><row><cell>UPLR [33]</cell><cell>79.4</cell><cell>34.6</cell><cell>83.5</cell><cell>19.3</cell><cell>2.8</cell><cell>35.3</cell><cell cols="3">32.1 26.9 78.8 79.6 66.6</cell><cell>30.3 86.1 36.6</cell><cell>19.5</cell><cell>56.9</cell><cell>48.0</cell><cell>54.6</cell></row><row><cell>CTF [38]</cell><cell>75.7</cell><cell>30.0</cell><cell>81.9</cell><cell>11.5</cell><cell>2.5</cell><cell>35.3</cell><cell cols="3">18.0 32.7 86.2 90.1 65.1</cell><cell>33.2 83.3 36.5</cell><cell>35.3</cell><cell>54.3</cell><cell>48.2</cell><cell>55.5</cell></row><row><cell>DACS [31]</cell><cell>80.6</cell><cell>25.1</cell><cell>81.9</cell><cell>21.5</cell><cell>2.9</cell><cell>37.2</cell><cell cols="3">22.7 24.0 83.7 90.8 67.6</cell><cell>38.3 82.9 38.9</cell><cell>28.5</cell><cell>47.6</cell><cell>48.3</cell><cell>54.8</cell></row><row><cell>IAST [67]</cell><cell>81.9</cell><cell>41.5</cell><cell>83.3</cell><cell>17.7</cell><cell>4.6</cell><cell>32.3</cell><cell cols="3">30.9 28.8 83.4 85.0 65.5</cell><cell>30.8 86.5 38.2</cell><cell>33.1</cell><cell>52.7</cell><cell>49.8</cell><cell>57.0</cell></row><row><cell>KATPAN [29]</cell><cell>82.3</cell><cell>40.8</cell><cell>83.7</cell><cell>19.2</cell><cell>1.8</cell><cell>34.6</cell><cell cols="3">29.5 32.7 82.9 83.4 67.3</cell><cell>32.8 86.1 41.2</cell><cell>33.5</cell><cell>52.1</cell><cell>50.2</cell><cell>57.6</cell></row><row><cell>SAC [35]</cell><cell>89.3</cell><cell>47.2</cell><cell>85.5</cell><cell>26.5</cell><cell>1.3</cell><cell>43.0</cell><cell cols="3">45.5 32.0 87.1 89.3 63.6</cell><cell>25.4 86.9 35.6</cell><cell>30.4</cell><cell>53.0</cell><cell>52.6</cell><cell>59.3</cell></row><row><cell>CorDA [40]</cell><cell>93.3</cell><cell>61.6</cell><cell>85.3</cell><cell>19.6</cell><cell>5.1</cell><cell>37.8</cell><cell cols="3">36.6 42.8 84.9 90.4 69.7</cell><cell>41.8 85.6 38.4</cell><cell>32.6</cell><cell>53.9</cell><cell>55.0</cell><cell>62.8</cell></row><row><cell>ProDA [36]</cell><cell>87.8</cell><cell>45.7</cell><cell>84.6</cell><cell>37.1</cell><cell>0.6</cell><cell>44.0</cell><cell cols="3">54.6 37.0 88.1 84.4 74.2</cell><cell>24.3 88.2 51.1</cell><cell>40.5</cell><cell>45.6</cell><cell>55.5</cell><cell>62.0</cell></row><row><cell>SePiCo (ProtoCL)</cell><cell>79.2</cell><cell>42.9</cell><cell>85.6</cell><cell>9.9</cell><cell>4.2</cell><cell>38.0</cell><cell cols="3">52.5 53.3 80.6 81.2 73.7</cell><cell>47.4 86.2 63.1</cell><cell>48.0</cell><cell>63.2</cell><cell>56.8</cell><cell>65.9</cell></row><row><cell>SePiCo (BankCL)</cell><cell>76.7</cell><cell>34.3</cell><cell>84.9</cell><cell>18.7</cell><cell>2.9</cell><cell>38.4</cell><cell cols="3">51.8 55.6 85.0 84.6 73.2</cell><cell>45.0 89.7 63.7</cell><cell>50.5</cell><cell>63.8</cell><cell>57.4</cell><cell>66.1</cell></row><row><cell>SePiCo (DistCL)</cell><cell>77.0</cell><cell>35.3</cell><cell>85.1</cell><cell>23.9</cell><cell>3.4</cell><cell>38.0</cell><cell cols="3">51.0 55.1 85.6 80.5 73.5</cell><cell>46.3 87.6 69.7</cell><cell>50.9</cell><cell>66.5</cell><cell>58.1</cell><cell>66.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>r o a d</cell><cell>s id e .</cell><cell>b u il .</cell><cell>w a ll</cell><cell>f e n c e</cell><cell>p o le</cell><cell>li g h t</cell><cell>s ig n</cell><cell>v e g .</cell><cell>t e r r .</cell><cell>s k y</cell><cell>p e r s .</cell><cell>r id e r</cell><cell>c a r</cell><cell>t r u c k</cell><cell>b u s</cell><cell>t r a in</cell><cell>m b ik e</cell><cell>b ik e</cell><cell>mIoU</cell></row><row><cell>Source Only</cell><cell>R</cell><cell cols="10">68.8 23.2 46.8 20.8 12.6 29.8 30.4 26.9 43.1 14.3</cell><cell>0.3</cell><cell cols="3">36.9 49.7 63.6</cell><cell>6.8</cell><cell cols="3">0.2 24.0 33.6</cell><cell>9.3</cell><cell>28.5</cell></row><row><cell>DMAda [53]</cell><cell>R</cell><cell cols="10">75.5 29.1 48.6 21.3 14.3 34.3 36.8 29.9 49.4 13.8</cell><cell>0.4</cell><cell cols="8">43.3 50.2 69.4 18.4 0.0 27.6 34.9 11.9</cell><cell>32.1</cell></row><row><cell>GCMA [82]</cell><cell>R</cell><cell cols="19">81.7 46.9 58.8 22.0 20.0 41.2 40.5 41.6 64.8 31.0 32.1 53.5 47.5 75.5 39.2 0.0 49.6 30.7 21.0</cell><cell>42.0</cell></row><row><cell>MGCDA [54]</cell><cell>R</cell><cell cols="3">80.3 49.3 66.2</cell><cell>7.8</cell><cell cols="15">11.0 41.4 38.9 39.0 64.1 18.0 55.8 52.1 53.5 74.7 66.0 0.0 37.5 29.1 22.7</cell><cell>42.5</cell></row><row><cell>DANNet [55]</cell><cell>R</cell><cell cols="19">90.0 54.0 74.8 41.0 21.1 25.0 26.8 30.2 72.0 26.2 84.0 47.0 33.9 68.2 19.0 0.3 66.4 38.3 23.6</cell><cell>44.3</cell></row><row><cell>CDAda [89]</cell><cell>R</cell><cell cols="19">90.5 60.6 67.9 37.0 19.3 42.9 36.4 35.3 66.9 24.4 79.8 45.4 42.9 70.8 51.7 0.0 29.7 27.7 26.2</cell><cell>45.0</cell></row><row><cell>Source Only</cell><cell>D</cell><cell cols="14">79.0 21.8 53.0 13.3 11.2 22.5 20.2 22.1 43.5 10.4 18.0 37.4 33.8 64.1</cell><cell>6.4</cell><cell cols="3">0.0 52.3 30.4</cell><cell>7.4</cell><cell>28.8</cell></row><row><cell>AdaptSeg [20]</cell><cell>D</cell><cell cols="4">86.1 44.2 55.1 22.2</cell><cell>4.8</cell><cell>21.1</cell><cell>5.6</cell><cell cols="2">16.7 37.2</cell><cell>8.4</cell><cell>1.2</cell><cell cols="8">35.9 26.7 68.2 45.1 0.0 50.1 33.9 15.6</cell><cell>30.4</cell></row><row><cell>AdvEnt [21]</cell><cell>D</cell><cell cols="9">85.8 37.9 55.5 27.7 14.5 23.1 14.0 21.1 32.1</cell><cell>8.7</cell><cell>2.0</cell><cell cols="8">39.9 16.6 64.0 13.8 0.0 58.8 28.5 20.7</cell><cell>29.7</cell></row><row><cell>BDL [37]</cell><cell>D</cell><cell cols="9">85.3 41.1 61.9 32.7 17.4 20.6 11.4 21.3 29.4</cell><cell>8.9</cell><cell>1.1</cell><cell cols="8">37.4 22.1 63.2 28.2 0.0 47.7 39.4 15.7</cell><cell>30.8</cell></row><row><cell>DANNet [55]</cell><cell>D</cell><cell cols="19">88.6 53.4 69.8 34.0 20.0 25.0 31.5 35.9 69.5 32.2 82.3 44.2 43.7 54.1 22.0 0.1 40.9 36.0 24.1</cell><cell>42.5</cell></row><row><cell>SePiCo (ProtoCL)</cell><cell>D</cell><cell cols="19">87.3 50.9 64.5 25.6 12.1 38.3 40.8 37.5 61.0 21.9 77.6 37.4 47.0 67.8 54.5 0.0 33.7 27.0 23.7</cell><cell>42.6</cell></row><row><cell>SePiCo (BankCL)</cell><cell>D</cell><cell cols="19">88.5 54.8 66.5 25.1 13.5 40.0 39.6 40.8 62.5 25.1 79.0 37.8 54.8 70.4 63.7 0.0 36.8 15.6 23.4</cell><cell>44.1</cell></row><row><cell>SePiCo (DistCL)</cell><cell>D</cell><cell cols="19">91.2 61.3 67.0 28.5 15.5 44.7 44.3 41.3 65.4 22.5 80.4 41.3 52.4 71.2 39.3 0.0 39.6 27.5 28.8</cell><cell>45.4</cell></row></table><note>Comparison results of Cityscapes ? Dark Zurich. The DeepLab-V2 (D) [8] and RefineNet (R) [88] architecture with ResNet-101 trained on Cityscapes are used as Source Only baselines. The best result is highlighted in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 4 :</head><label>4</label><figDesc>Comparison results using Swin-B ViT</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Swin-B ViT (88M) 63.3 28.6 68.3 16.8 23.4 37.8 51.0 34.3 83.8 42.1 85.7 68.5 25.4 83.5 36.3 17.7 2.9 36.1 42.3 44.6 TransDA-B [90] 94.7 64.2 89.2 48.1 45.8 50.1 60.2 40.8 90.4 50.2 93.7 76.7 47.6 92.5 56.8 60.1 47.6 49.6 55.4 63.9 SegF. MiT-B5 (84.7M) 77.1 15.2 83.8 30.8 32.0 27.9 41.5 18.5 86.5 42.5 86.8 62.6 22.2 87.0 42.7 36.8 6.1 33.5 12.5 44.5 DAFormer [28] 95.7 70.2 89.4 53.5 48.1 49.6 55.8 59.4 89.9 47.9 92.5 72.2 44.7 92.3 74.5 78.2 65.1 55.9 61.8 fence * pole * light sign veg. sky pers. rider car bus mbike bike mIoU mIoU * Implement according to source code.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>68.3</cell></row><row><cell cols="2">SePiCo (ProtoCL)</cell><cell cols="20">96.1 72.9 89.7 54.4 48.8 53.5 60.4 65.3 90.0 48.4 91.6 75.2 47.1 93.3 74.4 74.6 41.2 58.8 65.9</cell><cell>68.5</cell></row><row><cell cols="2">SePiCo (BankCL)</cell><cell cols="20">96.3 73.6 89.6 53.7 47.8 53.8 60.8 60.0 89.9 48.8 91.5 74.6 45.1 93.1 74.8 73.8 51.5 60.3 65.3</cell><cell>68.7</cell></row><row><cell cols="2">SePiCo (DistCL)</cell><cell cols="20">96.9 76.7 89.7 55.5 49.5 53.2 60.0 64.5 90.2 50.3 90.8 74.5 44.2 93.3 77.0 79.5 63.6 61.0 65.3</cell><cell>70.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(b) SYNTHIA ? Cityscapes</cell><cell></cell><cell></cell></row><row><cell cols="10">Method road side. buil. wall  Swin-B ViT (88M) 57.3 33.8 56.0 6.3</cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell>33.8</cell><cell cols="5">35.5 18.9 79.9 74.8 63.1</cell><cell cols="2">10.9 78.3 39.0</cell><cell>20.8</cell><cell>19.4</cell><cell>39.2</cell><cell>45.2</cell></row><row><cell cols="2">TransDA-B [90]</cell><cell>90.4</cell><cell></cell><cell cols="2">54.8</cell><cell></cell><cell>86.4</cell><cell cols="2">31.1</cell><cell></cell><cell></cell><cell>1.7</cell><cell></cell><cell>53.8</cell><cell cols="5">61.1 37.1 90.3 93.0 71.2</cell><cell cols="2">25.3 92.3 66.0</cell><cell>44.4</cell><cell>49.8</cell><cell>59.3</cell><cell>66.3</cell></row><row><cell cols="3">SegF. MiT-B5 (84.7M) 69.9</cell><cell></cell><cell cols="2">27.8</cell><cell></cell><cell>82.9</cell><cell cols="2">21.6</cell><cell></cell><cell></cell><cell>2.3</cell><cell></cell><cell>39.2</cell><cell cols="5">36.3 29.9 84.2 84.9 61.6</cell><cell cols="2">22.6 83.8 48.0</cell><cell>14.9</cell><cell>19.7</cell><cell>45.6</cell><cell>51.3</cell></row><row><cell cols="2">DAFormer [28]</cell><cell>84.5</cell><cell></cell><cell cols="2">40.7</cell><cell></cell><cell>88.4</cell><cell cols="2">41.5</cell><cell></cell><cell></cell><cell>6.5</cell><cell></cell><cell>50.0</cell><cell cols="5">55.0 54.6 86.0 89.8 73.2</cell><cell cols="2">48.2 87.2 53.2</cell><cell>53.9</cell><cell>61.7</cell><cell>60.9</cell><cell>67.4</cell></row><row><cell cols="2">SePiCo (ProtoCL)</cell><cell>85.9</cell><cell></cell><cell cols="2">45.5</cell><cell></cell><cell>88.9</cell><cell cols="2">38.2</cell><cell></cell><cell></cell><cell>2.5</cell><cell></cell><cell>52.3</cell><cell cols="5">57.7 58.2 89.3 88.4 74.0</cell><cell cols="2">50.5 92.3 70.6</cell><cell>56.2</cell><cell>56.7</cell><cell>62.9</cell><cell>70.3</cell></row><row><cell cols="2">SePiCo (BankCL)</cell><cell>88.2</cell><cell></cell><cell cols="2">49.3</cell><cell></cell><cell>88.6</cell><cell cols="2">36.1</cell><cell></cell><cell></cell><cell>4.7</cell><cell></cell><cell>53.1</cell><cell cols="5">58.9 58.4 88.5 84.8 72.4</cell><cell cols="2">49.3 92.8 76.3</cell><cell>55.5</cell><cell>55.2</cell><cell>63.3</cell><cell>70.6</cell></row><row><cell cols="2">SePiCo (DistCL)</cell><cell>87.0</cell><cell></cell><cell cols="2">52.6</cell><cell></cell><cell>88.5</cell><cell cols="2">40.6</cell><cell></cell><cell cols="2">10.6</cell><cell></cell><cell>49.8</cell><cell cols="5">57.0 55.4 86.8 86.2 75.4</cell><cell cols="2">52.7 92.4 78.9</cell><cell>53.0</cell><cell>62.6</cell><cell>64.3</cell><cell>71.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(c) Cityscapes ? Dark Zurich</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell cols="2">r o a d</cell><cell cols="2">s i d e .</cell><cell cols="2">b u i l .</cell><cell cols="2">w a l l</cell><cell cols="2">f e n c e</cell><cell cols="2">p o l e</cell><cell>l i g h t</cell><cell>s i g n</cell><cell>v e g .</cell><cell>t e r r .</cell><cell>s k y</cell><cell>p e r s .</cell><cell>r i d e r</cell><cell>c a r</cell><cell>t r u c k</cell><cell>b u s</cell><cell>t r a i n</cell><cell>m b i k e</cell><cell>b i k e</cell><cell>mIoU</cell></row><row><cell cols="9">SegF. MiT-B5 (84.7M) 80.3 37.1 57.5 28.1</cell><cell></cell><cell>7.9</cell><cell></cell><cell cols="6">35.5 33.2 29.3 41.7 14.8</cell><cell>4.7</cell><cell cols="3">48.9 48.0 66.6</cell><cell>5.7</cell><cell>7.9 63.3 31.4 23.3</cell><cell>35.0</cell></row><row><cell cols="2">DAFormer [28]</cell><cell cols="20">92.0 63.0 67.2 28.9 13.1 44.0 42.0 42.3 70.7 28.2 83.6 51.1 39.1 76.4 31.7 0.0 78.3 43.9 26.5</cell><cell>48.5</cell></row><row><cell cols="2">SePiCo (ProtoCL)</cell><cell cols="20">90.1 57.7 75.0 34.9 16.4 53.5 47.0 47.8 70.1 31.7 84.1 57.3 53.3 80.5 42.4 2.3 83.6 42.6 30.1</cell><cell>52.7</cell></row><row><cell cols="2">SePiCo (BankCL)</cell><cell cols="20">91.1 61.2 73.4 31.9 18.0 51.6 48.6 47.7 72.8 33.0 85.5 57.0 51.1 80.6 48.4 3.1 84.6 45.3 28.2</cell><cell>53.3</cell></row><row><cell cols="2">SePiCo (DistCL)</cell><cell cols="20">93.2 68.1 73.7 32.8 16.3 54.6 49.5 48.1 74.2 31.0 86.3 57.9 50.9 82.4 52.2 1.3 83.8 43.9 29.8</cell><cell>54.2</cell></row><row><cell>GTAV ? Cityscapes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>road terrain</cell><cell>sidewalk sky</cell><cell>building person</cell><cell cols="2">wall rider</cell><cell cols="2">fence car</cell><cell cols="2">pole truck</cell><cell cols="2">light bus</cell><cell cols="2">sign train</cell><cell cols="2">vegetation motocycle</cell><cell>unlabeled bike</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SYNTHIA ? Cityscapes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 5 :</head><label>5</label><figDesc>Ablation study on GTAV ? Cityscapes. All models are trained from scratch in total of 40k iterations. L ssl L cl L reg CBC mIoU ?</figDesc><table><row><cell></cell><cell>38.6</cell><cell>-</cell></row><row><cell>w/o self-training</cell><cell>48.5 49.2</cell><cell>9.9 10.6</cell></row><row><cell></cell><cell>49.8</cell><cell>11.2</cell></row><row><cell></cell><cell>52.1</cell><cell>-</cell></row><row><cell>SePiCo (DistCL)</cell><cell>59.3 60.4</cell><cell>7.2 8.3</cell></row><row><cell></cell><cell>61.0</cell><cell>8.9</cell></row></table><note>Method</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 6 :</head><label>6</label><figDesc>Effect of the teacher network.</figDesc><table><row><cell></cell><cell>w/o teacher</cell><cell></cell><cell cols="2">w/ teacher</cell></row><row><cell>?</cell><cell>0.0</cell><cell>0.9</cell><cell cols="3">0.99 0.999 0.9995</cell></row><row><cell>mIoU</cell><cell>56.1</cell><cell cols="2">59.3 60.4</cell><cell>61.0</cell><cell>60.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 7 :</head><label>7</label><figDesc>Effect of the bank size B.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">SePiCo (BankCL)</cell><cell></cell><cell>SePiCo (DistCL)</cell></row><row><cell>B</cell><cell>10</cell><cell>50</cell><cell>100</cell><cell>200</cell><cell>500</cell><cell>?</cell></row><row><cell cols="6">mIoU 59.2 59.4 59.5 59.8 59.7</cell><cell>61.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 8 :</head><label>8</label><figDesc>Ablation of feature selection in SePiCo variants for GTAV ? Cityscapes based on DeepLab-V2.</figDesc><table><row><cell>Method</cell><cell cols="2">layer 1 layer 2 layer 3 layer 4 mIoU</cell></row><row><cell></cell><cell></cell><cell>58.7</cell></row><row><cell></cell><cell></cell><cell>58.6</cell></row><row><cell>SePiCo (ProtoCL)</cell><cell></cell><cell>58.6</cell></row><row><cell></cell><cell></cell><cell>58.8</cell></row><row><cell></cell><cell>{1,2,3,4}-fusion</cell><cell>58.4</cell></row><row><cell></cell><cell></cell><cell>58.5</cell></row><row><cell></cell><cell></cell><cell>58.4</cell></row><row><cell>SePiCo (BankCL)</cell><cell></cell><cell>58.0</cell></row><row><cell></cell><cell></cell><cell>59.8</cell></row><row><cell></cell><cell>{1,2,3,4}-fusion</cell><cell>58.6</cell></row><row><cell></cell><cell></cell><cell>60.7</cell></row><row><cell></cell><cell></cell><cell>59.3</cell></row><row><cell>SePiCo (DistCL)</cell><cell></cell><cell>58.8</cell></row><row><cell></cell><cell></cell><cell>61.0</cell></row><row><cell></cell><cell>{1,2,3,4}-fusion</cell><cell>60.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 9 :</head><label>9</label><figDesc>Study on ? cl . mIoU 58.9 59.9 60.9 61.0 59.3</figDesc><table><row><cell>? cl</cell><cell>0.01</cell><cell>0.1</cell><cell>0.5</cell><cell>1.0</cell><cell>2.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 10 :</head><label>10</label><figDesc>Study on ? reg . mIoU 59.5 59.3 59.3 61.0 58.7</figDesc><table><row><cell>? reg</cell><cell>0.01</cell><cell>0.1</cell><cell>0.5</cell><cell>1.0</cell><cell>2.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE 11 :</head><label>11</label><figDesc>Study on L w .</figDesc><table><row><cell>L w</cell><cell>0</cell><cell cols="4">1,500 3,000 5,000 10,000</cell></row><row><cell cols="2">mIoU 59.2</cell><cell>59.5</cell><cell>61.0</cell><cell>59.7</cell><cell>58.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://competitions.codalab.org/competitions/23553</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Computer vision for autonomous vehicles: Problems, datasets and state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>G?ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Comput. Graph. Vis</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="1" to="308" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep learning techniques for medical image segmentation: Achievements and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Hesamian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Digit. Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="582" to="596" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The SYNTHIA dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>V?zquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>L?pez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dataset Shift in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quionero-Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CyCADA: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICML</title>
		<imprint>
			<biblScope unit="page" from="1989" to="1998" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Transferable representation learning with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3071" to="3085" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generalized domain conditioned adaptation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Domainadversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Domain stylization: A fast covariance matching framework towards domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zedlewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2360" to="2372" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">FDA: fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4084" to="4094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7472" to="7481" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2517" to="2526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised intra-domain adaptation for semantic segmentation through selfsupervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rameau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3764" to="3773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Categorylevel adversarial adaptation for semantic segmentation using purified features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Transferable semantic augmentation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Differential treatment for stuff and things: A simple unsupervised domain adaptation method for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="635" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Classes matter: A fine-grained adversarial approach to cross-domain semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="642" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Category anchor-guided unsupervised domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="433" to="443" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Daformer: Improving network architectures and training strategies for domain-adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno>abs/2111.14887</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Where and how to transfer: Knowledge aggregation-induced transferability perception for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V K V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5982" to="5991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">DACS: domain adaptation via cross-domain mixed sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV, 2021</title>
		<imprint>
			<biblScope unit="page" from="1378" to="1388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1106" to="1120" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Uncertainty-aware pseudo label refinery for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9092" to="9101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Confidence estimation via auxiliary models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Corbiere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saporta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-supervised augmentation consistency for adapting semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Araslanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="page" from="15" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6936" to="6945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Coarse-to-fine domain adaptive semantic segmentation with photometric alignment and categorycenter regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4051" to="4060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">DADA: depthaware domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7363" to="7372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Domain adaptive semantic segmentation with self-supervised depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8515" to="8525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9726" to="9735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Joint contrastive learning with infinite possibilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="833" to="851" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="173" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno>abs/2111.06377</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dark model adaptation: Semantic image segmentation from daytime to nighttime</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ITSC</title>
		<imprint>
			<biblScope unit="page" from="3819" to="3824" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Map-guided curriculum domain adaptation and uncertainty-aware evaluation for semantic nighttime image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dannet: A one-stage domain adaptation network for unsupervised nighttime semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="137" to="144" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Bridging theory and algorithm for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7404" to="7413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Active learning for domain adaptation: An energy-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/2112.01406</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">An empirical comparison of domain adaptation methods for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kurohashi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="385" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Domain adaptive faster R-CNN for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="3339" to="3348" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Pareto domain adaptation,&quot; in NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Dual path learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9082" to="9091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Pixmatch: Unsupervised domain adaptation via pixelwise consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Melas-Kyriazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Manrai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">445</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning texture invariant representation for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12" to="975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2507" to="2516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Category contrast for unsupervised domain adaptation in visual tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno>abs/2106.02885</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Instance adaptive selftraining for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="415" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Pixellevel cycle association: A new perspective for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced selftraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="289" to="305" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1807.03748</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Dense contrastive learning for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3024" to="3033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Exploring cross-image pixel contrast for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7303" to="7313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with pixel-level contrastive learning from a class-wise memory bank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sabater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Montesano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8219" to="8228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="1195" to="1204" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Semisupervised semantic segmentation via adaptive equalization learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Classmix: Segmentation-based data augmentation for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV, 2021</title>
		<imprint>
			<biblScope unit="page" from="1368" to="1377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6028" to="6039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Regularizing deep networks with semantic data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Metacorrection: Domainaware meta loss correction for unsupervised domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3927" to="3936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Guided curriculum model adaptation and uncertainty-aware evaluation for semantic nighttime image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7373" to="7382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Contributors</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/mmsegmentation" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="8024" to="8035" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Cdada: A curriculum domain adaptation for nighttime semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCVW</publisher>
			<biblScope unit="page" from="2962" to="2971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Smoothing matters: Momentum transformer for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<idno>abs/2203.07988</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">86</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
