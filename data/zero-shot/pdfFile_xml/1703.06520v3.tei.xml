<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detecting Oriented Text in Natural Images by Linking Segments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
							<email>shibaoguang@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
							<email>xbai@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cornell Tech</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Detecting Oriented Text in Natural Images by Linking Segments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most state-of-the-art text detection methods are specific to horizontal Latin text and are not fast enough for real-time applications. We introduce Segment Linking (SegLink), an oriented text detection method. The main idea is to decompose text into two locally detectable elements, namely segments and links. A segment is an oriented box covering a part of a word or text line; A link connects two adjacent segments, indicating that they belong to the same word or text line. Both elements are detected densely at multiple scales by an end-to-end trained, fully-convolutional neural network. Final detections are produced by combining segments connected by links. Compared with previous methods, SegLink improves along the dimensions of accuracy, speed, and ease of training. It achieves an f-measure of 75.0% on the standard ICDAR 2015 Incidental (Challenge 4) benchmark, outperforming the previous best by a large margin. It runs at over 20 FPS on 512?512 images. Moreover, without modification, SegLink is able to detect long lines of non-Latin text, such as Chinese.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Reading text in natural images is a challenging task under active research. It is driven by many real-world applications, such as Photo OCR <ref type="bibr" target="#b1">[2]</ref>, geo-location, and image retrieval <ref type="bibr" target="#b8">[9]</ref>. In a text reading system, text detection, i.e. localizing text with bounding boxes of words or text lines, is usually the first step of great significance. In a sense, text detection can be seen as object detection applied to text, where words/characters/text lines are taken as the detection targets. Owing to this, a new trend has emerged recently that state-of-the-art text detection methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30]</ref> are heavily based on the advanced general object detection or segmentation techniques, e.g. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15</ref>].</p><p>Despite the great success of the previous work, we argue that the general detection methods are not well suited * Corresponding author. for text detection, for two main reasons. First, word/text line bounding boxes have much larger aspect ratios than those of general objects. An (fast/faster) R-CNN <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19</ref>]or SSD <ref type="bibr" target="#b13">[14]</ref>-style detector may suffer from the difficulty of producing such boxes, owing to its proposal or anchor box design. In addition, some non-Latin text does not have blank spaces between words, hence the even larger bounding box aspect ratios, which make the problem worse. Second, unlike general objects, text usually has a clear definition of orientation <ref type="bibr" target="#b24">[25]</ref>. It is important for a text detector to produce oriented boxes. However, most general object detection methods are not designed to produce oriented boxes. To overcome the above challenges, we tackle the text detection problem in a new perspective. We propose to decompose long text into two smaller and locally-detectable elements, namely segment and link. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>  <ref type="figure">Figure 2</ref>. Network Architecture. The network consists of convolutional feature layers (shown as gray blocks) and convolutional predictors (thin gray arrows). Convolutional filters are specified in the format of "(#filters),k(kernel size)s(stride)". A multi-line filter specification means a hidden layer between. Segments (yellow boxes) and links (not displayed) are detected by convolutional predictors on multiple feature layers (indexed by l = 1 . . . 6) and combined into whole words by a combining algorithm.</p><p>(for clarity we use "word" here and later on, but segments also work seamlessly on text lines that comprise multiple words); A link connects a pair of adjacent segments, indicating that they belong to the same word. Under the above definitions, a word is located by a number of segments with links between them. During detection, segments and links are densely detected on an input image by a convolutional neural network. Then, the segments are combined into whole words according to the links.</p><p>The key advantage of this approach is that long and oriented text is now detected locally since both basic elements are locally-detectable: Detecting a segment does not require the whole word to be observed. And neither does a link since the connection of two segments can be inferred from a local context. Thereafter, we can detect text of any length and orientation with great flexibility and efficiency.</p><p>Concretely, we propose a convolutional neural network (CNN) model to detect both segments and links simultaneously, in a fully-convolutional manner. The network uses VGG-16 <ref type="bibr" target="#b20">[21]</ref> as its backbone. A few extra feature layers are added onto it. Convolutional predictors are added to 6 of the feature layers to detect segments and links at different scales. To deal with redundant detections, we introduce two types of links, namely within-layer links and cross-layer links. A within-layer link connects a segment to its neighbors on the same layer. A cross-layer link, on the other hand, connects a segment to its neighbors on the lower layer. In this way, we connect segments of adjacent locations as well as scales. Finally, we find connected segments with a depth-first search (DFS) algorithm and combine them into whole words.</p><p>Our main contribution is the novel segment-linking de-tection method. Through experiments, we show that the proposed method possesses several distinctive advantages over the other state-of-the-art methods: 1) Robustness: SegLink models the structure of oriented text in a simple and elegant way, with robustness against complex backgrounds. Our method achieves highly competitive results on standard datasets. In particular, it outperforms the previous best by a large margin in terms of f-measure (75.0% vs 64.8%) on the ICDAR 2015 Incidental (Challenge 4) benchmark <ref type="bibr" target="#b11">[12]</ref>; 2) Efficiency: SegLink is highly efficient due to its single-pass, fully-convolutional design. It processes more than 20 images of 512x512 size per second; 3) Generality: Without modification, SegLink is able to detect long lines of non-Latin text, such as Chinese. We demonstrate this capability on a multi-lingual dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Text Detection Over the past few years, much research effort has been devoted to the text detection problem <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26]</ref>. Based on the basic detection targets, the previous methods can be roughly divided into three categories: character-based, word-based and line-based. Character-based methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> detect individual characters and group them into words. These methods find characters by classifying candidate regions extracted by region extraction algorithms or by classifying sliding windows. Such methods often involve a postprocessing step of grouping characters into words. Wordbased methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b5">6]</ref> directly detect word bounding boxes. They often have a similar pipeline to the recent CNN-based general object detection networks. Though achieving excellent detection accuracies, these methods may suffer from performance drop when applied to some non-Latin text such as Chinese, as we mentioned earlier. Line-based methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b25">26]</ref> find text regions using some image segmentation algorithms. They also require a sophisticated post-processing step of word partitioning and/or false positive removal. Compared with the previous approaches, our method predicts segments and links jointly in a single forward network pass. The pipeline is much simpler and cleaner. Moreover, the network is end-to-end trainable.</p><p>Our method is similar in spirit to a recent work <ref type="bibr" target="#b21">[22]</ref>, which detects text lines by finding and grouping a sequence of fine-scale text proposals through a CNN coupled with recurrent neural layers. In contrast, we detect oriented segments only using convolutional layers, yielding better flexibility and faster speed. Also, we detect links explicitly using the same strong CNN features for segments, improving the robustness.</p><p>Object Detection Text detection can be seen as a particular instance of general object detection, which is a fundamental problem in computer vision. Most state-of-the-art detection systems either classify some class-agnostic object proposals with CNN <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19]</ref> or directly regress object bounding boxes from a set of preset boxes (e.g. anchor boxes) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>The architecture of our network inherits that of SSD <ref type="bibr" target="#b13">[14]</ref>, a recent object detection model. SSD proposed the idea of detecting objects on multiple feature layers with convolutional predictors. Our model also detects segments and links in a very similar way. Despite the model similarity, our detection strategy is drastically different: SSD directly outputs object bounding boxes. We, on the other hand, adopt a bottom-up approach by detecting the two comprising elements of a word or text line and combine them together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Segment Linking</head><p>Our method detects text with a feed-forward CNN model. Given an input image I of size w I ? h I , the model outputs a fixed number of segments and links, which are then filtered by their confidence scores and combined into whole word bounding boxes. A bounding box is a ro-</p><formula xml:id="formula_0">tated rectangle denoted by b = (x b , y b , w b , h b , ? b ), where x b , y b</formula><p>are the coordinates of the center, w b , h b the width and height, and ? b the rotation angle. <ref type="figure">Fig. 2</ref> shows the network architecture. Our network uses a pretrained VGG-16 network <ref type="bibr" target="#b20">[21]</ref> as its backbone (conv1 through pool5). Following <ref type="bibr" target="#b13">[14]</ref>, the fully-connected layers of VGG-16 are converted into convolutional layers (fc6 to conv6; fc7 to conv7). They are followed by a few extra convolutional layers (conv8 1 to conv11), which extract even deeper features with larger receptive fields. Their configurations are specified in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">CNN Model</head><p>Segments and links are detected on 6 of the feature layers, which are conv4 3, conv7, conv8 2, conv9 2, conv10 2, and conv11. These feature layers provide high-quality deep features of different granularity (conv4 3 the finest and conv11 the coarsest). A convolutional predictor with 3 ? 3 kernels is added to each of the 6 layers to detect segments and links. We index the feature layers and the predictors by l = 1, . . . , 6.</p><p>Segment Detection Segments are also oriented boxes, denoted by s = (x s , y s , w s , h s , ? s ). We detect segments by estimating the confidence scores and geometric offsets to a set of default boxes <ref type="bibr" target="#b13">[14]</ref> on the input image. Each default box is associated with a feature map location, and its score and offsets are predicted from the features at that location. For simplicity, we only associate one default box with a feature map location.</p><p>Consider the l-th feature layer whose feature map size is w l ? h l . A location (x, y) on this map corresponds to a default box centered at (x a , y a ) on the image, where</p><formula xml:id="formula_1">x a = w I w l (x + 0.5); y a = h I h l (y + 0.5)<label>(1)</label></formula><p>The width and height of the default box are both set to a constant a l . The convolutional predictor produces 7 channels for segment detection. Among them, 2 channels are further softmax-normalized to get the segment score in (0, 1). The rest 5 are the geometric offsets. Considering a location (x, y) on the map, we denote the vector at this location along the depth by (?x s , ?y s , ?w s , ?h s , ?? s ). Then, the segment at this location is calculated by:</p><formula xml:id="formula_2">x s = a l ?x s + x a (2) y s = a l ?y s + y a (3) w s = a l exp(?w s ) (4) h s = a l exp(?h s ) (5) ? s = ?? s<label>(6)</label></formula><p>Here, the constant a l controls the scale of the output segments. It should be chosen with regard to the receptive field size of the l-th layer. We use an empirical equation for choosing this size: a l = ? w I w l , where ? = 1.5.</p><p>Within-Layer Link Detection A link connects a pair of adjacent segments, indicating that they belong to the same word. Here, adjacent segments are those detected at adjacent feature map locations. Links are not only necessary for combining segments into whole words but also helpful for separating two nearby words -between two nearby words, the links should be predicted as negative. We explicitly detect links between segments using the same features for detecting segments. Since we detect only one segment at a feature map location, segments can be indexed by their map locations (x, y) and layer indexes l, denoted by s (x,y,l) . As illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>.a, we define the within-layer neighbors of a segment as its 8-connected neighbors on the same feature layer:</p><formula xml:id="formula_3">N w s (x,y,l) = {s (x ,y ,l) } x?1?x ?x+1,y?1?y ?y+1 \ s (x,y,l)<label>(7)</label></formula><p>As segments are detected locally, a pair of neighboring segments are also adjacent on input image. Links are also detected by the convolutional predictors. A predictor outputs 16 channels for the links to the 8-connected neighboring segments. Every 2 channels are softmax-normalized to get the score of a link.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Layer Link Detection</head><p>In our network, segments are detected at different scales on different feature layers. Each layer handles a range of scales. We make these ranges overlap in order not to miss scales at their edges. But as a result, segments of the same word could be detected on multiple layers at the same time, producing redundancies.</p><p>To address this problem, we further propose another type of links, called cross-layer links. A cross-layer link connects segments on two feature layers with adjacent indexes. For example, cross-layer links are detected between conv4 3 and conv7, because their indexes are l = 1 and l = 2 respectively.</p><p>An important property of such a pair is that the first layer always has twice the size as the second one, because of the down-sampling layer (max-pooling or stride-2 convolution) between them. Note that this property only holds when all feature layers have even-numbered sizes. In practice, we ensured this property by having the width and height of the input image both dividable by 128. For example, an 1000 ? 800 image is resized to 1024 ? 768, which is the nearest valid size.</p><p>As illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>.b, we define the cross-layer neighbors of a segment as</p><formula xml:id="formula_4">(8) N c s (x,y,l) = {s (x ,y ,l?1) } 2x?x ?2x+1,2y?y ?2y+1 ,</formula><p>which are the segments on the preceeding layer. Every segment has 4 cross-layer neighbors. The correspondence is ensured by the double-size relationship between the two layers.</p><p>Again, cross-layer links are detected by the convolutional predictor. The predictor outputs 8 channels for crosslayer links. Every 2 channels are softmax-normalized to produce the score of a cross-layer link. Cross-layer links are detected on feature layer l = 2 . . . 6, but not on l = 1 (conv4 3) since it has no preceeding feature layer.</p><p>With cross-layer links, segments of different scales can be connected and later combined. Compared with the traditional non-maximum suppression, cross-layer linking provides a trainable way of joining redundancies. Besides, it fits seamlessly into our linking strategy and is easy to implement under our framework. Outputs of a Convolutional Predictor Putting things together, <ref type="figure" target="#fig_2">Fig. 4</ref> shows the output channels of a convolutional predictor. A predictor is implemented by a convolutional layer followed by some softmax layers that normalize the segment and link scores respectively. Thereafter, all layers in our network are convolutional layers. Our network is fully-convolutional.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Combining Segments with Links</head><p>After feed-forwarding, the network produces a number of segments and links (the number depends on the image size). Before combination, the output segments and links are filtered by their confidence scores. We set different filtering thresholds for segment and link, respectively ? and ?.</p><p>Empirically, the performance of our model is not very sensitive to these thresholds. A 0.1 deviation on either thresholds from their optimal values results in less than 1% f-measure drop.</p><p>Taking the filtered segments as nodes and the filtered links as edges, we construct a graph over them. Then, a depth-first search (DFS) is performed over the graph to find its connected components. Each component contains a set of segments that are connected by links. Denoting a connected component by B, segments within this component are combined following the procedures in Alg. 1. </p><formula xml:id="formula_5">(x p + x q ) 7: y b := 1 2 (y p + y q ) 8: w b := (x p ? x q ) 2 + (y p ? y q ) 2 + 1 2 (w p + w q ) 9: h b := 1 |B| B h (i) s 10: b := (x b , y b , w b , h b , ? b ) 11: Output: b is the combined bounding box.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Groundtruths of Segments and Links</head><p>The network is trained by the direct supervision of groundtruth segments and links. The groundtruths include the labels of all default boxes (i.e. the label of their corresponding segments), their offsets to the default boxes, and the labels of all within-and cross-layer links. We calculate them from the groundtruth word bounding boxes.</p><p>First, we assume that there is only one groundtruth word on the input image. A default box is labeled as positive iff 1) the center of the box is inside the word bounding box; 2) the ratio between the box size a l and the word height h satisfies:</p><p>max( a l h , h a l ) ? 1.5</p><p>Otherwise, the default box is labeled as negative. Next, we consider the case of multiple words. A default box is labeled as negative if it does not meet the abovementioned criteria for any word. Otherwise, it is labeled as positive and matched to the word that has the closest size, i.e. the one with the minimal value at the left-hand side of Eq. 9. Offsets are calculated on positive default boxes. First, we calculate the groundtruth segments following the steps illustrated in <ref type="figure">Fig. 5</ref>. Then, we solve Eq. 2 to Eq. 6 to get the groundtruth offsets.</p><p>A link (either within-layer or cross-layer) is labeled as positive iff 1) both of the default boxes connected to it are labeled as positive; 2) the two default boxes are matched to the same word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Optimization</head><p>Objective Our network model is trained by simultaneously minimizing the losses on segment classification, offsets regression, and link classification. Overall, the loss function is a weighted sum of the three losses:</p><formula xml:id="formula_7">L(y s , c s , y l , c l ,?, s) = 1 N s L conf (y s , c s ) + ? 1 1 N s L loc (?, s) + ? 2 1 N l L conf (y l , c l )<label>(10)</label></formula><p>Here, y s is the labels of all segments. y Online Hard Negative Mining For both segments and links, negatives take up most of the training samples. Therefore, hard negative mining is necessary for balancing the positive and negative samples. We follow the online hard negative mining strategy proposed in <ref type="bibr" target="#b19">[20]</ref> to keep the ratio between the negatives and positives 3:1 at most. Hard negative mining is performed separately for segments and links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Augmentation</head><p>We adopt an online augmentation pipeline that is similar to that of SSD <ref type="bibr" target="#b13">[14]</ref> and YOLO <ref type="bibr" target="#b17">[18]</ref>. Training images are randomly cropped to a patch that has a minimum Jaccard overlap of o with any groundtruth word Crops are resized to the same size before loaded into a batch. For oriented text, the augmentation is performed on the axis-aligned bounding boxes of the words. The overlap o is randomly chosen from 0 (no constraint), 0.1, 0.3, 0.5, 0.7, and 0.9 for every sample. The crop size is randomly chosen from [0.1, 1] of the original image size. Training images are not horizontally flipped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we evaluate the proposed method on three public datasets, namely ICDAR 2015 Incidental Text (Challenge 4), MSRA-TD500, and ICDAR 2013, using the standard evaluation protocol of each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>SynthText in the Wild (SynthText) <ref type="bibr" target="#b5">[6]</ref> contains 800,000 synthetic training images. They are created by blending natural images with text rendered with random fonts, size, orientation, and color. Text is rendered and aligned to carefully chosen image regions in order have a realistic look. The dataset provides very detailed annotations for characters, words, and text lines. We only use the dataset for pretraining our network.</p><p>ICDAR 2015 Incidental Text (IC15) <ref type="bibr" target="#b11">[12]</ref> is the Challenge 4 of the ICDAR 2015 Robust Reading Competition. This challenge features incidental scene text images taken by Google Glasses without taking care of positioning, image quality, and viewpoint. Consequently, the dataset exhibits large variations in text orientation, scale, and resolution, making it much more difficult than previous IC-DAR challenges. The dataset contains 1000 training images and 500 testing images. Annotations are provided as word quadrilaterals.</p><p>MSRA-TD500 (TD500) <ref type="bibr" target="#b24">[25]</ref> is the first standard dataset that focuses on oriented text. The dataset is also multilingual, including both Chinese and English text. The dataset consists of 300 training images and 200 testing images. Different from IC15, TD500 is annotated at the level of text lines.</p><p>ICDAR 2013 (IC13) <ref type="bibr" target="#b12">[13]</ref> contains mostly horizontal text, with some text slightly oriented. The dataset has been widely adopted for evaluating text detection methods previously. It consists of 229 training images and 233 testing images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>Our network is pre-trained on SynthText and finetuned on real datasets (specified later). It is optimized by the standard SGD algorithm with a momentum of 0.9. For both pretraining and finetuning, images are resized to 384?384 after random cropping. Since our model is fully-convolutional, we can train it on a certain size and apply it to other sizes during testing. Batch size is set to 32. In pretraining, the learning is set to 10 ?3 for the first 60k iterations, then decayed to 10 ?4 for the rest 30k iterations. During finetuning, the learning rate is fixed to 10 ?4 for 5-10k iterations. The number of finetuning iterations depends on the size of the dataset.</p><p>Due to the precision-recall tradeoff and the difference between evaluation protocols across datasets, we choose the best thresholds ? and ? to optimize f-measure. Except for IC15, the thresholds are chosen separately on different datasets via a grid search with 0.1 step on a hold-out validation set. IC15 does not offer an offline evaluation script, so the only way for us is to submit multiple results to the evaluation server.</p><p>Our method is implemented using TensorFlow <ref type="bibr" target="#b0">[1]</ref> r0.11. All the experiments are carried out on a workstation with an Intel Xeon 8-core CPU (2.8 GHz), 4 Titan X Graphics Cards, and 64GB RAM. Running on 4 GPUs in parallel, training a batch takes about 0.5s. The whole training process takes less than a day.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Detecting Oriented English Text</head><p>First, we evaluate SegLink on IC15. The pretrained model is finetuned for 10k iterations on the training dataset of IC15. Testing images are resized to 1280 ? 768. We set the thresholds on segments and links to 0.9 and 0.7, respectively. Performance is evaluated by the official central submission server (http://rrc.cvc.uab.es/?ch=4). In order to meet the requirements on submission format, the output oriented rectangles are converted into quadrilaterals. <ref type="table" target="#tab_1">Table 1</ref> lists and compares the results of the proposed method and other state-of-the-art methods. Some results are obtained from the online leaderboard. SegLink outperforms the others by a large margin. In terms of f-measure, it outperforms the second best by 10.2%. Considering that some methods have close or even higher precision than SegLink,  the improvement mainly comes from the recall. As shown in <ref type="figure" target="#fig_6">Fig. 6</ref>, our method is able to distinguish text from very cluttered backgrounds. In addition, owing to its explicit link prediction, SegLink correctly separates words that are very close to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Detecting Multi-Lingual Text in Long Lines</head><p>We further demonstrate the ability of SegLink to detect long text in non-Latin scripts. TD500 is taken as the dataset for this experiment, as it consists of oriented and multilingual text. The training set of TD500 only has 300 images, which are not enough for finetuning our model. We mix the training set of TD500 with the training set of IC15, in the way that every batch has half of its images coming from each dataset. The pretrained model is finetuned for 8k iterations. The testing images are resized to 768 ? 768. The thresholds ? and ? are set to 0.9 and 0.5 respectively. Performance scores are calculated by the official development toolkit.</p><p>According to <ref type="table" target="#tab_2">Table 2</ref>, SegLink achieves the highest scores in terms of precision and f-measure. Benefiting from its fully-convolutional design, SegLink runs at 8.9 FPS, a much faster speed than the others. SegLink also enjoys simplicity. The inference process of SegLink is a single forward pass in the detection network, while the previous methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref> involve sophisticated rule-based grouping or filtering steps. TD500 contains many long lines of text in mixed languages (English and Chinese). <ref type="figure" target="#fig_7">Fig. 7</ref> shows how SegLink handles such text. As can be seen, segments and links are densely detected along text lines. They result in long bounding boxes that are hard to obtain from a conventional object detector. Despite the large difference in appearance between English and Chinese text, SegLink is able to handle them simultaneously without any modifications in its structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Detecting Horizontal Text</head><p>Lastly, we evaluate the performance of SegLink on horizontal-text datasets. The pretrained model is finetuned for 5k iterations on the combined training sets of IC13 and IC15. Since the most text in IC13 has relatively larger sizes, the testing images are resized to 512 ? 512. The thresholds ? and ? are set to 0.6 and 0.3, respectively. To match the submission format, we convert the detected oriented boxes into their axis-aligned bounding boxes. <ref type="table">Table 3</ref> compares SegLink with other state-of-the-art methods. The scores are calculated by the central submission system using the "Deteval" evaluation protocol. SegLink achieves very competitive results in terms of fmeasure. Only one approach <ref type="bibr" target="#b21">[22]</ref> outperforms SegLink in terms of f-measure. However, <ref type="bibr" target="#b21">[22]</ref> is mainly designed for detecting horizontal text and is not well-suited for oriented text. In terms of speed, SegLink runs at over 20 FPS on 512 ? 512 images, much faster than the other methods. <ref type="table">Table 3</ref>. Results on IC13. P, R, F stand for precision, recall and f-measure respectively. *These methods are only evaluated under the "ICDAR 2013" evaluation protocol, the rest under "Deteval". The two protocols usually yield very close scores. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Limitations</head><p>A major limitation of SegLink is that two thresholds, ? and ?, need to be set manually. In practice, the optimal values of the thresholds are found by a grid search. Simplifying the parameters would be part of our future work. Another weakness is that SegLink fails to detect text that has very large character spacing. <ref type="figure" target="#fig_8">Fig. 8.a,b</ref> show two such cases. The detected links connect adjacent segments but fail to link distant segments.  <ref type="figure" target="#fig_8">Fig. 8</ref>.c shows that SegLink fails to detect text of curved shape. However, we believe that this is not a limitation of the segment linking strategy, but the segment combination algorithm, which can only produce rectangles currently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have presented SegLink, a novel text detection strategy implemented by a simple and highly-efficient CNN model. The superior performance on horizontal, oriented, and multi-lingual text datasets well demonstrate that SegLink is accurate, fast, and flexible. In the future, we will further explore its potentials on detecting deformed text such as curved text. Also, we are interested in extending SegLink into a end-to-end recognition system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>SegLink Overview. The upper row shows an image with two words of different scales and orientations. (a) Segments (yellow boxes) are detected on the image. (b) Links (green lines) are detected between pairs of adjacent segments. (c) Segments connected by links are combined into whole words. (d-f) SegLink is able to detect long lines of Latin and non-Latin text, such as Chinese.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Within-Layer and Cross-Layer Links. (a) A location on conv8 2 (yellow block) and its 8-connected neighbors (blue blocks with and without fill). The detected within-layer links (green lines) connect a segment (yellow box) and its two neighboring segments (blue boxes) on the same layer. (b) The crosslayer links connect a segment on conv9 2 (yellow box) and two segments on conv8 2 (blue boxes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Output channels of a convolutional predictor. The block shows a w l ? h l map of depth 31. The predictor of l = 1 does not output the channels for corss-layer links.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 Segments 1 : 2 : 3 : 4 :</head><label>11234</label><figDesc>Combining Input: B = {s (i) } |B| i=1 is a set of segments connected by links, where s (i) = (x Find the average angle ? b := 1 For a straight line (tan ? b )x + b, find the b that minimizes the sum of distances to all segment centers (x Find the perpendicular projections of all segment centers onto the straight line. 5: From the projected points, find the two with the longest distance. Denote them by (x p , y p ) and (x q , y q ). 6: x b := 1 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 1 )( 2 ) 3 )( 4 )Figure 5 .</head><label>12345</label><figDesc>Default box, word bounding box, and the center of the default box (blue dot) Rotate word clockwise by along the center of the default box (Crop word bounding box to remove the parts to the left and right of the default box Rotate the cropped box anticlockwise by along the center of the default box The steps of calculating a groundtruth segment given a default box and a word bounding box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>s = 1</head><label>1</label><figDesc>if the i-th default box is labeled as positive, and 0 otherwise. Likewise, y l is the labels of the links. L conf is the softmax loss over the predicted segment and link scores, respectively c s and c l . L loc is the Smooth L1 regression loss<ref type="bibr" target="#b3">[4]</ref> over the predicted segment geometries? and the groundtruth s. The losses on segment classification and regression are normalized by N s , which is the number of positive default boxes. The loss on link classification is normalized by the number of positive links N l . The weight constants ? 1 and ? 2 are both set to 1 in practice.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Example Results on IC15. Green regions are correctly detected text regions. Red ones are either false positive or false negative. Gray ones are detected but neglected by the evaluation algorithm. Visualizations are generated by the central submission system. Yellow frames contain zoom-in image regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Example Results on TD500. The first row shows the detected segments and links. The within-layer and cross-layer links are visualized as red and green lines, respectively. Segments are shown as rectangles in different colors, denoting different connected components. The second row shows the combined boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Failure cases on TD500. Red boxes are false positives. (a)(b) SegLink fails to link the characters with large character spacing. (c) SegLink fails to detect curved text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, a segment is an oriented box that covers a part of a word arXiv:1703.06520v3 [cs.CV] 13 Apr 2017</figDesc><table><row><cell>= 1</cell><cell>= 2</cell><cell></cell><cell>= 3</cell><cell></cell><cell cols="2">= 4</cell><cell></cell><cell></cell><cell>= 5</cell><cell></cell><cell></cell><cell></cell><cell>= 6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Combining</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Segments</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">3x3 conv</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">predictors</cell><cell></cell></row><row><cell></cell><cell>VGG16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>through pool5</cell><cell>64</cell><cell>conv 4_3</cell><cell>32</cell><cell>conv7 (fc7)</cell><cell>16</cell><cell>conv 8_2</cell><cell>8</cell><cell>conv 9_2</cell><cell>4</cell><cell>conv 10_2</cell><cell>2</cell><cell>conv 11</cell></row><row><cell></cell><cell></cell><cell>512</cell><cell cols="2">1024</cell><cell cols="2">1024</cell><cell></cell><cell>512</cell><cell></cell><cell>256</cell><cell cols="2">256</cell></row><row><cell>Input Image (512x512)</cell><cell></cell><cell></cell><cell>1024,k1s1 1024,k3s1</cell><cell></cell><cell>512,k3s2 256,k1s1</cell><cell></cell><cell>256,k3s2 128,k1s1</cell><cell></cell><cell>256,k3s2 128,k1s1</cell><cell></cell><cell>256,k3s2</cell><cell></cell><cell>Detections</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Results on ICDAR 2015 Incidental Text</figDesc><table><row><cell>Method</cell><cell cols="3">Precision Recall F-measure</cell></row><row><cell>HUST MCLAB</cell><cell>47.5</cell><cell>34.8</cell><cell>40.2</cell></row><row><cell>NJU Text</cell><cell>72.7</cell><cell>35.8</cell><cell>48.0</cell></row><row><cell>StradVision-2</cell><cell>77.5</cell><cell>36.7</cell><cell>49.8</cell></row><row><cell>MCLAB FCN [30]</cell><cell>70.8</cell><cell>43.0</cell><cell>53.6</cell></row><row><cell>CTPN [22]</cell><cell>51.6</cell><cell>74.2</cell><cell>60.9</cell></row><row><cell>Megvii-Image++</cell><cell>72.4</cell><cell>57.0</cell><cell>63.8</cell></row><row><cell>Yao et al. [26]</cell><cell>72.3</cell><cell>58.7</cell><cell>64.8</cell></row><row><cell>SegLink</cell><cell>73.1</cell><cell>76.8</cell><cell>75.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Results on MSRA-TD500</figDesc><table><row><cell>Method</cell><cell cols="3">Precision Recall F-measure</cell><cell>FPS</cell></row><row><cell>Kang et al. [11]</cell><cell>71</cell><cell>62</cell><cell>66</cell><cell>-</cell></row><row><cell>Yao et al. [25]</cell><cell>63</cell><cell>63</cell><cell>60</cell><cell>0.14</cell></row><row><cell>Yin et al. [27]</cell><cell>81</cell><cell>63</cell><cell>74</cell><cell>0.71</cell></row><row><cell>Yin et al. [28]</cell><cell>71</cell><cell>61</cell><cell>65</cell><cell>1.25</cell></row><row><cell>Zhang et al. [30]</cell><cell>83</cell><cell>67</cell><cell>74</cell><cell>0.48</cell></row><row><cell>Yao et al. [26]</cell><cell>77</cell><cell>75</cell><cell>76</cell><cell>?1.61</cell></row><row><cell>SegLink</cell><cell>86</cell><cell>70</cell><cell>77</cell><cell>8.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Neumann et al. [16] * 81.8 72.4 77.1 3 Neumann et al. [17] * 82.1 71.3 76.3 3 Busta et al. [3] SegLink 87.7 83.0 85.3 20.6</figDesc><table><row><cell></cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>FPS</cell></row><row><cell></cell><cell cols="3">84.0 69.3 76.8</cell><cell>6</cell></row><row><cell>Zhang et al. [29]</cell><cell>88</cell><cell>74</cell><cell>80</cell><cell>&lt;0.1</cell></row><row><cell>Zhang et al. [30]</cell><cell>88</cell><cell>78</cell><cell>83</cell><cell>&lt;1</cell></row><row><cell>Jaderberg et al. [9]</cell><cell cols="3">88.5 67.8 76.8</cell><cell>&lt;1</cell></row><row><cell>Gupta et al. [6]</cell><cell cols="3">92.0 75.5 83.0</cell><cell>15</cell></row><row><cell>Tian et al. [22]</cell><cell cols="3">93.0 83.0 87.7</cell><cell>7.1</cell></row></table><note>*</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported in part by National Natural Science Foundation of China (61222308 and 61573160), a Google Focused Research Award, AWS Cloud Credits for Research, a Microsoft Research Award and a Facebook equipment donation. The authors also thank China Scholarship Council (CSC) for supporting this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensor-Flow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Man?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vi?gas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org. 6</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Photoocr: Reading text in uncontrolled conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fastext: Efficient unconstrained scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Busta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Text localization in natural images using stroke feature transform and text covariance descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust scene text detection with convolution neural network induced MSER trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep features for text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Orientation robust text line detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
		<title level="m">ICDAR 2015 competition on robust reading</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almaz?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heras</surname></persName>
		</author>
		<title level="m">ICDAR 2013 robust reading competition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient scene text localization and recognition with local character refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Real-time lexicon-free scene text localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1872" to="1885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>abs/1506.02640</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detecting text in natural image with connectionist text proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Word spotting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-end text recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Scene text detection via holistic, multi-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<idno>abs/1606.09002</idno>
		<imprint>
			<date type="published" when="2003" />
			<publisher>CoRR</publisher>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Multi-orientation scene text detection with adaptive clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1930" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Robust text detection in natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="970" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Symmetry-based text line detection in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-oriented text detection with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
