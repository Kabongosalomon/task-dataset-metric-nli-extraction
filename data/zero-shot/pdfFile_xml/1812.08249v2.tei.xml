<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">D3D: Distilled 3D Networks for Video Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">C</forename><surname>Stroud</surname></persName>
							<email>stroud@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan ? Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
							<email>dross@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan ? Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
							<email>chensun@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan ? Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
							<email>jiadeng@cs.princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan ? Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
							<email>sukthankar@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan ? Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename><surname>Google</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan ? Princeton University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">D3D: Distilled 3D Networks for Video Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art methods for video action recognition commonly use an ensemble of two networks: the spatial stream, which takes RGB frames as input, and the temporal stream, which takes optical flow as input. In recent work, both of these streams consist of 3D Convolutional Neural Networks, which apply spatiotemporal filters to the video clip before performing classification. Conceptually, the temporal filters should allow the spatial stream to learn motion representations, making the temporal stream redundant. However, we still see significant benefits in action recognition performance by including an entirely separate temporal stream, indicating that the spatial stream is "missing" some of the signal captured by the temporal stream. In this work, we first investigate whether motion representations are indeed missing in the spatial stream of 3D CNNs. Second, we demonstrate that these motion representations can be improved by distillation, by tuning the spatial stream to predict the outputs of the temporal stream, effectively combining both models into a single stream. Finally, we show that our Distilled 3D Network (D3D) achieves performance on par with two-stream approaches, using only a single model and with no need to compute optical flow.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Motion is often a necessary cue for recognizing actions in a video clip. For example, it may be difficult to tell two actions apart from a single video frame, like "open a door" and "close a door", because the interpretation of the action depends on the direction of motion. To handle motion, much recent work on action recognition treats recognition from motion as a task separate from recognition from appearance. Typically, these two tasks are performed by separate networks, the "temporal stream" and "spatial stream",</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optical Flow</head><p>Temporal Stream RGB D3D Actions Student Network Teacher Network <ref type="figure">Figure 1</ref>: Distilled 3D Networks (D3D). We train a 3D CNN to recognize actions from RGB video while distilling knowledge from a network that recognizes actions from optical flow sequences. During inference, only D3D is used.</p><p>which are then ensembled, a technique first introduced by Two-Stream Networks <ref type="bibr" target="#b22">[23]</ref>. In Two-Stream Networks, the spatial stream only observes a single RGB video frame at a time, while the temporal stream observes a brief sequence of optical flow frames, meaning the temporal stream is solely responsible for capturing features from motion. However, in more recent work, the spatial stream consists of a 3D Convolutional Neural Network, which observes an entire video clip <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b34">35]</ref>. Conceptually, the spatiotemporal filters in a 3D CNN have the ability to respond to movement, which should allow them to learn motion features, a claim echoed in the literature <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref>. However, we still see strong gains in accuracy by ensembling these 3D CNNs with "temporal" 3D CNNs which take explicit motion representations as input. For example, we see a 6.6% increase in accuracy on HMDB-51 by ensembling a 3D CNN that takes RGB frames with a 3D CNN that takes optical flow frames <ref type="bibr" target="#b2">[3]</ref>. It is unclear why both streams are necessary. Is the temporal stream capturing motion features which the spatial stream is missing? If so, why is the 3D CNN miss-ing this information? In this work, we examine the spatial streams in 3D CNNs to see what motion representations they learn, and we introduce a method, depicted in <ref type="figure">Figure 1</ref>, that combines the spatial and temporal streams into a single RGB-only model that achieves comparable performance.</p><p>Because 3D CNNs include temporal filters, we hypothesize that they should be able to capture motion representations if optimized to do so. Recent work has shown that it is possible for 3D CNNs to learn motion representations such as optical flow, but the network structure was designed specifically for this purpose <ref type="bibr" target="#b17">[18]</ref>. Instead of designing a 3D network specifically for learning motion representations, we examine state-of-the-art 3D CNNs designed for action recognition, with minimal modifications to their structure, to see what motion representations they are capable of learning. To do this, we train 3D CNNs on an optical flow prediction task, described in Section 3.1, and we demonstrate experimentally that 3D CNNs are capable of learning motion representations in this way.</p><p>However, even if 3D CNNs are capable of learning motion representations when optimized for optical flow prediction, it is not necessarily true that these motion representations will arise naturally when 3D CNNs are trained to perform other tasks, such as action recognition. To answer whether this is the case, we evaluate the same state-of-theart 3D CNNs on the optical flow prediction task, but we use models with fixed spatiotemporal filters that are pretrained on an action recognition task. We find that these models underperform models that are fully fine-tuned for optical flow prediction, suggesting that 3D CNNs have much room for improvement to learn higher-quality motion representations.</p><p>In order to improve these motion representations, we propose to distill knowledge from the temporal stream into the spatial stream, effectively compressing the two-stream architecture into a single model. In Section 4, we train this Distilled 3D Network (D3D) by optimizing the spatial stream to match the temporal stream's output, a technique often used for model compression <ref type="bibr" target="#b9">[10]</ref>. During inference we only use the distilled spatial stream, and we find that this spatial stream has improved performance on the optical flow prediction task. This suggests that distillation improves motion representations in 3D CNNs.</p><p>We apply D3D to several benchmark datasets, and we find in Section 5 that D3D strongly outperforms singlestream baselines, achieving accuracy on par with the twostream model with only a single stream. We train and evaluate D3D on Kinetics <ref type="bibr" target="#b13">[14]</ref>, and show that the weights learned by distillation also transfer to other tasks, including HMDB-51 <ref type="bibr" target="#b14">[15]</ref>, and UCF-101 <ref type="bibr" target="#b23">[24]</ref>. D3D does not require any optical flow computation during inference, making it less computationally expensive than two-stream approaches. D3D can also benefit from ensembling for better performance, still without the need for optical flow. We compare D3D to a number of other strong RGB-only baselines, and find that D3D outperforms these approaches.</p><p>In summary, we make the following contributions: 1. We investigate whether motion representations arise naturally in the appearance stream of 3D CNNs trained on action recognition. 2. We introduce a method, Distilled 3D Networks (D3D), for improving these motion representations using knowledge distillation from the temporal stream. 3. We demonstrate that D3D achieves competitive results on Kinetics, UCF-101, HMDB-51, and AVA, without the need to compute optical flow during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We broadly categorize video action recognition methods into two approaches. First, there are 2D CNN approaches, where single-frame models are used to process each frame individually. Second, there are 3D CNN approaches, where a model learns video-level features using 3D filters. As we will see, both categories of methods often take a two-stream approach, where one stream captures features from appearance, and another stream captures features from motion. 2D CNNs. Many approaches leverage the strength of single-image (2D) CNNs by applying a CNN to each individual video frame and pooling the predictions across time <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b21">22]</ref>. However, na?ve average pooling ignores the temporal dynamics of video. To capture temporal features, Two-Stream Networks introduce a second network called the temporal stream, which takes a sequence of consecutive optical flow frames as input <ref type="bibr" target="#b22">[23]</ref>. The outputs of these networks are then combined by averaging or a linear SVM. Other methods have taken different approaches to incorporating motion by changing the way the features are pooled across time, for example, with an LSTM or CRF <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22]</ref>. These approaches have proven very effective, particularly in the case where video data is limited and therefore training a 3D CNN is challenging. However, recent advances have enabled 3D CNN approaches, which require large video datasets to train, to be effective. 3D CNNs. Single-frame CNNs can be generalized to video by expanding the filters to three dimensions and applying them temporally, an approach called 3D CNNs. Conceptually, 3D filters should allow CNNs to model motion, but because of the increased number of parameters, 3D CNNs require large amounts of data to train. Large-scale video datasets such as Sports-1M enabled the first 3D CNNs, but these were often not much more accurate than 2D CNNs applied frame-by-frame, calling into question whether 3D CNNs actually model motion <ref type="bibr" target="#b12">[13]</ref>. To compensate, many 3D CNN approaches use additional techniques for incorporating motion. In C3D, motion is incorporated using Improved Dense Trajectory (IDT) features, which leads to a substantial improvement of 5.2% absolute accuracy on UCF-101 <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b31">32]</ref>. In I3D, S3D-G, and R(2+1)D, using a two-stream approach leads to absolute improvements of 3.1%, 2.5%, and 1.1% on Kinetics, respectively <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b30">31]</ref>. The fact that 3D CNNs benefit from a separate temporal stream suggests that 3D CNNs do not learn to model motion naturally when trained on action recognition tasks. More evidence has shed light on this, for example recent work discovered that 3D CNNs are largely unaffected in accuracy on Kinetics when their input is reversed <ref type="bibr" target="#b34">[35]</ref>. In addition, it has been shown that using only a single frame from Kinetics videos with C3D achieves only 5% lower accuracy than using all frames <ref type="bibr" target="#b10">[11]</ref>. These results suggest that 3D CNNs do not sufficiently model motion, a hypothesis we explore further in this work. Why Optical Flow? If 3D CNNs do not model motion when trained on action recognition, we naturally ask whether motion is even necessary for this task, and if not, what benefits optical flow may offer other than motion. Recent work explored several possible explanations for why optical flow is effective for 3D CNNs <ref type="bibr" target="#b20">[21]</ref>. One hypothesis is that optical flow is invariant to texture and color, making it difficult to overfit to video datasets when using optical flow representations as inputs. To support this, they demonstrate that action recognition performance is not well correlated with optical flow accuracy, except near motion boundaries and areas of small displacement <ref type="bibr" target="#b20">[21]</ref>. This work, as well as others, have shown that better or cheaper motion representations can be used in place of optical flow, suggesting that optical flow itself is not necessary for action recognition <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21]</ref>. Alternatively, optical flow can be used as a source of additional supervision, which is shown by ActionFlowNet <ref type="bibr" target="#b17">[18]</ref>, and explored further in this work. Incorporating Motion in 3D CNNs. Many approaches have been proposed to incorporate motion features into 3D CNNs without the use of optical flow inputs. Motion Feature Networks, Optical Flow-Guided Features, and Representation Flow all accomplish this by introducing modules into the network architecture which explicitly compute motion representations <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b18">19]</ref>. Alternatively, several approaches have proposed to replace the optical flow inputs for the temporal stream with a CNN which produces optical flow. For example, Hidden Two-Stream and TVNet use a motion representation that is trained end-to-end for action recognition <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">38]</ref>. However, these methods, as well many other methods that propose to use CNNs to predict optical flow, do not use "vanilla" 3D CNNs. Instead, they use specialized layers, such as correlations or cost volumes, so they do not answer whether vanilla 3D CNNs can learn motion representations <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref>. Distillation. In this work we propose to incorporate motion representations into 3D CNNs using distillation. Distillation was first introduced as a way of transferring knowl-edge from a teacher network to a (typically smaller) student network by optimizing the student network to reconstruct the output of the teacher network <ref type="bibr" target="#b9">[10]</ref>. Recent work on distillation has demonstrated that this technique is widely applicable and can be used to transfer knowledge between different tasks or modalities <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b16">17]</ref>. Very related to our work is Motion Vector CNNs, which distill knowledge from the temporal stream into a new motion stream which uses a cheaper motion representation in place of optical flow <ref type="bibr" target="#b36">[37]</ref>. Our work differs from this work because we demonstrate that we can distill the temporal stream into the spatial stream and still benefit from its motion representations, without ever explicitly computing motion vectors or evaluating a temporal stream 3D CNN at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Evaluating 3D CNN Motion Representations</head><p>The success of two-stream methods suggests that 3D CNNs do not learn sufficient motion representations on their own. Instead, these methods require that we first estimate optical flow from the RGB frames, and then use it as input to the temporal stream. This avoids the need to learn motion representations, since optical flow already serves as a representation of motion. We ask whether 3D CNNs are capable of learning this representation on their own, which would allow us to eliminate the temporal stream.</p><p>We hypothesize that 3D CNNs applied to RGB frames can indeed learn motion representations, but that they do not learn these representations naturally when trained to perform action recognition. To test this, we examine the hidden feature representations of 3D CNNs designed for action recognition. Using these features, we train a decoder to predict optical flow sequences, allowing us to see whether motion representations were captured in these features. We use the performance on the optical flow task as a measure of the architecture's capability to learn motion representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Predicting Optical Flow</head><p>To predict optical flow, we take features from an intermediate layer in a 3D CNN and pass them through a decoder, as depicted in <ref type="figure">Figure 2</ref>. For the 3D CNN, we use S3D-G <ref type="bibr" target="#b34">[35]</ref>. For the decoder, we experiment with three types, which we call "Simple", "Spatial', and "PWC", illustrated in <ref type="figure">Figure 3</ref>. None of these decoders contain temporal filters, cost volume layers, or any other way of incorporating temporal information. This ensures that the decoder is unable to learn motion representations other than what is already represented in the hidden features.</p><p>Using the notation C ?(T ?H ?W ), we denote the number and size of convolutional filters, where C is the number of output channels, and T , H, and W describe the size of the kernels in the time, height, and width dimensions. The Simple decoder consists of a single 3 ? (1 ? 1 ? 1) layer. The Spatial decoder consists of two <ref type="figure">Figure 2</ref>: The network used to predict optical flow from 3D CNN features. We apply the decoder at hidden layers in the 3D CNN (depicted here at layer 3A). This diagram shows the structure of I3D/S3D-G, where blue boxes represent convolution (dashed lines) or Inception blocks (solid lines), and gray boxes represent pooling blocks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b34">35]</ref>.</p><formula xml:id="formula_0">C ? (1 ? 3 ? 3) RGB 1A 2A 2B 2C 3A 3B 3C 4A 4B 4C 4D 4E 4F 5A 5B 5C P L Decoder Flow</formula><p>Layer names are the same as those used in Inception <ref type="bibr" target="#b27">[28]</ref>.</p><formula xml:id="formula_1">Conv Cx(1x3x3) Conv 3x(1x1x1) Spatial Conv Cx(1x3x3) Conv 3x(1x1x1) Conv C L x(1x3x3) PWC x6</formula><p>Simple <ref type="figure">Figure 3</ref>: Three decoders used to predict optical flow. The PWC decoder resembles the optical flow prediction network from PWC-net <ref type="bibr" target="#b25">[26]</ref>. No decoder makes use of temporal filters. See Section 3.1 for more details.</p><p>layers, where C is the number of input channels, followed by a 3 ? (1 ? 1 ? 1) layer. The PWC decoder consists of six C L ? (1 ? 3 ? 3) filters, where C L is the number of channels at layer L. For the six layers (from 1 to 6), C L = {128, 128, 96, 64, 32, 3}. The PWC decoder is equivalent to the optical flow prediction network in PWC-Net without the cost volume and warping layers <ref type="bibr" target="#b25">[26]</ref>.</p><p>The output is a three-channel optical flow representation with the same height and width as the input features. We use the (mag, sin ?, cos ?) representation used by Im2Flow, where mag and ? are the magnitude and angle, respectively, of the flow vector at each pixel <ref type="bibr" target="#b6">[7]</ref>. The decoder is trained to minimize the squared error between this representation and the target optical flow, which is also represented as three channels. For numerical stability, we weight the loss for the sin ?, cos ? channels by mag.</p><p>We use TV-L1 optical flow in place of ground truth optical flow during training <ref type="bibr" target="#b35">[36]</ref>. TV-L1 optical flow is commonly used as the input to the temporal stream in many two-stream approaches, indicating that TV-L1 optical flow is useful for action recognition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21]</ref>. Therefore, the abil-ity to reconstruct TV-L1 optical flow with a 3D CNN serves as a measure of how well motion representations for action recognition can be learned with a 3D CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluating Motion Representations</head><p>After training the optical flow decoder, we evaluate the learned optical flow using endpoint error (EPE). The endpoint error is also computed on the estimated TV-L1 optical flow rather than ground truth. In Section 5.2, we evaluate the predicted optical flow at every layer in S3D-G and compare this with that of our proposed method.</p><p>We evaluate in two settings. In the first setting, we only train the decoder, and leave the 3D CNN fixed. This settings tests what motion representations are learned by the 3D CNN naturally by training on action recognition. In the second setting, we fine-tune the 3D CNN and decoder endto-end. This tests what motion representation can be learned by a 3D CNN when optimized specifically for this purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Distilled 3D Networks</head><p>Our goal is to incorporate motion representations from the temporal stream into the spatial stream. We approach this using distillation, that is, by optimizing the spatial stream to behave similarly to the temporal stream. Our approach uses the learned temporal stream from the typical two-stream pipeline as a teacher network, and distills the knowledge from this teacher network into a student network, the spatial stream, as depicted in <ref type="figure">Figure 1</ref>. This is accomplished by introducing a new loss function, which penalizes the outputs of the spatial stream if they are not similar to those of the temporal stream. More concretely, we train the network parameters ? to minimize the sum of two losses L a and L d ,</p><formula xml:id="formula_2">L(?) = L a (?) + ?L d (?)<label>(1)</label></formula><p>where the action classification loss L a is the cross-entropy and the distillation loss L d is the mean squared error between the pre-softmax outputs of the spatial stream f s (x; ?) and that of the fixed temporal stream f t (x), i.e.</p><formula xml:id="formula_3">L d (?) = 1 N ? 1 N i=0 (f s (x (i) ; ?) ? f t (x (i) )) 2 ,<label>(2)</label></formula><p>where {x (0) , ..., x (N ?1) } are the video clips. The hyperparameter ? allows us to flexibly rescale the contribution of the distillation loss. In our experiments, we find that ? = 1 conveniently serves as a good setting in many cases. We refer to a spatial stream f s trained using distillation as Distilled 3D Networks (D3D). During inference, we discard the temporal stream f t , skipping the included optical flow computation step, and rely only on RGB input. As we show in Section 5, D3D is able to achieve accuracy on par with two-stream methods without the need for two separate spatial and temporal streams. In addition, unlike other approaches for incorporating motion representations, we add no additional computational overhead to the spatial stream <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b15">16]</ref>. We use S3D-G as the architecture for both D3D and the teacher network, since it achieves comparable accuracy at lower computational cost than competing architectures such as I3D and Non-local I3D <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b33">34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Alternatives to Distillation</head><p>In Section 5.6, we experiment with two alternatives to distillation which underperform D3D. These approaches are described here in detail. Flow as Supervision. We introduce an approach similar to ActionFlowNet <ref type="bibr" target="#b17">[18]</ref>, which uses TV-L1 optical flow as a source of auxiliary supervision. The network is identical to the method for predicting optical flow with the "Simple" decoder at layer "3A", described in Section 3.1, but we optimize both the 3D CNN and decoder to minimize the sum of the flow prediction loss and the action classification loss. This is a more direct way of encouraging the network to learn motion representations, since we directly penalize the 3D CNN for producing feature representations from which the decoder cannot predict optical flow. However, we find that this does not generally lead to better results on action classification. One possible reason is that the optical flow prediction loss is dominated by the loss applied at background pixels, which covers most of the field of view, whereas prior work demonstrated that accurate optical flow estimation is only correlated with action classification performance near motion boundaries <ref type="bibr" target="#b20">[21]</ref>. Distillation avoids this because we train the spatial stream to replicate the outputs created by the temporal stream, rather than to directly match its motion representations. Flow as a Learnable Input Representation. Recent approaches, such as TVNet and Hidden Two-Stream networks, improve upon the temporal stream by learning motion representations specifically for action recognition <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b4">5]</ref>. We introduce an approach similar to these, which uses the first few layers of a 3D CNN as an optical flow prediction network, and passes this predicted flow into a temporal stream, which is all trained end-to-end. We use our optical flow prediction network as in Section 3.1 up to the "3A" layer with the "Simple" decoder, and for the temporal stream, we use S3D-G pretrained to predict actions from optical flow. In our experiments, we find that this approach outperforms the temporal stream applied to TV-L1 optical flow, but still underperforms the spatial stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We train and evaluate D3D on several datasets, demonstrating that D3D outperforms other single-stream models  <ref type="table">Table 1</ref>: Effect of decoder on optical flow prediction. We add the optical flow decoder to the "3A" layer of S3D-G and train it to predict optical flow. and achieves accuracy on par with that of two-stream models that require explicit optical flow computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>Kinetics. Kinetics is a large-scale video classification dataset with approximately 500K 10-second clips annotated with one of 600 action categories <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b1">2]</ref>. Kinetics has two variants: Kinetics-600 is the full dataset, and Kinetics-400 is a subset containing 400 of the total categories. Kinetics consists of publicly available YouTube videos, which can be deleted by their owners at any time. Thus, Kinetics, like similar large-scale Internet datasets, gradually decays over time. Our experiments were conducted in October 2018, when Kinetics-400 contained 226K of the original 247K training examples (-8.4%) and Kinetics-600 contained 369K of the original 393K training examples (-6.1%). The change in both training and validation sets generates a small discrepancy between experiments conducted at different times. We explicitly denote results on the original Kinetics dataset with an asterisk (*) in all tables and provide the list of videos available at the time of our experiments to enable others to reproduce our results 1 . HMDB-51 and UCF-101. HMDB-51 and UCF-101 are action classification datasets composed of brief video clips, each containing one action <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref>. HMDB-51 contains 7,000 videos from 51 classes, and UCF-101 contains 13,320 videos from 101 classes. For both datasets, we report classification accuracy on the first test split. AVA. AVA is a large-scale spatiotemporal action localization dataset that consists of 430 15-minute movie clips <ref type="bibr" target="#b8">[9]</ref>. Each clip contains bounding box annotations at 1-second intervals for all actors in frame, and each actor is annotated with one or more action labels. In our experiments, we train on AVA v2.1, and report results on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Predicting Optical Flow</head><p>As described in Section 3.2, we evaluate the ability of 3D CNNs trained for action recognition (specifically on Kinetics-400) to capture motion by attempting to decode  <ref type="figure">Figure 2</ref>) is used as input to the decoder. D3D features are able to more accurately reproduce optical flow across the board, particularly in earlier layers. Fine-tuning either network end-to-end (indicated "ft"), leads to better performance.</p><p>optical flow from their hidden feature representations, measuring performance using endpoint error (EPE) compared with the TV-L1 optical flow pseudo-groundtruth. For these experiments, we train each model on 2 GPUs with a batchsize of 6 for 100K iterations, and otherwise we use the same hyperparameters as S3D-G <ref type="bibr" target="#b34">[35]</ref>.</p><p>In <ref type="table">Table 1</ref>, we explore the effect of changing the decoder. To bracket performance, we evaluate two baselines: a trivial flow model that predicts "All zeros", and a decoder trained on the activations of a temporal stream model, which is provided TV-L1 flow as input. Compared to the baselines, the "PWC" decoder trained on spatial stream S3D-G is able approximately estimate optical flow. However, we find that the "Simple" decoder is unable to capture any motion signal from the S3D-G features, meaning that motion representations are not readily available from these hidden features.</p><p>In <ref type="figure" target="#fig_0">Figure 4</ref>, we compare the performance of S3D-G and D3D when the "PWC" decoder is applied at each layer. We observed lower error across the board when attempting to predict optical flow from D3D activations versus S3D-G activations. Interestingly, this appears to be strongest in the first few layers, farthest from where the distillation actually takes place. This indicates that distillation is able to improve motion representations even in the earliest layers of 3D CNNs, allowing abstract features to be built from these motion representations in later layers. These results confirm that D3D improves motion representations in 3D CNNs.</p><p>When models are fine-tuned end-to-end on the flow prediction task-indicated by (ft) in the figure-the estimates are further improved, and the optical flow performance gap between S3D-G and D3D disappears. When combined with results from <ref type="table">Table 1</ref>, these results mostly confirm our original hypothesis: 3D CNNs provided with RGB input have a limited natural tendency to capture the motion signal present in optical flow when trained on action classification. The ability to capture motion signal can be significantly enhanced with modified training objectives, such as distillation loss or by fine-tuning for optical flow prediction.</p><p>In <ref type="figure" target="#fig_1">Figure 5</ref>, we give examples of optical flow estimates given using our method. Both S3D-G and D3D can capture coarse motion, but miss fine details. Results using D3D appear to have slightly more accurate motion boundaries. We provide more examples in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Distillation on Kinetics</head><p>We train D3D on the Kinetics training set using a twostep procedure. First, we train the teacher S3D-G temporal stream using identical settings to those described in prior work, with TV-L1 optical flow provided by the Kinetics creators <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b2">3]</ref>. Second, we train the student D3D network using the distillation procedure described in Section 4. For fair comparison with S3D-G, we use the same S3D-G hyperparameters when training D3D, with the distillation loss L d added to the action loss L a with scaling parameter ? = 1. We train the model for 140k steps on 64 GPUs with a batch size of 6. For details about the hyperparameters, please refer to prior work on S3D-G <ref type="bibr" target="#b34">[35]</ref>. During inference, the teacher</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Modality</head><p>Kinetics-400 ARTNet <ref type="bibr" target="#b32">[33]</ref> RGB+Flow 72.4* TSN <ref type="bibr" target="#b29">[30]</ref> RGB+Flow 73.9* R(2+1)D <ref type="bibr" target="#b30">[31]</ref> RGB+Flow 75.4* NL I3D <ref type="bibr" target="#b33">[34]</ref> RGB 77.7* SAN <ref type="bibr" target="#b0">[1]</ref> RGB+Flow+Audio 77.7* I3D <ref type="bibr" target="#b2">[3]</ref> RGB 70.6 / 71.1* I3D <ref type="bibr" target="#b2">[3]</ref> Flow 62.1 / 63.9* I3D <ref type="bibr" target="#b2">[3]</ref> RGB+Flow 72.6 / 74.1* S3D-G <ref type="bibr" target="#b34">[35]</ref> RGB 74.0 / 74.7* S3D-G <ref type="bibr" target="#b34">[35]</ref> Flow 67.3 / 68.0* S3D-G <ref type="bibr" target="#b34">[35]</ref> RGB+Flow 76.2 / 77.2* D3D RGB 75.9 D3D+S3D-G RGB+RGB 76.5 <ref type="table">Table 2</ref>: Performance of D3D on Kinetics-400. All numbers given are top-1 accuracy on the validation set. "D3D+S3D-G" refers to an ensemble of D3D and S3D-G. Numbers marked with an asterisk (*) are reported on the full Kinetics-400 set, those without are reported on the subset available as of October 2018 as described in Section 5.1.</p><p>network is not used, optical flow does not need to be computed, and only one network evaluation is performed.</p><p>In <ref type="table">Table 2</ref>, we compare the performance of D3D with several competitive baselines. We report accuracy for I3D and S3D-G trained and evaluated on the reduced Kinetics-400 dataset described in Section 5.1. These replications were run with code provided by the original authors and use identical settings to the published papers. Direct comparison with S3D-G shows that the distillation procedure leads to a 1.9% improvement in top-1 accuracy, without any additional computational cost during inference. Per-class accuracy is provided in the appendix. Furthermore, we ensemble D3D with S3D-G ("D3D+S3D-G") by averaging their softmax scores, and achieve a small boost in performance over the two-stream S3D-G approach which uses optical flow. Our ensemble achieves better performance without the need to compute optical flow. The only RGB-only model that outperforms D3D is Non-local I3D, which uses an inflated Resnet-101 backbone architecture with added nonlocal blocks, which introduce computational overhead during inference <ref type="bibr" target="#b33">[34]</ref>.</p><p>In <ref type="table">Table 3</ref>, we compare the performance of D3D with baseline methods on Kinetics-600. Both the teacher and student network are trained using Kinetics-600 in these experiments. We achieve a 1.3% improvement in single-model performance using D3D, and further improvements by ensembling D3D and S3D-G together, outperforming twostream S3D-G without the need for optical flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Modality Kinetics-600 I3D <ref type="bibr" target="#b1">[2]</ref> RGB 73.6 / 71.9* S3D-G <ref type="bibr" target="#b34">[35]</ref> RGB 76.6 S3D-G <ref type="bibr" target="#b34">[35]</ref> Flow 69.7 S3D-G <ref type="bibr" target="#b34">[35]</ref> RGB+Flow 78.6 D3D RGB 77.9 D3D+S3D-G RGB+RGB 79.1 <ref type="table">Table 3</ref>: Performance of D3D on Kinetics-600. All numbers given are top-1 accuracy on the validation set. "D3D+S3D-G" refers to an ensemble of D3D and S3D-G. Numbers marked with an asterisk (*) are reported on the full Kinetics-400 set, those without are reported on the subset available as of October 2018 as described in Section 5.1.</p><p>Results on I3D use different settings than in <ref type="table">Table 2</ref>  <ref type="bibr" target="#b1">[2]</ref>.</p><p>Method UCF-101 HMDB-51 P3D <ref type="bibr" target="#b19">[20]</ref> 88.6 -C3D <ref type="bibr" target="#b28">[29]</ref> 82.3 51.6 Res3D <ref type="bibr" target="#b29">[30]</ref> 85.8 54.9 ARTNet <ref type="bibr" target="#b32">[33]</ref> 94.3 70.9 I3D <ref type="bibr" target="#b2">[3]</ref> 95.6 74.8 R(2+1)D <ref type="bibr" target="#b30">[31]</ref> 96.8 74.5 S3D-G <ref type="bibr" target="#b34">[35]</ref> 96.8 75.9 I3D Two-Stream <ref type="bibr" target="#b2">[3]</ref> 98.0 80.7 ActionFlowNet <ref type="bibr" target="#b17">[18]</ref> 83.9 56.4 MFNet <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19]</ref> -56.8 Rep. Flow <ref type="bibr" target="#b18">[19]</ref> -65.4 MV-CNN <ref type="bibr" target="#b36">[37]</ref> 86.4 -TVNet+IDT <ref type="bibr" target="#b4">[5]</ref> 95.4 72.6 Hidden Two-Stream <ref type="bibr" target="#b37">[38]</ref> 97.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Transfer to UCF101, HMDB51</head><p>To demonstrate the transferability of D3D, we fine-tune D3D on UCF-101 and HMDB-51. For these experiments, we initialize using D3D pretrained on Kinetics with distillation. However, during fine-tuning, we use only the action classification loss, and not distillation. This avoids the need for a temporal stream altogether, during both training and inference. While we could potentially benefit from applying distillation during fine-tuning as well, these experiments demonstrate that it is not necessary to do so. Each model is fine-tuned for 10k steps on 10 GPUs with a batch size of 6, as described in <ref type="bibr" target="#b34">[35]</ref>.</p><p>In <ref type="table" target="#tab_2">Table 4</ref>, we demonstrate that fine-tuning D3D outperforms many competitive baselines. The models in the top</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Pretraining AVA I3D w/ RPN <ref type="bibr" target="#b7">[8]</ref> Kinetics-600 21.9 I3D w/ RPN + JFT <ref type="bibr" target="#b7">[8]</ref> Kinetics-400 22.8 S3D-G w/ ResNet RPN <ref type="bibr" target="#b8">[9]</ref> Kinetics-400 22.0 D3D w/ ResNet RPN Kinetics-400 23.0 <ref type="table">Table 5</ref>: Performance on AVA using different backbone networks. All numbers are frame-mAP on the validation set. Models with "+ ResNet RPN" use a separate pretrained RPN stream based on ResNet, while the others use the 3D features directly for the RPN. The S3D-G baseline includes changes over the previously published numbers, described in Section 5.5.</p><p>section of the table are strong RGB-only baselines based on 3D CNNs, including S3D-G, which serves as a direct comparison to show that the benefit of distillation during pretraining persists after fine-tuning. The models in the middle section of the table all specifically address the problem of learning motion features without the use of optical flow. D3D outperforms all baselines and achieves essentially equal performance to Hidden Two-Stream when pretrained on Kinetics-400. Hidden Two-Stream uses two I3D models plus an optical flow prediction network, requiring over 2x the cost of D3D for the same accuracy <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Transfer to AVA</head><p>We fine-tune D3D on the spatiotemporal localization dataset AVA. We use a similar approach to the baseline described in the original AVA paper <ref type="bibr" target="#b8">[9]</ref>, but adopt some changes introduced by a top entry in the 2018 AVA competition <ref type="bibr" target="#b7">[8]</ref>. Like the original AVA baseline, we use a Faster RCNN-style approach, with a separate pretrained region proposal network (RPN) based on ResNet, and video feature extractor backbone network based on 3D CNNs. Unlike this work, we use D3D in place of I3D as the backbone network. We also adopt the three key changes introduced in the competition entry <ref type="bibr" target="#b7">[8]</ref>. First, we regress only one set of bounding box offsets per region proposal, rather than a different set of offsets per action class. Second, we train for 500k steps using synchronous training on 11 GPUs using a higher learning rate. Third, we add cropping and flipping augmentation during training. Unlike <ref type="bibr" target="#b7">[8]</ref>, we do not remove the ResNet RPN in either D3D our the S3D-G baseline.</p><p>In <ref type="table">Table 5</ref>, we compare the use of D3D as a backbone network with S3D-G and I3D. Our approaches use 50 RGB frames and no optical flow. Direct comparison between S3D-G and D3D shows that using D3D leads to a 1% improvement in Frame-mAP over S3D-G. We also see comparable gains over I3D, and still outperform the I3D-based approach when it includes additional ResNet features pretrained on JFT, an internal Google dataset <ref type="bibr" target="#b24">[25]</ref>.  <ref type="table">Table 6</ref>: Ablation studies. All numbers given are top-1 accuracy on the reduced Kinetics-400 validation set described in Section 5.1. D3D using our proposed approach outperforms all other approaches listed. See Section 5.6 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Ablation study</head><p>We explore alternative approaches to distillation and how these effect D3D. These results are given in <ref type="table">Table 6</ref>.</p><p>The top section of the table explores non-distillation approaches for improving S3D-G. "S3D-G temporal stream" uses TV-L1 optical flow inputs to S3D-G instead of RGB frames, as described in prior work <ref type="bibr" target="#b34">[35]</ref>. "S3D-G with 3D CNN flow" uses optical flow predicted by a separate S3D-G using the approach we introduce in Section 3.1 instead of TV-L1. The flow prediction network is pretrained to reproduce TV-L1 optical flow, and then is fine-tuned end-to-end with the S3D-G temporal stream on top. This outperforms the TV-L1 optical flow temporal stream, confirming the results of similar end-to-end approaches TVNet and Hidden Two-Stream <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">38]</ref>. Finally, "S3D-G with flow loss" uses the same flow prediction network as before, but this loss is added to the action loss and both losses are minimized simultaneously as the full S3D-G model is optimized. This leads to slight improvements over S3D-G but does not outperform D3D. Both flow approaches predict at layer "3A" and use the "PWC" decoder, which we find gives the best results. See Section 4.1 for more details on these approaches.</p><p>The middle section demonstrates applying the distillation loss at intermediate layers. For these experiments, we use ? = 1 except for distillation at the 4f layer, in which case we use ? = 100. We find that we achieve the best performance when setting ? such that the scale of the distillation loss roughly matches that of the cross entropy loss. We find that applying the distillation loss at intermediate layers is not as effective as doing so at the network outputs.</p><p>The bottom section explores two variants on D3D. "D3D without action loss" uses the distillation loss only, and not the cross-entropy loss. "D3D distilled from spatial stream" uses the S3D-G spatial stream as the teacher network in place of the temporal stream. Both of these approaches un-derperform D3D, showing that both losses are useful for D3D, and that distillation alone does not explain the improvement of D3D over S3D-G. Crucially, we only see benefits of distillation when distilling from the temporal stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We introduce D3D, a single-stream distilled 3D CNN which does not require optical flow during inference and still performs on par with two-stream approaches. Furthermore, we show that D3D transfers to other action recognition datasets without the need for further distillation. We study the ability to predict optical flow with 3D CNNs, and we show that 3D CNNs have some limited capacity to learn motion representations, and that D3D reconstructs motion representations better than its non-distilled counterparts.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Predicted Optical Flow Visualizations</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Predicting optical flow from each layer in S3D-G and D3D. The horizontal axis indicates which layer (see</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>(a) RGB frame (b) TV-L1 optical flow (c) S3D-G predicted flow (d) D3D predicted flow Examples of optical flow produced by S3DG and D3D (without fine-tuning) using the PWC decoder applied at layer 3A. The color and saturation of each pixel corresponds to the angle and magnitude of motion, respectively. TV-L1 optical flow is displayed at 28 ? 28px, the output resolution of the decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Examples of optical flow produced by S3DG and D3D by adding the PWC decoder applied at layer 3A. From top to bottom: RGB Frames, TV-L1 optical flow, S3D-G flow, D3D flow, D3D flow with finetuning. TV-L1 optical flow is shown downsampled to 28 ? 28 px, which is the decoder output resolution used during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>Accuracy difference on individual Kinetics-400 categories by adding distillation. We compare the difference between per-class accuracy for D3D and per-class accuracy for S3D-G. Only the top and bottom 25 classes are shown. In total, D3D leads to improvements on 203 of the 400 classes (50.8%) and degradations on 103 of the 400 classes (27.3%), with less than a ?.1% difference on the remaining classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>Performance after fine-tuning D3D on UCF-101</cell></row><row><cell>and HMDB-51. Our numbers are top-1 accuracy on test</cell></row><row><cell>split 1 for both datasets. No distillation is performed during</cell></row><row><cell>fine-tuning.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>B. Performance on Kinetics-400 Categories Accuracy on individual Kinetics-400 categories using D3D. We show the per-class accuracy for D3D trained on Kinetics-400. Only the top and bottom 25 classes are shown.</figDesc><table><row><cell>dunking basketball</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>exercising arm</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>sticking tongue out</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>doing nails wrestling</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>riding mechanical bull tasting food</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>surfing crowd salsa dancing</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>playing chess finger snapping</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>sled dog racing tossing coin</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>windsurfing catching or throwing</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>playing squash or driving car</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>pull ups bending back</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>diving cliff playing organ</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>front raises stretching arm</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>golf putting pushing car</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>skateboarding long jump</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>milking cow drop kicking</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>paragliding sign language interpreting</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>shearing sheep air drumming</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>feeding goats dribbling basketball punching bag robot dancing scuba diving answering questions filling eyebrows baby waking up hurling (sport) dancing charleston presenting weather shaking head pole vault jetskiing reading book</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>playing harp shaving head country line dancing folding paper</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>moving furniture playing basketball</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>yawning unboxing</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>shaking hands parasailing</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hugging catching or throwing</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>sticking tongue out drumming fingers</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>somersaulting playing drums</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tying bow tie garbage collecting</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>jogging writing</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>making a sandwich dining</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>singing petting cat</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>answering questions eating carrots</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>throwing ball making a sandwich</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>applauding playing guitar</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>sneezing cooking egg</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>faceplanting doing laundry</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>headbutting playing keyboard making a cake playing kickball eating doughnuts riding or walking with sniffing drinking shots recording music sweeping floor slapping eating cake fixing hair drinking drinking smoking drinking beer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>drinking beer smoking hookah -0.400</cell><cell>-0.200</cell><cell>0.000</cell><cell>0.200</cell><cell>0.400</cell></row><row><cell cols="5">D3D per-class accuracy 0.250 0.500 0.750 Figure 7: D3D per-class accuracy -S3D-G per-class accuracy drinking shots 0.000 1.000</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code and list of Kinetics videos used are available at the project page: jonathancstroud.com/d3d</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>Work was completed while JCS was an intern at Google Research. We thank our colleagues for their helpful feedback, including Cordelia Schmid, George Toderici, and Carl Vondrick.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Revisiting the effectiveness of off-theshelf temporal modeling approaches for large-scale video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03805</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A short note about kinetics-600</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01340</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end learning of motion representation for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Born again neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Im2flow: Motion hallucination from static images for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.10066</idno>
		<title level="m">A better baseline for ava</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ava: A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What makes a video a video: Analyzing temporal information in video understanding models and datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Motion feature network: Fixed motion filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Graph distillation for action detection with privileged modalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Actionflownet: Learning motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Representation flow for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01455</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.08416</idno>
		<title level="m">On the integration of optical flow and action recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Asynchronous temporal fields for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Optical flow guided feature: A fast and robust motion representation for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05038</idno>
		<title level="m">Convnet architecture search for spatiotemporal feature learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Appearance-andrelation networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Realtime action recognition with enhanced motion vector cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Hidden two-stream convolutional networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00389</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
