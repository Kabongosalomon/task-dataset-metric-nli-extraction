<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Speechformer: Reducing Information Loss in Direct Speech Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Papi</surname></persName>
							<email>spapi@fbk.eu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gaido</surname></persName>
							<email>mgaido@fbk.eu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
							<email>negri@fbk.eu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
							<email>turchi@fbk.eu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fondazione</forename><forename type="middle">Bruno</forename><surname>Kessler</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Speechformer: Reducing Information Loss in Direct Speech Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer-based models have gained increasing popularity achieving state-of-the-art performance in many research fields including speech translation. However, Transformer's quadratic complexity with respect to the input sequence length prevents its adoption as is with audio signals, which are typically represented by long sequences. Current solutions resort to an initial sub-optimal compression based on a fixed sampling of raw audio features. Therefore, potentially useful linguistic information is not accessible to higher-level layers in the architecture. To solve this issue, we propose Speechformer, an architecture that, thanks to a reduced memory usage in the attention layers, avoids the initial lossy compression and aggregates information only at a higher level according to more informed linguistic criteria. Experiments on three language pairs (en?de/es/nl) show the efficacy of our solution, with gains of up to 0.8 BLEU on the standard MuST-C corpus and of up to 4.0 BLEU in a low resource scenario.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Speech-to-text translation (ST) has been traditionally approached with cascade architectures consisting of a pipeline of two sub-components <ref type="bibr" target="#b8">(Stentiford and Steer, 1988;</ref><ref type="bibr" target="#b13">Waibel et al., 1991)</ref>: an automatic speech recognition (ASR), which transforms the audio input into a textual representation, and a machine translation (MT) model, which projects the transcript into the target language. A more recent approach consists in directly translating speech into target text using a single model <ref type="bibr">(B?rard et al., 2016;</ref><ref type="bibr" target="#b16">Weiss et al., 2017)</ref>. This direct solution has interesting advantages <ref type="bibr" target="#b7">(Sperber and Paulik, 2020)</ref>: i) it can better exploit audio information (e.g. prosody) during the translation phase, ii) it has lower latency, and iii) it is not affected by error propagation.</p><p>The authors contributed equally.</p><p>Thanks to these advantages, the initially huge performance gap with cascade systems has gradually closed <ref type="bibr">(Ansari et al., 2020)</ref>, motivating research towards further improvements.</p><p>Direct ST models are fed with features extracted from the audio with high frequency (usually every 10ms). This, on average, makes the resulting input sequence of vectors ?10 times longer than the corresponding text, leading to an intrinsically redundant (i.e. long and repetitive) representation. For this reason, it is not possible to process speech data with a vanilla Transformer encoder <ref type="bibr" target="#b12">(Vaswani et al., 2017)</ref>, whose self-attention layers have quadratic memory complexity with respect to the input length. State-of-the-art architectures tackle the problem by collapsing adjacent vectors in a fixed way, i.e. by mapping a predefined number of vectors (usually 4) into a single one, either using strided convolutional layers <ref type="bibr">(B?rard et al., 2018;</ref><ref type="bibr">Di Gangi et al., 2019;</ref><ref type="bibr" target="#b14">Wang et al., 2020a)</ref> or by stacking them <ref type="bibr" target="#b5">(Sak et al., 2015)</ref>. As a positive side effect, these length reduction solutions lower input redundancy. As a negative side effect, they disregard the variability over time of the amount of linguistic and phonetic information in audio signals (e.g. due to pauses and speaking rate variations) by giving equal weight to all features. In doing this, relevant features are penalized and considered equally important to the irrelevant ones, resulting in an information loss.</p><p>Recently, <ref type="bibr" target="#b6">Salesky et al. (2019)</ref> obtained considerable translation quality gains by collapsing consecutive vectors with the same phonetic content instead of compressing them in a fixed way. <ref type="bibr" target="#b18">Zhang et al. (2020)</ref> also showed that selecting a small percentage (?16%) of input time steps based on their informativeness improves ST quality. On the downside, these approaches respectively require adding a model that performs phoneme classification and a pre-trained adaptive feature selection layer on top of an ASR encoder, losing the compactness of direct solutions at the risk of error propagation.</p><p>In direct ST, <ref type="bibr">Liu et al. (2020)</ref> and <ref type="bibr">Gaido et al. (2021)</ref> addressed the problem with a transcript/phoneme-based compression leveraging Connectionist Temporal Classification <ref type="bibr">(CTC -Graves et al. 2006</ref>). However, since these methods are applied to the representation encoded by Transformer layers, the initial content-unaware downsampling of the input is still required for memory reasons, at the risk of losing important information.</p><p>To avoid initial fixed compression, we propose Speechformer: the first Transformer-based architecture that processes full audio content maintaining the original dimensions of the input sequence. Inspired by recent work on reducing the memory complexity of the attention mechanism <ref type="bibr" target="#b15">(Wang et al., 2020b)</ref>, we introduce a novel attention layer -the ConvAttention -whose memory requirements are reduced by means of convolutional layers. As the benefits of avoiding the initial lossy compression might be outweighed by the increased redundancy of the encoded audio features, we aggregate the high-level representation of the input sequence in a linguistically informed way, as in <ref type="bibr">(Liu et al., 2020;</ref><ref type="bibr">Gaido et al., 2021)</ref>. In other words, we collapse vectors representing the same linguistic atomic content (words, sub-words, pauses) into a single element, since they express the same linguistic information. The usage of the ConvAttention and of the linguistically motivated compression produces a considerably shorter, yet informative, sequence that fits the memory requirements of vanilla Transformer encoder layers. Experiments on three language directions (en?de/es/nl) show that the proposed architecture outperforms a state-of-the-art ST model by up to 0.8 BLEU points on the standard MuST-C corpus and obtains significantly larger gains (up to 4.0 BLEU) in a low resource setting where the amount of training data is reduced to 100 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>In this section, we first introduce a novel attention layer that enables to process raw audio features without downsampling ( ?2.1). Then, we present an architecture that leverages this attention mechanism in the first encoder layers and reduces the redundancy of the more informative but longer resulting sequences with CTC compression ( ?2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">ConvAttention layer</head><p>State-of-the-art ST models employ convolutional neural networks to sample the feature sequence to a lower dimension (typically by a factor of 4), enabling the use of Transformer layers otherwise impossible given their memory consumption. Outside ST, the Linformer architecture <ref type="bibr" target="#b15">(Wang et al., 2020b)</ref> has been recently proposed to reduce the quadratic complexity of the product between the attention matrix (resulting from the product of the query -Q -and key -K -matrices) and the value (V) matrix by applying a linear projection to K and V. These projections bring the dimension of the sequence length of K and V to a fixed value, yielding a linear memory complexity. However, a direct application of this architecture to ST is problematic due to the high variability in audio lengths. On one side, mapping those sequences to a fixed dimension can cause an excessive information loss, with a consequent performance drop. On the other, it poses technical issues: the linear projection matrix has size n ? k, where n is the maximum input length and k is the fixed dimension. If the input has a length n shorter than n, which is a common case due to the high variability in length of audio sequences, only the first n weights of the matrix are updated. This results in gradients of different dimensions across GPUs, leading to training failures due to inconsistencies.</p><p>To avoid the aforementioned problems, we propose the adoption of ConvAttention <ref type="figure" target="#fig_0">(Figure 1</ref>), in which the linear projections of the Linformer architecture are substituted, both in K and V , with a single 1D convolutional layer. Hence, the length of the sequences used in the scaled dot-product attention depends on the stride of the convolution, a hyper-parameter we named compression factor (?), which controls the memory complexity of the ConvAttention. Namely, being n the temporal dimension of K and V, the convolution output length is n ? and the complexity of the ConvAttention is O(( n ? ) 2 ), i.e. a 1 ? 2 factor lower than a vanilla Transformer self-attention. For instance, setting ? to 4 leads to the same memory consumption as standard ST models with an initial ?4 subsampling (i.e. with two initial convolutional layers with stride 2).</p><p>Notice that the output sequence length is still equal to the input sequence length as it depends on the length of Q that is not modified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Speechformer</head><p>The introduction of ConvAttention layers allows us to avoid sub-optimal fixed compressions that disregard the variability over time in the amount of audio information. However, since an encoder consisting only of ConvAttention layers does not compress the length of the original input sequence, the decoder will be fed with long and redundant sequences that are difficult to attend, leading to potential performance degradation.</p><p>To overcome this problem, as in <ref type="bibr">(Liu et al., 2020;</ref><ref type="bibr">Gaido et al., 2021)</ref>, we apply a content-informed compression to high-level hidden states trained using the CTC loss <ref type="bibr">(Graves et al., 2006)</ref> to represent the linguistic content. Specifically, the CTC loss produces a prediction for each input time step and then merges equal predictions for consecutive time steps. The resulting sequence is compared with the reference, which is the sequence of subwords representing the transcript of the input utterance. CTC compression, similarly to the loss computation, collapses consecutive features corresponding to the same predictions, averaging them. After this operation, the sequence is reduced to a representation dimensionally closer to its textual content, which can be processed by the original attention mechanism without the need of approximations.</p><p>Speechformer (see <ref type="figure" target="#fig_1">Figure 2</ref>), is composed of E L ConvAttention layers up to a CTC compression layer, after which there are E T Transformer encoder layers. The E L ConvAttention layers are meant to learn the linguistic content of the input audio while the E T Transformer encoder layers are in charge of learning higher-level semantic representations, i.e. the encoder outputs, which the decoder has to convert into a text in the target language. We also maintain the two 1D convolutional layers before the ConvAttention layers but without striding, so that no sub-sampling is applied to the input. We make this choice both to keep the number of parameters comparable to the existing architectures, and to let the model learn a better representation of the  <ref type="table">Table 1</ref>: BLEU on MuST-C en-de dev set varying the compression factor ? and 1D convolutional kernel size. The scores are obtained without label smoothing. input before feeding it to the attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Settings</head><p>We experimented on three languages of MuST-C (Cattoni et al., 2021): English-German, English-Spanish, and English-Dutch. To ensure the reproducibility of our work, all training details are provided in the Appendix and the code -based on fairseq <ref type="bibr" target="#b2">(Ott et al., 2019)</ref> -is released open source. <ref type="bibr">1</ref> Following <ref type="bibr" target="#b15">(Wang et al., 2020b)</ref>, we share the convolution parameters of the ConvAttention layers both among K and V and among the attention heads. We select the compression factor and the 1D convolution kernel size with a set of preliminary experiments on the en-de validation set. The compression factor (?) is chosen among 4, 8, and 16, since 4 is the minimum value that avoids out-ofmemory issues. The kernel size is set either equal to or twice as the value of ?. <ref type="table">Table 1</ref> shows that the combination of a compression factor of 4 and a kernel size of 8 leads to better performance compared to the other combinations. Consequently, in all our experiments we use this setting.</p><p>We initialize the ConvAttention weights of Speechformer with those of a pre-trained ST model  <ref type="table">Table 2</ref>: BLEU score (average over 3 runs) on English?Dutch (en-nl), English?German (en-de), and English?Spanish (en-es) of MuST-C tst-COMMON (tst) and the dev (validation) set. The * symbol indicates statistically significant improvements over the baseline. Statistical significance is computed with a t-test <ref type="bibr" target="#b9">(Student, 1908)</ref>, whose null hypothesis is that the mean of the considered experiment is not higher than the mean of the baseline. We consider the result statistically significant if we can reject the null hypothesis with 95% confidence.</p><p>having only ConvAttention layers in the encoder, since, in the initial random state, the CTC-based compression might not properly reduce the input sequence, leading to out-of-memory issues in the following Transformer encoder layers. Notice that the pre-training does not improve performance. Indeed, Gaido et al. <ref type="formula">(2021)</ref> already showed that the encoder pre-training improves the baseline performance only without the additional CTC loss and that the results obtained by training without CTC loss and with encoder pre-training are identical to those achieved with the additional CTC loss. These findings have been confirmed in our experiments: i) initializing the encoder of the baseline with either an ASR or an ST encoder did not bring any improvement, and ii) our results are on par with those obtained with encoder pre-training and no additional CTC loss. We do not include the results with encoder pre-training of the baselines, as they do not bring any additional insight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We compare our proposed model to a strong baseline represented by a Transformer-based model with initial fixed sub-sampling <ref type="bibr" target="#b14">(Wang et al., 2020a)</ref> and its baseline+compression variant that includes the average CTC compression strategy, as per <ref type="bibr">(Gaido et al., 2021)</ref>. We choose to also develop the second baseline to make the comparison with Speechformer fair since they both use the CTC compression strategy. <ref type="table">Table 2</ref> reports the results computed with SacreBLEU 2 <ref type="bibr" target="#b4">(Post, 2018)</ref>. For each experiment, we report the average over 3 runs to ensure that performance differences do not depend on the fluctuations of particularly good or bad runs. First, it can be noticed that our baseline is in line with state-of-the-art architectures trained only 2 BLEU+c.mixed+#.1+s.exp+tok.13a+v.1.5.0</p><p>on MuST-C <ref type="bibr" target="#b14">(Wang et al., 2020a;</ref><ref type="bibr">Inaguma et al., 2020)</ref>. Second, the addition of CTC compression to the baseline model does not bring benefits. This confirms the findings of Gaido et al. <ref type="formula">(2021)</ref>, who showed that applying CTC compression using transcripts produces differences in score that are not statistically significant. Speechformer, instead, results in statistically significant improvements over the baseline in all language directions, with BLEU gains ranging from 0.5 (for en-nl) to 0.8 (for ende). As the CTC compression is not helpful for the baseline, we also evaluate a model (Plain ConvAttention) whose encoder is a stack of ConvAttention layers, i.e. without vanilla Transformer-encoder layers and any form of compression. The drop in performance with respect to Speechformer varies between 0.4 and 0.8 BLEU on all language pairs, supporting our hypothesis that a non-compressed encoder output is too redundant to be effectively attended by the decoder. Low-Resource Settings. We suppose that the higher gains on en-de may be related to the size of the training data. Indeed, the en-de section of MuST-C used for training is the smallest one, containing 20% fewer data than the en-es section and 10% less than the en-nl one. Thus, we study Speechformer's performance in different data conditions by progressively reducing the amount of training data. For this analysis, we select the enes section of MuST-C as it contains the highest number of hours (478h) among the three languages, and we experiment with three subsets, respectively containing 385h (corresponding to the amount of training data for en-de), 200h, and 100h (which can be considered a limited quantity given that the number of hours is respectively less than half and one fourth of the available data). <ref type="figure" target="#fig_2">Figure 3</ref> shows that the gains obtained by Speechformer over the baseline do not vary significantly between 385h and 478h (0.5 vs 0.6 BLEU). We can then conclude that the gain variation between en-de and en-es does not depend on the smaller size of the en-de training set. However, in the low resource settings (200h and 100h), the gains obtained by the Speechformer are much larger, amounting to 1.1 BLEU with 200h and 4.0 BLEU with 100h. To validate the robustness of these results, we also experimented on the en-de language pair and obtained consistent results: Speechformer outperforms the baseline by 1.5 BLEU (19.6 vs 18.1 BLEU) with 200h of training data and by 1.9 BLEU (9.7 vs 7.8 BLEU) with 100h of training data, achieving a considerable relative improvement of more than 24%. Although it brings consistent and significant gains in higher resource scenarios, these experiments show that Speechformer is particularly fruitful in low-resource settings. We leave to future work the assessment of the behavior of Speechformer in unrestricted data conditions (e.g. when using large ASR corpora to generate pseudo-labelled ST training data). Inference Time. The ConvAttention layers process the whole input sequences, which are 4 times larger than those elaborated by the baseline attention mechanism. Thereby, a slow-down at inference time is expected, especially for the Plain Con-vAttention, whose encoder layers are all ConvAttention layers. The last column of <ref type="table">Table 2</ref> confirms that the Plain ConvAttention architecture is 1.8 times slower than the baseline, i.e. the inference time is nearly twice. Speechformer is also slower than the baseline, but the overhead amounts to only 30% instead of 80%. Moreover, it can be noticed that the size of the attention matrix -and therefore the corresponding computational cost -can be controlled in the Speechformer with the compression factor (?) hyper-parameter. We leave to future studies the analysis of the trade-off between overall translation quality and inference time, which is usually irrelevant in offline ST, but becomes critical in simultaneous scenarios. Manual Analysis. Lastly, we inspected the baseline and Speechformer outputs to better understand the reason behind the improvements brought by our architecture. This qualitative analysis was conducted on a sample of 200 sentences of the ende test set -the language direction showing the largest gap between the systems (+0.8, see <ref type="table">Table  2</ref>) -by a professional linguist with C2 German level. It emerged (see the Appendix for examples) that Speechformer tends to have better wordordering, a typical problem arising when translating from an SVO language like English to an SOV language like German. Furthermore, Speechformer outputs display a better punctuation positioningattributable to an improved handling of pauses and prosody -and a reduction of the number of audio misunderstandings and omissions. Together with the overall BLEU gains, these findings provide us with interesting hints about the potential of Speechformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In the wake of previous works showing the benefits of a content-informed compression over fixed downsampling of the audio features, we proposed Speechformer: the first ST Transformer-based model able to encode the whole raw audio features without any sub-optimal initial subsampling typical of current state-of-the-art models. Our solution is made possible by the introduction of a modified attention mechanism -the ConvAttention -that reduces the memory complexity to O(( n ? ) 2 ). As the plain application of ConvAttention layers leads to redundant sequences, high-level hidden states are compressed with a CTC-based strategy to obtain a compact, yet informative representation that can be processed by vanilla Transformer encoder layers. Experiments on three language pairs show that Speechformer significantly outperforms a stateof-the-art ST model by 0.5-0.8 BLEU, reaching a peak of 4 BLEU points in a low resource scenario. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Training Details</head><p>All our models are composed by 12 encoder layers and 6 decoder layers with 8 attention heads and are trained using label smoothed cross entropy <ref type="bibr" target="#b11">(Szegedy et al., 2016)</ref> with the auxiliary CTC loss <ref type="bibr">(Kim et al., 2017;</ref><ref type="bibr">Bahar et al., 2019)</ref> and Adam optimizer <ref type="bibr">(Kingma and Ba, 2015)</ref>. The number of parameters is ?77M for the baseline and ?79M for the Speechformer. The CTC is computed at the 8th encoder layer and its role is to predict the source transcription (lowercased and without punctuation), as in <ref type="bibr">(Liu et al., 2020)</ref>. The learning rate is set to 1e-3 with an inverse square-root scheduler and 10,000 warm-up updates. Mini-batches contain up to 5,000 tokens and we update gradients every 16 mini-batches. We apply SpecAugment <ref type="bibr" target="#b3">(Park et al., 2019)</ref> and utterance-level cepstral mean and variance normalization. We filter out samples with duration exceeding 30s. The text is segmented in sub-word units with transcript and target Sentencepiece (Kudo and Richardson, 2018) unigram language models (Kudo, 2018) with size 5,000 and 8,000 respectively. We average 7 checkpoints around the best on the validation loss. Trainings were performed with 4 GPUs NVIDIA Tesla K-80 with 12GB of RAM and lasted about 3 days. It was a way that parents could figure out which were the right public schools for their kids.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Output examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference</head><p>Es ging um eine Methode, mit der Eltern herausfinden k?nnen, welche die richtigen ?ffentlichen Schulen f?r ihre Kinder sind.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline</head><p>Es war eine M?glichkeit, dass Eltern herausfinden konnten, welche f?r ihre Kinder die richtige ?ffentliche Schule war.</p><p>It was an opportunity for the parents to find out which were for their children the right public schools. Speechformer Es war eine Methode, mit der Eltern herausfinden konnten, welche die richtigen ?ffentlichen Schulen f?r ihre Kinder waren.</p><p>It was a method with which the parents could find out which were the right public schools for their children. Aluminum was the most valuable metal on the planet, more than gold and platinum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(d) Omission Audio</head><p>But the amazing thing about cities is they're worth so much more than it costs to build them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference</head><p>Aber das Erstaunliche an St?dten ist, dass sie so viel mehr wert sind, als es kostet sie zu bauen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline</head><p>Aber das Faszinierende an St?dten ist, dass es viel mehr wert ist, als es zu bauen.</p><p>But the fascinating thing about cities is that it's worth a lot more than building it. Speechformer Aber das Erstaunliche an St?dten ist, dass sie viel mehr wert sind als sie es kostet, sie zu bauen.</p><p>But the amazing thing about cities is that they are worth a lot more than it costs to build them. been selected to highlight the specific aspects that are better handled by Speechformer. Example (a) exhibits a wrong word ordering present in the baseline output, i.e. it anticipates "f?r ihre Kinder" (for their kids) with respect to "die richtigen ?ffentlichen Schulen" (the right public schools). Our proposed architecture, instead, translates the sentence in the correct order, making the translation easier to be read and understood.</p><p>Example (b) displays that Speechformer shows better punctuation handling, which -we hypothesize -is the result of an improved representation of prosody and pauses. In this example, for instance, our architecture is capable of detecting a question (i.e. So can you help me?) and translating it, while the baseline does not translate the input in question form and omits the last part of the audio content. Listening to the audio, we noticed a long pause after the question. We suppose that this pause led the baseline to conclude the sentence, while Speechformer managed to translate the remaining part of the utterance by going beyond that pause.</p><p>Our architecture shows an improved encoding of audio features that is reflected in its superior understanding of audio content. This emerges, indeed, from example (c), where the word Platinum is correctly recognized and translated by our system, while the baseline misunderstands and translates it in another word, "Pflanzen" (plants), with a completely different meaning. The better audio understanding of Speechformer is present in example (d) as well. On the contrary, the baseline omits part of the original sentence (i.e. it costs), with a huge impact on the meaning of the resulting sentence, while Speechformer does not lose audio details and produces a complete translation. In this example, we can also notice that our system better solves pronominal resolution as it chooses sie, which follows the grammatical gender and number of Staedten (i.e. plural feminine), while the baseline uses es, which wrongly agrees with das Faszinierende (i.e. singular neuter).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Effect of Label Smoothing</head><p>Label smoothing <ref type="bibr" target="#b11">(Szegedy et al., 2016</ref>) is a widely adopted regularization factor <ref type="bibr" target="#b19">(Zhang et al., 2021)</ref>. As such, a more complex architecture that pro-  cesses longer and potentially more redundant inputs -like our proposed Speechformer -can benefit more from its adoption. Hence, to validate that our gains are not due to a better regularization of the models and to assess the effect of label smoothing, we run experiments using the cross entropy loss without smoothing factor. The results are reported in <ref type="table" target="#tab_6">Table 4</ref>. Compared with the scores reported in Section 4 of the paper, we can see that label smoothing brings significant gains for all the systems (ranging from 1.5 to 2.0 BLEU points). Most importantly, the improvements of the Speechformer architecture (0.5-1.1 BLEU) are similar to those achieved with label smoothing (0.5-0.8 BLEU). The minimal difference can be explained by statistical variations of the results, considering that those obtained without label smoothing are computed on a single run. We can conclude that these results confirm the efficacy of our architecture and the validity of our experiments, showing that they are not biased by a higher regularization that might favor our solution over the baseline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Attention mechanism with the proposed convolutional compression of K and V.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Speechformer architecture with E L ConvAttention Layers and E T Transformer Encoder Layers. 7 20.6 20.5 21.3 20.2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Architecture comparison varying the amount of en-es training data (478h, 385h, 200h, and 100h).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>+0.6 23.2 +0.4 31.5 +0.3 27.7 -0.2 24.8 * +0.6 26.9 -0.3 1.8x Speechformer 23.3 * +0.8 23.6 * +0.8 31.8 * +0.6 28.5 * +0.6 24.9 * +0.7 27.7 * +0.5 1.3x</figDesc><table><row><cell>Model</cell><cell></cell><cell>en-de</cell><cell></cell><cell>en-es</cell><cell></cell><cell>en-nl</cell><cell></cell><cell>Inference</cell></row><row><cell></cell><cell>dev</cell><cell>tst-COMMON</cell><cell>dev</cell><cell>tst-COMMON</cell><cell>dev</cell><cell cols="2">tst-COMMON</cell><cell>Time</cell></row><row><cell>(Inaguma et al., 2020)</cell><cell>-</cell><cell>22.9</cell><cell>-</cell><cell>28.0</cell><cell>-</cell><cell>27.4</cell><cell></cell><cell>-</cell></row><row><cell>(Wang et al., 2020a)</cell><cell>-</cell><cell>22.7</cell><cell>-</cell><cell>27.2</cell><cell>-</cell><cell>27.3</cell><cell></cell><cell>-</cell></row><row><cell>Our baseline</cell><cell>22.5</cell><cell>22.8</cell><cell>31.2</cell><cell>27.9</cell><cell>24.2</cell><cell>27.2</cell><cell></cell><cell>1.0x</cell></row><row><cell>+ compression</cell><cell cols="6">22.3 -0.2 22.8 +0.0 31.1 -0.1 27.9 +0.0 24.2 +0.0 27.0</cell><cell>-0.2</cell><cell>0.9x</cell></row><row><cell cols="2">Plain ConvAttention 23.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>provides examples of the German trans-</cell></row><row><cell>lations generated by the baseline and by Speech-</cell></row><row><cell>former for four utterances of the MuST-C test set,</cell></row><row><cell>chosen among 200 sentences manually inspected</cell></row><row><cell>by a C2 German speaker. These sentences have</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Aluminum was the most valuable metal on the Planet, worth more than Gold and Platinum. Reference Aluminium war zu dieser Zeit das wertvollste Metall auf dem Planeten, wertvoller als Gold und Platin. Baseline Aluminium war die wertvollste Metallart auf dem Planeten, mehr als Gold und Pflanzen. Aluminum was the most valuable type of metal on the planet, more than gold and plants. Speechformer Aluminium war das wertvollste Metall auf dem Planeten, mehr als Gold und Platin.</figDesc><table><row><cell></cell><cell>Punctuation handling</cell></row><row><cell>Audio</cell><cell>So, sir, can you help me? I need help.</cell></row><row><cell>Reference</cell><cell>Also, mein Herr, k?nnen Sie mir helfen? Ich brauche Hilfe.</cell></row><row><cell>Baseline</cell><cell>Es ist also m?glich, mir zu helfen.</cell></row><row><cell></cell><cell>So it is possible to help me.</cell></row><row><cell cols="2">Speechformer Also, k?nnen Sie mir helfen? Ich habe keine Hilfe.</cell></row><row><cell></cell><cell>So can you help me? I have no help.</cell></row><row><cell></cell><cell>(c) Audio misunderstanding</cell></row><row><cell>Audio</cell><cell>You see</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Examples of translation problems -(a), (b), (c) -and omissions -(d) -that Speechformer does not suffer from while baseline does.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>+0.0 26.0 -0.2 25.1 -0.4 Plain ConvAttention 21.6 +0.4 25.6 * -0.6 25.6 +0.1 Speechformer 22.3 * +1.1 26.7 * +0.5 26.2 * +0.7</figDesc><table><row><cell>Model</cell><cell>en-de</cell><cell>en-es</cell><cell>en-nl</cell></row><row><cell>baseline</cell><cell>21.2</cell><cell>26.2</cell><cell>25.5</cell></row><row><cell>+ compression</cell><cell>21.2</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>BLEU score on three language pairs of MuST-C tst-COMMON. The * symbol indicates statistically significant improvements over the baseline computed with bootstrap resampling(Koehn, 2004)  with 10,000 samples, 1,000 sample size and 95% significance level.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/sarapapi/ FBK-fairseq/tree/speechformer_emnlp2021.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank Alina Karakanta for the support and help with the manual analysis.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2020. FINDINGS OF THE IWSLT 2020 EVALU-ATION CAMPAIGN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ebrahim</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amittai</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roldano</forename><surname>Cattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahim</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Nagesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Salesky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>St?ker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.iwslt-1.1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Spoken Language Translation</title>
		<meeting>the 17th International Conference on Spoken Language Translation</meeting>
		<imprint>
			<biblScope unit="page" from="1" to="34" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Zhu</surname></persName>
		</author>
		<title level="m">Jiajun Zhang, and Chengqing Zong. 2020. Bridging the Modality Gap for Speechto-Text Translation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">fairseq: A Fast, Extensible Toolkit for Sequence Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-4009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)<address><addrLine>Minneapolis</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2019-2680</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech 2019</title>
		<meeting>Interspeech 2019<address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Call for Clarity in Reporting BLEU Scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<meeting>the Third Conference on Machine Translation: Research Papers<address><addrLine>Belgium, Brussels</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ha?im</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?oise</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech<address><addrLine>Dresden, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploring Phoneme-Level Speech Representations for End-to-End Speech Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Salesky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Sperber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1179</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1835" to="1841" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Speech translation and the end-to-end promise: Taking stock of where we are</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Sperber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Paulik</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.661</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7409" to="7421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Machine Translation of Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Frederick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">G</forename><surname>Stentiford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Telecom Technology Journal</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="116" to="122" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The probable error of a mean</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Student</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Las Vegas, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems 30 (NIPS)</title>
		<meeting>Advances in Neural Information Processing Systems 30 (NIPS)<address><addrLine>Long Beach, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">JANUS: A Speech-to-Speech Translation System Using Connectionist and Symbolic Processing Strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><forename type="middle">N</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><forename type="middle">E</forename><surname>Mcnair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Tebelskis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the International Conference on Acoustics, Speech and Signal Processing<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="793" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fairseq S2T: Fast Speech-to-Text Modeling with Fairseq</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: System Demonstrations</title>
		<meeting>the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: System Demonstrations<address><addrLine>Suzhou, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="33" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Linformer: Self-Attention with Linear Complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Sequence-to</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sequence Models Can Directly Translate Foreign Speech</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech 2017</title>
		<meeting>Interspeech 2017<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="2625" to="2629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adaptive Feature Selection for End-to-End Speech Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.230</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2533" to="2544" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Delving Deep Into Label Smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Bin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Tao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2021.3089942</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5984" to="5996" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
