<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2021 GEOMETRY-AWARE GRADIENT ALGORITHMS FOR NEURAL ARCHITECTURE SEARCH</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Determined AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Khodak</surname></persName>
							<email>khodak@cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>* denotes equal contribution</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>* denotes equal contribution</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
							<email>talwalkar@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Determined AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>* denotes equal contribution</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2021 GEOMETRY-AWARE GRADIENT ALGORITHMS FOR NEURAL ARCHITECTURE SEARCH</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent state-of-the-art methods for neural architecture search (NAS) exploit gradient-based optimization by relaxing the problem into continuous optimization over architectures and shared-weights, a noisy process that remains poorly understood. We argue for the study of single-level empirical risk minimization to understand NAS with weight-sharing, reducing the design of NAS methods to devising optimizers and regularizers that can quickly obtain high-quality solutions to this problem. Invoking the theory of mirror descent, we present a geometry-aware framework that exploits the underlying structure of this optimization to return sparse architectural parameters, leading to simple yet novel algorithms that enjoy fast convergence guarantees and achieve state-of-the-art accuracy on the latest NAS benchmarks in computer vision. Notably, we exceed the best published results for both CIFAR and ImageNet on both the DARTS search space and NAS-Bench-201; on the latter we achieve near-oracle-optimal performance on CIFAR-10 and CIFAR-100. Together, our theory and experiments demonstrate a principled way to co-design optimizers and continuous relaxations of discrete NAS search spaces.</p><p>Published as a conference paper at ICLR 2021 1. We argue for studying NAS with weight-sharing as a single-level objective over a structured function class in which architectural decisions are treated as learned parameters rather than hyperparameters. Our setup clarifies recent concerns about rank disorder and makes clear that proper regularization and optimization of this objective is critical to obtaining high-quality solutions. 2. Focusing on optimization, we propose to improve existing NAS algorithms by re-parameterizing architecture parameters over the simplex and updating them using exponentiated gradient, a variant of mirror descent that converges quickly over this domain and enjoys favorable sparsity properties. This simple modification-which we call the Geometry-Aware Exponentiated Algorithm (GAEA)-is easily applicable to numerous methods, including first-order DARTS Liu et al. <ref type="formula">(2019)</ref>, GDAS Dong &amp; Yang (2019), and PC-DARTS (Xu et al., 2020). 3. To show correctness and efficiency of our scheme, we prove polynomial-time stationary-point convergence of block-stochastic mirror descent-a family of geometry-aware gradient algorithms that includes GAEA-over a continuous relaxation of the single-level NAS objective. To the best of our knowledge these are the first finite-time convergence guarantees for gradient-based NAS. 4. We demonstrate that GAEA improves upon state-of-the-art methods on three of the latest NAS benchmarks for computer vision. Specifically, we beat the current best results on NAS-Bench-201 (Dong &amp; Yang, 2020) by 0.18% on CIFAR-10, 1.59% on CIFAR-100, and 0.82% on ImageNet-16-120; we also outperform the state-of-the-art on the DARTS search space Liu et al. <ref type="formula">(2019)</ref>, for both CIFAR-10 and ImageNet, and match it on NAS-Bench-1Shot1 (Zela et al., 2020a). 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Neural architecture search has become an important tool for automating machine learning (ML) but can require hundreds of thousands of GPU-hours to train. Recently, weight-sharing approaches have achieved state-of-the-art performance while drastically reducing the computational cost of NAS to just that of training a single shared-weights network <ref type="bibr" target="#b32">(Pham et al., 2018;</ref>. Methods such as DARTS , GDAS <ref type="bibr" target="#b11">(Dong &amp; Yang, 2019)</ref>, and many others <ref type="bibr" target="#b32">(Pham et al., 2018;</ref><ref type="bibr" target="#b24">Liu et al., 2018;</ref><ref type="bibr" target="#b20">Laube &amp; Zell, 2019;</ref><ref type="bibr" target="#b5">Cai et al., 2019;</ref><ref type="bibr" target="#b0">Akimoto et al., 2019;</ref> combine weight-sharing with a continuous relaxation of the discrete search space to allow cheap gradient updates, enabling the use of popular optimizers. However, despite some empirical success, weight-sharing remains poorly understood and has received criticism due to (1) rank-disorder <ref type="bibr" target="#b44">(Yu et al., 2020;</ref><ref type="bibr" target="#b46">Zela et al., 2020b;</ref><ref type="bibr" target="#b33">Pourchot et al., 2020)</ref>, where the shared-weights performance is a poor surrogate of standalone performance, and (2) poor results on recent benchmarks <ref type="bibr" target="#b12">(Dong &amp; Yang, 2020;</ref><ref type="bibr" target="#b45">Zela et al., 2020a)</ref>.</p><p>Motivated by the challenge of developing simple and efficient methods that achieve state-of-the-art performance, we study how to best handle the goals and optimization objectives of NAS. We start by observing that weight-sharing subsumes architecture hyperparameters as another set of learned parameters of the shared-weights network, in effect extending the class of functions being learned. This suggests that a reasonable approach towards obtaining high-quality NAS solutions is to study how to regularize and optimize the empirical risk over this extended class. While many regularization approaches have been implicitly proposed in recent NAS efforts, we focus instead on the question of optimizing architecture parameters, which may not be amenable to standard procedures such as SGD that work well for standard neural network weights. In particular, to better-satisfy desirable properties such as generalization and sparsity of architectural decisions, we propose to constrain architecture parameters to the simplex and update them using exponentiated gradient, which has favorable convergence properties due to the underlying problem structure. Theoretically, we draw upon the mirror descent meta-algorithm <ref type="bibr" target="#b29">(Nemirovski &amp; Yudin, 1983;</ref><ref type="bibr" target="#b1">Beck &amp; Teboulle, 2003)</ref> to give convergence guarantees when using any of a broad class of such geometry-aware gradient methods to optimize the weight-sharing objective; empirically, we show that our solution leads to strong improvements on several NAS benchmarks. We summarize these contributions below:</p><p>Related Work. Most optimization analyses of NAS show monotonic improvement <ref type="bibr" target="#b0">(Akimoto et al., 2019)</ref>, asymptotic guarantees <ref type="bibr" target="#b42">(Yao et al., 2020)</ref>, or bounds on auxiliary quantities disconnected from any objective <ref type="bibr" target="#b6">Carlucci et al., 2019)</ref>. In contrast, we prove polynomial-time stationary-point convergence on a single-level objective for weight-sharing NAS, so far only studied empirically <ref type="bibr" target="#b22">Li et al., 2019)</ref>. Our results draw upon the mirror descent meta-algorithm <ref type="bibr" target="#b29">(Nemirovski &amp; Yudin, 1983;</ref><ref type="bibr" target="#b1">Beck &amp; Teboulle, 2003)</ref> and extend recent nonconvex convergence results <ref type="bibr" target="#b47">Zhang &amp; He (2018)</ref> to handle alternating descent. While there exist related results <ref type="bibr" target="#b9">(Dang &amp; Lan, 2015)</ref> the associated guarantees do not hold for the algorithms we propose. Finally, we note that a variant of GAEA that modifies first-order DARTS is related to XNAS , whose update also involves exponentiated gradient; however, GAEA is simpler and easier to implement. 2 Furthermore, the regret guarantees for XNAS do not relate to any meaningful performance measure for NAS such as speed or accuracy, whereas we guarantee convergence on the ERM objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE WEIGHT-SHARING OPTIMIZATION PROBLEM</head><p>In supervised ML we have a dataset T of labeled pairs (x, y) drawn from a distribution D over input/output spaces X and Y . The goal is to use T to search a function class H for h w : X ? Y parameterized by w ? R d that has low expected test loss (h w (x), y) when using x to predict the associated y on unseen samples drawn from D, as measured by some loss : Y ? Y ? [0, ?). A common way to do so is by approximate (regularized) empirical risk minimization (ERM), i.e. finding w ? R d with the smallest average loss over T , via some iterative method Alg, e.g. SGD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">THE BENEFITS AND CRITICISMS OF WEIGHT-SHARING FOR NAS</head><p>NAS is often viewed as hyperparameter optimization on top of Alg, with each architecture a ? A corresponding to a function class H a = {h w,a : X ? Y, w ? R d } to be selected by using validation data V ? X ? Y to evaluate the predictor obtained by fixing a and doing approximate ERM over T : </p><p>Since training individual sets of weights for any sizeable number of architectures is prohibitive, weight-sharing methods instead use a single set of shared weights to obtain validation signal about many architectures at once. In its most simple form, RS-WS <ref type="bibr" target="#b22">(Li &amp; Talwalkar, 2019)</ref>, these weights are trained to minimize a non-adaptive objective, min w?R d E a (x,y)?T (h wa,a (x), y), where the expectation is over a fixed distribution over architectures A. The final architecture a is then chosen to maximize the outer (validation) objective in (1) subject to w a = w. More frequently used is a bilevel objective over some continuous relaxation ? of the architecture space A, after which a valid architecture is obtained via a discretization step Map : ? ? A <ref type="bibr" target="#b32">(Pham et al., 2018;</ref>:</p><formula xml:id="formula_1">min ??? (x,y)?V (h w,? (x), y) s.t. w ? arg min u?R d (x,y)?T (h u,? (x), y)<label>(2)</label></formula><p>This objective is not significantly different from <ref type="formula" target="#formula_1">(2)</ref>, since Alg(T, a) approximately minimizes the empirical risk w.r.t. T ; the difference is replacing discrete architectures with relaxed architecture parameters ? ? ?, w.r.t. which we can take derivatives of the outer objective. This allows (2) to be approximated via alternating gradient updates w.r.t. w and ?. Relaxations can be stochastic, so that Map(?) is a sample from a ?-parameterized distribution <ref type="bibr" target="#b32">(Pham et al., 2018;</ref><ref type="bibr" target="#b11">Dong &amp; Yang, 2019)</ref>, or a mixture, in which case Map(?) selects architectural decisions with the highest weight in a convex combination given by ? . We overview this in more detail in Appendix A.</p><p>While weight-sharing significantly shortens search <ref type="bibr" target="#b32">(Pham et al., 2018)</ref>, it draws two main criticisms: ? Rank disorder: this describes when the rank of an architecture a according to the validation risk evaluated with fixed shared weights w is poorly correlated with the one using "standalone" weights w a = Alg(T, a). This causes suboptimal architectures to be selected after shared weights search <ref type="bibr" target="#b44">(Yu et al., 2020;</ref><ref type="bibr" target="#b46">Zela et al., 2020b;</ref><ref type="bibr" target="#b33">Pourchot et al., 2020)</ref>. ? Poor performance: weight-sharing can converge to degenerate architectures <ref type="bibr" target="#b45">(Zela et al., 2020a)</ref> and is outperformed by regular hyperparameter tuning on NAS-Bench-201 <ref type="bibr" target="#b12">(Dong &amp; Yang, 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SINGLE-LEVEL NAS AS A BASELINE OBJECT OF STUDY</head><p>Why are we able to apply weight-sharing to NAS? The key is that, unlike regular hyperparameters such as step-size, architectural hyperparameters directly affect the loss function without requiring a dependent change in the model weights w. Thus we can distinguish architectures without retraining simply by changing architectural decisions. Besides enabling weight-sharing, this point reveals that the goal of NAS is perhaps better viewed as a regular learning problem over an extended class H A = a?A H a = {h w,a : X ? Y, w ? R d , a ? A} that subsumes the architectural decisions as parameters of a larger model class, an unrelaxed "supernet." The natural approach to solving this is by approximate empirical risk minimization, e.g. by approximating continuous objective below on the right using a gradient algorithm and passing the output ? through Map to obtain a valid architecture:</p><formula xml:id="formula_2">min w?R d ,a?A (x,y)?T (h w,a (x), y) discrete (unrelaxed) supernet (NAS ERM) min w?R d ,??? (x,y)?T (h w,? (x), y) continuous relaxation (supernet ERM)<label>(3)</label></formula><p>Several works have optimized this single-level objective as an alternative to bilevel (2) <ref type="bibr" target="#b22">Li et al., 2019)</ref>. We argue for its use as the baseline object of study in NAS for three reasons: 1. As discussed above, it is the natural first approach to solving the statistical objective of NAS: finding a good predictor h w,a ? H A in the extended function class over architectures and weights. 2. The common alternating gradient approach to the bilevel problem (2) is in practice very similar to alternating block approaches to ERM (3); as we will see, there are established ways of analyzing such methods for the latter objective, while for the former convergence is known only under very strong assumptions such as uniqueness of the inner minimum <ref type="bibr" target="#b13">(Franceschi et al., 2018)</ref>. 3. While less frequently used in practice than bilevel, single-level optimization can be very effective:</p><p>we use it to achieve new state-of-the-art results on NAS-Bench-201 <ref type="bibr" target="#b12">(Dong &amp; Yang, 2020)</ref>.</p><p>Understanding NAS as single-level optimization-the usual deep learning setting-makes weightsharing a natural, not surprising, approach. Furthermore, for methods-both single-level and bilevelthat adapt architecture parameters during search, it suggests that we need not worry about rank disorder as long as we can use optimization to find a single feasible point that generalizes well; we explicitly do not need a ranking. Non-adaptive methods such as RS-WS still do require rank correlation to select good architectures after search, but they are explicitly not changing ? and so have no variant solving (3). The single-level formulation thus reduces search method design to well-studied questions of how to best regularize and optimize ERM. While there are many techniques for regularizing weight-sharing-including partial channels  and validation Hessian penalization <ref type="bibr" target="#b45">(Zela et al., 2020a</ref>)-we focus on the second question of optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GEOMETRY-AWARE GRADIENT ALGORITHMS</head><p>We seek to minimize the (possibly regularized) empirical risk f (w, ?) = 1 |T | (x,y)?T (h w,? (x), y) over shared-weights w ? R d and architecture parameters ? ? ?. Assuming we have noisy gradients of f w.r.t. w or ? at any point</p><formula xml:id="formula_3">(w, ?) ? R d ? ?-i.e.? w f (w, ?) or? ? f (w, ?) satisfying E? w f (w, ?) = ? w f (w, ?) or E? ? f (w, ?) = ? ? f (w, ?)</formula><p>, respectively-our goal is a point where f , or at least its gradient, is small, while taking as few gradients as possible. Our main complication is that architecture parameters lie in a constrained, non-Euclidean domain ?. Most search spaces A are product sets of categorical decisions-which operation o ? O to use at edge e ? E-so the natural relaxation is a product of |E| |O|-simplices. However, NAS methods often re-parameterize ? to be unconstrained using a softmax and then SGD or Adam <ref type="bibr" target="#b17">(Kingma &amp; Ba, 2015)</ref>. Is there a better parameterization-algorithm co-design? We consider a geometry-aware approach that uses mirror descent to design NAS methods with better properties depending on the domain; a key desirable property is to return sparse architectural parameters to reduce loss from post-search discretization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">BACKGROUND ON MIRROR DESCENT</head><p>Mirror descent has many formulations <ref type="bibr" target="#b29">(Nemirovski &amp; Yudin, 1983;</ref><ref type="bibr" target="#b1">Beck &amp; Teboulle, 2003;</ref><ref type="bibr" target="#b37">Shalev-Shwartz, 2011)</ref>; the proximal starts by noting that, in the unconstrained case, an SGD update at a point ? ? ? = R k given gradient estimate?f (?) with step-size ? &gt; 0 is equivalent to</p><formula xml:id="formula_4">? ? ??f (?) = arg min u?R k ??f (?) ? u + 1 2 u ? ? 2 2<label>(4)</label></formula><p>Here the first term aligns the output with the gradient while the second (proximal) term regularizes for closeness to the previous point as measured by the Euclidean distance. While the SGD update has been found to work well for unconstrained high-dimensional optimization, e.g. deep nets, this choice of proximal regularization may be sub-optimal over a constrained space with sparse solutions. The canonical such setting is optimization over the unit simplex, i.e. when ? = {? ? [0, 1] k : ? 1 = 1}.</p><p>Replacing the 2 -regularizer in Equation 4 by the relative entropy u ? (log u ? log ?), i.e. the KL-divergence, yields the exponentiated gradient (EG) update ( denotes element-wise product):</p><formula xml:id="formula_5">? exp(???f (?)) ? arg min u?? ??f (?) ? u + u ? (log u ? log ?)<label>(5)</label></formula><p>Note that the full EG update is obtained by 1 -normalizing the l.h.s. It is well-known that EG over the k-dimensional simplex requires only O(log k)/? 2 iterations to achieve a function value ?-away from optimal <ref type="bibr" target="#b1">(Beck &amp; Teboulle, 2003</ref>, Theorem 5.1), compared to the O(k/? 2 ) guarantee of gradient descent. This nearly dimension-independent iteration complexity is achieved by choosing a regularizer-the KL divergence-well-suited to the underlying geometry-the simplex. More generally, mirror descent is specified by a distance-generating function (DGF) ? that is strongly- <ref type="bibr" target="#b3">(Bregman, 1967)</ref>, a notion of distance on ? that acts as a regularizer in the mirror descent update:</p><formula xml:id="formula_6">convex w.r.t. some norm. ? induces a Bregman divergence D ? (u||v) = ?(u)??(v)???(v)?(u?v)</formula><formula xml:id="formula_7">arg min u?? ??f (?) ? u + D ? (u||?)<label>(6)</label></formula><p>For example, to recover SGD (4) we set ?(u) = 1 2 u 2 2 , which is strongly-convex w.r.t. the Euclidean norm, while EG (5) is recovered by setting ?(u) = u ? log u, strongly-convex w.r.t. the 1 -norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">BLOCK-STOCHASTIC MIRROR DESCENT</head><p>In the previous section we saw how mirror descent can perform better over certain geometries such as the simplex. However, in weight-sharing we are interested in optimizing over a hybrid geometry containing both the shared weights in an unconstrained Euclidean space and the architecture parameters in a non-Euclidean domain. Thus we focus on optimization over two blocks: shared weights w ? R d and architecture parameters ? ? ?, the latter associated with a DGF ? that is strongly-convex w.r.t. some norm ? . In NAS a common approach is to perform alternating gradient steps on each domain; for example, both ENAS <ref type="bibr" target="#b32">(Pham et al., 2018)</ref> and first-order DARTS  alternate between SGD on the shared weights and Adam on architecture parameters. This approach is encapsulated in the block-stochastic algorithm described in Algorithm 1, which at each step chooses one block at random to update using mirror descent (recall that SGD is a variant) and after T steps returns a random iterate. Algorithm 1 generalizes the single-level variant of both ENAS and first-order DARTS if SGD is used to update ? instead of Adam, with some mild caveats: in practice blocks are picked cyclically and the algorithm returns the last iterate, not a a random one. To analyze the convergence of Algorithm 1 we first state some regularity assumptions on the function:</p><p>Algorithm 1: Block-stochastic mirror descent optimization of a function f :</p><formula xml:id="formula_8">R d ? ? ? R. Input: initialization (w (1) , ? (1) ) ? R d ? ?, strongly-convex DGF ? : ? ? R, number of iterations T ? 1, step-size ? &gt; 0 for iteration t = 1, . . . , T do sample bt ? Unif{w, ?} // randomly select update block if block bt = w then w (t+1) ? w (t) ? ??wf (w (t) , ? (t) ) // SGD update to shared weights ? (t+1) ? ? (t) // no update to architecture params else w (t+1) ? w (t) // no update to shared weights ? (t+1) ? arg min u?? ?? ? f (w (t) , ? (t) ) ? u + D ? (u||? (t) )</formula><p>// update architecture params</p><p>Output: (w (r) , ? (r) ) for r ? Unif{1, . . . , T } // return random iterate Assumption 1. Suppose ? is strongly-convex w.r.t. some norm ? on a convex set ? and the objective function f :</p><formula xml:id="formula_9">R d ? ? ? [0, ?) satisfies the following: 1. ?-relatively-weak-convexity: f (w, ?) + ??(?) is convex on R d ? ? for some ? &gt; 0. 2. gradient bound: E ? w f (w, ?) 2 2 ? G 2 w and E ? ? f (w, ?) 2 * ? G 2 ? for some G w , G ? ? 0.</formula><p>The second assumption is a standard bound on the gradient norm while the first is a generalization of smoothness that allows all smooth and some non-smooth non-convex functions <ref type="bibr" target="#b47">(Zhang &amp; He, 2018)</ref>.</p><p>Our aim will be to show (first-order) ?-stationary-point convergence of Algorithm 1, a standard metric indicating that it has reached a point with no feasible descent direction, up to error ?; for example, in the unconstrained Euclidean case an ?-stationary-point is simply one where the gradient has squared-norm ? ?. The number of steps required to obtain such a point thus measures how fast a first-order method terminates. Stationarity is also significant as a necessary condition for optimality.</p><p>In our case ? may be constrained and so the gradient may never be small, thus necessitating a measure other than gradient norm. We use Bregman stationarity <ref type="bibr">(Zhang &amp; He, 2018, Equation 2</ref>.11), which measures stationary at a point (w, ?) using the Bregman divergence between the point and its proximal map prox ? (w, ?) = arg min u?R d ?? ?f (u) + D 2,? (u||w, ?) for some ? &gt; 0:</p><formula xml:id="formula_10">? ? (w, ?) = D 2,? (w, ?|| prox ? (w, ?)) + D 2,? (prox ? (w, ?)||w, ?) ? 2<label>(7)</label></formula><p>Here ? = 1 2? and the Bregman divergence D 2,? is that of the DGF 1 2 w 2 2 + ?(?) that encodes the geometry of the joint optimization domain over w ? R d and ?; note that the dependence of the stationarity measure on ? is standard <ref type="bibr" target="#b9">(Dang &amp; Lan, 2015;</ref><ref type="bibr" target="#b47">Zhang &amp; He, 2018)</ref>.</p><p>To understand why reaching a point (w, ?) with small Bregman stationarity is a reasonable goal, note that the proximal operator prox ? has the property that its fixed points, i.e. those satisfying (w, ?) = prox ? (w, ?), correspond to points where f has no feasible descent direction. Thus measuring how close (w, ?) is to being a fixed point of prox ? -as is done using the Bregman divergence in <ref type="formula" target="#formula_10">(7)</ref>-is a good measure of how far away the point is from being a stationary point of f . Finally, note that if f is smooth, ? is Euclidean, and ? is unconstrained-i.e. if we are running SGD over architecture parameters as well-then ? 1 2? ? ? implies an O(?)-bound on the squared gradient norm, recovering the standard definition of ?-stationarity. More intuition on proximal operators can be found in Parikh &amp; Boyd (2013, Section 1.2), while further details on Bregman stationarity and how it relates to other notions of convergence can be found in <ref type="bibr">Zhang &amp; He (2018, Section 2.3</ref>).</p><p>The following result shows that Algorithm 1 needs polynomially many iterations to finds a point (w, ?) with ?-small Bregman stationarity in-expectation:</p><formula xml:id="formula_11">Theorem 1. Let F = f (w (1) , ? (1) ) be the value of f at initialization. Under Assumption 1, if we run Algorithm 1 for T = 16?F ? 2 (G 2 w + G 2 ? ) iterations with step-size ? = 4F ?(G 2 w +G 2 ? )T then E? 1 2? (w (r) , ? (r) ) ? ?.</formula><p>Here the expectation is over the randomness of the algorithm and gradients.</p><p>The proof in the appendix follows from single-block analysis <ref type="bibr" target="#b47">(Zhang &amp; He, 2018</ref>, Theorem 3.1) and in fact holds for the general case of any number of blocks associated to any set of strongly-convex DGFs. Although there are prior results for the multi-block case (Dang &amp; Lan, 2015), they do not hold for nonsmooth Bregman divergences such as the KL divergence needed for exponentiated gradient.  <ref type="figure" target="#fig_3">Figure 1</ref>: Sparsity: Evolution over search phase epochs of the average entropy of the operationweights for GAEA and approaches it modifies when run on the DARTS search space (left), NAS-Bench-1Shot1 Search Space 1 (middle), and NASBench-201 on CIFAR-10 (right). GAEA reduces entropy much more quickly, allowing it to quickly obtain sparse architecture weights. This leads to both faster convergence to a single architecture and a lower loss when pruning at the end of search.</p><p>Thus Algorithm 1 returns an ?-stationary-point given</p><formula xml:id="formula_12">T = O(G 2 w + G 2 ? )/? 2 iterations, where G 2 w</formula><p>bounds the squared 2 -norm of the shared-weights gradient? w and G 2 ? bounds the squared magnitude of the architecture gradient? ? , as measured by the dual norm ? * of ? . Only the last term G ? is affected by our choice of DGF ?. The DGF of SGD is strongly-convex w.r.t. the 2 -norm, which is its own dual, so G 2 w is defined via 2 . However, for EG the DGF ?(u) = u ? log u is strongly-convex w.r.t. the 1 -norm, whose dual is ? . Since the 2 -norm of a k-dimensional vector can be ? k times its ? -norm, picking this DGF can lead to better bound on G ? and thus on the number of iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">GAEA: A GEOMETRY-AWARE EXPONENTIATED ALGORITHM</head><p>Equipped with these single-level guarantees, we turn to designing methods that can in-principle be applied to both the single-level and bilevel objectives, seeking parameterizations and algorithms that converge quickly and encourage favorable properties; in particular, we focus on returning architecture parameters that are sparse to reduce loss due to post-search discretization. EG is often considered to converge quickly to sparse solutions over the simplex <ref type="bibr" target="#b2">(Bradley &amp; Bagnell, 2008;</ref><ref type="bibr" target="#b4">Bubeck, 2019)</ref>, which makes it a natural choice for the architecture update. We thus propose GAEA, a Geometry-Aware Exponentiated Algorithm in which operation weights on each edge are constrained to the simplex and trained using EG; as in DARTS, the shared weights w are trained using SGD. GAEA can be used as a simple, principled modification to the many NAS methods that treat architecture parameters ? ? ? = R |E|?|O| as real-valued "logits" to be passed through a softmax to obtain mixture weights or probabilities for simplices over the operations O. Such methods include DARTS, PC-DARTS , and GDAS <ref type="bibr" target="#b11">(Dong &amp; Yang, 2019)</ref>. To apply GAEA, first re-parameterize ? to be the product set of |E| simplices, each associated to an edge (i, j) ? E; thus ? i,j,o corresponds directly to the weight or probability of operation o ? O for edge (i, j), not a logit. Then, given a stochastic gradient? ? f (w (t) , ? (t) ) and step-size ? &gt; 0, replace the architecture update by EG:</p><formula xml:id="formula_13">? (t+1) ? ? (t) exp ??? ? f (w (t) , ? (t) ) (multiplicative update) ? (t+1) i,j,o ?? (t+1) i,j,o o ?O? (t+1) i,j,o ? o ? O, ? (i, j) ? E (simplex projection)<label>(8)</label></formula><p>These two simple modifications, re-parameterization and exponentiation, suffice to obtain state-ofthe-art results on several NAS benchmarks, as shown in Section 4. Note that to obtain a bilevel algorithm we simply replace the gradient w.r.t. ? of the training loss with that of the validation loss.</p><p>GAEA is equivalent to Algorithm 1 with ?(?)</p><formula xml:id="formula_14">= (i,j)?E o?O ? i,j,o log ? i,j,o , which is strongly- convex w.r.t. ? 1 / |E| over the product of |E| |O|-simplices. The dual is |E| ? ? , so if G w</formula><p>bounds the shared-weights gradient and we have an entry-wise bound on the architecture gradient then GAEA reach ?-stationarity in O(G 2 w + |E|)/? 2 iterations. This can be up to a factor |O| improvement over SGD, either over the simplex or the logit space. In addition, GAEA encourages sparsity in the architecture weights by using a multiplicative update over simplices and not an additive update over R |E|?|O| . Obtaining sparse architecture parameters is critical for good performance, both for the mixture relaxation, where it alleviates the effect of discretization on the validation loss, and for the stochastic relaxation, where it reduces noise when sampling architectures. <ref type="table">Table 1</ref>: DARTS: Comparison with SOTA NAS methods on the DARTS search space, plus three results on different search spaces with a similar number of parameters reported at the top for comparison. All evaluations and reported performances of models found on the DARTS search space use similar training routines; this includes auxiliary towers and cutout but no other modifications, e.g. label smoothing <ref type="bibr" target="#b27">(M?ller et al., 2019)</ref>, <ref type="bibr">AutoAugment (Cubuk et al., 2019)</ref>, Swish <ref type="bibr" target="#b34">(Ramachandran et al., 2017)</ref>, Squeeze &amp; Excite <ref type="bibr" target="#b15">(Hu et al., 2018)</ref>, etc. The specific training procedure we use is that of PC-DARTS, which differs slightly from the DARTS routine by a small change to the drop-path probability; PDARTS tunes both this and batch-size. Our results are averaged over 10 random seeds. Search cost is hardware-dependent; we used Tesla V100 GPUs. For more details see <ref type="table" target="#tab_8">Tables 4 &amp; 5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EMPIRICAL RESULTS USING GAEA</head><p>We evaluate GAEA on three different computer vision benchmarks: the large and heavily studied search space from DARTS  and two smaller oracle evaluation benchmarks, NAS-Bench-1Shot1 <ref type="bibr" target="#b45">(Zela et al., 2020a)</ref>, and NAS-Bench-201 <ref type="bibr" target="#b12">(Dong &amp; Yang, 2020)</ref>. NAS-Bench-1Shot1 differs from the others by applying operations per node instead of per edge, while NAS-Bench-201 differs by not requiring edge-pruning. Since GAEA can modify a variety of methods, e.g. DARTS, PC-DARTS , and GDAS <ref type="bibr" target="#b11">(Dong &amp; Yang, 2019)</ref>, on each benchmark we start by evaluating the GAEA variant of the current best method on that benchmark. We show that despite the diversity of search spaces, GAEA improves upon this state-of-the-art across all three. Note that we use the same step-size for GAEA variants of DARTS/PC-DARTS and do not require weight-decay on architecture parameters. We defer experimental details and hyperparameter settings to the appendix and release all code, hyperparameters, and random seeds for reproducibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CONVERGENCE AND SPARSITY OF GAEA</head><p>We first examine the impact of GAEA on convergence and sparsity. <ref type="figure" target="#fig_3">Figure 1</ref> shows the entropy of the operation weights averaged across nodes for a GAEA-variant and its base method across the three benchmarks, demonstrating that it decreases much faster for GAEA-modified approaches. This validates our expectation that GAEA encourages sparse architecture parameters, which should alleviate the mismatch between the continuously relaxed architecture parameters and the discrete architecture returned. Indeed, we find that post-search discretization on the DARTS search space causes the validation accuracy of the PC-DARTS supernet to drop from 72.17% to 15.27%, while for GAEA PC-DARTS the drop is only 75.07% to 33.23%; note that this is shared-weights accuracy, obtained without retraining the final network. The numbers demonstrate that GAEA both (1) achieves better supernet optimization of the weight-sharing objective and (2) suffers less due to discretization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">GAEA ON THE DARTS SEARCH SPACE</head><p>Here we evaluate GAEA on the task of designing CNN cells for CIFAR-10 (Krizhevksy, 2009) and ImageNet <ref type="bibr" target="#b36">(Russakovsky et al., 2015)</ref> by using it to modify PC-DARTS , the current state-of-the-art method. We follow the same three stage process used by both DARTS and RS-WS for search and evaluation. <ref type="table">Table 1</ref> displays results on both datasets and demonstrates that GAEA's parameterization and optimization scheme improves upon PC-DARTS. In fact, GAEA PC-DARTS The dashed line is the final regret of the best weight-sharing method according to <ref type="bibr" target="#b46">Zela et al. (2020b)</ref>; note that in our reproduction PC-DARTS performed better than their evaluation on spaces 1 and 3.</p><p>outperforms all search methods except ProxylessNAS, which uses 1.5 times as many parameters on a different search space. Thus we improve the state-of-the-art on the DARTS search space. To meet a higher bar for reproducibility on CIFAR-10, in Appendix C we report "broad reproducibility" <ref type="bibr" target="#b22">(Li &amp; Talwalkar, 2019)</ref> by repeating our pipeline with new seeds. While GAEA PC-DARTS consistently finds good networks when selecting the best of four independent trials, multiple trials are required due to sensitivity to initialization, as is true for many approaches .</p><p>On ImageNet, we follow  by using subsamples containing 10% and 2.5% of the training images from ILSVRC-2012 <ref type="bibr" target="#b36">(Russakovsky et al., 2015)</ref> as training and validation sets, respectively. We fix architecture parameters for the first 35 epochs, then run GAEA PC-DARTS with step-size 0.1. All other hyperparameters match those of . <ref type="table">Table 1</ref> shows the final performance of both the architecture found by GAEA PC-DARTS on CIFAR-10 and the one found directly on ImageNet when trained from scratch for 250 epochs using the same settings as . GAEA PC-DARTS achieves a top-1 test error of 24.0%, which is state-of-the-art performance in the mobile setting when excluding additional training modifications, e.g. those in the caption. Additionally, the architecture found by GAEA PC-DARTS for CIFAR-10 and transferred achieves a test error of 24.2%, comparable to the 24.2% error of the one found by PC-DARTS directly on ImageNet. Top architectures found by GAEA PC-DARTS are depicted in <ref type="figure" target="#fig_4">Figure 3</ref> in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">GAEA ON NAS-BENCH-1SHOT1</head><p>NAS-Bench-1Shot1 <ref type="bibr" target="#b45">(Zela et al., 2020a</ref>) is a subset of NAS-Bench-101 <ref type="bibr" target="#b43">(Ying et al., 2019</ref>) that allows benchmarking weight-sharing methods on three search spaces over CIFAR-10 that differ in the number of nodes considered and the number of input edges per node. Of the weight-sharing methods benchmarked by <ref type="bibr" target="#b45">Zela et al. (2020a)</ref>, we found that PC-DARTS achieves the best performance on 2 of 3 search spaces, so we again evaluate GAEA PC-DARTS here. <ref type="figure" target="#fig_1">Figure 2</ref> shows that GAEA PC-DARTS consistently finds better architectures on average than PC-DARTS and thus exceeds the performance of the best method from Zela et al. (2020a) on 2 of 3 search spaces. We hypothesize that the benefits of GAEA are limited here due to the near-saturation of NAS methods. In particular, existing methods obtain within 1% test error of the top network in each space, while the latters' test errors when evaluated with different initializations are 0.37%, 0.23% and 0.19%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">GAEA ON NAS-BENCH-201</head><p>NAS-Bench-201 has one search space on three datasets-CIFAR-10, CIFAR-100, and ImageNet-16-120-that includes 4-node architectures with an operation from O = {none, skip connect, 1x1 convolution, 3x3 convolution, 3x3 avg pool} on each edge, yielding 15625 possible networks. <ref type="bibr" target="#b12">Dong &amp; Yang (2020)</ref> report results for several algorithms in the transfer NAS setting, where search is conducted on CIFAR-10 and the resulting networks are trained on a possibly different target dataset. <ref type="table" target="#tab_2">Table 2</ref> reports a subset of these results alongside evaluations of our implementation of several existing and GAEA-modified NAS methods in both the transfer and direct setting. Both the results from <ref type="bibr" target="#b12">Dong &amp; Yang (2020)</ref> and our reproductions show that GDAS is the best previous weight-sharing method; we evaluate GAEA GDAS and find that it achieves better results on CIFAR-100 and similar results on the other two datasets. Since we are interested in improving upon not only GAEA GDAS but also upon traditional hyperparameter optimization methods, we also investigate the performance of GAEA applied to first-order DARTS. We evaluate GAEA DARTS with both single-level (ERM) and bilevel optimization; recall that in the latter case we optimize architecture parameters w.r.t. the validation loss and the shared weights w.r.t. the training loss, whereas in ERM there is no data split. GAEA DARTS (ERM) achieves state-of-the-art performance on all three datasets in both the transfer and direct setting, exceeding the test accuracy of both weight-sharing and traditional hyperparameter tuning by a wide margin. GAEA DARTS (bilevel) performs worse but still exceeds all other methods on CIFAR-100 and ImageNet-16-120 in the direct search setting. The result thus also confirms the relevance of studying the single-level case to understand NAS; notably, the DARTS (ERM) baseline also improves substantially upon the DARTS (bilevel) baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper we take an optimization-based view of NAS, arguing that the design of good NAS algorithms is largely a matter of successfully optimizing and regularizing the supernet. In support of this, we develop GAEA, a simple modification of gradient-based NAS that attains state-of-the-art performance on several computer vision benchmarks while enjoying favorable speed and sparsity properties. We believe that obtaining high-performance NAS algorithms for a wide variety of applications will continue to require a similar co-design of search space parameterizations and optimization methods, and that our geometry-aware framework can help accelerate this process. In particular, most modern NAS algorithms search over products of categorical decision spaces, to which our approach is directly applicable. More generally, as the field moves towards more ambitious search spaces, e.g. full-network topologies or generalizations of operations such as convolution or attention, these developments may result in new architecture domains for which our work can inform the design of appropriate, geometry-aware optimization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A BACKGROUND ON NAS WITH WEIGHT-SHARING</head><p>Here we review the NAS setup motivating our work. Weight-sharing methods almost exclusively use micro cell-based search spaces for their tractability and additional structure <ref type="bibr" target="#b32">(Pham et al., 2018;</ref>. These search spaces can be represented as directed acyclic graphs (DAGs) with a set of ordered nodes N and edges E. Each node x (i) ? N is a feature representation and each edge (i, j) ? E is associated with an operation on the feature of node j passed to node i and aggregated with other inputs to form x (j) , with the restriction that a given node j can only receive edges from prior nodes as input. Hence, the feature at node i is Gradient-based weight-sharing methods apply continuous relaxations to the architecture space A in order to compute gradients in a continuous space ?. Methods like DARTS  and its variants <ref type="bibr" target="#b20">Laube &amp; Zell, 2019;</ref><ref type="bibr" target="#b16">Hundt et al., 2019;</ref> relax the search space by considering a mixture of operations per edge. For example, we will consider a relaxation where the architecture space A = {0, 1} |E|?|O| is relaxed into ? = [0, 1] |E|?|O| with the constraint that o?O ? i,j,o = 1, i.e. the operation weights on each edge sum to 1. The feature at node i is then x (i) = j&lt;i o?O ? i,j,o o(x (j) ). To get a valid architecture a ? A from a mixture ?, rounding and pruning are typically employed after the search phase.</p><formula xml:id="formula_15">x (i) = j&lt;i o (i,j) (x (j)</formula><p>An alternative, stochastic approach, such as that used by GDAS <ref type="bibr" target="#b11">(Dong &amp; Yang, 2019)</ref>, instead uses ?-parameterized distributions p ? over A to sample architectures <ref type="bibr" target="#b32">(Pham et al., 2018;</ref><ref type="bibr" target="#b0">Akimoto et al., 2019;</ref><ref type="bibr" target="#b5">Cai et al., 2019)</ref>; unbiased gradients w.r.t. ? ? ? can be computed using Monte Carlo sampling. The goal of all these relaxations is to use simple gradient-based approaches to approximately optimize (1) over a ? A by optimizing (2) over ? ? ? instead. However, both the relaxation and the optimizer critically affect the convergence speed and solution quality. We next present a principled approach for understanding both mixture and stochastic methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B OPTIMIZATION</head><p>This section contains proofs and generalizations of the non-convex optimization results in Section 3. Throughout this section, V denotes a finite-dimensional real vector space with Euclidean inner product ?, ? , R + denotes the set of nonnegative real numbers, and R denotes the set of extended real numbers R ? {??}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 PRELIMINARIES</head><p>Definition 1. Consider a closed and convex subset X ? V. For any ? &gt; 0 and norm ? :</p><formula xml:id="formula_16">X ? R + an everywhere-subdifferentiable function f : X ? R is called ?-strongly-convex w.r.t. ? if ? x, y ? X we have f (y) ? f (x) + ?f (x), y ? x + ? 2 y ? x 2</formula><p>Definition 2. Consider a closed and convex subset X ? V. For any ? &gt; 0 and norm ? :</p><formula xml:id="formula_17">X ? R + an continuously-differentiable function f : X ? R is called ?-strongly-smooth w.r.t. ? if ? x, y ? X we have f (y) ? f (x) + ?f (x), y ? x + ? 2 y ? x 2</formula><p>Definition 3. Let X be a closed and convex subset of V. The Bregman divergence induced by a strictly convex, continuously-differentiable distance-generating function (DGF) ? :</p><formula xml:id="formula_18">X ? R is D ? (x||y) = ?(x) ? ?(y) ? ??(y), x ? y ? x, y ? X</formula><p>By definition, the Bregman divergence satisfies the following properties: </p><formula xml:id="formula_19">1. D ? (x||y) ? 0 ? x, y ? X and D ? (x||y) = 0 ?? x = y. 2. If ? is ?-strongly-convex w.r.t. norm ? then so is D ? (?||y) ? y ? X . Furthermore, D ? (x||y) ? ? 2 x ? y 2 ? x, y ? X . 3. If ? is ?-strongly-smooth w.r.t. norm ? then so is D ? (?||y) ? y ? X . Furthermore, D ? (x||y) ? ? 2 x ? y 2 ? x, y ? X .</formula><formula xml:id="formula_20">? ? (x) = D ? (x|| prox ? (x)) + D ? (prox ? (x)||x) ? 2 B.2 RESULTS Throughout this subsection let V = ? b i=1 V i be a product space of b finite-dimensional real vector spaces V i , each with an associated norm ? i : V i ? R + , and X = ? b i=1 X i be a product set of b subsets X i ? V i , each with an associated 1-strongly-convex DGF ? i : X i ? R w.r.t. ? i .</formula><p>For each i ? [b] will use ? i, * to denote the dual norm of ? i and for any element x ? X we will use x i to denote its component in block i and x ?i to denote the component across all blocks other than i. Define the functions ? : V ? R + and ? * V ? R + for any x ? V by</p><formula xml:id="formula_21">x 2 = b i=1 x i 2 i and x 2 * = b i=1 x i 2</formula><p>i, * , respectively, and the function ? : X ? R for any</p><formula xml:id="formula_22">x ? X by ?(x) = b i=1 ? i (x).</formula><p>Finally, for any n ? N we will use [n] to denote the set {1, . . . , n}. Setting 1. For some fixed constants ? i , L i &gt; 0 for each i ? [b] we have the following:</p><formula xml:id="formula_23">1. f : X ? R is everywhere-subdifferentiable with minimum f * &gt; ?? and for all x ? X and each i ? [b] the restriction f (?, x ?i ) is ? i -RWC w.r.t. ? i . 2. For each i ? [b]</formula><p>there exists a stochastic oracle G i that for input x ? X outputs a random vector</p><formula xml:id="formula_24">G i (x, ?) s.t. E ? G i (x, ?) ? ? i f (x), where ? i f (x) is the subdifferential set of the restriction f (?, x ?i ) at x i . Moreover, E ? G i (x, ?) 2 i, * ? L 2 i . Define ? = max i?[b] ? i and L 2 = b i=1 L 2 i . Claim 1. ? is a norm on V.</formula><p>Proof. Positivity and homogeneity are trivial. For the triangle inequality, note that for any ? ? [0, 1] and any x, y ? X we have that</p><formula xml:id="formula_25">?x + (1 ? ?)y = b i=1 ?x i + (1 ? ?)y i 2 i ? b i=1 (? x i i + (1 ? ?) y i i ) 2 ? ? b i=1 x i 2 i + (1 ? ?) b i=1 y i 2 i = ? x + (1 ? ?) y</formula><p>where the first inequality follows by convexity of the norms ? i ? i ? [b] and the fact that the Euclidean norm on R b is nondecreasing in each argument, while the second inequality follows by convexity of the Euclidean norm on R b . Setting ? = 1 2 and multiplying both sides by 2 yields the triangle inequality.</p><p>Algorithm 2: Block-stochastic mirror descent over X = ? b i=1 X i given associated DGFs ? i : X i ? R.</p><formula xml:id="formula_26">Input: initialization x (1) ? X , number of steps T ? 1, step-size sequence {? t } T t=1 for iteration t ? [T ] do sample i ? Unif[b] set x (t+1) ?i = x (t) ?i get g = G i (x (t) , ? t ) set x (t+1) i = arg min u?Xi ? t g, u + D ?i (u||x (t) i ) Output:x = x (t) w.p. ?t T t=1 ?t . Claim 2. 1 2 ? 2 * is the convex conjugate of 1 2 ? 2 .</formula><p>Proof. Consider any u ? V. To upper-bound the convex conjugate note that</p><formula xml:id="formula_27">sup x?V u, x ? x 2 2 = sup x?V b i=1 u i , x i ? x i 2 i 2 ? sup x?V b i=1 u i i, * x i i ? x i 2 i 2 = 1 2 b i=1 u i 2 i, * = u 2 * 2</formula><p>where the first inequality follows by definition of a dual norm and the second by maximizing each term w.r.t. x i i . For the lower bound, pick x ? V s.t. u i , x i = u i i, * x i i and</p><formula xml:id="formula_28">x i i = u i i, * ? i ? [b]</formula><p>, which must exist by the definition of a dual norm. Then</p><formula xml:id="formula_29">u, x ? x 2 2 = b i=1 u i , x i ? x i 2 i 2 = 1 2 b i=1 u i 2 i, * 2 = u 2 * 2</formula><p>so sup x?V u, x ? 1 2 x 2 ? 1 2 u 2 * , completing the proof.</p><p>Theorem 2. Letx be the output of Algorithm 2 after T iterations with non-increasing step-size sequence {? t } T t=1 . Then under Setting 1, for any? &gt; ? we have that</p><formula xml:id="formula_30">E? 1 ? (x) ?? b ? ? ? min u?X f (u) +?D ? (u||x (1) ) ? f * +? L 2 2b T t=1 ? 2 t T t=1 ? t</formula><p>where the expectation is w.r.t. ? t and the randomness of the algorithm.</p><formula xml:id="formula_31">Proof. Define transforms U i , i ? [b] s.t. U T i x = x i and x = b i=1 U i x i ? x ? X . Let G be a stochastic oracle that for input x ? X outputs G(x, i, ?) = bU i G i (x, ?). This implies E i,? G(x, i, ?) = 1 b b i=1 bU i E ? G i (x, ?) ? b i=1 U i ? i f (x) = ?f (x) and E i,? G(x, i, ?) 2 * = 1 b b i=1 b 2 E ? U i G i (x, ?) 2 i, * ? b b i=1 L 2 i = bL 2 . Then x (t+1) = U i arg min u?Xi ? t g, u + D ?i (u||x (t) i ) = U i U T i arg min u?X ? t U i G i (x (t) , ? t ), u + b i=1 D ?i (u i ||x (t) i ) = arg min u?X ? t b G(x, i, ? t ), u + D ? (u||x (t) )</formula><p>Thus Algorithm 2 is equivalent to Zhang &amp; He (2018, Algorithm 1) with stochastic oracle G(x, i, ?), step-size sequence {? t /b} T t=1 , and no regularizer. Note that ? is 1-strongly-convex w.r.t. ? and f is ?-RWC w.r.t. ?, so in light of Claims 1 and 2 our setup satisfies Assumption 3.1 of <ref type="bibr" target="#b47">Zhang &amp; He (2018)</ref>. The result then follows from Theorem 3.1 of the same.  <ref type="figure" target="#fig_3">1)</ref> ). Then we have</p><formula xml:id="formula_32">= 2b(f (1) ?f * ) ?L 2 T ? t ? [T ], where f (1) = f (x (</formula><formula xml:id="formula_33">E? 1 2? (x) ? 2L 2b?(f (1) ? f * ) T</formula><p>where the expectation is w.r.t. ? t and the randomness of the algorithm. Equivalently, we can reach a pointx satisfying E? 1 2? (x) ? ? in 8?bL 2 (f (1) ?f * ) ? 2 stochastic oracle calls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 A SINGLE-LEVEL ANALYSIS OF ENAS AND DARTS</head><p>In this section we apply our analysis to understanding two existing NAS algorithms, ENAS <ref type="bibr" target="#b32">(Pham et al., 2018)</ref> and DARTS . For simplicity, we assume objectives induced by architectures in the relaxed search space are ?-smooth, which excludes components such as ReLU. However, such cases can be smoothed via Gaussian convolution, i.e. adding noise to every gradient; thus given the noisiness of SGD training we believe the following analysis is still informative <ref type="bibr" target="#b18">(Kleinberg et al., 2018)</ref>.</p><p>ENAS continuously relaxes A via a neural controller that samples architectures a ? A, so ? = R O(h 2 ) , where h is the number of hidden units. The controller is trained with Monte Carlo gradients. On the other hand, first-order DARTS uses a mixture relaxation similar to the one in Section A but using a softmax instead of constraining parameters to the simplex. Thus ? = R |E|?|O| for E the set of learnable edges and O the set of possible operations. If we assume that both algorithms use SGD for the architecture parameters then to compare them we are interested in their respective values of G ? , which we will refer to as G ENAS and G DARTS . Before proceeding, we note again that our theory holds only for the single-level objective and when using SGD as the architecture optimizer, whereas both algorithms specify the bilevel objective and Adam <ref type="bibr" target="#b17">(Kingma &amp; Ba, 2015)</ref>, respectively.</p><p>At a very high level, the Monte Carlo gradients used by ENAS are known to be high-variance, so G ENAS may be much larger than G DARTS , yielding faster convergence for DARTS, which is reflected in practice . We can also do a simple low-level analysis under the assumption that all architecture gradients are bounded entry-wise, i.e. in ? -norm, by some constant; then since the squared 2 -norm is bounded by the product of the dimension and the squared ? -norm we have</p><formula xml:id="formula_34">G 2 ENAS = O(h 2 ) while G 2 DARTS = O(|E||O|).</formula><p>Since ENAS uses a hidden state size of h = 100 and the DARTS search space has |E| = 14 edges and |O| = 7 operations, this also points to DARTS needing fewer iterations to converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EXPERIMENTAL DETAILS</head><p>We provide additional detail on the experimental setup and hyperparameter settings used for each benchmark studied in Section 4. We also provide a more detailed discussion of how XNAS differs from GAEA, along with empirical results for XNAS on the NAS-Bench-201 benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 DARTS SEARCH SPACE</head><p>We consider the same search space as DARTS , which has become one of the standard search spaces for CNN cell search . Following the evaluation procedure used in  and  , our evaluation of GAEA PC-DARTS consists of three stages:</p><p>? Stage 1: In the search phase, we run GAEA PC-DARTS with 5 random seeds to reduce variance from different initialization of the shared-weights network. 3 ? Stage 2: We evaluate the best architecture identified by each search run by training from scratch. ? Stage 3: We perform a more thorough evaluation of the best architecture from stage 2 by training with ten different random seed initializations.</p><p>For completeness, we describe the convolutional neural network search space considered. A cell consists of 2 input nodes and 4 intermediate nodes for a total of 6 nodes. The nodes are ordered and subsequent nodes can receive the output of prior nodes as input so for a given node k, there are k ? 1 possible input edges to node k. Therefore, there are a total of 2 + 3 + 4 + 5 = 14 edges in the weight-sharing network.</p><p>An architecture is defined by selecting 2 input edges per intermediate node and also selecting a single operation per edge from the following 8 operations: (1) 3 ? 3 separable convolution, (2) 5 ? 5 separable convolution, (3) 3 ? 3 dilated convolution, (4) 5 ? 5 dilated convolution, (5) max pooling, (6) average pooling, (7) identity (8) zero. We use the same search space to design a "normal" cell and a "reduction" cell; the normal cells have stride 1 operations that do not change the dimension of the input, while the reduction cells have stride 2 operations that half the length and width dimensions of the input. In the experiments, for both cell types, , after which the output of all intermediate nodes are concatenated to form the output of the cell.  <ref type="table" target="#tab_5">Table 3</ref> show the final stage 3 evaluation performance of GAEA PC-DARTS for 2 additional sets of random seeds from stage 1 search. The performance of GAEA PC-DARTS for one set is similar to that reported in <ref type="table">Table 1</ref>, while the other is on par with the performance reported for PC-DARTS in . We do observe non-negligible variance in the performance of the architecture found by different random seed initializations of the shared-weights network, necessitating running multiple searches before selecting an architecture. We also found that it was possible to identify and eliminate poor performing architectures in just 20 epochs of training during stage 2 intermediate evaluation, thereby reducing the total training cost by over 75% (we only trained 3 out of 10 architectures for the entire 600 epochs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 3 Evaluation Set 1 (Reported)</head><p>Set 2 Set 3 2.50 ? 0.07 2.50 ? 0.09 2.60 ? 0.09 We depict the top architectures found by GAEA PC-DARTS for CIFAR-10 and ImageNet in <ref type="figure" target="#fig_4">Figure 3</ref> and detailed results in <ref type="table" target="#tab_8">Tables 4 and 5</ref>.  <ref type="bibr" target="#b5">(Cai et al., 2019)</ref> 2.08 N/A 5.7 4 N gradient ENAS <ref type="bibr" target="#b32">(Pham et al., 2018)</ref> 2.89 N/A 4.6 0.5 Y RL Random search WS ? <ref type="bibr" target="#b22">(Li &amp; Talwalkar, 2019)</ref> 2.71 2.85 ? 0.08 3.8 0.7 Y random ASNG-NAS <ref type="bibr" target="#b0">(Akimoto et al., 2019)</ref> N/A 2.83 ? 0.14 3.9 0.1 Y gradient SNAS  N/A 2.85 ? 0.02 2.8 1.5 Y gradient DARTS (1st-order) ?  N/A 3.00 ? 0.14 3.3 0.4 Y gradient DARTS (2nd-order) ?  N      <ref type="figure">Figure 4</ref>: NAS-Bench-201: Learning Curves. Evolution over search phase epochs of the best architecture according to the NAS method. DARTS (first-order) converges to nearly all skip connections while GAEA is able to suppress overfitting to the mixture relaxation by encouraging sparsity in operation weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 NAS-BENCH-1SHOT1</head><p>The NAS-Bench-1Shot1 benchmark <ref type="bibr" target="#b45">(Zela et al., 2020a)</ref> contains 3 different search spaces that are subsets of the NAS-Bench-101 search space. The search spaces differ in the number of nodes and the number of input edges selected per node. We refer the reader to <ref type="bibr" target="#b45">(Zela et al., 2020a)</ref> for details about each individual search space.</p><p>Of the NAS methods evaluated in <ref type="bibr" target="#b45">Zela et al. (2020a)</ref>, PC-DARTS had the most robust performance across the three search spaces and converged to the best architecture in search spaces 1 and 3. GDAS, a probabilistic gradient NAS method, achieved the best performance on search space 2. Hence, we focused on applying a geometry-aware approach to PC-DARTS. We implemented GAEA PC-DARTS within the repository provided by the authors of <ref type="bibr" target="#b45">Zela et al. (2020a)</ref> available at https: //github.com/automl/nasbench-1shot1. We used the same hyperparameter settings for training the weight-sharing network as that used by <ref type="bibr" target="#b45">Zela et al. (2020a)</ref> for PC-DARTS. Similar to the previous benchmark, we initialize architecture parameters to allocate equal weight to all options. For the architecture updates, the only hyperparameter for GAEA PC-DARTS is the learning rate for exponentiated gradient, which we set to 0.1.</p><p>As mentioned in Section 4, the search spaces considered in this benchmark differ in that operations are applied after aggregating all edge inputs to a node instead of per edge input as in the DARTS and NAS-Bench-201 search spaces. This structure inherently limits the size of the weight-sharing network to scale with the number of nodes instead of the number of edges (O(|nodes| 2 )), thereby limiting the degree of overparameterization. Understanding the impact of overparameterization on the performance of weight-sharing NAS methods is a direction for future study. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 COMPARISON TO XNAS</head><p>As discussed in Section 1, XNAS is similar to GAEA in that it uses an exponentiated gradient update but is motivated from a regret minimization perspective.  provides regret bounds for XNAS relative to the observed sequence of validation losses, however, this is not equivalent to the regret relative to the best architecture in the search space, which would have generated a different sequence of validation losses.</p><p>XNAS also differs in its implementation in two ways: (1) a wipeout routine is used to zero out operations that cannot recover to exceed the current best operation within the remaining number of iterations and (2) architecture gradient clipping is applied per data point before aggregating to form the update. These differences are motivated from the regret analysis and meaningfully increase the complexity of the algorithm. Unfortunately, the authors do not provide the code for architecture search in their code release at https://github.com/NivNayman/XNAS. Nonetheless, we implemented XNAS for the NAS-Bench-201 search space to provide a point of comparison to GAEA.</p><p>Our results shown in <ref type="figure" target="#fig_6">Figure 5</ref> demonstrate that XNAS exhibits much of the same behavior as DARTS in that the operations all converge to skip connections. We hypothesize that this is due to the gradient clipping, which obscures the signal kept by GAEA in favor of convolutional operations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>,a (x), y) s.t.w a = Alg(T, a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>NAS-Bench-1Shot1: Online comparison of PC-DARTS and GAEA PC-DARTS in terms of the test regret at each epoch of shared-weights training, i.e. the difference between the ground truth test error of the proposed architecture and that of the best architecture in the search space. The dark lines indicate the mean of four random trials and the light colored bands ? one standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Definition 4. (Zhang &amp; He, 2018, Definition 2.1) Consider a closed and convex subset X ? V. For any ? &gt; 0 and ? : X ? R an everywhere-subdifferentiable function f : X ? R is called ?-relatively-weakly-convex (?-RWC) w.r.t. ? if f (?) + ??(?) is convex on X . Definition 5. (Zhang &amp; He, 2018, Definition 2.3) Consider a closed and convex subset X ? V. For any ? &gt; 0, function f : X ? R, and DGF ? : X ? R the Bregman proximal operator of f is prox ? (x) = arg min u?X ?f (u) + D ? (u||x) Definition 6. (Zhang &amp; He, 2018, Equation 2.11) Consider a closed and convex subset X ? V. For any ? &gt; 0, function f : X ? R, and DGF ? : X ? R the Bregman stationarity of f at any point x ? X is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Corollary 1 .</head><label>1</label><figDesc>Under Setting 1 letx be the output of Algorithm 2 with constant step-size ? t</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>The best normal and reduction cells found by GAEA PC-DARTS on CIFAR-10 (top) and ImageNet (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>DARTS (bilevel) CIFAR100: GAEA DARTS (bilevel) IMAGENET16-120: GAEA DARTS (bilevel)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>NAS-Bench-201: XNAS Learning Curves. Evolution over search phase epochs of the best architecture according 4 runs of XNAS. XNAS exhibits the same behavior as DARTS and converges to nearly all skip connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>.</figDesc><table><row><cell></cell><cell cols="2">CIFAR-10 Error</cell><cell>Search Cost</cell><cell cols="2">ImageNet Error</cell><cell>Search Cost</cell><cell></cell></row><row><cell>Search Method (source)</cell><cell>Best</cell><cell>Average</cell><cell>(GPU Days)</cell><cell>top-1</cell><cell>top-5</cell><cell>(GPU Days)</cell><cell>method</cell></row><row><cell>NASNet-A  *  (Zoph et al., 2018)</cell><cell>-</cell><cell>2.65</cell><cell>2000</cell><cell>26.0</cell><cell>8.4</cell><cell>1800</cell><cell>RL</cell></row><row><cell>AmoebaNet-B  *  (Real et al., 2019)</cell><cell>-</cell><cell>2.55 ? 0.05</cell><cell>3150</cell><cell>24.3</cell><cell>7.6</cell><cell>3150</cell><cell>evolution</cell></row><row><cell>ProxylessNAS  *  (Cai et al., 2019)</cell><cell>2.08</cell><cell>-</cell><cell>4</cell><cell>24.9</cell><cell>7.5</cell><cell>8.3</cell><cell>gradient (WS)</cell></row><row><cell>ENAS (Pham et al., 2018)</cell><cell>2.89</cell><cell>-</cell><cell>0.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>RL (WS)</cell></row><row><cell>RS-WS  ? (Li &amp; Talwalkar, 2019)</cell><cell>2.71</cell><cell>2.85 ? 0.08</cell><cell>0.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>random (WS)</cell></row><row><cell>ASNG (Akimoto et al., 2019)</cell><cell>-</cell><cell>2.83 ? 0.14</cell><cell>0.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>gradient (WS)</cell></row><row><cell>SNAS (Xie et al., 2019)</cell><cell>-</cell><cell>2.85 ? 0.02</cell><cell>1.5</cell><cell>27.3</cell><cell>9.2</cell><cell>1.5</cell><cell>gradient (WS)</cell></row><row><cell>DARTS (1st)  ? (Liu et al., 2019)</cell><cell>-</cell><cell>3.00 ? 0.14</cell><cell>0.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>gradient (WS)</cell></row><row><cell>DARTS (2nd)  ? (Liu et al., 2019)</cell><cell>-</cell><cell>2.76 ? 0.09</cell><cell>1</cell><cell>26.7</cell><cell>8.7</cell><cell>4.0</cell><cell>gradient (WS)</cell></row><row><cell>PDARTS (Chen et al., 2019)</cell><cell>2.50</cell><cell>-</cell><cell>0.3</cell><cell>24.4</cell><cell>7.4</cell><cell>0.3</cell><cell>gradient (WS)</cell></row><row><cell>PC-DARTS  ? (Xu et al., 2020)</cell><cell>-</cell><cell>2.57 ? 0.07</cell><cell>0.1</cell><cell>24.2</cell><cell>7.3</cell><cell>3.8</cell><cell>gradient (WS)</cell></row><row><cell>GAEA PC-DARTS  ? (ours)</cell><cell>2.39</cell><cell>2.50 ? 0.06</cell><cell>0.1</cell><cell>24.0</cell><cell>7.3</cell><cell>3.8</cell><cell>gradient (WS)</cell></row><row><cell>PC-DARTS  ? (Xu et al., 2020)</cell><cell cols="3">(search on CIFAR-10, train on ImageNet)</cell><cell>25.1</cell><cell>7.8</cell><cell>0.1</cell><cell>gradient (WS)</cell></row><row><cell>GAEA PC-DARTS  ? (ours)</cell><cell cols="3">(search on CIFAR-10, train on ImageNet)</cell><cell>24.3</cell><cell>7.3</cell><cell>0.1</cell><cell>gradient (WS)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* Search space/backbone differ from the DARTS setting; we show results for networks with a comparable number of parameters.? For fair comparison to other work, we show the search cost for training the shared-weights network with a single initialization.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>: Results are separated into traditional hyperparameter optimization algorithms with search run on CIFAR-10 (top block), weight-sharing methods with search run on CIFAR-10 (middle block), and weight-sharing methods run directly on the dataset used for training (bottom block). The use of transfer NAS follows the evaluations conducted by<ref type="bibr" target="#b12">Dong &amp; Yang (2020)</ref>; unless otherwise stated all non-GAEA results are from their paper. The best results in the transfer and direct settings on each dataset are bolded.</figDesc><table><row><cell></cell><cell></cell><cell>Search  *  (seconds)</cell><cell>CIFAR-10 (test)</cell><cell>CIFAR-100 (test)</cell><cell>ImageNet-16-120 (test)</cell></row><row><cell>Regular</cell><cell>REA</cell><cell>N/A</cell><cell>93.92 ? 0.30</cell><cell>71.84 ? 0.99</cell><cell>45.54 ? 1.03</cell></row><row><cell>HO,</cell><cell>RS</cell><cell>N/A</cell><cell>93.70 ? 0.36</cell><cell>71.04 ? 1.08</cell><cell>44.57 ? 1.25</cell></row><row><cell>search on</cell><cell>REINFORCE</cell><cell>N/A</cell><cell>93.85 ? 0.37</cell><cell>71.71 ? 1.09</cell><cell>45.25 ? 1.18</cell></row><row><cell>CIFAR-10</cell><cell>BOHB</cell><cell>N/A</cell><cell>93.61 ? 0.52</cell><cell>70.85 ? 1.28</cell><cell>44.42 ? 1.49</cell></row><row><cell></cell><cell>RSPS</cell><cell>7587</cell><cell>87.66 ? 1.69</cell><cell>58.33 ? 4.34</cell><cell>31.14 ? 3.88</cell></row><row><cell></cell><cell>DARTS (bilevel)</cell><cell>35781</cell><cell>54.30 ? 0.00</cell><cell>15.61 ? 0.00</cell><cell>16.32 ? 0.00</cell></row><row><cell>Weight</cell><cell>SETN</cell><cell>34139</cell><cell>87.64 ? 0.00</cell><cell>59.05 ? 0.24</cell><cell>32.52 ? 0.21</cell></row><row><cell>sharing,</cell><cell>GDAS</cell><cell>31609</cell><cell>93.61 ? 0.09</cell><cell>70.70 ? 0.30</cell><cell>41.71 ? 0.98</cell></row><row><cell>search on</cell><cell>DARTS  ? (bilevel)</cell><cell>10683  ?</cell><cell>54.30 ? 0.00</cell><cell>15.32 ? 0.00</cell><cell>16.38 ? 0.00</cell></row><row><cell>CIFAR-10</cell><cell>GAEA DARTS (bilevel)</cell><cell>7930  ?</cell><cell>91.63 ? 2.57</cell><cell>68.39 ? 4.47</cell><cell>41.59 ? 4.20</cell></row><row><cell></cell><cell>DARTS  ? (ERM)</cell><cell>18112  ?</cell><cell>84.39 ? 3.82</cell><cell>54.81 ? 7.08</cell><cell>31.82 ? 4.78</cell></row><row><cell></cell><cell>GAEA DARTS (ERM)</cell><cell>9061  ?</cell><cell>94.10 ? 0.29</cell><cell>72.60 ? 0.89</cell><cell>45.81 ? 0.51</cell></row><row><cell></cell><cell>GDAS  ?</cell><cell>27923  ?</cell><cell>93.52 ? 0.15</cell><cell>67.52 ? 0.15</cell><cell>40.91 ? 0.12</cell></row><row><cell>Weight</cell><cell>GAEA GDAS</cell><cell>16754  ?</cell><cell>93.55 ? 0.13</cell><cell>70.47 ? 0.47</cell><cell>40.91 ? 0.12</cell></row><row><cell>sharing,</cell><cell>DARTS  ? (bilevel)</cell><cell>10683  ?</cell><cell>54.30 ? 0.00</cell><cell>15.32 ? 0.00</cell><cell>28.96 ? 10.22</cell></row><row><cell>direct</cell><cell>GAEA DARTS (bilevel)</cell><cell>7930  ?</cell><cell>91.63 ? 2.57</cell><cell>71.87 ? 0.57</cell><cell>45.69 ? 0.56</cell></row><row><cell>search</cell><cell>DARTS  ? (ERM)</cell><cell>18112  ?</cell><cell>84.39 ? 3.82</cell><cell>51.26 ? 6.14</cell><cell>31.35 ? 7.46</cell></row><row><cell></cell><cell>GAEA DARTS (ERM)</cell><cell>9061  ?</cell><cell>94.10 ? 0.29</cell><cell>73.43 ? 0.13</cell><cell>46.36 ? 0.00</cell></row><row><cell></cell><cell>ResNet</cell><cell>N/A</cell><cell>93.97</cell><cell>70.86</cell><cell>43.63</cell></row><row><cell></cell><cell>Optimal</cell><cell>N/A</cell><cell>94.37</cell><cell>73.51</cell><cell>47.31</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* Search cost reported for running the search algorithm on CIFAR-10.? Search cost measured on NVIDIA P100 GPUs.? Our reproduction or implementation of a non-GAEA method.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>). Search spaces are specified by the number of nodes, the number of edges per node, and the set of operations O that can be applied at each edge. Thus for NAS, A ? {0, 1} |E|?|O| is the set of all valid architectures for encoded by edge and operation decisions. Treating both the shared weights w ? R d and architecture decisions a ? A as parameters, weight-sharing methods train a single network subsuming all possible functions within the search space.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>For GAEA PC-DARTS, we initialize the architecture parameters with equal weight across all options (equal weight across all operations per edge and equal weight across all input edges per node). Then, we train the shared-weights network for 10 epochs without performing any architecture updates to warmup the weights. Then, we use a learning rate of 0.1 for the exponentiated gradient update for GAEA PC-DARTS.</figDesc><table><row><cell>C.1.2 STAGE 2 AND 3: ARCHITECTURE EVALUATION</cell></row><row><cell>For stages 2 and 3, we train each architecture for 600 epochs with the same hyperparameter settings</cell></row><row><cell>as PC-DARTS:</cell></row><row><cell>train:</cell></row><row><cell>scheduler: cosine</cell></row><row><cell>lr_anneal_cycles: 1</cell></row><row><cell>smooth_cross_entropy: false</cell></row><row><cell>batch_size: 128</cell></row><row><cell>learning_rate: 0.025</cell></row><row><cell>learning_rate_min: 0.0</cell></row><row><cell>momentum: 0.9</cell></row><row><cell>weight_decay: 0.0003</cell></row><row><cell>init_channels: 36</cell></row><row><cell>layers: 20</cell></row><row><cell>autoaugment: false</cell></row><row><cell>cutout: true</cell></row><row><cell>cutout_length: 16</cell></row><row><cell>auxiliary: true</cell></row><row><cell>auxiliary_weight: 0.4</cell></row><row><cell>drop_path_prob: 0.3</cell></row><row><cell>grad_clip: 5</cell></row><row><cell>C.1.3 BROAD REPRODUCIBILITY</cell></row><row><cell>Our 'broad reproducibility' results in</cell></row><row><cell>C.1.1 STAGE 1: ARCHITECTURE SEARCH</cell></row><row><cell>For stage 1, as is done by DARTS and PC-DARTS, we use GAEA PC-DARTS to update architecture</cell></row><row><cell>parameters for a smaller shared-weights network in the search phase with 8 layers and 16 initial</cell></row><row><cell>channels. All hyperparameters for training the weight-sharing network are the same as that used by</cell></row><row><cell>PC-DARTS:</cell></row><row><cell>train:</cell></row><row><cell>scheduler: cosine</cell></row><row><cell>lr_anneal_cycles: 1</cell></row><row><cell>smooth_cross_entropy: false</cell></row><row><cell>batch_size: 256</cell></row><row><cell>learning_rate: 0.1</cell></row><row><cell>learning_rate_min: 0.0</cell></row><row><cell>momentum: 0.9</cell></row><row><cell>weight_decay: 0.0003</cell></row><row><cell>init_channels: 16</cell></row><row><cell>layers: 8</cell></row><row><cell>autoaugment: false</cell></row><row><cell>cutout: false</cell></row><row><cell>auxiliary: false</cell></row><row><cell>drop_path_prob: 0</cell></row><row><cell>grad_clip: 5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>GAEA PC-DARTS Stage 3 Evaluation for 3 sets of random seeds.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>We show results for networks with a comparable number of parameters. ? For fair comparison to other work, we show the search cost for training the shared-weights network with a single initialization. ? Search space and backbone architecture (PyramidNet) differ from the DARTS setting.PDARTS results not reported for multiple seeds. Additionally, PDARTS uses deeper weight-sharing networks during search, on which PC-DARTS has also been shown to improve performance, so we GAEA PC-DARTS to further improve if modified similarly.</figDesc><table><row><cell></cell><cell>/A</cell><cell>2.76 ? 0.09</cell><cell>3.3</cell><cell>1</cell><cell>Y</cell><cell>gradient</cell></row><row><cell>PDARTS # (Chen et al., 2019)</cell><cell>2.50</cell><cell>N/A</cell><cell>3.4</cell><cell>0.3</cell><cell>Y</cell><cell>gradient</cell></row><row><cell>PC-DARTS  ? (Xu et al., 2020)</cell><cell>N/A</cell><cell>2.57 ? 0.07</cell><cell>3.6</cell><cell>0.1</cell><cell>Y</cell><cell>gradient</cell></row><row><cell>GAEA PC-DARTS  ? (Ours)</cell><cell>2.39</cell><cell>2.50 ? 0.06</cell><cell>3.7</cell><cell>0.1</cell><cell>Y</cell><cell>gradient</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*#</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>DARTS (CIFAR-10): Comparison with manually designed networks and those found by SOTA NAS methods, mainly on the DARTS search space. Results grouped by the type of search method: manually designed, full-evaluation NAS, and weight-sharing NAS. All test errors are for models trained with auxiliary towers and cutout (parameter counts exclude auxiliary weights). Test errors we report are averaged over 10 seeds. "-" indicates that the field does not apply while "N/A" indicates unknown. Note that search cost is hardware-dependent; our results used Tesla V100 GPUs.</figDesc><table><row><cell>Test Error</cell><cell>Params</cell><cell>Search Cost</cell><cell>Search</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>DARTS (ImageNet): Comparison with manually designed networks and those found by SOTA NAS methods, mainly on the DARTS search space. Results are grouped by the type of search method: manually designed, full-evaluation NAS, and weight-sharing NAS. All test errors are for models trained with auxiliary towers and cutout but no other modifications, e.g. label smoothing<ref type="bibr" target="#b27">(M?ller et al., 2019)</ref>,AutoAugment (Cubuk  et al., 2019), Swish<ref type="bibr" target="#b34">(Ramachandran et al., 2017)</ref>, squeeze and excite modules<ref type="bibr" target="#b15">(Hu et al., 2018)</ref>, etc. "-" indicates that the field does not apply while "N/A" indicates unknown. Note that search cost is hardware-dependent; our results used Tesla V100 GPUs.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code to obtain these results has been made available in the supplementary material. 2 XNAS code does not implement search and, as with previous efforts(Li et al., 2019, OpenReview), we cannot reproduce results after correspondence with the authors. XNAS's best architecture achieves an average test error of 2.70% under the DARTS evaluation, while GAEA achieves 2.50%. For details see Appendix C.4.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note trains the weight-sharing network with 4 random seeds. However, since PC-DARTS is significantly faster than DARTS, the cost of an additional seed is negligible.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Jeremy Cohen, Jeffrey Li, and Nicholas Roberts for helpful feedback. This work was supported in part by DARPA under cooperative agreements FA875017C0141 and HR0011202000, NSF grants CCF-1535967, CCF-1910321, IIS-1618714, IIS-1705121, IIS-1838017, IIS-1901403, and  IIS-2046613, a Microsoft  Research Faculty Fellowship, a Bloomberg Data Science research grant, an Amazon Research Award, an AWS Machine Learning Research Award, a Facebook Faculty Research Award, funding from Booz Allen Hamilton Inc., a Block Center Grant, a Carnegie Bosch Institute Research Award, and a Two Sigma Fellowship Award. Any opinions, findings and conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA, NSF, or any other funding agency.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 NAS-BENCH-201</head><p>The NAS-Bench-201 benchmark <ref type="bibr" target="#b12">(Dong &amp; Yang, 2020</ref>) evaluates a single search space across 3 datasets: CIFAR-10, CIFAR-100, and a miniature version of ImageNet . ImageNet-16-120 is a downsampled version of ImageNet with 16 ? 16 images and 120 classes for a total of 151.7k training images, 3k validation images, and 3k test images. The authors of <ref type="bibr" target="#b12">Dong &amp; Yang (2020)</ref> evaluated the architecture search performance of multiple weight-sharing methods and traditional hyperparameter optimization methods on all three datasets. According to the results from <ref type="bibr" target="#b12">Dong &amp; Yang (2020)</ref>, GDAS outperformed other weight-sharing methods by a large margin. Hence, we first evaluated the performance of GAEA GDAS on each of the three datasets. Our implementation of GAEA GDAS uses an architecture learning rate of 0.1, which matches the learning rate used for GAEA approaches in the previous two benchmarks. Additionally, we run GAEA GDAS for 150 epochs instead of 250 epochs used for GDAS in the original benchmarked results; this is why the search cost is lower for GAEA GDAS. All other hyperparameter settings are the same. Our results for GAEA GDAS is comparable to the reported results for GDAS on CIFAR-10 and CIFAR-100 but slightly lower on ImageNet-16-120. Compared to our reproduced results for GDAS, GAEA GDAS outperforms GDAS on CIFAR-100 and matches it on CIFAR-10 and ImageNet-16-120.</p><p>Next, to see if we can use GAEA to further improve the performance of weight-sharing methods, we evaluated GAEA DARTS (first order) applied to both the single-level (ERM) and bi-level optimization problems. Again, we used a learning rate of 0.1 and trained GAEA DARTS for 25 epochs on each dataset. The one additional modification we made was to exclude the zero operation, which limits GAEA DARTS to a subset of the search space. To isolate the impact of this modification, we also evaluated first-order DARTS with this modification. Similar to <ref type="bibr" target="#b12">(Dong &amp; Yang, 2020)</ref>, we observe DARTS with this modification to also converge to architectures with nearly all skip connections, resulting in similar performance as that reported in <ref type="bibr" target="#b12">Dong &amp; Yang (2020)</ref>. We present the learning curves of the oracle architecture recommended by DARTS and GAEA DARTS (when excluding zero operation) over the training horizon for 4 different runs in <ref type="figure">Figure 4</ref>. For GAEA GDAS and GAEA DARTS, we train the weight-sharing network with the following hyperparameters: train: scheduler: cosine lr_anneal_cycles: 1 smooth_cross_entropy: false batch_size: 64 learning_rate: 0.025 learning_rate_min: 0.001 momentum: 0.9 weight_decay: 0.0003 init_channels: 16 layers: 5 autoaugment: false cutout: false auxiliary: false auxiliary_weight: 0.4 drop_path_prob: 0 grad_clip: 5</p><p>Surprisingly, we observe single-level optimization to yield better performance than solving the bilevel problem with GAEA DARTS on this search space. In fact, the performance of GAEA DARTS (ERM) not only exceeds that of GDAS, but also outperforms traditional hyperparameter optimization approaches on all three datasets, nearly reaching the optimal accuracy on all three datasets. In contrast, GAEA DARTS (bi-level) outperforms GDAS on CIFAR-100 and ImageNet-16-120 but underperforms slightly on CIFAR-10. The single-level results on this benchmark provides concrete support for our convergence analysis, which only applies to the ERM problem. As noted in Section 4, the search space considered in this benchmark differs from the prior two in that there is no subsequent edge pruning. Additionally, the search space is fairly small with only 3 nodes for which architecture decisions must be made. The success of GAEA DARTS (ERM) on this benchmark indicate the need for a better understanding of when single-level optimization should be used in favor of the default bi-level optimization problem and how the search space impacts the decision.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adaptive stochastic natural gradient method for one-shot neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youhei</forename><surname>Akimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinichi</forename><surname>Shirakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nozomu</forename><surname>Noshinari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kento</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shota</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kouhei</forename><surname>Nishida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mirror descent and nonlinear projected subgradient methods for convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research Letters</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="167" to="175" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Differentiable sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Andrew</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><forename type="middle">M</forename><surname>Bregman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">USSR Computational Mathematics and Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="200" to="217" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Lectures on Some Geometric Aspects of Randomized Online Decision Making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Bubeck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Five miracles of mirror descent</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations</title>
		<meeting>the 7th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Maria</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M</forename><surname>Esperan?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Gabillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">MANAS: Multi-agent neural architecture search. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Progressive differentiable architecture search: Bridging the depth gap between search and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">AutoAugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stochastic block mirror descent methods for nonsmooth and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="856" to="881" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<title level="m">Improved regularization of convolutional neural networks with cutout. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four GPU hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">NAS-Bench-201: Extending the scope of reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Representations</title>
		<meeting>the 8th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bilevel programming for hyperparameter optimization and meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saverio</forename><surname>Salzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Grazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimilano</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<title level="m">MobileNets: Efficient convolutional neural networks for mobile vision applications. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<title level="m">sharpDARTS: Faster and more accurate differentiable architecture search. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An alternative view: When does SGD escape local minima?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bobby</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevksy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Prune and replace NAS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Alexander Laube</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Zell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Machine Learning and Applications</title>
		<meeting>the IEEE International Conference on Machine Learning and Applications</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">StacNAS: Towards stable and consistent differentiable neural architecture search. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingqiu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kechen</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<title level="m">DARTS+: Improved differentiable architecture search with early stopping. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations</title>
		<meeting>the 7th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ShuffleNet V2: Practical guidelines for efficient CNN architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">When does label smoothing help?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">XNAS: Neural architecture search with expert advice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Nayman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Problem Complexity and Method Efficiency in Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkadi</forename><surname>Nemirovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yudin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Nayman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivan</forename><surname>Doveh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">Raja Giryes, and Lihi Zelnik-Manor. ASAP: Architecture search, anneal and prune</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Proximal algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Optimization</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="123" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melody</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">To share or not to share: A comprehensive appraisal of weight-sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Alo Is Pourchot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Ducarouge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sigaud</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Searching for activation functions. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 33rd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Online learning and online convex optimization. Foundations and Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="107" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SNAS: Stochastic neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations</title>
		<meeting>the 7th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">PC-DARTS: Partial channel connections for memory-efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Representations</title>
		<meeting>the 8th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiro</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iawmura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koichi</forename><surname>Kise</surname></persName>
		</author>
		<title level="m">ShakeDrop regularization. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">CARS: Continuous evolution for efficient neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via proximal iterations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Wei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangxing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 34th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">NAS-Bench-101: Towards reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Evaluating the search phase of neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaicheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Sciuto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudiu</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Representations</title>
		<meeting>the 8th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Understanding and robustifying differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arber</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Marrakchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Representations</title>
		<meeting>the 8th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">NAS-Bench-1Shot1: Benchmarking and dissecting oneshot neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arber</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Siems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Representations</title>
		<meeting>the 8th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">On the convergence rate of stochastic mirror descent for nonsmooth nonconvex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niao</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Deeper insights into weight sharing in neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanlu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaming</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multinomial distribution learning for effective neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiawu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
