<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transformer Models for Text Coherence Assessment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Abhishek</surname></persName>
							<email>tushar.abhishek@research.iiit.ac.in</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Information Retrieval and Extraction Lab</orgName>
								<orgName type="institution">IIIT Hyderabad</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daksh</forename><surname>Rawat</surname></persName>
							<email>daksh.rawat@students.iiit.ac.in</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Information Retrieval and Extraction Lab</orgName>
								<orgName type="institution">IIIT Hyderabad</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manish</forename><surname>Gupta</surname></persName>
							<email>manish.gupta@iiit.ac.in</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Information Retrieval and Extraction Lab</orgName>
								<orgName type="institution">IIIT Hyderabad</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasudeva</forename><surname>Varma</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Information Retrieval and Extraction Lab</orgName>
								<orgName type="institution">IIIT Hyderabad</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transformer Models for Text Coherence Assessment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Coherence is an important aspect of text quality and is crucial for ensuring its readability. It is an essential desirable for outputs from text generation systems like summarization, question answering, machine translation, question generation, table-to-text, etc. An automated coherence scoring model is also helpful in essay scoring or providing writing feedback. A large body of previous work has leveraged entity-based methods, syntactic patterns, discourse relations and more recently traditional deep learning architectures for text coherence assessment. We hypothesize that coherence assessment is a cognitively complex task which requires deeper models and can benefit from other related tasks. Accordingly, in this paper, we propose four different Transformerbased architectures for the task: vanilla Transformer, hierarchical Transformer, multi-task learning-based model, and a model with factbased input representation. Our experiments with popular benchmark datasets across multiple domains on four different coherence assessment tasks demonstrate that our models achieve state-of-the-art results outperforming existing models by a good margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Coherence is a crucial metric for text quality analysis. It assimilates how well the sentences are connected and how well the document is organized. Coherent documents have a clear topic transitions that are discussed throughout the text with a smooth flow of concepts, typically in an increasing order of complexity. Ideas are first introduced in preceding sentences and are referred to later in document. Connectives are often used to assist the structure as well as smooth transitions within the document. Overall, coherence leads to better text clarity.</p><p>Coherence is vital for multiple NLP (Natural Language Processing) applications like summarization <ref type="bibr" target="#b1">(Barzilay and Elhadad, 2002;</ref><ref type="bibr" target="#b36">Parveen et al., *</ref> The author also works as a researcher at Microsoft 2016), question answering <ref type="bibr" target="#b40">(Verberne et al., 2007)</ref>, machine translation <ref type="bibr" target="#b43">(Xiong et al., 2019;</ref><ref type="bibr">Mohiuddin et al., 2020)</ref>, question generation <ref type="bibr" target="#b7">(Desai et al., 2018)</ref>, language assessment for essay scoring <ref type="bibr" target="#b6">(Burstein et al., 2010;</ref><ref type="bibr" target="#b38">Somasundaran et al., 2014;</ref><ref type="bibr" target="#b12">Farag et al., 2018)</ref>, story generation <ref type="bibr" target="#b26">(McIntyre and Lapata, 2010)</ref>, readability assessment <ref type="bibr" target="#b37">(Pitler et al., 2010;</ref><ref type="bibr" target="#b32">Muangkammuen et al., 2020)</ref> and other text generation <ref type="bibr" target="#b35">(Park and Kim, 2015;</ref><ref type="bibr" target="#b18">Kiddon et al., 2016;</ref><ref type="bibr" target="#b17">Holtzman et al., 2018)</ref>.</p><p>Many formal theories of coherence <ref type="bibr" target="#b15">(Grosz et al., 1995;</ref><ref type="bibr" target="#b25">Mann and Thompson, 1988;</ref><ref type="bibr" target="#b0">Asher et al., 2003)</ref> have been proposed leading to further development of various coherence models. Based on such theories, multiple text coherence models like entity-grid <ref type="bibr" target="#b2">(Barzilay and Lapata, 2008</ref>) and its extensions have been proposed. Other linguistic approaches for text coherence include coreferences, discourse relations, lexical cohesion, and syntactic features. However, such models are incapable of handling long transitions. Also, feature engineering is decoupled from the prediction task thus limiting model performance. Recently, various models have been proposed which leverage deep learning architectures like convolutional neural networks (CNNs), recurrent neural networks (RNNs), long short-term memory networks (LSTMs). Although, Transformer <ref type="bibr" target="#b39">(Vaswani et al., 2017)</ref> based models have revolutionized the field of NLP, surprisingly, there is no rigorous study which investigates the effectiveness of Transformer architectures for text coherence. In this paper, we study four different kinds of Transformer architectures.</p><p>We propose models for four different coherence assessment tasks: 2-way classification, 3-way classification, sentence ordering, and coherence score prediction. For each of these tasks, we investigate effectiveness of four Transformer architectures: (1) vanilla Transformer models, (2) hierarchical Transformers with two levels, (3) multi-task learning with text coherence assessment as primary task and textual entailment as auxiliary task, and (4) fact based input representation where besides the original document, we also pass extracted facts as input. We assess the extent to which these architectures generalize to different domains and prediction tasks, establishing a new state-of-the-art (SOTA).</p><p>Overall, in this paper, we make the following main contributions. (1) We investigate the effectiveness of four Transformer architectures. (2) We assess the extent to which the information encoded in the network generalizes to multiple domains and four prediction tasks, and demonstrate the effectiveness of our approach not only on standard sentence ordering tasks but also on more realistic task like predicting coherence of varying degrees in people's everyday writings. (3) Experiments on popular benchmark datasets (GCDC and WSJ) indicate that our proposed methods establish SOTA across multiple (task, dataset) combinations. On GCDC, our best method beats the existing SOTA by: 0.15 F 0.5 on the 2-way classification task, 3.00 absolute percent points accuracy 3-way classification task, 6.40 absolute percent points accuracy on sentence ordering task and 0.18 Spearman correlation on coherence score prediction resp. On WSJ, our best method provides an improvement of 1.28 absolute percent points on sentence ordering task. We make the code and dataset publicly available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Entity-Grid based methods: Discourse coherence has been studied widely using both deep learning as well as non-deep learning models. <ref type="bibr" target="#b2">(Barzilay and Lapata, 2008)</ref> proposed the entity grid model, which is based on Centering Theory <ref type="bibr" target="#b15">(Grosz et al., 1995)</ref>. It captures the distribution of discourse entities and transition of grammatical roles (subject, object, neither) across the sentences. Several extensions were proposed by utilising entity specific features <ref type="bibr" target="#b9">(Elsner and Charniak, 2008)</ref>, modifying ranking scheme <ref type="bibr" target="#b13">(Feng and Hirst, 2012)</ref> or transforming problem into bipartite graph <ref type="bibr" target="#b27">(Mesgar and Strube, 2015)</ref>. The entity grid method as well as extensions suffer from two main drawbacks: (1) they use discrete representation for grammatical roles and features, which prevents the model from considering sufficiently long transitions due to the curse of dimensionality problem. (2) Feature engineering is decoupled from the prediction task, 1 https://tinyurl.com/mry9464u which limits the model's capacity to learn taskspecific features.</p><p>Other feature engineering methods: Besides entity grid, other linguistic approaches for text coherence include coreferences, discourse relations, lexical cohesion, and syntactic features. <ref type="bibr" target="#b9">(Elsner and Charniak, 2008)</ref> proposed a maximum-entropy based discourse-new classifier that classifies mentions of all referring expression as first mention (discourse-new) or subsequent (discourse-old) mentions. <ref type="bibr" target="#b24">(Louis and Nenkova, 2012)</ref> proposed a coherence model based on syntactic patterns by assuming that sentences in a coherent discourse should share the same structural syntactic patterns. Other approaches have used syntactic patterns <ref type="bibr" target="#b24">(Louis and Nenkova, 2012)</ref>, lexical cohesion <ref type="bibr" target="#b31">(Morris and Hirst, 1991;</ref><ref type="bibr" target="#b38">Somasundaran et al., 2014)</ref> or capturing topic shifts via Hidden Markov Models <ref type="bibr" target="#b3">(Barzilay and Lee, 2004)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep learning methods:</head><p>Recently, multiple deep learning approaches have been proposed. <ref type="bibr" target="#b21">(Li and Hovy, 2014)</ref> propose a neural framework to compute the coherence score of a document by estimating a coherence probability for each clique of L sentences. <ref type="bibr" target="#b22">(Li and Jurafsky, 2017)</ref> propose generative methods to capture global topic information. <ref type="bibr" target="#b34">(Nguyen and Joty, 2017;</ref><ref type="bibr" target="#b28">Mohiuddin et al., 2018)</ref> transform entity-grid based methods into deep learning versions that obtains better result than traditional counterparts. <ref type="bibr" target="#b11">(Farag and Yannakoudakis, 2019)</ref> propose a hierarchical attention model with multi-task learning objective. <ref type="bibr" target="#b30">(Xu et al., 2019;</ref><ref type="bibr" target="#b30">Moon et al., 2019)</ref> show that modeling local coherence with discriminative models could capture both the local and the global contexts of coherence. <ref type="bibr" target="#b16">(Guz et al., 2020)</ref> propose an RST-Recursive model, which takes advantage of the text's RST features. <ref type="bibr">(Farag et al., 2020)</ref> extend some of the previous discriminative models using BERT (Bidirectional Encoder Representations from Transformers) <ref type="bibr" target="#b8">(Devlin et al., 2018)</ref> embeddings. Further details on some of the above mentioned methods are provided in Section 4.2.</p><p>Except for the last work, none of the previous methods have investigated the use of Transformer based models for text coherence modeling. Even <ref type="bibr">(Farag et al., 2020)</ref> simply experiment with BERT embeddings (they do not train the Transformer architecture), and that too only on one task with synthetic data. We experiment with multiple Transformer architectures. Our experiments establish a new state of the art outperforming these models by a good margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Text Coherence Assessment Models</head><p>In this section, we first discuss details of the text assessment tasks, and then present the proposed Transformer-based architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Text Coherence Assessment Tasks</head><p>We propose models for four different coherence assessment tasks: (i) 3-way classification, (ii) 2-way classification, (iii) sentence ordering, (iv) coherence score prediction. 3-way classification: Given a document, the task is to classify it into one of the three different labels (high, medium and low) which denotes the textual coherence level of the given document. 2-way classification: This task is similar to the 3way classification task, except that there are only two labels: non-coherent or other. Sentence ordering: For this task, every original document is assumed to be coherent. 20 random permutations (other than the original document) of sentences in the document are then obtained and labeled as negative. The dataset is created by pairing the original document and the permuted document. The task is to rank the original document higher than the permuted one in terms of coherence. Coherence score prediction: A 3-point coherence score might not reflect the range of coherence that actually exists in the data. Hence, we also experiment with a coherence score prediction task. It is a regression task where, given a document, the task is to predict the degree of the textual coherence.</p><p>Note that manual judgment labels are required for 3-way classification, 2-way classification and coherence score prediction tasks. Dataset for the sentence ordering task can be generated in a synthetic manner. Also note that for all tasks, the input is a single document except for the sentence ordering task where the input is a pair of documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proposed Transformer based architectures</head><p>Transformer <ref type="formula">(</ref> (1) vanilla Transformer models, (2) hierarchical Transformers with two levels, (3) multi-task learning with text coherence assessment and textual entailment, and (4) fact based input representation where besides the original document, we also pass extracted facts as input. We discuss details of each of these methods in the following. For each of these architectures 2 , we use RoBERTa as the basic Transformer model; we tried using BERT as well, but found RoBERTa to be better across all experiments. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the basic architecture of each of these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Vanilla Transformer</head><p>The simplest way of performing text coherence tasks is to feed the input text to a Transformer model as shown in <ref type="figure" target="#fig_0">Fig. 1(A)</ref>. We choose RoBERTa <ref type="bibr" target="#b23">(Liu et al., 2019)</ref> which is an extension of BERT with changes to the pretraining procedure. The modifications include: (1) training the model longer, with bigger batches, over more data (2) removing the next sentence prediction objective (3) training on longer sequences, and (4) dynamically changing the masking pattern applied to the training data. RoBERTa has been shown to perform very well across many NLP tasks. RoBERTa however cannot handle very long sequences. Thus, for long sequence tasks (especially sentence ordering), we resort to Longformer <ref type="bibr" target="#b4">(Beltagy et al., 2020)</ref>. Longformer has an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Hierarchical Transformer</head><p>In our hierarchical Transformer architecture, we leverage a two-level hierarchical RoBERTa model for obtaining document representation as shown in <ref type="figure" target="#fig_0">Fig. 1(B)</ref>. To obtain the representation of document D, we use two encoders: a sentence encoder to transform each sentence in D to a vector and a document encoder to learn sentence representations given their surrounding sentences as context. Both the sentence encoder and document encoder are based on the Transformer encoder. The sentence encoder takes individual sentences (separated by SEP token) as input. The outputs from each position of the last layer of the sentence encoder are pooled (separately for each sentence) using various pooling strategies like min, max, mean, sum, attention or none. In the "none" pooling strategy, the representation for the last sentence token is used to represent the sentence. Each sentence in the document is encoded by the sentence encoder and passed as input to the document encoder. Finally, the CLS token's transformed representation from the document encoder is fed to a dense layer with ReLU activation which is then connected to a taskspecific output layer. The two encoders can either be initialized randomly or using transfer learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">MTL with Transformers</head><p>When multiple related prediction tasks need to be performed, multi-task learning (MTL) has been found to be very effective. Hard parameter sharing is the most commonly used approach to MTL in neural networks. It is generally applied by sharing the hidden layers between all tasks, while keeping several task-specific output layers. Given a pair of sentences, the textual entailment task aims to predict whether the second sentence (hypothesis) is an entailment with respect to the first one (premise) or not. Textual entailment and text coherence assessment are very similar tasks. As shown in <ref type="figure" target="#fig_0">Fig. 1(C)</ref>, we share the Transformer encoder weights across the two tasks. Task specific layers for each task are conditioned on the shared Transformer encoder. For the sentence entailment task, we form input by concatenating the hypothesis and premise with sentence separator token SEP placed between them. For both the tasks (coherence and entailment), we use a fully-connected layer with ReLU, and then a softmax output layer.</p><p>The final loss is computed as a sum of the individual losses for the two tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Fact-aware Transformer</head><p>We leverage MinIE, an Open Information Extraction system <ref type="bibr" target="#b14">(Gashteovski et al., 2017)</ref> to generate a set of facts for each sentence. Open Information Extraction systems aim to exploit linguistic information including dependency relations in sentences to extract facts in a knowledge-agnostic manner. A fact is essentially an ordered 3-tuple (subject, verb and object) extracted from a particular sentence. A single sentence can produce multiple facts. Consider the sentence "They are trying to determine whether it was used to attack Steenkamp, if she used the bat in self-defense." Two facts that can be extracted from this sentence are ("it", "was used to attack", "Steenkamp") and ("she", "used bat in", "self-defense"). Each of the three components of a fact triple can contain multiple words.</p><p>Let the length of sequence of tokens present in document D be denoted by L and the number of distinct facts obtained from the document using MinIE system be denoted by M . As shown in <ref type="figure" target="#fig_0">Fig. 1(D)</ref>, our fact-aware Transformer method consists of three kinds of encoders: a document encoder, M fact encoders (F E 1 , F E 2 . . . F E M ) and a fact-aware document encoder. Each encoder uses a vanilla Transformer model. Document encoder and all the fact encoders share weights. Document encoder encodes the document expressed using standard sub-word tokens. Fact encoder F E i encodes the i th fact using the (subject, verb, object) for the fact as input delimited by SEP. Document encoder produces document representation T , while each of the fact encoders F E i produce fact representation f i . These fact representations and the document representation form the input for the fact-aware document encoder. Finally, the representation from the last layer of the fact-aware encoder corresponding to the CLS token, is connected to a fully-connected layer with ReLU, and then a softmax output layer.</p><p>For all tasks except sentence ordering, we pass the document representation obtained from proposed models to a dense layer with ReLU activation which is then connected to a task-specific output layer, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. For sentence ordering task, we apply Siamese network <ref type="bibr" target="#b5">(Bromley et al., 1993)</ref> or twin neural network approach with each of our four proposed architectures, as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. In this approach, the coherent and incoher- ent document representation is obtained by using any of the four document encoders (from <ref type="figure" target="#fig_0">Fig. 1)</ref>. The document encoder for the coherent as well as the incoherent document, share weights. Further, both the document representations are separately connected to a dense layer with shared weights. The outputs of the dense layers are used to calculate the margin ranking loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We experiment with two popular benchmark datasets: Wall Street Journal (WSJ) and Grammarly Corpus of Discourse Coherence (GCDC). GCDC is a real dataset while WSJ is a synthetic dataset. We use the Recognizing Textual Entailment (RTE) dataset  for training an auxiliary task for our MTL model (2490 train and 277 validation instances) for experiments on GCDC. For WSJ, we found MTL to perform better when we use the Multi-Genre Natural Language Inference (MNLI) dataset <ref type="bibr" target="#b42">(Williams et al., 2018)</ref> for training the auxiliary task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">WSJ</head><p>The WSJ portion of the Penn Treebank <ref type="bibr" target="#b9">(Elsner and Charniak, 2008;</ref><ref type="bibr" target="#b34">Nguyen and Joty, 2017)</ref> is one of the most popular datasets for the sentence ordering task. It contains long articles without any constraint on style. Following previous work <ref type="bibr" target="#b2">(Barzilay and Lapata, 2008;</ref><ref type="bibr" target="#b34">Nguyen and Joty, 2017)</ref>, we also use the sections 00 -13 for training and 14 -24 for testing (documents consisting of one sentence are removed). We create 20 permutations per document, making sure to exclude duplicates or versions that happen to have the same ordering of sentences as the original article. We present the basic statistics of the dataset in <ref type="table" target="#tab_1">Table 1</ref>   calculates the fraction of correct pairwise rankings in the test data (i.e., the original coherent text should be ranked higher than its permuted noncoherent counterpart).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">GCDC</head><p>The GCDC dataset contains emails and reviews written with varying degrees of proficiency and care <ref type="bibr" target="#b20">(Lai and Tetreault, 2018)</ref>. The WSJ dataset contains documents that have been professionally written and extensively edited. In contrast to WSJ, the GCDC dataset contains writing from nonprofessional writers in everyday contexts. Rather than using permuted or machine generated texts as examples of low coherence, GCDC has real sentences in which people try but fail to write coherently. GCDC is a corpus that contains texts from four domains, covering a range of coherence, each annotated with a document-level coherence score. Specifically, the dataset contains texts from four domains: Yahoo online forum posts, emails from Hillary Clinton's office, emails from Enron and Yelp business reviews. We present the basic statistics of the dataset in <ref type="table" target="#tab_2">Table 2</ref>. 3-way classification: For each of these domains, a fixed split of 1000 and 200 was used for train and test respectively as specified in <ref type="bibr" target="#b20">(Lai and Tetreault, 2018)</ref>. Of the 1000 documents, we use 200 documents for validation and remaining 800 for training. For our experiments, we use the consensus rating of the expert scores as calculated by <ref type="bibr" target="#b20">(Lai and Tetreault, 2018)</ref>, and train our models for all the four tasks. To evaluate model performance, we use 3-way classification accuracy.  <ref type="table">Table 3</ref>: Basic statistics of the GCDC dataset for the sentence ordering task. We used the same train and test split as specified in <ref type="bibr" target="#b20">[Lai and Tetreault, 2018</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2-way classification:</head><p>A text is labeled as noncoherent if at least two expert annotators judged the text to be low coherence, or labeled as "other" otherwise. For 2-way classification task, we report the F 0.5 score of low coherence class where precision is emphasized twice as much as recall. This is in line with evaluation standards in writing feedback applications <ref type="bibr" target="#b33">(Ng et al., 2014)</ref> and also in previous text coherence studies <ref type="bibr" target="#b20">(Lai and Tetreault, 2018)</ref>. Sentence ordering: Similar to <ref type="bibr" target="#b20">(Lai and Tetreault, 2018)</ref> we created the synthetic dataset for this task in the same way as for WSJ. Since the sentence ordering task assumes well-formed texts, we use only the high coherence texts. Statistics of the GCDC dataset wrt this task are shown in <ref type="table">Table 3</ref>. We use PRA as the metric for this task as well. Coherence Score Prediction: For this task, gold score is the mean of 3 expert rater judgments (low coherence = 1, medium = 2, high = 3). We report Spearman's rank correlation coefficient between the gold scores and the predicted coherence scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We experiment with the following baselines. While Flesch-Kincaid grade level <ref type="bibr" target="#b19">(Kincaid et al., 1975)</ref> is a readability measure, previous work has treated readability and text coherence as overlapping tasks <ref type="bibr" target="#b2">(Barzilay and Lapata, 2008;</ref><ref type="bibr" target="#b27">Mesgar and Strube, 2015</ref>  <ref type="bibr" target="#b21">(Li and Hovy, 2014)</ref>, sentences are encoded with a recurrent or recursive layer and a filter of weights is applied over each window of sentence vectors to extract scores that are aggregated to calculate overall document coherence score. Paragraph sequence (PARSEQ) <ref type="bibr" target="#b20">(Lai and Tetreault, 2018)</ref> contains three stacked LSTMs to represent sentence, paragraph and document. Hierachical LSTM <ref type="bibr" target="#b11">(Farag and Yannakoudakis, 2019)</ref> is very similar to PARSEQ, but with attention and uses BiLSTMs. Coh+GR <ref type="bibr" target="#b11">(Farag and Yannakoudakis, 2019)</ref> extends Hierachical LSTM by training it to predict word-level labels indicating the predicted grammatical role (GR) type at the bottom layers of the network, along with the document-level coherence score. Coh+SOX <ref type="bibr" target="#b11">(Farag and Yannakoudakis, 2019)</ref> is same as Coh+GR where, for each word, we only predict subject (S), object (O) and 'other' (X) roles. Seq2Seq (Li and Jurafsky, 2017) consists of two LSTM generative language models and uses the difference between conditional log likelihood of a sentence given its preceding/succeeding context, and the marginal log likelihood of the current/next sentence to assess coherence. Local Coherence Discriminator (LCD-G) <ref type="bibr" target="#b30">(Xu et al., 2019)</ref> uses averaged GloVe embeddings as the sentence representations. A representation for two consecutive sentences is then computed by concatenating the output of a set of linear transformations applied to the two sentences. This is fed to a dense layer and used to predict a local coherence score. LCD-L <ref type="bibr" target="#b30">(Xu et al., 2019)</ref> is similar to LCD-G, but applies max-pooling on the hidden state of the language model to get the sentence representation.</p><p>Coh+GR_BERT <ref type="bibr">(Farag et al., 2020)</ref> is similar to Coh+GR, except that BERT embeddings are used instead of GloVe embeddings as input to BiL-STMs. LCD_BERT <ref type="bibr">(Farag et al., 2020)</ref> is similar to LCD-G but uses averaged BERT (instead of GloVe) embeddings as the sentence representations.</p><p>Unified (Moon et al., 2019) uses a combination of LSTMs and CNNs. Sentence averaging (SEN-TAVG) <ref type="bibr" target="#b20">(Lai and Tetreault, 2018)</ref> ignores sentence order and is similar to 2-level (sentence and document) hierarchical LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Settings</head><p>All experiments were run on a machine equipped with four 32GB V100 GPUs. For all Transformer models, we use 12-layer models, and embedding layer was frozen except for the sentence ordering task on WSJ. For Hierarchical Transformer models, we used pretrained model for the sentence en-coder and a randomly initialized RoBERTa for document encoder. For fact-aware Transformer models, we used pretrained model for the fact encoders and document encoder, and a randomly initialized RoBERTa for fact-aware document encoder. For all experiments we run 10 epochs, weight decay of 0.01 and use a dropout of 0.1. We use Adam optimizer for all the experiments, except for WSJ experiments where we use AdamW. For all the baseline models, we report results from their original papers. For all of our models, the reported results are the mean of 10 runs with different random seeds. When we use margin ranking loss, where margin is set to 1. For MTL based Transformers, categorical cross entropy loss was used for the auxiliary task. For Longformer, we fixed max sequence length to 2048. For RoBERTa, we fixed it to 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>Tables 4 to 8 show the results across all the four text coherence tasks for WSJ and GCDC datasets. Broadly we observe that our proposed Transformerbased methods significantly outperform baselines, establishing a new SOTA across all tasks. We also experimented with a fact-aware hierarchical MTL model (or a combined model) but observed that the combination does not lead to any better results. Sentence ordering results: <ref type="table" target="#tab_6">Tables 4 and 5</ref> show results for the sentence ordering task for WSJ and GCDC datasets respectively. We make the following observations for WSJ results <ref type="table" target="#tab_6">(Table 4</ref>): (1) Fact aware transformer outperforms hierarchical model as it can incorporate the factual information flow (subject in discourse) in addition to textual information which helps it to correctly determine the coherent sentences.</p><p>(2) MTL with Transformers outperforms other variants as the auxiliary task helps in better generalization over test set.  We make the following observations for GCDC results <ref type="table" target="#tab_8">(Table 5</ref>): (1) RTE and coherence tasks help improve accuracy of each other and this leads to best results using MTL with Transformers. (2) Among the baselines, LCD models perform better with respect to other baselines. This shows that local coherence is a good indicator of global document coherence. <ref type="formula">(3)</ref> LCD_BERT is better than all other LCD models indicating that using just Transformer embeddings can also be helpful. (4) Models built using training data across all the four domains lead to better results compared to models trained on individual domain data. (5) While baseline methods work reasonably well on other three domains, their accuracy on Yahoo is very low. Our method has massive gains on Yahoo compared to baselines.  3-way classification results: <ref type="table" target="#tab_10">Table 6</ref> shows 3-way classification results on GCDC. We make the following observations: (1) While MTL with Transformers leads to best results on average across all domains, there is no unique winner for individual domains. Still, our proposed methods outperformed the baselines.</p><p>(2) Re-ordering of associated facts helps the fact-aware Transformer model in identifying the change in sentence order (in WSJ synthetic data) but seems ineffective on realistic (GCDC) documents.</p><p>(3) Out of the three gold coherence labels (low, medium, high) all the models have difficulty in correctly classifying documents of medium level coherence, which can be attributed to the smaller number of training examples for that particular class. Coherence Score Prediction results: <ref type="table">Table 7</ref> shows Coherence Score Prediction results on GCDC. We observe that <ref type="formula">(1)</ref>    <ref type="table">Table 7</ref>: Coherence Score Spearman correlation Prediction results on GCDC. "(all)" indicates that training data across all four domains was used. <ref type="table">Table 8</ref> shows 2-way classification results on GCDC. (1) Vanilla Transformer trained on individual domain datasets work best on average. For Yahoo and Yelp, training on all domains is beneficial. (2) On average, our best method leads to F 0.5 of 0.506 as compared to the existing SOTA (SENTAVG) of 0.351. Thus, the improvements are significant.  <ref type="table">Table 8</ref>: 2-way classification F 0.5 results on GCDC. "(all)" indicates that training data across all four domains was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2-way classification results:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we investigate the efficacy of four different transformer architectures: vanilla, hierarchical, MTL with transformer, and fact-aware transformer on four different coherence assessment tasks: 2-way classification, 3-way classification, sentence ordering, and coherence score prediction. We observe that all the transformer based approaches outperforms existing models. It indicate that deeper models like transformer captures textual coherence signal well. Our work also shows that inductive transfer between MTL-based transformer models' tasks: textual coherence assessment and textual entailment, helps it achieve the best performance for most coherence assessment tasks.</p><p>In the future, we plan to extend this work to evaluate the text coherence in an open domain setting. We also plan to evaluate the impact of coherence estimation on downstream applications like essay evaluation and summarization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc><ref type="bibr" target="#b39">Vaswani et al., 2017</ref>)-based deep learning models like BERT<ref type="bibr" target="#b8">(Devlin et al., 2018)</ref>, RoBERTa<ref type="bibr" target="#b23">(Liu et al., 2019)</ref>, etc. have transformed the field of NLP in the past two years. They have been shown to be effective across a large number of NLP tasks. For each of these tasks mentioned in the previous subsection, we investigate effective-Architectures of various proposed methods:(A) Vanilla Transformers (B) Hierarchical Transformers (C) MTL with Transformers (D) Fact-aware Transformer ness of four Transformer architectures:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of Siamese neural approach applied for sentence ordering task. Document encoder weights are shared. Dense layer weights are also shared.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Basic statistics of the WSJ dataset. #Docs represents the number of original articles and #Synthetic Docs represents the number of original articles and their permuted versions.</figDesc><table><row><cell></cell><cell cols="2">#Docs Avg</cell><cell>Avg</cell><cell>Vocab</cell><cell>L, M, H In-</cell><cell>NC(%)</cell></row><row><cell></cell><cell></cell><cell>#Words</cell><cell>#Sents</cell><cell>Size</cell><cell>stances (%)</cell><cell></cell></row><row><cell>Yahoo</cell><cell>1200</cell><cell>162.1</cell><cell>7.5</cell><cell>13235</cell><cell>46.6,17.4,37.0</cell><cell>30.0</cell></row><row><cell cols="2">Clinton 1200</cell><cell>189.0</cell><cell>6.6</cell><cell>15564</cell><cell>28.2,20.6,51.2</cell><cell>16.6</cell></row><row><cell>Enron</cell><cell>1200</cell><cell>196.2</cell><cell>7.7</cell><cell>13694</cell><cell>29.9,19.4,50.7</cell><cell>18.4</cell></row><row><cell>Yelp</cell><cell>1200</cell><cell>183.1</cell><cell>7.5</cell><cell>12201</cell><cell>27.1,21.8,51.1</cell><cell>14.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Basic statistics of the GCDC dataset. For</cell></row><row><cell>each of these domains, a fixed split of 1000 and 200</cell></row><row><cell>was used for train and test respectively as specified in</cell></row><row><cell>[Lai and Tetreault, 2018]. NC=Non-coherent, L=low,</cell></row><row><cell>M=medium, H=High.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Sentence ordering task PRA results on WSJ</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Sentence ordering PRA results on GCDC. "(all)" indicates that training data across all four domains was used.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6</head><label>6</label><figDesc>Hierarchical Transformer (all) 0.671 0.673 0.587 0.499 0.608 Vanilla Transformer (all) 0.690 0.689 0.599 0.493 0.618 MTL with Transformers (all) 0.684 0.709 0.598 0.495 0.622</figDesc><table><row><cell></cell><cell cols="2">: 3-way classification accuracy results on</cell></row><row><cell cols="3">GCDC. "(all)" indicates that training data across all</cell></row><row><cell cols="2">four domains was used.</cell><cell></cell></row><row><cell cols="3">Transformers helps them outperform the PARSEQ</cell></row><row><cell cols="2">model.</cell><cell></cell></row><row><cell></cell><cell>Model</cell><cell>Yahoo Clinton Enron Yelp Average</cell></row><row><cell>Baselines</cell><cell>EGRID CNN-Egrid Flesch-Kincaid grade level SENTAVG</cell><cell>0.110 0.146 0.168 0.121 0.136 0.204 0.251 0.258 0.104 0.204 0.089 0.323 0.244 0.200 0.214 0.466 0.505 0.438 0.311 0.430</cell></row><row><cell></cell><cell>PARSEQ</cell><cell>0.519 0.448 0.454 0.329 0.438</cell></row><row><cell></cell><cell>Fact-aware Transformer</cell><cell>0.638 0.610 0.474 0.426 0.537</cell></row><row><cell></cell><cell>Hierarchical Transformer</cell><cell>0.662 0.599 0.512 0.452 0.556</cell></row><row><cell>Ours</cell><cell>MTL with Transformers Vanilla Transformer</cell><cell>0.655 0.666 0.561 0.468 0.588 0.668 0.653 0.560 0.484 0.591</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">except for WSJ where we use Longformer. Note that Longformer has been pretrained from the RoBERTa checkpoint.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Logics of conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Asher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">Michael</forename><surname>Asher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lascarides</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inferring strategies for sentence ordering in multidocument news summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noemie</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="35" to="55" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling local coherence: An entity-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COL-ING</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Catching the drift: Probabilistic content models, with applications to generation and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<idno>cs/0405039</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Signature verification using a &quot;siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roopak</forename><surname>S?ckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="669" to="688" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using entity-based features to model coherence in student essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slava</forename><surname>Andreyev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="681" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generating questions for reading comprehension using coherence relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takshak</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parag</forename><surname>Dakle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on NLP Techniques for Educational Applications</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Coreference-inspired coherence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Elsner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="41" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youmna</forename><surname>Farag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Valvoda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.06306</idno>
		<title level="m">Helen Yannakoudakis, and Ted Briscoe. 2020. Analyzing neural discourse coherence models</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multi-task learning for coherence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youmna</forename><surname>Farag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02427</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Neural automated essay scoring and coherence modeling for adversarially crafted input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youmna</forename><surname>Farag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06898</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Extending the entity-based coherence model with multiple ranks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanessa</forename><surname>Wei Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="315" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Minie: Minimizing facts in open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiril</forename><surname>Gashteovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciano</forename><forename type="middle">Del</forename><surname>Corro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2620" to="2630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Centering: a framework for modeling the local coherence of discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barbara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Grosz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind K</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COLING</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="225" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorii</forename><surname>Guz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Bateni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darius</forename><surname>Muglich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14463</idno>
		<title level="m">Neural rst-based evaluation of discourse coherence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06087</idno>
		<title level="m">Learning to write with cooperative discriminators</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Globally coherent text generation with neural checklist models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chlo?</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="329" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert P Fishburne</forename><surname>Peter Kincaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">L</forename><surname>Jr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brad</forename><forename type="middle">S</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chissom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
		<respStmt>
			<orgName>Naval Technical Training Command Millington TN Research Branch</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04993</idno>
		<title level="m">Discourse coherence in the wild: A dataset, evaluation and methods</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A model of coherence based on distributed sentence representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2039" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural net models of open-domain discourse coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="198" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A coherence model based on syntactic patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1157" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rhetorical structure theory: Toward a functional theory of text organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="281" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Plot induction and evolutionary search for story generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Mcintyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1562" to="1572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graphbased coherence modeling for assessing readability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Mesgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint conf. on lexical and computational semantics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="309" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Coherence modeling of asynchronous conversations: A neural entity grid approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tasnim</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat Tien</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="558" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tasnim</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prathyusha</forename><surname>Jwalapuram</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14626</idno>
		<title level="m">Xiang Lin, and Shafiq Joty. 2020. Coheval: Benchmarking coherence models</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A unified neural coherence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Han Cheol Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Tasnim Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2262" to="2272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Lexical cohesion computed by thesaural relations as an indicator of the structure of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COLING</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="48" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A neural local coherence analysis model for clarity text scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panitan</forename><surname>Muangkammuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumiyo</forename><surname>Fukumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyi</forename><surname>Kanda Runapongsa Saikaew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2138" to="2143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The conll-2014 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">Hendy</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Comp. N. L. Learning: Shared Task</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A neural local coherence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Dat Tien Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1320" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Expressing an image stream with a sequence of natural sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cesc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="73" to="81" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generating coherent summaries of scientific articles using coherence patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daraksha</forename><surname>Parveen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Mesgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="772" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Automatic evaluation of linguistic quality in multidocument summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="544" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Lexical chaining for measuring discourse coherence quality in test-taker essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swapna</forename><surname>Somasundaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="950" to="961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Evaluating discoursebased answer extraction for why-question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzan</forename><surname>Verberne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lou</forename><surname>Boves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelleke</forename><surname>Oostdijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter-Arno</forename><surname>Coppen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="735" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3266" to="3280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Modeling coherence for discourse neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7338" to="7345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamidreza</forename><surname>Saghir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Sung</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Long</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11912</idno>
		<title level="m">Avishek Joey Bose, Yanshuai Cao, and Jackie Chi Kit Cheung. 2019. A cross-domain transferable neural coherence model</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
