<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ASSANet: An Anisotropic Separable Set Abstraction for Efficient Point Cloud Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Guocheng</surname></persName>
							<email>guocheng.qian@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abed</forename><surname>Hasan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kader</forename><surname>Al</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Hammoud</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
							<email>bernard.ghanem@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ASSANet: An Anisotropic Separable Set Abstraction for Efficient Point Cloud Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Access to 3D point cloud representations has been widely facilitated by LiDAR sensors embedded in various mobile devices. This has led to an emerging need for fast and accurate point cloud processing techniques. In this paper, we revisit and dive deeper into PointNet++, one of the most influential yet under-explored networks, and develop faster and more accurate variants of the model. We first present a novel Separable Set Abstraction (SA) module that disentangles the vanilla SA module used in PointNet++ into two separate learning stages: (1) learning channel correlation and (2) learning spatial correlation. The Separable SA module is significantly faster than the vanilla version, yet it achieves comparable performance. We then introduce a new Anisotropic Reduction function into our Separable SA module and propose an Anisotropic Separable SA (ASSA) module that substantially increases the network's accuracy. We later replace the vanilla SA modules in PointNet++ with the proposed ASSA module, and denote the modified network as ASSANet. Extensive experiments on point cloud classification, semantic segmentation, and part segmentation show that ASSANet outperforms PointNet++ and other methods, achieving much higher accuracy and faster speeds. In particular, ASSANet outperforms PointNet++ by 7.4 mIoU on S3DIS Area 5, while maintaining 1.6? faster inference speed on a single NVIDIA 2080Ti GPU. Our scaled ASSANet variant achieves 66.8 mIoU and outperforms KPConv, while being more than 54? faster.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Among the various 3D object representations, point clouds have been surging in popularity, becoming one of the most fundamental 3D representations. This popularity stems from the increased availability of 3D sensors, like LiDAR, which produce point clouds as their raw output. The growing presence of point cloud data has been accompanied by the development of many 3D deep learning methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b21">22]</ref>. Even though these methods achieve impressive performance, they are generally computationally expensive <ref type="figure">(Figure 1</ref>). With the integration of LiDAR sensors into hardware-limited devices, such as mobile devices and AR headsets, interest in efficient models for point cloud processing has grown significantly. Given the limited computational power of mobile devices and embedded systems, the design of mobile-friendly point cloud-based algorithms should not only focus on providing good accuracy, but also on maintaining high computational efficiency.</p><p>When processing point cloud data, one can always opt to convert the data into representations that are more accessible to deep learning frameworks. Popular options are multi-view methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b41">42]</ref> and voxel-based methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b46">47]</ref>. Converting to these representations generally requires additional  <ref type="figure">Figure 1</ref>: Tradeoffs between accuracy (mIoU on S3DIS Area-5) and inference speed (instances/second). Speed is reported as the mean value of 200 runs on a single GTX 2080Ti GPU. The proposed ASSANet scaled with different widths and depths shown in outperform the state-of-the-art methods in with better accuracies and faster speeds. Refer to Section 5.2 for details. computation and memory, and can lead to geometric information loss <ref type="bibr" target="#b22">[23]</ref>. It is therefore more desirable to operate directly on point clouds. To that extent, we are currently witnessing a surge in point-based methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b21">22]</ref>. The first of such methods was introduced by Qi et al. through the seminal PointNet <ref type="bibr" target="#b26">[27]</ref> architecture. PointNet operates directly on point clouds, without the need for an intermediate representation. Despite its efficiency, PointNet merely learns per-point features individually and discards local information, which restrains its performance. As a variant of PointNet, PointNet++ <ref type="bibr" target="#b27">[28]</ref> presents a novel Set Abstraction module that sub-samples the point cloud, groups the neighborhood, extracts local information via a set of multi-layer perceptrons (MLPs), and then aggregates the local information by a reduction layer (i.e. pooling). <ref type="figure">Figure 1</ref> shows how PointNet++ outperforms the pioneering PointNet <ref type="bibr" target="#b26">[27]</ref> by a large margin. PointNet++ also obtains better accuracy than the graph-based method DeepGCN <ref type="bibr" target="#b18">[19]</ref>, and does so with a 100? speed gain. PointNet++ provided a good balance between accuracy and efficiency, and was therefore widely utilized in various tasks like normal estimation <ref type="bibr" target="#b7">[8]</ref>, segmentation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b13">14]</ref>, and object detection <ref type="bibr" target="#b31">[32]</ref>. After PointNet++, graph-based <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b18">19]</ref>, pseudo-grid based <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b37">38]</ref> and adaptive weight-based <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b43">44]</ref>, became the state-of-the-art in point cloud tasks. As shown in <ref type="figure">Figure  1</ref>, nearly all of these methods improve performance at the cost of speed. In this work, we focus on designing point cloud networks that are both fast and accurate. Inspired by its success, both in terms of the accuracy-speed balance and its wide adoption, we take a deep dive into PointNet++. We conduct extensive analysis of its architectural design (Section 3.1) and latency decomposition ( <ref type="figure">Figure  2</ref>). Interestingly, we demonstrate that both its efficiency and accuracy can be improved sharply by minimal modifications to the architecture. These modifications lead to a new architecture design that is faster and more accurate than currently available point methods (shown in in <ref type="figure">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions. (1)</head><p>We demonstrate that the MLPs performed on the neighborhood features in the Set Abstraction (SA) module of PointNet++ reduce the inference speed. We introduce a new separable SA module that processes on point features directly allowing for a significant improvement in inference speed. <ref type="bibr" target="#b1">(2)</ref> We discover that all operations for processing neighbors in the SA module are isotropic which limits the performance (accuracy wise). We present a novel Anisotropic Reduction layer that treats each neighbor differently. We then insert Anisotropic Reduction into our Separable SA and propose the Anisotropic Separable Set Abstraction (ASSA) module that greatly increases accuracy.</p><p>(3) We present ASSANet by replace the vanilla SA in PointNet++ with the proposed ASSA. ASSANet shows a much higher accuracy and a faster speed compared to PointNet++ and previous methods on various tasks (point cloud classification, semantic segmentation, and part segmentation). We further study two regimes for up-scaling ASSANet. As shown in <ref type="figure">Figure 1</ref>, our scaled ASSANet outperforms the previous state-of-the-art with a much faster inference speed. In particular, scaled ASSANet achieves better accuracy than the graph-based method DeepGCN <ref type="bibr" target="#b18">[19]</ref> with an increase in speed of 294?, the pseudo grid-based method KPConv <ref type="bibr" target="#b37">[38]</ref> (54? faster), the adaptive weight-based method PosPool*(S) <ref type="bibr" target="#b21">[22]</ref> (9? faster), and the efficient 3D method PVCNN <ref type="bibr" target="#b22">[23]</ref> (2? faster).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Projection-based methods. Due to the unstructured nature of point clouds, convolutional neural networks (CNNs) that tend to work impressively well on grid stuctured data (e.g. images, texts and videos) fail to apply directly on point clouds. One common solution for processing point clouds is to project them into collections of images (views) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b41">42]</ref> or 3D voxels <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b35">36]</ref>.  <ref type="bibr" target="#b27">[28]</ref> that learns multilevel representations and reduces the required computations. After PointNet++, numerous point-based methods considering neighborhood information were proposed. Graph-based methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29]</ref> represent point clouds as graphs and process point clouds with graph neural networks. Pseudo grid-based methods project neighborhood features onto different forms of pseudo grids such as tangent planes <ref type="bibr" target="#b36">[37]</ref>, grid cells <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b37">38]</ref> and spherical grid points <ref type="bibr" target="#b48">[49]</ref> which allow convolving with regular kernel weights like CNNs. Adaptive weight-based methods perform weighted neighborhood aggregation by considering the relative positions of the points <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22]</ref> or point density <ref type="bibr" target="#b43">[44]</ref>. These methods rely either on designing sophisticated and customized modules, which usually require expensive parameter tuning for different applications <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24]</ref>, or on performing expensive graph kernels <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b18">19]</ref> that achieve better performance than PointNet and PointNet++ at the expense of computational complexity.</p><p>Efficient Neural Networks. Efficient neural networks is a class of architectures that target mobile and embedded systems applications. These networks are usually designed to provide a balance between accuracy and efficiency (e.g. latency, FLOPs, memory, and power). MobileNet <ref type="bibr" target="#b8">[9]</ref> utilizes depth-wise separable convolutions to reduce the required FLOPs and latency of a regular CNN for image processing. Depth-wise separable convolutions disentangle convolutions into learning channel correlations using point-wise convolutions and learning spatial correlations using depthwise convolutions. Other efficient neural networks usually leverage either depth-wise separable convolutions with better designed architectures to improve performance <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b47">48]</ref> or study new efficient operations to replace the regular convolutions <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b42">43]</ref>. In 3D, efficient neural networks include ShellNet <ref type="bibr" target="#b48">[49]</ref>, PVCNN <ref type="bibr" target="#b22">[23]</ref>, Grid-GCN <ref type="bibr" target="#b44">[45]</ref>, RandLA-Net <ref type="bibr" target="#b9">[10]</ref>, SegGCN <ref type="bibr" target="#b16">[17]</ref> and LPNs <ref type="bibr" target="#b15">[16]</ref>. ShellNet <ref type="bibr" target="#b48">[49]</ref> and SegGCN <ref type="bibr" target="#b16">[17]</ref> speed up the pseudo grid-based methods by aggregating neighborhood features through efficient 1D convolutions or fuzzy spherical convolutions on the predefined pseudo grids like shells. PVCNN <ref type="bibr" target="#b22">[23]</ref> and Grid-GCN <ref type="bibr" target="#b44">[45]</ref> reduce the time spent in querying a neighborhood by combining voxelization in point-based methods. RandLA-Net <ref type="bibr" target="#b9">[10]</ref> reduces the subsampling complexity by leveraging random sampling and further improves the speed by operating on a large-scale point cloud directly without chunking. LPN <ref type="bibr" target="#b15">[16]</ref> improves the speed of convolving neighborhood features by a simple group-wise matrix multiplication. Nevertheless, all efficient methods mentioned above require performing convolutions on neighborhood features, which we deem through extensive experiments as unnecessary. Therefore, our algorithm achieves much faster speeds compared to these methods (ref to <ref type="bibr">Section 4)</ref>. It is also worthwhile to mention that our method can be made even faster with the voxelization trick in PVCNN and Grid-GCN to further reduce the latency of neighborhood querying. We leave that as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminary: PointNet++</head><p>PointNet++ <ref type="bibr" target="#b27">[28]</ref> improves PointNet <ref type="bibr" target="#b26">[27]</ref> by providing two main contributions: (1) developing a U-Net <ref type="bibr" target="#b29">[30]</ref> like architecture to process a set of points, which are sampled in a metric space in a hierarchical fashion. This mechanism captures multi-scale features and reduces the required computation.</p><p>(2) Developing a Set Abstraction (SA) module to process and abstract the locality from the local neighbors to a new set of points with fewer elements. The SA module is used as the basic building block to be stacked to form the backbone of PointNet++.</p><p>Analysis of the Set Abstraction Module. As illustrated in <ref type="figure" target="#fig_2">Figure 3a</ref>, the vanilla SA module proposed in PointNet++ consists of two parts: point subsampling and feature aggregation, . The subsampling layer takes a point cloud X = {P, F } as an input and leverages iterative farthest-point sampling to acquire X , a subset of X. P and F denote the coordinates and features, respectively. The feature aggregation block is built for learning locality from local neighbors and is composed of a grouping layer, an MLP block, and a reduction layer. The grouping layer obtains the neighborhood composed of K neighbors for each point in X using the ball query, with X as the support set. The resulting point neighborhood is denoted as N (X ), which contains K repeated features. The MLP block consists of L layers of MLPs, and each MLP is followed by a Batch Normalization (BN <ref type="bibr" target="#b11">[12]</ref>) layer and a ReLU activation. By default, PointNet++ sets L = 3. The number of feature aggregation blocks inside one SA module, referred to as depth D in this paper, is set to D = 2. The reduction layer (a.k.a, pooling) aggregates the neighborhood information by a reduction function, e.g. mean, max, or sum. The feature aggregation is formulated as shown in Equation <ref type="formula" target="#formula_0">(1)</ref>:</p><formula xml:id="formula_0">f l+1 i = R MLPs((p j ? p i )||f l j )|j ? N (i) ,<label>(1)</label></formula><p>where R is the reduction function across the neighborhood dimension, which is used for aggregating the neighborhood information. p i , f l i , N (i) and || denote the coordinates, the features in the l th layer of the network, the neighborhood of the i th point, and the concatenation operator across the channel dimension, respectively. The main issues with the vanilla SA module are: (1) the computational cost is unnecessarily high. MLPs are unnecessarily performed on the neighborhood features, which causes a considerable amount of latency in PointNet++. One straightforward remedy is to use MLPs to learn a feature embedding on the point features directly instead of doing so on the neighborhood features. This reduces the FLOPs of each MLP by a factor of K. (2) All operations on neighbors are unnecessarily isotropic. In other words, the MLPs and the reduction layer treat all local neighbors equally. This severely limits the representation capability of the network.</p><p>Latency Decomposition. <ref type="figure">Figure 2</ref> shows the latency decomposition of PointNet++ <ref type="bibr" target="#b27">[28]</ref> with different numbers of points as input. Here, the latency, which is the overall run time for the inference stage, was measured using a single Nvidia GeForce RTX 2080Ti GPU and one Intel(R) Xeon(R) CPU E5-2687W v4 @ 3.00GHz. We note here that latency is measured on the same hardware setting throughout this work. The latency of PointNet++ can be decomposed into three main contributing factors: (1) point subsampling, (2) grouping, (3) actual computations. The actual computations of PointNet++ mainly come from processing neighborhood features by MLPs shown in Equation 1. Note that we consider the time spent on data access implicitly in each part. Point clouds with four different input sizes were studied: 1024, 4096, 10, 000, and 15, 000. The first two input sizes are commonly encountered in classification tasks <ref type="bibr" target="#b1">[2]</ref>, and the last two are usually input sizes for patch-based segmentation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b37">38]</ref> and LiDAR-based object detection <ref type="bibr" target="#b31">[32]</ref>. Clearly, computations contribute to the majority of latency (over 70 %). This suggests that the computational complexity could be the major speed bottleneck for networks involving PointNet++.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Anisotropic Separable Set Abstraction (ASSA)</head><p>In this section, we gradually introduce the modified vanilla SA modules. Initially, we focus on speeding up vanilla SA. This is achieved through proposing two modules, namely, PreConv SA module and Separable SA module. Later, we focus our attention on improving the accuracy by proposing an Anisotropic SA module.</p><p>PreConv Set Abstraction module. Vanilla SA repeatedly performs shared MLPs on point neighborhood features. To solve this issue, we modify the feature aggregation layer in vanilla SA, and propose PreConv SA to performs all MLPs on point features directly (not on the k local neighbors) before the grouping layer. The PreConv SA is shown in Appendix <ref type="figure">Figure S1</ref>, and its feature aggregation is formulated as follows:</p><formula xml:id="formula_1">f i = MLPs f l i , f l+1 i = R f j | j ? N (i)<label>(2)</label></formula><p>PreConv SA reduces the required FLOPs by K times. PreConv SA speeds up PointNet++ by ? 55% (15,000 points), as shown in Section 5.1. Additionally, PreConv SA is equivalent to vanilla SA in the case where the (p j ? p i ) term is not included in Equation <ref type="formula" target="#formula_0">(1)</ref>. Proof is available in the Appendix.  Separable Set Abstraction module. Next, we present Separable Set Abstraction (refer to Appendix <ref type="figure">Figure S1</ref>), which is more accurate than PreConv SA, yet requires the same latency. The aggregation layer in Separable Set Abstraction is formulated by:</p><formula xml:id="formula_2">v h 5 O N J Y e f D 6 Q X 0 v U q z h E = " &gt; A A A B 6 n i c b Z D L S g M x F I b P e K 3 j r e r S T b B I X Z U Z F + p G L L p x W d F e o B 1 K J s 2 0 o Z l M S D J C G f o I b l w o 4 l L f x b 0 b 8 W 1 M L w t t / S H w 8 f / n k H N O K D n T x v O + n Y X F p e W V 1 d y a u 7 6 x u b W d 3 9 m t 6 S R V h F Z J w h P V C L G m n A l a N c x w 2 p C K 4 j j k t B 7 2 r 0 Z 5 / Z 4 q z R J x Z w a S B j H u C h Y x g o 2 1 b h v F Y j t f 8 E r e W G g e / C k U L j 7 c c / n 2 5 V b a + c 9 W J y F p T I U h H G v d 9 D 1 p g g w r w w i n Q 7 e V a i o x 6 e M u b V o U O K Y 6 y M a j D t G h d T o o S p R 9 w q C x + 7 s j w 7 H W g z i 0 l T E 2 P T 2 b j c z / s m Z q o r M g Y 0 K m h g o y + S h K O T I J G u 2 N O k x R Y v j A A i a K 2 V k R 6 W G F i b H X c e 0 R / N m V 5 6 F 2 X P J P S v 6 N V y h f w k Q 5 2 I c D O A I f T q E M 1 1 C B K h D o w</formula><formula xml:id="formula_3">f res i = MLPs f l i , f l+1 i = f res i + MLPs R f res j | j ? N (i)<label>(3)</label></formula><p>The main idea of Separable SA is borrowed from depth-wise separable convolutions <ref type="bibr" target="#b8">[9]</ref>, where the regular convolution is split into one point-wise convolution (MLPs), one depth-wise convolution (channel shared convolution), and then another point-wise convolution. Separable SA evenly separates the MLPs before the grouping layer and after the reduction layer and further adds a residual connection between the outputs of the two parts of the MLPs. The main reasons why the Separable Set Abstraction module is better than PreConv are: (1) after reduction MLPs further process the aggregated neighborhood information; (2) The residual connection not only stabilizes training, but also provides better feature embedding by fusing the aggregated local information with the point information. Another minor change from PreConv SA to Separable SA is that we query the neighborhood using the subsampled point cloud X as the support set to further reduce the computational complexity of the second aggregation block.</p><p>Anisotropic Separable Set Abstraction module. PreConv SA and Separable SA cut down computational complexity at the expense of accuracy, e.g. Separable SA leads to a reduction of 3 mIoU on S3DIS Area 5 compared to PointNet++ (Section 5.1). There are two reasons for the drop in accuracy. First, the geometric information is not well encoded in the current variants of the SA module. The geometric information can be represented by any relative information (edge information) between the neighbor and the center, e.g. the relative position (p j ? p i ) in PointNet++. The experiments in Section 5.1 show that geometric information is essential for point feature embedding. Second, the reduction layer is an isotropic operation that treats each neighbor the same and thus leads to a sub-optimal representation. Recall in a depth-wise separable convolution, the depth-wise convolution uses different weights to summarize features from a 3 ? 3 receptive field. However, simply introducing the depth-wise convolution kernel to point neighborhood aggregation does not work, as:</p><p>(1) the neighbors are not necessarily ordered for the sake of efficiency; (2) the convolution kernel is shared by all points and neighbors and leads to poor neighborhood aggregation where the local geometric varies. We propose an efficient geometric-aware Anisotropic Reduction layer to effectively aggregate the point neighborhood information. The term "Anisotropic" indicates that our reduction layer considers each neighbor differently. We insert Anisotropic Reduction into the separable SA module and present our final variant of the SA module, the Anisotropic Separable Set Abstraction (ASSA) module, and show it in <ref type="figure" target="#fig_2">Figure 3b</ref>. The feature aggregation of ASSA is formulated as:</p><formula xml:id="formula_4">f res i = MLPs(f l i ) f l+1 i = LN (f res i ) + MLPs R ?x ij f res j ||?y ij f res j ||?z ij f res j r |j ? N (i)<label>(4)</label></formula><p>?x ij = x j ?x i , ?y ij and ?z ij are the relative positions between the neighbor j and the center i in the x, y, z dimension, respectively. The relative positions are used as scaling weights for aggregating the features across the neighborhood dimension, and they are normalized by the radius of the ball query r.</p><p>The neighborhood features are scaled by the three corresponding relative positions individually. The three scaled neighborhood features are then concatenated together and passed into the reduction layer.</p><p>To reduce the computational complexity caused by the concatenation of the three scaled features, we set the last MLP before reduction as a bottleneck layer. This layer reduces the number of channels by a factor of 3. The output of the reduction layer is then processed by another MLP block and is added to the output before reduction. Due to channel mismatch, the output of before grouping MLPs is mapped by a linear layer LN (a.k.a the shortcut layer) before the addition. We highlight that our Anisotropic Reduction does not rely on any heuristic grouping (as done in PosPool <ref type="bibr" target="#b21">[22]</ref>), and we make full use of the information from the neighborhood features. The pseudo code for ASSA in PyTorch-like style is available in the Appendix.</p><p>It is worth noting that all MLPs in ASSA are processed on the point features directly, not on the neighborhood, which greatly reduces the computations compared to Equation (1). In particular, for one aggregation block with L = 3 MLPs, ASSA roughly reduces the FLOPs consumed in vanilla SA by:</p><p>C?C?N ?K?L C?C?N ?L+C?N ?K ? K times. Typically, K is around 32. All of our SA variants are permutation invariant, which favors 3D deep learning on point clouds. More details of the ASSA module and its comparison with previous modules are provided in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ASSANet</head><p>We now replace the vanilla SA module in PointNet++ <ref type="bibr" target="#b27">[28]</ref> with our proposed ASSA module. All other parts are kept the same as PointNet++, including the number of SA modules (4), the number of aggregation blocks in SA (D = 2), the layers of MLPs in an aggregation block (L = 3), the channel sizes, the neighborhood querying configurations (ball query algorithm with maximum neighborhood size K and radius r) and the subsampling configurations (farthest point sampling). The modified architecture of PointNet++ is referred to as ASSANet. Section 4 shows that ASSANet can achieve much higher accuracies compared to PointNet and PointNet++ and is faster on various vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Scaling ASSANet</head><p>Since the ASSANet is much faster than both PointNet++ <ref type="bibr" target="#b27">[28]</ref> and the state-the-of-art networks, we now present two ways to up-scale ASSANet to improve its accuracy: width scaling and depth scaling. We show the performance of each scaling regime in the ablation study presented in Section 5.2.</p><p>Width Scaling Regime. In width scaling regime, we modified the channel size of ASSANet. ASSANet is built upon PointNet++ <ref type="bibr" target="#b27">[28]</ref>, which uses hand-crafted channel sizes for each convolution layer. To make the scaling more programmable and user-friendly for the scaled ASSANet, the output of each feature aggregation block inside one ASSA module is set to have the same channel size, and is then concatenated as the output of the module. After this modification, we can easily study the effect of width scaling on the accuracy and the speed, by simply changing the initial channel size C.</p><p>Depth Scaling Regime. The second way to scale is to increase the depth of the network, which can be achieved by changing the number of aggregation blocks D stacked in each ASSA module. D is set to 2 by default in ASSANet. We can decrease D to 1 to make ASSANet faster or increase D to improve its accuracy. Among all width or depth scaled versions of ASSANet, we emphasize ASSANet (L), a large ASSANet network with C = 128 and D = 3. In most of the experiments, we compare ASSANet and ASSANet (L) with the state-of-the-art. We studied the accuracy and speed of ASSANet and ASSANet (L) on S3DIS semantic segmentation <ref type="bibr" target="#b0">[1]</ref>, ShapeNet part segmentation <ref type="bibr" target="#b2">[3]</ref>, and ModelNet40 point cloud classification <ref type="bibr" target="#b1">[2]</ref>. To enable a fair comparison, the same data processing and evaluation protocols adopted by the state-of-the-art method PosPool <ref type="bibr" target="#b21">[22]</ref> were used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">3D Scene Segmentation</head><p>Setups. We conducted extensive experiments on the Stanford large-scale 3D Indoor Spaces (S3DIS) dataset <ref type="bibr" target="#b0">[1]</ref>. Following <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22]</ref>, we trained all our models on Area 1, 2, 3, 4, and 6 and tested them on Area 5. We optimized all of our networks using SGD with weight decay 0.001, momentum 0.98 and initial learning rate (LR) 0.02. We trained the models for 600 epochs and used an exponential LR decay. At each inference time, a single RTX 2080Ti GPU was used to measure the speed for each method using a batch size of 16; each item in the batch has 15, 000 points (16 ? 15, 000). If the batch size was too large to feed into the GPU, we lowered the batch size. Note that we focus on the speed since FLOPs and the model parameter size are not indicative of the actual latency <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23]</ref>. The inference speed is calculated as the number of instances evaluated in one second (ins./sec.). The average speed over 200 runs is reported. Other methods were measured in a similar manner. Note that KPConv <ref type="bibr" target="#b37">[38]</ref> has to compute the pseudo kernels for each point cloud during data preprocessing. For a fair comparison, we show the speed of calculating the pseudo kernels on the fly. We also include the speed of KPConv with preprocessed pseudo kernels in () in the table.</p><p>Comparison with state-of-the-art. <ref type="table">Table 1</ref> compares the proposed ASSANet and ASSANet (L) with PointNet++ <ref type="bibr" target="#b27">[28]</ref> and the state-of-the-art on S3DIS. ASSANet outperforms PointNet++ by 7 mIoU and is 1.6? faster. ASSANet also achieves much better accuracy than the two efficient point cloud processing algorithms PVCNN <ref type="bibr" target="#b22">[23]</ref> and Grid-GCN <ref type="bibr" target="#b44">[45]</ref>, while also being over 1.5? faster. ASSANet (L) achieves state-of-the-art performance with a mIoU of 66.8% on S3DIS, with very high speed. ASSANet (L) is 294? faster than the graph-based method DeepGCN <ref type="bibr" target="#b18">[19]</ref>, 54.6? faster than the state-of-the-art pesudo grid-based method KPConv <ref type="bibr" target="#b37">[38]</ref>, 7.9? faster than the state-of-the-art adaptive weight-based method PosPool* <ref type="bibr" target="#b21">[22]</ref>, and 2.2? faster than the best-performing efficient method SegGCN <ref type="bibr" target="#b16">[17]</ref>. Note that PosPool* refers to PosPool with sinusoidal position weight, and that PosPool* (S) denotes the small model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">3D Object Classification</head><p>Setup. As a common practice, we benchmark ASSANet on the ModelNet40 [2] object classification dataset. We adopted a similar training setting as that of S3DIS except that we used LR 0.001 and a cosine LR decay. At the inference time, a single RTX 2080Ti GPU was used to measure the speed for the classification task using 16 ? 10, 000 points as input.</p><p>Comparison with state-of-the-art. <ref type="table" target="#tab_4">Table 2</ref> compares ASSANet and ASSANet (L) with the state-ofthe-art. ASSANet outperforms PointNet++ by 1.7 units in overall accuracy and is 2.1? faster than PointNet++. ASSANet (L) achieves on par accuracy as the state-of-the-art methods KPConv <ref type="bibr" target="#b37">[38]</ref> and PosPool* <ref type="bibr" target="#b21">[22]</ref> while being 5.0? and 4.4? faster, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">3D Part Segmentation</head><p>Data. ShapeNetPart is a commonly used benchmark for 3D part segmentation. The networks were optimized using Adam <ref type="bibr" target="#b12">[13]</ref> with momentum 0.9. The other training parameters were the same as  <ref type="bibr" target="#b19">[20]</ref> 92.5 183.4 PosPool * (S) <ref type="bibr" target="#b21">[22]</ref> 92. <ref type="bibr">6 48.8 DGCNN [41]</ref> 92.9 11.6 KPConv <ref type="bibr" target="#b37">[38]</ref> 92.9 (30.1) Grid-GCN <ref type="bibr" target="#b44">[45]</ref> 93   Comparison with state-of-the-art. <ref type="table" target="#tab_5">Table 3</ref> shows that ASSANet again outperforms PointNet++ with a sharp increase (2.2?) in speed on the ShapeNetPart part segmentation dataset. ASSANet (L) also achieves 1 unit higher mIoU than PoinetNet++ with a 1.3? faster speed. Additionally, ASSANet (L) attains on-par accuracy, 86.1% mIoU, with the state-of-the-art and is much faster. For example, ASSANet (L) is nearly 7.8? faster than KPConv <ref type="bibr" target="#b37">[38]</ref>.  <ref type="table" target="#tab_7">Table 4</ref>: Ablation study of the proposed SA variants. All proposed SA variants achieve a faster speed than the vanilla SA. Our ASSA further improves the accuracy of the Separable SA module and outperforms other methods, while also being faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ablation Study</head><p>An ablation study was conducted on S3DIS [1] Area-5. We show the effectiveness of the proposed SA variants and the effect of the two scaling regimes on ASSANet.  <ref type="bibr" target="#b21">[22]</ref>. Our method clearly outperforms both of these methods in terms of accuracy and inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation on Proposed SA variants</head><p>We also test simple addition of the relative positions ? x + ? y + ? z as the weights of the reduction layer, denoted as Relative Position, the obtained performance is worse than the proposed Anisotropic Reduction. To further show the benefits of the proposed ASSA module, we visualize the feature patterns before and after the ASSA module in <ref type="figure" target="#fig_2">Figure 3</ref>. ASSA module helps capture better geometric relationships among points constituting the point cloud (for example, in the second and fourth examples in the first row of <ref type="figure" target="#fig_2">Figure 3d</ref>, one can see that the ASSA module allows the network to learn relationships between the tail of the plane and its wings). Appendix provides a more detailed overview and a further set of examples of feature patterns visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation on Scaling Regimes</head><p>We now study the effects of ablating the width and depth of a network on its accuracy and inference speed. The initial channel size of the network is referred to as width (denoted by C), whereas the number of aggregation layers inside a single SA module is referred to as depth (denoted by D).</p><p>Width scaling. <ref type="figure" target="#fig_3">Figure 4 (left)</ref> shows the effect of the width scaling regime. When the width of the network is small, increasing the width leads to a significant improvement in accuracy. For example, simply increasing the width C from 3 to 8 sharply improves the accuracy from 41.21 mIoU to 53.95 with a negligible drop in speed. However, when the network is wide enough (C ? 128), increasing the width further only leads to a marginal improvement in accuracy, yet reduces the speed noticeably.</p><p>Depth scaling. <ref type="figure" target="#fig_3">Figure 4 (right)</ref> shows the effect of the depth scaling regime. We study the depth scaling with C = 128, which is the sweet point of width scaling. When the network is shallow, with a depth of D ? 3, increasing the depth leads to an obvious increase in accuracy. However, depth scaling rapidly saturates as the depth increases. Depth scaling leads to a linear reduction in speed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we dove deeper into the architecture of PointNet++. We noticed that PointNet++ suffers from a computational burden attributed to the MLPs that process the neighborhood features in the set abstraction (SA) module. We also found out that the accuracy of PointNet++ is limited by the isotropic nature of its operations. To solve these issues, we proposed a PreConv SA module, a Separable SA module, and finally an Anisotropic Separable SA (ASSA) module that aim to reduce the computational cost and improve the accuracy. We then replaced the vanilla SA module in PointNet++ with our ASSA module and proposed a fast and accurate architecture, namely, ASSANet. Extensive experiments were conducted to verify the presented claims and showed that ASSANet achieves largely improved accuracy and much faster speed on various point cloud tasks, such as classification, semantic segmentation, and part segmentation. We also studied up-scaling ASSANet. The scaled ASSANet set new state-of-the-art on various tasks with faster speeds. For future work, one could leverage both random sampling <ref type="bibr" target="#b9">[10]</ref> and voxelization tricks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b44">45]</ref> to further improve the inference speed. Alternatively, one could consider studying compound scaling, like that in EfficientNet <ref type="bibr" target="#b34">[35]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>X</head><label></label><figDesc>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I O K I G Q E l g G q T b a G C f M S C e O M b l k = " &gt; A A A B 6 X i c b Z D L S g M x F I b P 1 F s d b 1 W X b o J F d F V m X K g b s e j G Z R V 7 g X Y o m T T T h m Y y I c k I Z e g b u H G h i N s + j H s 3 4 t u Y X h b a + k P g 4 / / P I e e c U H K m j e d 9 O 7 m l 5 Z X V t f y 6 u 7 G 5 t b 1 T 2 N 2 r 6 S R V h F Z J w h P V C L G m n A l a N c x w 2 p C K 4 j j k t B 7 2 b 8 Z 5 / Z E q z R L x Y A a S B j H u C h Y x g o 2 1 7 h v H 7 U L R K 3 k T o U X w Z 1 C 8 + n A v 5 e j L r b Q L n 6 1 O Q t K Y C k M 4 1 r r p e 9 I E G V a G E U 6 H b i v V V G L S x 1 3 a t C h w T H W Q T S Y d o i P r d F C U K P u E Q R P 3 d 0 e G Y 6 0 H c W g r Y 2 x 6 e j 4 b m / 9 l z d R E F 0 H G h E w N F W T 6 U Z R y Z B I 0 X h t 1 m K L E 8 I E F T B S z s y L S w w o T Y 4 / j 2 i P 4 8 y s v Q u 2 0 5 J + V / D u v W L 6 G q f J w A I d w A j 6 c Q x l u o Q J V I B D B E 7 z A q 9 N 3 n p 0 3 5 3 1 a m n N m P f v w R 8 7 o B 3 e f k F A = &lt; / l a t e x i t &gt; X 0 Query Support Grouping &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E t u S / m P</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>g M 8 w b P D n U f n x X m d l C 4 4 0 5 4 9 + C P n / Q f Y I J C B &lt; / l a t e x i t &gt; X 00 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e X 0 E 2 T V R j l R h Y Y m J 8 F t J j T g 8 N 9 c = " &gt; A A A B 6 3 i c b Z D L S g M x F I b P 1 F s d b 1 W X b o J F 6 q r M u F A 3 Y t G N y w r 2 A m 0 p m T T T h i a Z I c k I Z e g r u H G h i D v x W d y 7 E d / G T N u F t v 4 Q + P j / c 8 g 5 J 4 g 5 0 8 b z v p 3 c 0 v L K 6 l p + 3 d 3 Y 3 N r e K e z u 1 X W U K E J r J O K R a g Z Y U 8 4 k r R l m O G 3 G i m I R c N o I h t d Z 3 r i n S r N I 3 p l R T D s C 9 y U L G c E m s 5 q l U q l b K H p l b y K 0 C P 4 M i p c f 7 k X 8 9 u V W u 4 X P d i 8 i i a D S E I 6 1 b v l e b D o p V o Y R T s d u O 9 E 0 x m S I + 7 R l U W J B d S e d z D p G R 9 b p o T B S 9 k m D J u 7 v j h Q L r U c i s J U C m 4 G e z z L z v 6 y V m P C 8 k z I Z J 4 Z K M v 0 o T D g y E c o W R z 2 m K D F 8 Z A E T x e y s i A y w w s T Y 8 7 j 2 C P 7 8 y o t Q P y n 7 p 2 X / 1 i t W r m C q P B z A I R y D D 2 d Q g R u o Q g 0 I D O A B n u D Z E c 6 j 8 + K 8 T k t z z q x n H / 7 I e f 8 B O N m Q s g = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " b J F r d 2 z h b 3 r O q s l 0 o F 0 C 7 1 Y L p K o = " &gt; A A A B 6 H i c b Z C 7 S g N B F I b P x l t c b 1 F L m 8 E g W I V d C 7 U R g z a W C Z g L J E u Y n Z x N x s x e m J k V w p I n s L F Q x F Y f x t 5 G f B s n l 0 I T f x j 4 + P 9 z m H O O n w i u t O N 8 W 7 m l 5 Z X V t f y 6 v b G 5 t b 1 T 2 N 2 r q z i V D G s s F r F s + l S h 4 B H W N N c C m 4 l E G v o C G / 7 g e p w 3 7 l E q H k e 3 e p i g F 9 J e x A P O q D Z W t d k p F J 2 S M x F Z B H c G x c s P + y J 5 / 7 I r n c J n u x u z N M R I M 0 G V a r l O o r 2 M S s 2 Z w J H d T h U m l A 1 o D 1 s G I x q i 8 r L J o C N y Z J w u C W J p X q T J x P 3 d k d F Q q W H o m 8 q Q 6 r 6 a z 8 b m f 1 k r 1 c G 5 l / E o S T V G b P p R k A q i Y z L e m n S 5 R K b F 0 A B l k p t Z C e t T S Z k 2 t 7 H N E d z 5 l R e h f l J y T 0 t u 1 S m W r 2 C q P B z A I R y D C 2 d Q h h u o Q A 0 Y I D z A E z x b d 9 a j 9 W K 9 T k t z 1 q x n H / 7 I e v s B F 0 e Q H w = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " E t u S / m P v h 5 O N J Y e f D 6 Q X 0 v U q z h E = " &gt; A A A B 6 n i c b Z D L S g M x F I b P e K 3 j r e r S T b B I X Z U Z F + p G L L p x W d F e o B 1 K J s 2 0 o Z l M S D J C G f o I b l w o 4 l L f x b 0 b 8 W 1 M L w t t / S H w 8 f / n k H N O K D n T x v O + n Y X F p e W V 1 d y a u 7 6 x u b W d 3 9 m t 6 S R V h F Z J w h P V C L G m n A l a N c x w 2 p C K 4 j j k t B 7 2 r 0 Z 5 / Z 4 q z R J x Z w a S B j H u C h Y x g o 2 1 b h v F Y j t f 8 E r e W G g e / C k U L j 7 c c / n 2 5 V b a + c 9 W J y F p T I U h H G v d 9 D 1 p g g w r w w i n Q 7 e V a i o x 6 e M u b V o U O K Y 6 y M a j D t G h d T o o S p R 9 w q C x + 7 s j w 7 H W g z i 0 l T E 2 P T 2 b j c z / s m Z q o r M g Y 0 K m h g o y + S h K O T I J G u 2 N O k x R Y v j A A i a K 2 V k R 6 W G F i b H X c e 0 R / N m V 5 6 F 2 X P J P S v 6 N V y h f w k Q 5 2 I c D O A I f T q E M 1 1 C B K h D o w g M 8 w b P D n U f n x X m d l C 4 4 0 5 4 9 + C P n / Q f Y I J C B &lt; / l a t e x i t &gt; X 00 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e X 0 E 2 T V R j l R h Y Y m J 8 F t J j T g 8 N 9 c = " &gt; A A A B 6 3 i c b Z D L S g M x F I b P 1 F s d b 1 W X b o J F 6 q r M u F A 3 Y t G N y w r 2 A m 0 p m T T T h i a Z I c k I Z e g r u H G h i D v x W d y 7 E d / G T N u F t v 4 Q + P j / c 8 g 5 J 4 g 5 0 8 b z v p 3 c 0 v L K 6 l p + 3 d 3 Y 3 N r e K e z u 1 X W U K E J r J O K R a g Z Y U 8 4 k r R l m O G 3 G i m I R c N o I h t d Z 3 r i n S r N I 3 p l R T D s C 9 y U L G c E m s 5 q l U q l b K H p l b y K 0 C P 4 M i p c f 7 k X 8 9 u V W u 4 X P d i 8 i i a D S E I 6 1 b v l e b D o p V o Y R T s d u O 9 E 0 x m S I + 7 R l U W J B d S e d z D p G R 9 b p o T B S 9 k m D J u 7 v j h Q L r U c i s J U C m 4 G e z z L z v 6 y V m P C 8 k z I Z J 4 Z K M v 0 o T D g y E c o W R z 2 m K D F 8 Z A E T x e y s i A y w w s T Y 8 7 j 2 C P 7 8 y o t Q P y n 7 p 2 X / 1 i t W r m C q P B z A I R y D D 2 d Q g R u o Q g 0 I D O A B n u D Z E c 6 j 8 + K 8 T k t z z q x n H / 7 I e f 8 B O N m Q s g = = &lt; / l a t e x i t &gt; Feature patterns before the first SA (d) Feature patterns after the first SA Comparison of proposed Anisotropical Separable Set Abstraction (ASSA) module and the Vanilla SA module. (a) Vanilla SA [28] applies MLPs on neighbor features. (b) The proposed ASSA module separates the MLPs before the grouping layer and after the reduction layer. Therefore the MLPs are applied directly on the point features not on the neighbor features. ASSA also replaces the reduction layer in vanilla SA with a new Anisotropic Reduction layer. X, N, C, K are the input point cloud, the number of points, the number of input features, and the number of neighbors. The shortcut layer in blue line is the residual connection with a linear mapping. (c) and (d) Show the point cloud feature patterns (activations) before and after the first ASSA module. The proposed ASSA module helps capture better geometric relationships. Refer to Section 5.1 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Effect of Width (left) and Depth Scaling (right). Increasing either the width or the depth leads to an improvement in accuracy and drop in inference speed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Latency Decomposition of PointNet++. We show the inference run time decomposition of PointNet++ under different numbers of points as input on one NVIDIA GTX2080Ti GPU. with additional cost of constructing intermediate representations. Moreover, the projection of point clouds causes loss of important geometric information [23].</figDesc><table><row><cell>Subsampling 5.5% Grouping 2.3% (a) 1024 points Computation 92.3% Subsampling 6.2% Grouping 3.1% (b) 4096 points Computation 90.6% Subsampling 12.6% Grouping 5.1% (c) 10,000 points Computation 82.4% Subsampling 24.5% Grouping 4.3%</cell><cell>Computation 71.1% (d) 15,000 points</cell></row><row><cell cols="2">Figure 2: (SA) to aggregate</cell></row><row><cell cols="2">features from the points' neighborhood, and a hierarchical architecture named PointNet++</cell></row></table><note>Common CNN backbones (using 2D or 3D convolutions) can be subsequently utilized to perform these intermediate representations. Although projection-based methods allow for utilizing the well studied convolutional neural networks to point-cloud applications, they are computationally expensive as they are associatedPoint-based methods. Pioneering work explored the possibility of processing point clouds directly. Qi et al. proposed PointNet [27] that leverages point-wise MLPs to extract per point features individually. To better encode locality, Qi et al. further presented Set Abstraction</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparison</figDesc><table><row><cell>Methods</cell><cell>mIoU</cell><cell>Inference Speed</cell></row><row><cell></cell><cell>%</cell><cell>instances/second</cell></row><row><cell>PointNet [27] PosPool  *  (S) [22]</cell><cell>83.7 85.1</cell><cell>1883.5 107.7</cell></row><row><cell>DGCNN [41]</cell><cell>85.2</cell><cell>151.4</cell></row><row><cell>LPN [16] PosPool  *  [22]</cell><cell>85.7 85.8</cell><cell>190.6 58.0</cell></row><row><cell>PointCNN [20]</cell><cell>86.1</cell><cell>626.4</cell></row><row><cell>RS-CNN [21]</cell><cell>86.2</cell><cell>&lt;350.4</cell></row><row><cell>KPConv [38]</cell><cell>86.2</cell><cell>(56.3)</cell></row><row><cell>PointNet++ [28]</cell><cell>85.1</cell><cell>350.4</cell></row><row><cell>ASSANet</cell><cell>85.4 (+0.3)</cell><cell>782.5 (2.2?)</cell></row><row><cell>ASSANet (L)</cell><cell>86.1 (+1.0)</cell><cell>438.5 (1.3?)</cell></row><row><cell>of our ASSANet and</cell><cell></cell><cell></cell></row><row><cell>ASSANet (L) with other methods on Mod-</cell><cell></cell><cell></cell></row><row><cell>elNet40 point cloud classification. AS-</cell><cell></cell><cell></cell></row><row><cell>SANet outperforms PointNet++ with 1.7</cell><cell></cell><cell></cell></row><row><cell>higher overall accuracy (OA) than PointNet++</cell><cell></cell><cell></cell></row><row><cell>and is 2.1 times faster. ASSANet (L) achieves</cell><cell></cell><cell></cell></row><row><cell>on par accuracy with the state-of-the-art while</cell><cell></cell><cell></cell></row><row><cell>maintaining a high speed.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison The speed of each method was measured with an input of 16 ? 2048 points. We report the part-averaged IoU (mIoU) as the evaluation metric for accuracy.</figDesc><table><row><cell>of the part-averaged</cell></row><row><cell>IoU (mIoU) of our ASSANet and AS-</cell></row><row><cell>SANet (L) with other methods on</cell></row><row><cell>ShapeNetPart part segmentation. Both</cell></row><row><cell>of ASSANet and ASSANet (L) outperform</cell></row><row><cell>PointNet++ with a higher speed. ASSANet</cell></row><row><cell>(L) achieves a comparable accuracy as the</cell></row><row><cell>state-of-the-art while being much faster.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc>shows the speed and the accuracy of the proposed PreConv Set Abstraction (SA) module, the Separable SA module, and the Anisotropic Separable SA (ASSA) module compared to the vanilla SA. All of our proposed SA modules lead to a sharp increase (over 1.6?) in inference speed. The proposed Separable SA module can boost the accuracy of PreConv by 3.7 mIoU, which verifies the effectiveness of separable MLPs and residual connections. Comparing the ASSA module with the Separable SA module, one can clearly see the importance of encoding the geometric information and the effect of the anisotropic operation to achieving higher accuracy. Additionally, we provide a comparison of our proposed Anisotropic Reduction with the Attentive Pooling used in RandLA-Net<ref type="bibr" target="#b9">[10]</ref> and the PosPool proposed in</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. The authors appreciate the anonymous NeurIPS reviewers for their constructive feedback (including the revised title, the feature pattern visualization, and the additional experiments). This work was supported by the KAUST Office of Sponsored Research (OSR) through the Visual Computing Center (VCC) funding.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Iro Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1534" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Three-dimensional point cloud classification in real-time using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhak</forename><surname>Ben-Shabat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anath</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3145" to="3152" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">ShapeNet: An Information-Rich 3D Model Repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Stanford University -Princeton University -Toyota Technological Institute at Chicago</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>cs.GR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gvcnn: Group-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="264" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Flex-convolution (million-scale point-cloud learning beyond grid-worlds)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Groh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Wieschollek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><forename type="middle">P A</forename><surname>Lensch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<meeting><address><addrLine>Dezember</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pcpnet learning local shape properties from raw point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanir</forename><surname>Kleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1704.04861</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11105" to="11114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pointwise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Binh-Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Khoi</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop and Conference Proceedings</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Going deeper with lean point networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric-Tuan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9500" to="9509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Seggcn: Efficient 3d point cloud segmentation with fuzzy spherical kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveed</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11608" to="11617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deepgcns: Making gcns go as deep as cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Itzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdulellah</forename><surname>Delgadillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">K</forename><surname>Abualshour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8887" to="8896" />
		</imprint>
	</monogr>
	<note>Bin Fan, Shiming Xiang, and Chunhong Pan</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A closer look at local aggregation operators in point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Point-voxel cnn for efficient 3d deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Interpolated convolutional networks for 3d point cloud understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiageng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1578" to="1587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9276" to="9285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pu-gcn: Point cloud upsampling using graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdulellah</forename><surname>Abualshour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="11683" to="11692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI (3)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>Mobilenetv2: Inverted residuals and linear bottlenecks</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Subhransu Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, volume 97 of Proceedings of Machine Learning Research</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Searching efficient 3d architectures with sparse point-voxel convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6410" to="6419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph attention convolution for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaolin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenman</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10296" to="10305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2589" to="2597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">View-gcn: View-based graph convolutional network for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1847" to="1856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Shift: A zero flop, zero parameter alternative to spatial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiangyu Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Golmant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gholaminejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9127" to="9135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9613" to="9622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Grid-gcn for fast and scalable point cloud learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Ying</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panqu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5660" to="5669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Efficient convolutions for real-time semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Shellnet: Efficient point cloud convolutional neural networks using concentric shells statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Binh-Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1607" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
