<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Scene Completion via Integrating Instances and Scene in-the-Loop</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjie</forename><surname>Cai</surname></persName>
							<email>caiyingjie@link.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuesong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Samsung Research Institute China -Beijing (SRC-B</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Research and Tetras.AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of CST</orgName>
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Scene Completion via Integrating Instances and Scene in-the-Loop</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b33">[34]</ref><p>, (e) results of Sketch <ref type="bibr" target="#b2">[3]</ref>, (f) results of the proposed method. Our method recovers better shape details like the legs of chairs and generates more reasonable results on easy confusing regions like windows and wall compared with the classic SSC method SSCNet <ref type="bibr" target="#b33">[34]</ref>, and the state-of-the-art method Sketch <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Semantic Scene Completion aims at reconstructing a complete 3D scene with precise voxel-wise semantics from a single-view depth or RGBD image. It is a crucial but challenging problem for indoor scene understanding. In this work, we present a novel framework named Scene-Instance-Scene Network (SISNet), which takes advantages of both instance and scene level semantic information. Our method is capable of inferring fine-grained shape details as well as nearby objects whose semantic categories are easily mixedup. The key insight is that we decouple the instances from a coarsely completed semantic scene instead of a raw input image to guide the reconstruction of instances and the overall scene. SISNet conducts iterative scene-to-instance (SI) and instance-to-scene (IS) semantic completion. Specifically, the SI is able to encode objects' surrounding context for effectively decoupling instances from the scene and each instance could be voxelized into higher resolution to capture finer details. With IS, fine-grained instance information can be integrated back into the 3D scene and thus leads to more accurate semantic scene completion. Utilizing such an iterative mechanism, the scene and instance completion benefits each other to achieve higher completion accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A comprehensive indoor scene understanding in 3D behavior is pivotal for many computer vision tasks, such as robot navigation, virtual/augmented reality and localization, to name a few. Semantic Scene Completion (SSC) aims at reconstructing full voxel-wise semantics of a 3D scene from a single-view image. However, as the real-world scenarios always have various object shapes/sizes, crowded placements, and object-to-object occlusions, precisely reconstructing and understanding the semantics of a whole 3D scene from partial observations is of great challenges.</p><p>To overcome the incomplete and complex nature of 3D scene understanding, various 3D scene/instance completion and segmentation techniques have been proposed in recent years. Most existing Semantic Scene Completion (SSC) methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b16">17]</ref> pass the incomplete scene from view frustum to a neural network. They design complex modules to aggregate multilevel context information to guide the prediction of volumetric occupancy of invisible regions and semantic categories over the whole scene. However, objects' shapes usu-ally cannot be reliably reconstructed in fine-details and the semantic categories of close-by objects are easily mixed-up, due to the limited resolution of voxelization and absence of instance-level constraints, as illustrated by the chairs and windows in <ref type="figure" target="#fig_0">Figure 1</ref> (e). Some recent Semantic Instance Completion (SIC) methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b0">1]</ref> attempt to reconstruct certain types of objects in the scene to help the understanding of the 3D scene. They general follow the pipeline of detecting instances and then completing each individuals or aligning with CAD models to achieve fine-grained reconstruction. Instance-level completion can greatly preserve the structures and details of the objects. However, detecting objects from partially observed scenes itself is a challenging problem. In addition, the surrounding semantic information of the scene and more complete shape are helpful for better distinguishing each individuals. Such valuable knowledge is much ignored in the current instance detection and completion methods.</p><p>In this paper, we aims to design a mechanism that can efficiently propagate and integrate the information between the scene and instances to achieve more accurate SSC prediction. To this end, we introduce a Scene-Instance-Scene Network, a novel framework that conducts iterative scene-to-instance and instance-to-scene semantic completion. Unlike existing SIC methods that conduct instance completion on the raw partial observations or SSC methods that only consider the scene-level knowledge, our framework takes full advantages of the 3D voxel-wise scene-level ground truths for assisting instance completion, which in turn, improves the whole scene semantic completion in an iterative manner.</p><p>Specifically, the proposed method first aggregates multimodal information from the visible regions' 2D semantic segmentation maps and truncated signed distance function (TSDF) 1 to generate a coarsely completed 3D semantic scene. Given the roughly completed scene, the follow-up scene-to-instance completion localizes each instances and locally voxelizes them into higher resolution to recover detailed 3D shapes, which are then placed back to the scene to further promote the scene's completion. The intuition behind this design is that objects are the main components of the scene and they are tightly correlated with the scene. For example, when the windows are well reconstructed, the surrounding walls can be easily inferred. It is worth noting that we iterate the scene-instance-scene completion in a weight-sharing manner by training with multi-stage data simultaneously. The iterative completion mechanism enables instances and scene information to be fully propagated and integrated without extra parameters, resulting in more accurate and comprehensive understanding of the 3D scene.</p><p>In summary, our contributions are three folds: ? We introduce a novel framework, Scene-Instance-Scene Network (SISNet), that fully takes advantages of both scene-and instance-level knowledge, to achieve high-quality semantic scene completion. Our method greatly boosts the instance completion performance especially for semantic confusing objects and shape's details with a coarsely completed 3D scene's knowledge in hand. And in turn, fine-detail instance completion leads to more accurate prediction of semantic scene completion. ? We design a scene-instance-scene iterative completion mechanism, which gradually improves the completion results to better reconstruct the 3D scene without increasing extra parameters. ? Experimental results demonstrate that the proposed method outperforms state-of-the-art semantic scene completion methods significantly on both synthetic and real datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Semantic Instance Completion</head><p>Semantic instance completion aims to detect individual instances in a scene and infer their complete object geometry. Most existing methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b0">1]</ref> attempt to reconstruct objects in the scene by detecting instances and then completing shape with CAD models or completion head. Here we briefly review recent works for detection and completion. For detection, since the irregularity of 3D point data, <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b49">50]</ref> group points into voxels and use 3D CNN to learn features to predict 3D boxes. However, the their voxelization suffer from information loss. Therefore, <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> are proposed to learn the point cloud features directly, which greatly boost the development of point-based 3D detection. <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b29">30]</ref> design end-to-end frameworks to detect 3D objects directly in raw point cloud. Compared with existing methods, our method utilizes initial completed scene results with point-level semantic information to yield more accurate 3D boxes, especially for semantic confusing objects. For instance shape completion, <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38]</ref> employ reasoning geometric cues, such as symmetries in meshes or point clouds, to complete partial inputs. Alternatively, some methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b14">15]</ref> make up missing geometry by matching with 3D CAD models from a large shape database. <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41]</ref> operate on raw partial point clouds directly to reconstruct a completed point cloud through end-to-end network. <ref type="bibr" target="#b42">[43]</ref> introduces 3D grids as intermediate representations to regularize point clouds and then employ a 3D CNN network for point cloud completion. Different from these methods, our instance completion exploits reliable and in hand semantic category priors provided by detection to recover fine-grained details. Overview of the Proposed Method. SISNet consists of iterative scene-to-instance completion and instance-to-scene completion stages. Given single-view RGBD images, TSDF from the depth map and semantic volume from the reprojection of 2D semantic segmentation are input into the initial scene completion (S0) to obtain a coarse completed scene, which is then fed into the first instance completion (I1) to locally recover the instance details. The completed instances are further merged into the semantic volume input to obtain better scene completion prediction (S1). More Ii-Si iterations are conducted to promote information integration between the instances and the scene in a weight-sharing manner without extra parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Semantic Scene Completion</head><p>Semantic scene completion aims to predict volumetric occupancy and semantic labels simultaneously. SSC-Net <ref type="bibr" target="#b33">[34]</ref> first proposes the combination of completion and segmentation and proves that they can promote each other in an end-to-end network. ForkNet <ref type="bibr" target="#b39">[40]</ref> proposes a multibranch architecture and utilizes generative models to solve the disadvantages of real scene data. SCFusion <ref type="bibr" target="#b41">[42]</ref> targets at real-time scene reconstruction from a sequence of depth maps. CCPNet <ref type="bibr" target="#b48">[49]</ref> proposes a cascaded context pyramid which progressively restores details of objects and improves the labeling coherence. As complementary information of depth map, RGB images are leveraged by some methods including <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20]</ref> where 3D CNN is employed to fuse two-stream information, including RGB-based semantic label and depth to improve the semantic discrimination of features. To better fused multi-scale RGB-D features, DDR-Net <ref type="bibr" target="#b18">[19]</ref> proposes a light-weight dimensional decomposition residual network. AICNet <ref type="bibr" target="#b16">[17]</ref> modifies the standard 3D convolution so that kernels with varying sizes. Sketch <ref type="bibr" target="#b2">[3]</ref> proposes a 3D sketch-aware feature embedding to explicitly encode geometric information to guide the prediction. However, our method effectively exploits the relations between instances reconstruction and scene completion in an interactive manner, which effectively recovers the details while guarantees the rigid shapes of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture Overview</head><p>The goal of our work is to precisely reconstruct the voxel-wise semantics of a 3D scene from single-view RGB-D images. Since the real-world scenes always contain objects with various sizes/shapes and crowded arrangements, the system must be able to distinguish close-by objects and infer the reasonable shape details of each of them. To this end, we propose the Scene-Instance-Scene Network (SIS-Net), which consists of a series of in-the-loop scene-toinstance (SI) and instance-to-scene (IS) completion stages. Taking a pair of RGB and depth images as inputs, the SIS-Net first identifies the visible surfaces by re-projecting each pixel and its 2D semantic label map onto the view frustum, forming two volumes. Then these volumes are fed into the proposed SISNet to produce a completed 3D voxel-wise representation storing volumetric occupancy and semantic labels. The overall architecture is illustrated in <ref type="figure">Figure 2</ref>.</p><p>Specifically, we first perform an initial scene completion (S 0 ), to aggregate multi-modal partial information from input volumes and output a roughly completed 3D scene prediction. This stage is much ignored by existing semantic instance completion methods and is shown to have crucial impacts to the final completion performance. Given the roughly completed scene from S 0 , a scene-to-instance completion stage S 0 ? I 1 is designed to localize each instance and voxlizes them into higher resolution to recover 3D shape details. We then conduct instance-to-scene completion (I 1 ? S 1 ), where the reconstructed instances with fine details are placed back into the 3D scene to form an enhanced input of next scene-to-instance completion. Utilizing such an in-the-loop scene-instance-scene iteration completion mechanism, we iterate scene-to-instance completion S i ? I i+1 and instance-to-scene completion I i+1 ? S i+1 in a weight-sharing manner until convergence. Such a framework enables the information between the instances and the scene to be effectively propagated and integrated without extra parameters, achieving more accurate reconstruction of the 3D scene. Details are described in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Initial Semantic Scene Completion</head><p>As the first step of our iterative scene-instance-scene completion framework, we perform an initial semantic scene completion S 0 to obtain a roughly completed 3D scene. Such a roughly completed 3D scene leverages the valuable voxel-wise semantic ground truths from SSC datasets so that it can capture more complete context and richer details, which helps the follow-up scene-to-instance completion. The initial scene completion S 0 takes a truncated signed distance function (TSDF) volume V T and a re-projected semantic volume V S0 as its inputs and outputs a 3D semantic volume, where each voxel is assigned with  <ref type="figure">Figure 3</ref>. Architecture details of initial scene completion. Given TSDF and semantic volume, to infer missing invisible shape details, we exploit DDR to encode features and propagate information with skip layers, yielding a whole scene completion results.</p><p>a semantic label c i , i ? [1, ? ? ? , C], where C denotes the number of semantic categories. Specifically, we reconstruct the depth map by reprojecting each pixel onto the view frustum and obtain a 60 ? 36 ? 60 ? 1 TSDF volume V T , where every voxel stores the signed distance value d to its closest surface. For the semantic volume V S0 , we exploit light-weight 2D semantic segmentation networks following <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b18">19</ref>] to obtain visible surface's semantic labels from the RGB image and then re-project it to the 3D space of 60?36?60?C, where every voxel records a C-class semantic segmentation confidence vector. The combination of V T and V S can generally provide rich clues as V S encodes explicit semantic knowledge compared with the raw RGB images. <ref type="figure">Figure 3</ref> shows the detailed network structure for initial scene completion S 0 . We first exploit two convolution blocks, consisting of several convolutions layers followed by ReLU and batch normalization, to transform V S and V T into high dimensional features F S and F T , respectively. Then we feed the summation of them to a 3D Convolution Neural Network (CNN) encoder that includes two blocks with each block consisting of 4 DDR [19] modules, which is computation-efficient compared with basic 3D residual block. After encoding, two deconvolution layers are used to upsample the feature embedding back to the original resolution of input and consequently obtain the semantic scene completion predictions after a classification convolution head. In addition, similar to <ref type="bibr" target="#b3">[4]</ref>, we add several skip connections between encoder and decoder layers for better information and gradient propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Scene-to-Instance Completion</head><p>Given the roughly completed scene, the scene-toinstance completion aims to localize each instance and recover their detailed 3D shapes, which consists of two components: proposal generation <ref type="figure">(Figure 4</ref> (a)) and instance shape completion <ref type="figure">(Figure 4 (b)</ref>), respectively.</p><p>For instance proposal generation, we follow the general design of VoteNet <ref type="bibr" target="#b23">[24]</ref>. M voxels are randomly sampled from the roughly completed 3D scene of S 0 with 129,600 (60 ? 36 ? 60) voxels and converted into a point cloud of size M ? (3 + C), where M is the number of points, 3 is the coordinates dimension of the points, and C denotes each point's confidences of the C semantic classes and one extra height channel. We utilize set abstraction (SA) lay-ers and feature propagation (FP) layers proposed by Point-Net++ <ref type="bibr" target="#b26">[27]</ref> to learn features from the partial instance points. Specifically, SA layer selects and groups a set of points and then aggregates features of local region points by max pooling. The FP layer consists of several fully connected layers for point feature encoding. We employ two SA layers to extract high-dimensional semantic features and location features, respectively. The combination of the two types of features are fed into the backbone, which consists of three SA layers and two FP layers, yielding a subset point set of size M ? (3 + C ). Then we follow VoteNet <ref type="bibr" target="#b23">[24]</ref> to learn each point's location offset to the center of an instance if the point belongs to a foreground instance. After offset prediction, the center-aware points are grouped into clusters, which are fed into a multi-layer perceptron (MLP) to generate the final proposals.</p><p>For instance completion (see <ref type="figure">Figure 4</ref> (b)), during training phase, we select up to K high-quality proposals in one scene whose distances to the center of ground truths boxes are less than a threshold ? and objectness confidences are higher than ?. For inference, we use Non-Maximum Suppression (NMS) operation to filter proposals, following <ref type="bibr" target="#b23">[24]</ref>. Then we pool N P points from each proposal and normalize them to canonical coordinates and locally voxelizes the partial points into the higher resolution 3D grid of shape H ? W ? D ? C, which enables the network to obtain instance-level supervision and prevents the local details of objects from being overwhelmed by the overall scene completion. The griding and re-griding operations proposed in <ref type="bibr" target="#b42">[43]</ref> are adopted to perform the conversion between point clouds and the 3D grid.</p><p>A 3D CNN encoder is followed to learn spatially-aware geometry embedding F IG from the 3D grid representation, which are then concatenated with instance-level semantic features F IS encoded from the instance's class confidences. It restrains the following decoder to reconstruct a complete instance that is consistent with its class's shape prior, obtaining better convergence results. Then, with the re-griding operation and a MLP, we can obtain the corresponding reconstructed point cloud of size N R ? 3 with detailed geometry structure, as shown in <ref type="figure">Figure 4</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Instance-to-Scene Completion and Iterative Refinement</head><p>With the reconstructed instances at hand, we can project them back into the 3D semantic scene volume to assist the completion of the overall 3D scene. Thus, we voxelize the completed instances so that they can match the resolution of the 3D semantic scene volume. which leads to an enhanced instance input for subsequent stage.</p><p>Specifically, we update the semantic category vector of the V S0 volume to obtain V S1 by replacing each instance's voxel labels with the reconstructed labels from I 1 . Then, the following scene completion stage S 1 , which shares the  <ref type="figure">Figure 4</ref>. Architecture details of scene-to-instance completion. We generate high-quality proposals with a backbone and proposal module to get instances' locations, sizes and categories and then locally voxelizes them into higher resolution to recover detailed 3D shapes.</p><p>same architecture of S 0 , can enjoy more detailed geometry structure information of partial instances than S 0 . Moreover, under the supervision of scene's semantic and occupied ground truth, the S 1 can further refine both reconstructed instances from the previous I 1 stage as well as the background in the overall 3D scene layout, resulting in a more comprehensive understanding of the scene. To further and fully integrate the instance-level and scene-level information, we propose a weight-sharing iteration framework. The same scene-to-instance completion (S i ? I i+1 ) and instance-to-scene completion (I i+1 ? S i+1 ) stages can be iterated to fully integrate information between the instances and the scene for better completion accuracy. During training, to deal with the different distributions at the multiple iterations, the earlier iterations would generate additional training data for the later stages. Specifically, S i would collect all the enhanced scene volumes from I i , I i?1 , . . . , I 1 as training data. I i would utilize all completed scene volumes from S i?1 , S i?2 , . . . , S 0 to provide training data for instance completion. In this way, the trained instance-to-scene completion and scene-to-instance completion stages can well handle the data variations across different iterations. During inference, the trained S N and I N would be iteratively and alternatively connected to gradually complete the whole 3D semantic scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss Function</head><p>For each stage in the method, we adopt the respective loss functions to perform instance and scene completion. For instance completion I i , the loss function consists of two terms for proposals generation and instance completion. Loss Function for Proposals Generation. The predicted location offset ?x i can be supervised by a regression loss</p><formula xml:id="formula_0">L loc-reg = 1 N total i on objects ?x i ? ?x * i ,<label>(1)</label></formula><p>where N total is the total number of instances' points, ?x * i is the ground truth offset from the point x i to its corresponding instance's center. To supervise the generation of proposals, we define positive and negative proposals generated from grouped clusters by determining whether these locations' distance to a ground truth object center is within 0.3 meters or more than 0.6 meters.</p><p>The positive proposals are supervised by a box loss L box and semantic classification loss L sem?cls . We use the standard cross entropy loss for L sem?cls and, following <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, we decouple L box into center regression, size estimation which use a hybrid of classification and regression formulations. The cross entropy loss is used for size classification and the robust L1-smooth loss is employed for regression:</p><formula xml:id="formula_1">L box = L center?reg + ? 1 L size?cls + L size?reg ,<label>(2)</label></formula><p>where ? 1 is a relative weight for the size classification term.</p><p>To supervise objectness of proposals, we employ a normalized cross entropy loss L obj?cls , which is performed on both positive and negative samples. In summary, the loss for proposal generation can be written as</p><formula xml:id="formula_2">L det = L loc?reg + L box + ? 2 L obj?cls + ? 3 L sem?cls ,</formula><p>where ? 2 and ? 3 are the relative weights for the loss terms. Loss Function for Instance Completion. To make the reconstructed points capture detailed geometry structures, we exploit the Chamfer Distance (CD) to supervise the reconstruction process.</p><formula xml:id="formula_3">Let T = {(x i , y i , z i )} n T i=1 be the ground truth and R = {(x i , y i , z i )} n R i=1</formula><p>be the reconstructed instance point set, where n T and n R denote the numbers of points of T and R. The CD loss can be written as</p><formula xml:id="formula_4">LCD = 1 nT t?T min r?R t ? r 2 2 + 1 nR r?R min t?T t ? r 2 2 . (3)</formula><p>Finally, through a combination of the proposal loss and the CD loss, we can jointly train our instance completion stages, which can be expressed as</p><formula xml:id="formula_5">Linstance = L det + LCD.<label>(4)</label></formula><p>Loss Function for Scene Completion. The scene completion is supervised by voxel-wise cross-entropy loss:</p><formula xml:id="formula_6">L scene = ce (S i (V T , V Si ), G) , i ? {0, ? ? ? , N } (5)</formula><p>where G is the ground truth semantic label and ce denotes the multi-class cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metrics</head><p>Datasets. We evaluate the proposed method on both real and synthetic datasets. The two real datasets are the popular NYU Depth V2 <ref type="bibr" target="#b30">[31]</ref> (denoted as NYU in the following) and NYUCAD <ref type="bibr" target="#b5">[6]</ref>. They both consists of 1449 indoor scenes   <ref type="table">Table 3</ref>. Results on SUNCG-RGBD dataset. Bold numbers represent the best scores. (a, b) means the input and output resolution. and there are 795 for training and 654 for test. We follow <ref type="bibr" target="#b33">[34]</ref> and use the 3D annotated labels provided by <ref type="bibr" target="#b27">[28]</ref> for semantic scene completion task. NYU dataset is a challenging dataset, as its depth measurements have errors compared with ground-truths. To address the misalignment of some depth maps and their corresponding label volumes. NYUCAD uses the depth maps generated from the projections of the 3D annotations, however, the RGB image and depth map are not aligned. SUNCG-RGBD <ref type="bibr" target="#b20">[21]</ref> is a subset dataset of SUNCG <ref type="bibr" target="#b33">[34]</ref>, consisting of 13,011 training samples and 499 testing samples and all samples provide depth images and corresponding RGB images which are not provided in SUNCG. Evaluation Metrics. We follow SSCNet <ref type="bibr" target="#b33">[34]</ref> which evaluates semantic scene completion with two types of metrics. The first one focuses on evaluating semantic performance i.e., semantic scene completion (SSC) and the other one cares more about scene completion (SC), which only measures the accuracy of occupancy instead of all categories. For SSC, we evaluate the IoU of each category on both sur-face and occluded voxels in the view frustum. For SC, we treat all voxels as binary predictions, i.e., free or non-free. We use recall, precision and voxel-wise intersection over union (IoU) as evaluation metrics to evaluate the binary IoU on occluded voxels in the view frustum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We use PyTorch framework to implement our experiments with 8 NVIDIA 1080 Ti GPUs. Specifically, for instance completion, we adopt an Adam optimizer with an initial learning rate (lr) 10 ?3 and 10 batch size per GPU to train detection backbone with 180 epochs. The lr is decreased by 10? at iterations 80, 120 and 160. Then, loading the pre-trained detection weights, we jointly train the detection backbone with 10 ?4 lr and instance shape completion with 10 ?3 lr for another 20 epochs. For scene completion, we employ an SGD optimizer with initial lr 0.1, batch size 2 per GPU to train NYU and SUNCG-RGBD with 250 and 160 epochs, respectively. We adopt a poly learning rate policy where the lr is changed with the iteration number t as  <ref type="table">Table  TVs</ref> Furn. Objects  of SSCNet <ref type="bibr" target="#b33">[34]</ref>, (e) results of Sketch <ref type="bibr" target="#b2">[3]</ref>, (f) baseline (without using instance completion), (g) our results. Our results achieve higher voxel-level accuracy compared with SSCNet <ref type="bibr" target="#b33">[34]</ref> and Sketch <ref type="bibr" target="#b2">[3]</ref>. Better viewed in color and zoom in.</p><p>(1 ? iter max iter ) 0.9 . Our final model employs 2 iterations (N = 2) to obtain the final results. We use ? 1 = ? 3 = 0.1, ? 2 = 0.5 for the combination of losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons with State-of-the-art Methods</head><p>We conduct experiments on NYU, NYUCAD and SUNCG-RGBD datasets, which demonstrates the superiority of our method to the existing state-of-the-art methods. Comparison on NYU and NYUCAD datasets. <ref type="table" target="#tab_2">Tables 1  and 2</ref> show the performances of different methods on the NYU and NYUCAD datasets. We can see that our SIS-Net achieves the best performance on both datasets. For the SC metric, the proposed method outperforms Sketch [4] by 6.5% and 2.3% on NYU and NYUCAD, respectively. For the SSC metric, our approach achieves 49.8% and 59.9%, and surpasses Sketch <ref type="bibr" target="#b3">[4]</ref> by 8.7% and 4.7%, respectively.</p><p>Our approach consistently outperforms state-of-theart methods with comparable parameters as Sketch <ref type="bibr" target="#b2">[3]</ref> (when using BiSeNet <ref type="bibr" target="#b44">[45]</ref>). When using our designed DeepLabv3 <ref type="bibr" target="#b1">[2]</ref> based method, as shown in <ref type="table" target="#tab_2">Tables 1, 2</ref> and 5, the improvements are even greater, achieving 11.3% and 8.3% in terms of SSC on NYU and NYUCAD, respectively. Also note that our SISNet's output resolution (60, 36, 60) is on par or lower than those of existing methods, denoting that our voxel-wise accuracy is much higher. Our design that locally increases the resolution of instance in the intermediate process of the network can achieves better results and dose not increase computation cost significantly. In addition, we observe that our instance-classes' completion performances consistently surpass existing methods like TVs, whose gains can reach up to 10%. We attribute the improvement to the exploitation of the completed 3D scene semantics, which greatly boost the performance of instance completion with well preserved shape details. Comparison on SUNCG-RGBD Dataset. We also conduct experiments on the SUNCG-RGBD <ref type="bibr" target="#b20">[21]</ref> dataset to validate the generalization of the proposed method on largescale dataset. As shown in <ref type="table">Table 3</ref>, with the same input and comparable computation cost, our method outperforms Sketch with considerable margins about 8.1% and 23.2% in terms of the overall IoUs of SC and SSC, respectively. Qualitative Results. <ref type="figure" target="#fig_6">Figure 5</ref> illustrates the qualitative results on NYU. More results please refer to supplementary materials. Although the previous methods work well for some scenes, they usually fail to deal with complex regions consisting of semantic confusing objects or instances with complex structures. By leveraging the complete 3D scene layout and semantics, the instance and scene level information can be effectively propagated and integrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>To verify the effects of each component of our SISNet, we employ the initial scene completion, i.e. S 0 , as our baseline and perform ablation studies on the three datasets. Effects of instance completion. To evaluate the benefits of introducing instance completion to SSC, we compare an alternative strategy that skips the instance completion and directly refine the roughly estimated results of S 0 with another scene completion S 1 .  fined again without extra parameters, there will be more obvious gains (i.e., +2-Iter I-S), which proves that the instance completion significantly boosts the scene prediction. Effects of scene-to-instance completion. We design a comparison experiment that directly performs instance completion without the initial scene completion (denoted by I 1 -S 1 ) to compare with our solution (denoted by S 0 -I 1 -S 1 ). We compare both instance-level detection accuracy and shape completion performance. The evaluation metrics of detection are mean average precision (mAP) and recall with 3D IoU threshold 0.25 following <ref type="bibr" target="#b31">[32]</ref>. The evaluation of instance shape completion uses the SSC metric and only considers instance-classes (excluding ceil, floor, and wall). <ref type="table" target="#tab_5">Table 7</ref> shows results of the above two setups on the three datasets. Without taking advantages of the roughly estimated 3D scene, I 1 -S 1 shows much worse performance on instance detection and shape completion, decreasing about 10% (as shown by the columns 3-5 of <ref type="table" target="#tab_5">Table 7</ref>). In contrast, our proposed S 0 -I 1 -S 1 shows significant improvements on both detection and instance shape completion. It shows about 1%, 2% and 1% gains on semantic scene completion with NYU, NYUCAD and SUNCG-RGBD, respectively. It demonstrates that the initial scene completion greatly assists the follow-up instance detection and completion by fully utilizing the valuable scene completion ground truths. Effects of scene-instance-scene iterations. To evaluate the effect of our proposed scene-instance-scene iteration mechanism, we stack different number of such iterations in a weight-sharing manner (denoted as +2-Iter I-S vs. +Iter I-S). As illustrated in <ref type="table">Table 4</ref>, based on the rough results of S 0 , one iteration can improve baseline SSC by 5.2%, 4.4% and 3.6% on NYU, NYUCAD and SUNCG-RGBD, respectively. Two iterations can consistently bring about 2% further improvement in terms of SSC. This demonstrates the effectiveness of our iteration mechanism. We also test using more iterations (N &gt;= 3) and the completion performance does not show obvious improvement (?1%). We therefore use N = 2 iterations for our final model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we present a novel framework that decouples the instances from a coarsely completed scene and exploits category priors to guide the reconstruction of the findgrained shape details as well as nearby objects whose semantic categories are easily mixed-up. Furthermore, we design an iterative mechanism, where the scene and instance completion benefits each other. Extensively experiments prove that the proposed method consistently achieves stateof-the-art performance on public benchmark.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Visualization of Semantic Scene Completion on NYU Dataset. From left to right: (a) RGB input, (b) depth map, (c) ground truth of semantic scene completion, (d) results of SSCNet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2. Overview of the Proposed Method. SISNet consists of iterative scene-to-instance completion and instance-to-scene completion stages. Given single-view RGBD images, TSDF from the depth map and semantic volume from the reprojection of 2D semantic segmentation are input into the initial scene completion (S0) to obtain a coarse completed scene, which is then fed into the first instance completion (I1) to locally recover the instance details. The completed instances are further merged into the semantic volume input to obtain better scene completion prediction (S1). More Ii-Si iterations are conducted to promote information integration between the instances and the scene in a weight-sharing manner without extra parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Semantic Scene Completion results on NYU dataset. From left to right: (a) RGB input, (b) Depth, (c) ground truth, (d) results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>caiyingjie@link.cuhk.edu.hk, cedarchen@pku.edu.cn</figDesc><table><row><cell>(a) RGB</cell><cell cols="2">(b) Depth</cell><cell cols="2">(c) Ground Truth</cell><cell></cell><cell>(d) SSCNet</cell><cell></cell><cell>(e) Sketch</cell><cell></cell><cell>(f) Proposed Method</cell></row><row><cell>Ceil</cell><cell>Floor</cell><cell>Wall</cell><cell>Window</cell><cell>Chair</cell><cell>Bed</cell><cell>Sofa</cell><cell>Table</cell><cell>TVs</cell><cell>Furn.</cell><cell>Objects</cell></row><row><cell>(a) RGB</cell><cell cols="2">(b) Depth</cell><cell cols="2">(c) Ground Truth</cell><cell></cell><cell>(d) SSCNet</cell><cell></cell><cell cols="2">(e) Sketch</cell><cell>(f) Proposed Method</cell></row><row><cell>Ceil</cell><cell>Floor</cell><cell>Wall</cell><cell>Window</cell><cell>Chair</cell><cell>Bed</cell><cell>Sofa</cell><cell>Table</cell><cell>TVs</cell><cell>Furn.</cell><cell>Objects</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>wall win. chair bed sofa table tvs furn. objs. avg. 94.8 28.0 12.2 19.6 57.0 50.5 17.6 11.9 35.6 15.3 32.9 90.8 32.3 14.8 18.2 51.1 44.8 15.2 22.4 38.3 15.7 33.Results on NYU dataset. Bold numbers represent the best scores. (a, b) means the input and output resolution.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Scene Completion (SC)</cell><cell></cell><cell></cell><cell cols="2">Semantic Scene Completion (SSC)</cell></row><row><cell cols="7">Methods ceil. floor SSCNet [34] Resolution prec. recall IoU (240, 60) 57.0 94.5 55.1 15.1 94.7 24.4 0.0</cell><cell>12.6 32.1 35.0 13.0</cell><cell>7.8</cell><cell>27.1 10.1 24.7</cell></row><row><cell>ESSCNet [47]</cell><cell>(240, 60)</cell><cell>71.9</cell><cell>71.9</cell><cell>56.2</cell><cell cols="2">17.5 75.4 25.8 6.7</cell><cell>15.3 53.8 42.4 11.2</cell><cell>0</cell><cell>33.4 11.8 26.7</cell></row><row><cell>DDRNet [19]</cell><cell>(60, 60)</cell><cell>71.5</cell><cell>80.8</cell><cell>61.0</cell><cell cols="2">21.1 92.2 33.5 6.8</cell><cell cols="3">14.8 48.3 42.3 13.2 13.9 35.3 13.2 30.4</cell></row><row><cell cols="10">VVNetR-120 [11] 19.3 AICNet [17] (120, 60) 69.8 83.1 61.1 (60, 60) 62.4 91.8 59.2 23.2 3</cell></row><row><cell>TS3D [8]</cell><cell>(240, 60)</cell><cell>-</cell><cell>-</cell><cell>60.0</cell><cell>9.7</cell><cell cols="4">93.4 25.5 21.0 17.4 55.9 49.2 17.0 27.5 39.4 19.3 34.1</cell></row><row><cell>SATNet [21]</cell><cell>(60, 60)</cell><cell>67.3</cell><cell>85.8</cell><cell>60.6</cell><cell cols="5">17.3 92.1 28.0 16.6 19.3 57.5 53.8 17.2 18.5 38.4 18.9 34.4</cell></row><row><cell>ForkNet [40]</cell><cell>(80, 80)</cell><cell>-</cell><cell>-</cell><cell>63.4</cell><cell cols="5">36.2 93.8 29.2 18.9 17.7 61.6 52.9 23.3 19.5 45.4 20.0 37.1</cell></row><row><cell>CCPNet [49]</cell><cell cols="2">(240, 240) 74.2</cell><cell>90.8</cell><cell>63.5</cell><cell cols="5">23.5 96.3 35.7 20.2 25.8 61.4 56.1 18.1 28.1 37.8 20.1 38.5</cell></row><row><cell>Sketch [3]</cell><cell>(60, 60)</cell><cell>85.0</cell><cell>81.6</cell><cell>71.3</cell><cell cols="5">43.1 93.6 40.5 24.3 30.0 57.1 49.3 29.2 14.3 42.5 28.6 41.1</cell></row><row><cell>Baseline (BiSeNet)</cell><cell>(60, 60)</cell><cell>87.6</cell><cell>78.9</cell><cell>71.0</cell><cell cols="5">46.9 93.3 41.3 26.7 30.8 58.4 49.5 27.2 22.1 42.2 28.7 42.5</cell></row><row><cell>Ours (BiSeNet)</cell><cell>(60, 60)</cell><cell>90.7</cell><cell>84.6</cell><cell>77.8</cell><cell cols="5">53.9 93.2 51.3 38.0 38.7 65.0 56.3 37.8 25.9 51.3 36.0 49.8</cell></row><row><cell>Baseline (DeepLabv3)</cell><cell>(60, 60)</cell><cell>88.7</cell><cell>77.7</cell><cell>70.8</cell><cell cols="5">46.8 93.4 42.0 32.4 36.0 61.1 55.8 28.2 27.6 45.7 32.9 45.6</cell></row><row><cell>Ours (DeepLabv3)</cell><cell>(60, 60)</cell><cell>92.1</cell><cell>83.8</cell><cell>78.2</cell><cell cols="5">54.7 93.8 53.2 41.9 43.6 66.2 61.4 38.1 29.8 53.9 40.3 52.4</cell></row><row><cell></cell><cell></cell><cell cols="3">Scene Completion (SC)</cell><cell></cell><cell></cell><cell cols="2">Semantic Scene Completion (SSC)</cell></row><row><cell>Methods</cell><cell cols="3">Resolution prec. recall</cell><cell>IoU</cell><cell cols="5">ceil. floor wall win. chair bed sofa table tvs furn. objs. avg.</cell></row><row><cell>SSCNet [34]</cell><cell>(240, 60)</cell><cell>75.4</cell><cell>96.3</cell><cell>73.2</cell><cell cols="2">32.5 92.6 40.2 8.9</cell><cell>33.9 57.0 59.5 28.3</cell><cell>8.1</cell><cell>44.8 25.1 40.0</cell></row><row><cell>DDRNet [19]</cell><cell>(60, 60)</cell><cell>88.7</cell><cell>88.5</cell><cell>79.4</cell><cell cols="3">54.1 91.5 56.4 14.9 37.0 55.7 51.0 28.8</cell><cell>9.2</cell><cell>44.1 27.8 42.8</cell></row><row><cell>AICNet [17]</cell><cell>(60, 60)</cell><cell>88.2</cell><cell>90.3</cell><cell>80.5</cell><cell cols="3">53.0 91.2 57.2 20.2 44.6 58.4 56.2 36.2</cell><cell>9.7</cell><cell>47.1 30.4 45.8</cell></row><row><cell>TS3D [8]</cell><cell>(240, 60)</cell><cell>-</cell><cell>-</cell><cell>76.1</cell><cell cols="5">25.9 93.8 48.9 33.4 31.2 66.1 56.4 31.6 38.5 51.4 30.8 46.2</cell></row><row><cell>CCPNet [49]</cell><cell cols="2">(240, 240) 91.3</cell><cell>92.6</cell><cell>82.4</cell><cell cols="5">56.2 94.6 58.7 35.1 44.8 68.6 65.3 37.6 35.5 53.1 35.2 53.2</cell></row><row><cell>Sketch [3]</cell><cell>(60, 60)</cell><cell>90.6</cell><cell>92.2</cell><cell>84.2</cell><cell cols="5">59.7 94.3 64.3 32.6 51.7 72.0 68.7 45.9 19.0 60.5 38.5 55.2</cell></row><row><cell>Baseline (BiSeNet)</cell><cell>(60, 60)</cell><cell>92.3</cell><cell>89.0</cell><cell>82.8</cell><cell cols="5">61.5 94.2 62.7 38.0 48.1 69.5 59.3 40.1 25.8 54.6 35.3 53.6</cell></row><row><cell>Ours (BiSeNet)</cell><cell>(60, 60)</cell><cell>94.2</cell><cell>91.3</cell><cell>86.5</cell><cell cols="5">65.6 94.4 67.1 45.2 57.2 75.5 66.4 50.9 31.1 62.5 42.9 59.9</cell></row><row><cell>Baseline (DeepLabv3)</cell><cell>(60, 60)</cell><cell>92.0</cell><cell>89.3</cell><cell>82.8</cell><cell cols="5">62.0 94.1 63.3 43.5 50.8 73.3 63.5 42.2 40.6 58.2 39.7 57.4</cell></row><row><cell>Ours (DeepLabv3)</cell><cell>(60, 60)</cell><cell>94.1</cell><cell>91.2</cell><cell>86.3</cell><cell cols="5">63.4 94.4 67.2 52.4 59.2 77.9 71.1 51.8 46.2 65.8 48.8 63.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Results on NYUCAD dataset. Bold numbers represent the best scores. (a, b) means the input and output resolution.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Scene Completion (SC)</cell><cell>Semantic Scene Completion (SSC)</cell></row><row><cell>Methods</cell><cell cols="3">Resolution prec. recall</cell><cell>IoU</cell><cell>ceil. floor wall win. chair bed sofa table tvs furn. objs. avg.</cell></row><row><cell>SATNet [21]</cell><cell>(60, 60)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>60.6 57.3 53.2 52.7 27.4 46.8 53.3 28.6 41.1 44.1 29.0 44.9</cell></row><row><cell>Sketch [3]</cell><cell>(60, 60)</cell><cell>94.1</cell><cell>86.2</cell><cell>81.8</cell><cell>77.9 82.3 68.4 57.9 35.7 71.8 63.7 45.1 12.8 64.2 32.0 55.6</cell></row><row><cell>Baseline (BiSeNet)</cell><cell>(60, 60)</cell><cell>90.6</cell><cell>94.5</cell><cell>86.0</cell><cell>71.0 86.6 78.4 78.7 53.2 77.4 77.3 60.6 76.4 83.7 60.0 73.0</cell></row><row><cell>Ours (BiSeNet)</cell><cell>(60, 60)</cell><cell>93.3</cell><cell>96.1</cell><cell>89.9</cell><cell>85.2 90.0 83.7 80.8 60.0 83.5 80.9 68.6 77.3 86.7 70.1 78.8</cell></row><row><cell>Baseline (DeepLabv3)</cell><cell>(60, 60)</cell><cell>90.1</cell><cell>94.5</cell><cell>85.6</cell><cell>70.5 86.2 77.5 79.0 55.5 82.8 78.8 62.4 69.5 81.7 57.8 72.9</cell></row><row><cell>Ours (DeepLabv3)</cell><cell>(60, 60)</cell><cell>92.6</cell><cell>96.3</cell><cell>89.3</cell><cell>85.4 90.6 82.6 80.9 62.9 84.5 82.6 71.6 72.6 85.6 69.7 79.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 Table 4 .Table 5 .Table 6 .</head><label>6456</label><figDesc>shows the quantitative results of the compared scheme, denoted as + S 1 only and +Iter I-S. Specifically, the SC drop 1.0%, 1.8% and 0.6% on NYU, NYUCAD and SUNCG-RGBD for +S 1 only scheme. Meanwhile, the SSC results decrease 2.8%, 3.8% and 2.1% on the dataset. When the instances are re-wall win. chair bed sofa table tvs furn. objs. avg. 93.3 41.3 26.7 30.8 58.4 49.5 27.2 22.1 42.2 28.7 42.5 +Iter 93.6 50.7 35.4 37.3 62.8 54.1 34.0 22.2 48.7 33.8 47.7 +2-Iter I-S 93.2 51.3 38.0 38.7 65.0 56.3 37.8 25.9 51.3 36.0 49.8 94.2 62.7 38.0 48.1 69.5 59.3 40.1 25.8 54.6 35.3 53.<ref type="bibr" target="#b5">6</ref> +Iter Ablation studies of the effects of scene-instance-scene iterations on NYU, NYUCAD and SUNCG-RGBD dataset. Efficiency with different methods on NYU dataset. Ablation studies of the effects of instance completion.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Scene Completion (SC)</cell><cell>Semantic Scene Completion (SSC)</cell></row><row><cell cols="7">Methods ceil. floor Baseline Trained on prec. recall IoU NYU 87.6 78.9 71.0 46.9 I-S NYU 91.1 82.5 76.4 52.5 NYU 90.7 84.6 77.8 53.9 Baseline NYUCAD 92.3 89.0 82.8 61.5 I-S NYUCAD 93.4 90.5 85.1 63.8 94.4 65.1 43.0 54.1 73.9 64.7 47.8 30.4 60.2 40.6 58.0</cell></row><row><cell>+2-Iter I-S</cell><cell>NYUCAD</cell><cell cols="2">94.2</cell><cell>91.3</cell><cell>86.5</cell><cell>65.6 94.4 67.1 45.2 57.2 75.5 66.4 50.9 31.1 62.5 42.9 59.9</cell></row><row><cell>Baseline</cell><cell cols="3">SUNCG-RGBD 90.6</cell><cell>94.5</cell><cell>86.0</cell><cell>71.0 86.6 78.4 78.7 53.2 77.4 77.3 60.6 76.4 83.7 60.0 73.0</cell></row><row><cell>+Iter I-S</cell><cell cols="3">SUNCG-RGBD 92.2</cell><cell>95.5</cell><cell>88.4</cell><cell>79.3 88.8 82.5 80.4 56.8 81.2 79.5 63.8 77.9 85.3 67.1 76.6</cell></row><row><cell cols="4">+2-Iter I-S SUNCG-RGBD 93.3</cell><cell>96.1</cell><cell>89.9</cell><cell>85.2 90.0 83.7 80.8 60.0 83.5 80.9 68.6 77.3 86.7 70.1 78.8</cell></row><row><cell cols="2">Method</cell><cell cols="5">SC(%) SSC(%) Params/M</cell></row><row><cell cols="2">Sketch [3]</cell><cell>71.3</cell><cell></cell><cell>41.1</cell><cell cols="2">28.0</cell></row><row><cell cols="2">Ours (BiSeNet)</cell><cell>77.8</cell><cell></cell><cell>49.8</cell><cell cols="2">30.5</cell></row><row><cell cols="2">Ours (DeepLabv3)</cell><cell>78.2</cell><cell></cell><cell>52.4</cell><cell cols="2">47.3</cell></row><row><cell>Scheme</cell><cell cols="2">Dataset</cell><cell cols="4">SC-IoU(%) SSC-mIoU(%)</cell></row><row><cell>Baseline (S 0 )</cell><cell>NYU</cell><cell></cell><cell></cell><cell>71.0</cell><cell></cell><cell>42.5</cell></row><row><cell>+S 1 only</cell><cell>NYU</cell><cell></cell><cell cols="2">75.4 (+4.4)</cell><cell cols="2">44.9 (+2.4)</cell></row><row><cell>+Iter I-S</cell><cell>NYU</cell><cell></cell><cell cols="2">76.4 (+5.4)</cell><cell cols="2">47.7 (+5.2)</cell></row><row><cell>+2-Iter I-S</cell><cell>NYU</cell><cell></cell><cell cols="2">77.8 (+6.8)</cell><cell cols="2">49.8 (+7.3)</cell></row><row><cell>Baseline (S 0 )</cell><cell cols="2">NYUCAD</cell><cell></cell><cell>82.8</cell><cell></cell><cell>53.6</cell></row><row><cell>+S 1 only</cell><cell cols="2">NYUCAD</cell><cell cols="2">83.3 (+0.5)</cell><cell cols="2">54.2 (+0.6)</cell></row><row><cell>+Iter I-S</cell><cell cols="2">NYUCAD</cell><cell cols="2">85.1 (+2.3)</cell><cell cols="2">58.0 (+4.4)</cell></row><row><cell>+2-Iter I-S</cell><cell cols="2">NYUCAD</cell><cell cols="2">86.5 (+3.7)</cell><cell cols="2">59.9 (+6.3)</cell></row><row><cell cols="3">Baseline (S 0 ) SUNCG-RGBD</cell><cell></cell><cell>86.0</cell><cell></cell><cell>73.0</cell></row><row><cell>+S 1 only</cell><cell cols="4">SUNCG-RGBD 87.8 (+1.8)</cell><cell cols="2">74.5 (+1.5)</cell></row><row><cell>+Iter I-S</cell><cell cols="4">SUNCG-RGBD 88.4 (+2.4)</cell><cell cols="2">76.6 (+3.6)</cell></row><row><cell>+2-Iter I-S</cell><cell cols="4">SUNCG-RGBD 89.9 (+3.9)</cell><cell cols="2">78.8 (+5.8)</cell></row><row><cell>Order</cell><cell>Dataset</cell><cell cols="5">Rec. mAP Shape SC SSC</cell></row><row><cell>Baseline (S 0 )</cell><cell>NYU</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>71.0 42.5</cell></row><row><cell>I 1 -S 1</cell><cell>NYU</cell><cell cols="3">61.7 25.9</cell><cell>25.9</cell><cell>75.5 46.6</cell></row><row><cell>S 0 -I 1 -S 1</cell><cell>NYU</cell><cell cols="3">68.8 34.6</cell><cell>35.1</cell><cell>76.4 47.7</cell></row><row><cell>Baseline (S 0 )</cell><cell>NYUCAD</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>82.8 53.6</cell></row><row><cell>I 1 -S 1</cell><cell>NYUCAD</cell><cell cols="3">75.0 41.1</cell><cell>30.1</cell><cell>83.1 55.5</cell></row><row><cell>S 0 -I 1 -S 1</cell><cell>NYUCAD</cell><cell cols="3">76.8 44.5</cell><cell>43.3</cell><cell>85.1 58.0</cell></row><row><cell cols="3">Baseline (S 0 ) SUNCG-RGBD</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>86.0 73.0</cell></row><row><cell>I 1 -S 1</cell><cell cols="4">SUNCG-RGBD 77.6 65.0</cell><cell>41.7</cell><cell>87.5 75.8</cell></row><row><cell>S 0 -I 1 -S 1</cell><cell cols="4">SUNCG-RGBD 81.2 70.4</cell><cell>50.1</cell><cell>88.4 76.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Ablation studies of effects of initial scene completion, where Rec. (recall) and mAP (IoU=0.25) denote the instance detection performance. Shape means the shape completion results.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">TSDF is a representation to encode depth volume, where every voxel stores the distance value d to its closest surface and the sign of the value indicates whether the voxel is in visible or invisible spaces.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Denver Dash</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Avetisyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Khanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.12622</idno>
	</analytic>
	<monogr>
		<title level="m">Angela Dai, and Matthias Nie?ner. Scenecad: Predicting object alignments and layouts in rgb-d scans</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gang Zeng, and Hongsheng Li. 3d sketch-aware semantic scene completion via semi-supervised structure prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gang Zeng, and Hongsheng Li. 3d sketch-aware semantic scene completion via semi-supervised structure prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aloisio</forename><surname>Dourado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teofilo</forename><surname>Emidio De Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hansung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02893</idno>
		<title level="m">Semantic scene completion from rgb-d images</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structured prediction of unobserved voxels from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Julier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5431" to="5440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Two stream 3d semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueh-Tung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johann</forename><surname>Sawatzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Two stream 3d semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johann</forename><surname>Sawatzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semantic scene completion combining colour and depth: preliminary experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Bernardes Soares Guedes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teofilo</forename><surname>Emidio De Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04735</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">View-volume network for semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05361</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">View-volume network for semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJ-CAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3d-sis: 3d semantic instance segmentation of rgb-d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Revealnet: Seeing behind objects in rgb-d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2098" to="2107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pf-net: Point fractal network for 3d point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7662" to="7670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Acquiring 3d indoor environments with variability and repetition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young Min Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Ming</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>TOG</publisher>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">138</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3d-rcnn: Instance-level 3d object reconstruction via render-andcompare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3559" to="3568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Anisotropic convolutional networks for 3d semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3351" to="3359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rgbd based dimensional decomposition residual network for 3d semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7693" to="7702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rgbd based dimensional decomposition residual network for 3d semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">See and think: Disentangling semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shice</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiankun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beibei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhe</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">See and think: Disentangling semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shice</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiankun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beibei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhe</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A search-classify approach for cluttered indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Sharf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">137</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discovering structural regularity in 3d geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Wallner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Pottmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TOG</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgbd data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Completing 3d object shape from one depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Thorsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2484" to="2493" />
		</imprint>
	</monogr>
	<note>JunYoung Gwak, Daeyun Shin, and Derek Hoiem</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An interactive approach to semantic modeling of indoor scenes with an rgbd camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjia</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">136</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03670</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="808" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1746" to="1754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1746" to="1754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A symmetry prior for convex variational 3d reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Speciale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="313" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Topnet: Structural point cloud decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="383" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Shape from symmetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Wegbreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1824" to="1831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cascaded refinement network for point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><forename type="middle">H</forename><surname>Ang</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="790" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Forknet: Multi-branch volumetric semantic completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">Joseph</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Point cloud completion by skip-attention network with hierarchical folding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1939" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Nassir Navab, and Federico Tombari. Scfusion: Real-time incremental scene reconstruction with semantic completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shun-Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tateno</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13662,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Grnet: Gridding residual network for dense point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiageng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03761</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Point-based 3d single stage object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="11040" to="11048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pcn: Point completion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="728" to="737" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient semantic scene completion network with spatial group convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongen</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="733" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Semantic scene completion with dense crf from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangming</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<editor>Syed Afaq Ali Shah, and Juan Song</editor>
		<imprint>
			<biblScope unit="volume">318</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="182" to="195" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cascaded context pyramid for full-resolution 3d semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinjie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="7801" to="7810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
