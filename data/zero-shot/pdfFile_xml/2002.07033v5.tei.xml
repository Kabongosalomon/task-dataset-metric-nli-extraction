<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards an Appropriate Query, Key, and Value Computation for Knowledge Tracing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 12-14, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngduck</forename><surname>Choi</surname></persName>
							<email>youngduck.choi@riiid.co</email>
							<affiliation key="aff0">
								<orgName type="institution">Riiid! AI Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngnam</forename><surname>Lee</surname></persName>
							<email>yn.lee@riiid.co</email>
							<affiliation key="aff0">
								<orgName type="institution">Riiid! AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junghyun</forename><surname>Cho</surname></persName>
							<email>jh.cho@riiid.co</email>
							<affiliation key="aff0">
								<orgName type="institution">Riiid! AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jineon</forename><surname>Baek</surname></persName>
							<email>jineon.baek@riiid.co</email>
							<affiliation key="aff0">
								<orgName type="institution">Riiid! AI Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungsoo</forename><surname>Kim</surname></persName>
							<email>byungsoo.kim@riiid.co</email>
							<affiliation key="aff0">
								<orgName type="institution">Riiid! AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeongmin</forename><surname>Cha</surname></persName>
							<email>ymcha@riiid.co</email>
							<affiliation key="aff0">
								<orgName type="institution">Riiid! AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmin</forename><surname>Shin</surname></persName>
							<email>dm.shin@riiid.co</email>
							<affiliation key="aff0">
								<orgName type="institution">Riiid! AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename><surname>Bae</surname></persName>
							<email>chan.bae@riiid.co</email>
							<affiliation key="aff0">
								<orgName type="institution">Riiid! AI Research</orgName>
							</affiliation>
							<affiliation key="aff3">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewe</forename><surname>Heo</surname></persName>
							<email>jwheo@riiid.co</email>
							<affiliation key="aff0">
								<orgName type="institution">Riiid! AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards an Appropriate Query, Key, and Value Computation for Knowledge Tracing</title>
					</analytic>
					<monogr>
						<meeting> <address><addrLine>Event, USA</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">August 12-14, 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3386527.3405945</idno>
					<note>ACM ISBN 978-1-4503-7951-9/20/08 ...$15.00.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Author Keywords Education</term>
					<term>Personalized learning</term>
					<term>Knowledge Tracing</term>
					<term>Deep Learning</term>
					<term>Transformer CCS Concepts</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge tracing, the act of modeling a student's knowledge through learning activities, is an extensively studied problem in the field of computer-aided education. Armed with attention mechanisms focusing on relevant information for target prediction, recurrent neural networks and Transformer-based knowledge tracing models have outperformed traditional approaches such as Bayesian knowledge tracing and collaborative filtering. However, the attention mechanisms of current state-of-the-art knowledge tracing models share two limitations. Firstly, the models fail to leverage deep self-attentive computations for knowledge tracing. As a result, they fail to capture complex relations among exercises and responses over time. Secondly, appropriate features for constructing queries, keys and values for the self-attention layer for knowledge tracing have not been extensively explored. The usual practice of using exercises and interactions (exercise-response pairs), as queries and keys/values, respectively, lacks empirical support.</p><p>In this paper, we propose a novel Transformer-based model for knowledge tracing, SAINT: Separated Self-AttentIve Neural Knowledge Tracing. SAINT has an encoder-decoder structure where the exercise and response embedding sequences separately enter, respectively, the encoder and the decoder. The encoder applies self-attention layers to the sequence of exercise embeddings, and the decoder alternately applies selfattention layers and encoder-decoder attention layers to the sequence of response embeddings. This separation of input allows us to stack attention layers multiple times, resulting in an improvement in area under receiver operating characteristic curve (AUC). To the best of our knowledge, this is the first work to suggest an encoder-decoder model for knowledge tracing that applies deep self-attentive layers to exercises and responses separately.</p><p>We empirically evaluate SAINT on a large-scale knowledge tracing dataset, EdNet, collected by an active mobile education application, Santa, which has 627,347 users, 72,907,005 response data points as well as a set of 16,175 exercises gathered since 2016. The results show that SAINT achieves state-of-theart performance in knowledge tracing with an improvement of 1.8% in AUC compared to the current state-of-the-art model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Creating a personalized educational agent that provides learning paths adapted to each student's ability and needs is a long-standing challenge of artificial intelligence in education. Knowledge tracing, a fundamental problem for developing such an agent, is the task of predicting a student's understanding of a target subject based on their learning activities over time. For example, one can predict the probability a student correctly answering a given exercise. Tracing the state of a student's understanding enables efficient assignment of resources tailored to their ability and needs.</p><p>Traditional approaches to knowledge tracing include Bayesian knowledge tracing <ref type="bibr" target="#b2">[3]</ref> and collaborative filtering <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b10">11]</ref>. With the advances in deep learning for applications to machine translation, healthcare and other modalities, neural network architectures such as Recurrent Neural Networks (RNNs) and Transformer <ref type="bibr" target="#b21">[22]</ref> have become the common building blocks in knowledge tracing models. These models effectively capture the complex nature of students' learning activities over time, which is often represented as high-dimensional and sequential data. In particular, <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b7">8]</ref> use RNNs with attention to predict the probability distribution of a student's response to an exercise. The model in <ref type="bibr" target="#b13">[14]</ref> serves the same purpose, but it is based on the Transformer model which leverages a self-attention mechanism. As a student goes through different exercises, their skills are correlated with the responses they gave to previous exercises. Therefore, attention mechanisms are a natural choice for knowledge tracing because they learn to capture the inter-dependencies among exercises and responses, and give more weight to entries relevant for prediction <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref>.  <ref type="figure">Figure 1</ref>. Architectures of previous models. There are two limitations of the attention mechanisms used in previous knowledge tracing models. First, the models have attention layers too shallow to capture the complex relationships among different exercises and responses. Second, the models rely on the same recipe: exercises for queries and interactions for keys and values.</p><p>Despite their strengths, the attention mechanisms applied to knowledge tracing currently have two limitations as shown in <ref type="figure">Figure 1</ref>. First, previous models have attention layers too shallow to capture the possibly complex relations among different exercises and responses. In particular, models in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14]</ref> have only one attention layer, and <ref type="bibr" target="#b13">[14]</ref> shows a decrease in performance when self-attention layer is stacked multiple times. Secondly, appropriate features for constructing queries, keys and values suited for knowledge tracing have not been explored thoroughly. Given a series of interactions (exerciseresponse pairs) of a student, previous works rely on the same recipe: exercises for queries and interactions for keys and values <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14]</ref>. Other choices may provide a substantial gain in performance and thus need to be tested.</p><p>In this paper, we address the problem of finding appropriate methods for constructing queries, keys and values for knowledge tracing. Supported by a series of extensive empirical explorations, we propose a novel Transformer-based model for knowledge tracing, SAINT: Separated Self-AttentIve Neural Knowledge Tracing. SAINT consists of an encoder and a decoder which are stacks of several identical layers composed of multi-head self-attention and point-wise feed-forward networks as shown in <ref type="figure">Figure 2</ref>. The encoder takes the sequence of exercise embeddings as queries, keys and values, and produces output through a repeated self-attention mechanism. The decoder takes the sequential input of response embeddings as queries, keys and values then alternately applies self-attention and attention layers to the encoder output. Compared to current state-of-the-art models, separating the exercise sequence and the response sequence, and feeding them to the encoder and decoder,respectively, are distinctive features that allow SAINT to capture complex relations among exercises and responses through deep self-attentive computations.</p><p>We conduct extensive experimental studies on a large scale knowledge tracing dataset, EdNet <ref type="bibr" target="#b1">[2]</ref>, collected by an active mobile education application, Santa, which has 627,347 users and 72,907,005 response data points on a set of 16,175 exercises gathered since 2016. We compare SAINT with current state-of-the-art models and Transformer-based variants of deep ...   <ref type="figure">Figure 2</ref>. The architecture of our proposed model, SAINT. Unlike previous models, separating the exercise sequence and the response sequence and applying the encoder and the decoder, respectively, are the distinctive features that allow SAINT to capture complex relations among exercises and responses through deep self-attentive computations.</p><p>knowledge tracing models. Our experimental results show that SAINT outperforms all other competitors and achieves stateof-the-art performance in knowledge tracing as measured by area under receiver operating characteristic curve (AUC) with an improvement of 1.8% compared to the current state-of-theart model, SAKT <ref type="bibr" target="#b13">[14]</ref>.</p><p>In summary, we make the following contributions:</p><p>? We propose SAINT, a novel Transformer based encoderdecoder model for knowledge tracing where the exercise embedding sequence and the response embedding sequence separately enter the encoder and the decoder respectively.</p><p>? We show that SAINT effectively captures complex relations among exercises and responses using deep self-attentive computations.</p><p>? We empirically show that SAINT achieves a 1.8% gain in AUC compared to the current state-of-the-art knowledge tracing model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RELATED WORKS</head><p>Knowledge tracing is the modeling of a student's state of knowledge over time as they go through different learning activities. It is an extensively studied problem in the field of computer-aided education. For example, one can predict the probability of a student correctly answering a given exercise. Such understanding enables efficient assignment of resources tailored to each student's ability and needs.</p><p>Traditional approaches to knowledge tracing include Bayesian Knowledge Tracing (BKT) <ref type="bibr" target="#b2">[3]</ref>, Collaborative Filtering (CF) <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b10">11]</ref> and many others. BKT is a classical, prominent approach to knowledge tracing that has been studied extensively over time <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19]</ref>. BKT represents a student's knowledge state as a tuple of binary values. Each value represents whether a student understands an individual concept or not. The values are updated by a hidden Markov model using actual student responses.</p><p>With the rise of Deep Learning (DL), DL-based knowledge tracing models such as Exercise-aware Knowledge Tracing (EKT) <ref type="bibr" target="#b7">[8]</ref>, Neural Pedagogical Agent (NPA) <ref type="bibr" target="#b11">[12]</ref> and Self-Attentive Knowledge Tracing (SAKT) <ref type="bibr" target="#b13">[14]</ref> have been shown to outperform traditional models. EKT and NPA are Bidirectional Long-Short Term Memory (Bi-LSTM) models with an attention mechanism built on the top layer. SAKT is a Transformer based model with exercises as attention queries and past interactions (exercise-response pairs) as attention keys/values. For effective knowledge tracing, one needs to analyze the relationships between the different parts of learning activity data. The attention mechanism serves that purpose by weighing the more relevant parts of data heavier for prediction.</p><p>However, DL-based knowledge tracing models currently have two limitations. First, the attention layer is shallow, so it is not able to capture the complex nature of students' learning activities over time which are often represented as highdimensional and sequential data. The networks in <ref type="bibr" target="#b13">[14]</ref> do not apply self-attention to interactions and exercises, but use the latent features of the embedding layer directly as the input to the attention layer. Also, they only use a single attention layer between interactions and exercises. The networks in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b7">8]</ref> have deep LSTM layers that analyze the sequential nature of interactions, but the exercises are embedded directly and, again, they only use a single layer of attention between interactions and exercises.</p><p>Second, different combinations of features for attention queries, keys and values suited for knowledge tracing have not been thoroughly explored. All of <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14]</ref> use the same recipe: Given a series of interactions of a student, they use exercises to build the queries and interactions to build the keys/values. Other possibilities, for instance, using selfattention on exercises or responses, to name one possibility among many, may provide substantial improvement and thus should be tested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PROPOSED MODEL Problem Definition</head><p>Given the history of how a student responded to a set of exercises, SAINT predicts the probability that the student will answer a particular new exercise correctly. Formally, the student activity is recorded as a sequence I 1 , ? ? ? , I n of interactions</p><formula xml:id="formula_0">I i = (E i , R i ).</formula><p>Here E i denotes the exercise information, the i-th exercise given to the student with related metadata such as the type of the exercise. Similarly, the response information R i denotes the student response r i to E i with related metadata such as the duration of time the student took to respond. The student response r i ? {0, 1} is equal to 1 if their i-th response is correct and 0 if it is not. Thus, SAINT is designed to predict the probability</p><formula xml:id="formula_1">P(r k = 1|I 1 , ? ? ? , I k?1 , E k )</formula><p>of a student answering the i'th exercise correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Representation</head><p>SAINT takes the sequences of exercise information E 1 , ? ? ? , E k and response information R 1 , ? ? ? , R k?1 as input, and predicts the k'th user response r k . The embedding layer in SAINT maps each E i and R i to a vector in latent space, producing a sequence of exercise embeddings E e 1 , ? ? ? , E e k and a sequence of response embeddings R e 1 , ? ? ? , R e k?1 . The layer embeds the following attributes of E i and R i .</p><p>? Exercise ID: A latent vector is assigned to an ID unique to each exercise.</p><p>? Exercise category: Each exercise belongs to a category of the domain subject. A latent vector is assigned to each category.</p><p>? Position: The position (1st, 2nd, ...) of an exercise or a response in the input sequence is represented as a position embedding vector. The position embeddings are shared across the exercise sequence and the response sequence.</p><p>? Response: A latent vector is assigned to each possible value (0 or 1) of a student's response r i .</p><p>? Elapsed time: The time a student took to respond in seconds is rounded to an integer value. A latent vector is assigned to each integer between 0 and 300, inclusive. Any time more than 300 seconds is capped off to 300 seconds.</p><p>? Timestamp: Month, day and hour of the absolute time when a student received each exercise is recorded. A unique latent vector is assigned for every possible combination of month, day and hour.</p><p>The final embedding E e i or R e i is the sum of all embedding vectors of its constituting attributes (see <ref type="figure" target="#fig_1">Figure 3</ref>). For example, the exercise information E i consists of the exercise ID, the exercise category, and the position, so we construct the exercise embedding E e i by summing the corresponding exercise ID embedding, category embedding, and position embedding. Although we do not directly embed the interaction I i in SAINT, we will use interaction embeddings in Transformer-based variants of deep knowledge tracing models for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Self-Attentive Encoder-Decoder</head><p>Our proposed model SAINT, based on the Transformer architecture proposed in <ref type="bibr" target="#b21">[22]</ref>, consists of an encoder and a decoder as shown in <ref type="figure">Figure 2</ref>. The encoder takes the sequence E e = [E e 1 , ? ? ? , E e k ] of exercise embeddings and feeds the processed output O = [O 1 , ? ? ? , O k ] to the decoder. The decoder takes O and another sequential input R e = [S, R e 1 , ? ? ? , R e k?1 ] of response embeddings with the start token embedding S, and produces the predicted responsesr = [r 1 , ? ? ? ,r k ].</p><formula xml:id="formula_2">O = Encoder(E e ) r = Decoder(O, R e )</formula><p>The encoder and decoder are combinations of multi-head attention networks, which are the core components of SAINT, followed by feed-forward networks. Unlike the original Transformer architecture, SAINT masks inputs corresponding to information from the future for all multi-head attention networks to prevent invalid attending. This ensures that the computation ofr k depends only on the previous exercises E 1 , ? ? ? , E k and responses R 1 , ? ? ? , R k?1 . We provide detailed explanations of the architecture in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-head Attention Networks</head><p>The multi-head attention networks take Q in , K in and V in , each representing the sequence of queries, keys and values respectively. The multi-head attention networks are simply the attention networks applied h times to the same input sequence with different projection matrices. An attention layer first projects each Q in , K in , and V in to a latent space by multiplying matrices W Q i , W K i and W V i . That is,</p><formula xml:id="formula_3">Q i = [q i 1 , ? ? ? , q i k ] = Q in W Q i K i = [k i 1 , ? ? ? , k i k ] = K in W K i V i = [v i 1 , ? ? ? , v i k ] = V in W V i</formula><p>where q, k and v are the projected queries, keys and values, respectively. The relevance of each value to a given query is determined by the dot-product between the query and the key corresponding to the value.</p><p>The attention networks in SAINT require a masking mechanism that prevents the current position from attending to subsequent positions. The masking mechanism replaces upper triangular part of matrix Q i K T i from the dot-product with ??, which, after the softmax operation, has the effect of zeroing out the attention weights of the subsequent positions. The attention head, head i , is the values V i multiplied by the masked attention weights. That is,</p><formula xml:id="formula_4">head i = Softmax Mask Q i K T i ? d V i</formula><p>where the division by square root of d, the dimension of q and k, is for scaling.</p><p>A concatenation of h attention heads is multiplied by W O to aggregate the outputs of different attention heads. This concatenated tensor is the final output of the multi-head attention networks.</p><formula xml:id="formula_5">MultiHead(Q in , K in ,V in ) = Concat(head 1 , ? ? ? , head h )W O</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feed-Forward Networks</head><p>Position-wise feed-forward networks are applied to the multihead attention output to add non-linearity to the model,</p><formula xml:id="formula_6">F = (F 1 , ? ? ? , F k ) = FFN(M) F i = ReLU M i W FF 1 + b FF 1 W FF 2 + b FF 2 where M = [M 1 , ? ? ? , M k ] = Multihead(Q in , K in ,V in ) and W FF 1 , W FF 2 , b FF 1 and b FF 2</formula><p>are weight matrices and bias vectors shared across different M i 's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder</head><p>The encoder is a stack of N identical layers which are the feed-forward networks followed by the multi-head attention networks. In formula, each identical layer is</p><formula xml:id="formula_7">M = SkipConct(Multihead(LayerNorm(Q in , K in ,V in ))) O = SkipConct(FFN(LayerNorm(M)))</formula><p>where skip connection <ref type="bibr" target="#b5">[6]</ref> and layer normalization <ref type="bibr" target="#b0">[1]</ref> are applied to each sub-layer. Note that Q in , K in and V in of the first layer are E e , the sequence of exercise embeddings, and those of subsequent layers are the output of the previous layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoder</head><p>The decoder is also a stack of N identical layers which consist of the multi-head attention networks followed by the feedforward networks. Similar to the encoder, skip connection and layer normalization are applied to each sub-layer.</p><p>Each identical layer is represented by following equations:</p><formula xml:id="formula_8">M 1 = SkipConct(Multihead(LayerNorm(Q in , K in ,V in ))) M 2 = SkipConct(Multihead(LayerNorm(M 1 , O, O))) L = SkipConct(FFN(LayerNorm(M 2 )))</formula><p>where O is the final output of the encoder. The Q in , K in and V in of the first layer in the decoder are all R e , the sequence of response with start token embeddings, and those of the following layers are the output of the previous layer.</p><p>Finally, a prediction layer, consisting of a linear transformation layer followed by a sigmoid operation, is applied to the output of last layer so that the decoder output is a series of probability values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer based Variants of Deep Knowledge Tracing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>In order to find a deep attention mechanism effective for knowledge tracing, we conduct extensive experiments on different ways to construct the query, key and value for attention mechanisms.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Masked Transformer with Interaction sequence (UTMTI) and</head><p>Stacked variant of SAKT <ref type="bibr" target="#b13">[14]</ref> (SSAKT) (see <ref type="figure" target="#fig_2">Figure 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LTMTI</head><p>The LTMTI model predicts the response r k by taking the exercise E k and the sequence of k ? 1 past interactions I = [I 1 , ? ? ? , I k?1 ] separately. Firstly, the encoder applies selfattention N times to the sequence of past interactions I plus a start token, producing the output O = [O 1 , ? ? ? , O k ]. Then, the first layer of the decoder takes the encoder output O as keys and values, and the constant sequence E k , ? ? ? , E k of size k as queries. The next N ? 1 layers of the decoder takes the encoder output O as keys and values, and the output of the previous layer as queries.</p><p>A major computational difference of LTMTI from SAINT is that LTMTI applies lower triangular masks to all attention layers, instead of the upper triangular masks in SAINT. This forces the i'th outputr k,i of LTMTI to be inferred only from E k and the most recent (i ? 1) interactions. That is,</p><formula xml:id="formula_9">r k,1 = argmax r P(r k = r|E k ) r k,2 = argmax r P(r k = r|I k?1 , E k )</formula><p>. . . r k,k = argmax r P(r k = r|I 1 , ? ? ? , I k?1 , E k ).</p><p>Note that this aspect of LTMTI is in effect an augmentation on the training data where histories are truncated to various lengths. That is, the model learns to infer r k with multiple past interaction histories (E k ), (I k?1 , E k ), ? ? ? , (I 1 , ? ? ? , I k?1 , E k ) from a single input. For testing, the predictionr k is defined to be the outputr k,i that uses the longest interaction sequence available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UTMTI</head><p>The UTMTI model follows the same architecture as SAINT and differs only on the choice of input sequence. Unlike SAINT, UTMTI takes the interaction sequence I 1 , ? ? ? , I k?1 as the encoder input and the exercise sequence E 1 , ? ? ? , E k as the decoder input. This method follows the attention mechanism of SAKT <ref type="bibr" target="#b13">[14]</ref> and EKT <ref type="bibr" target="#b7">[8]</ref>. In SAKT, exercises are embedded as queries and interactions are embedded as keys and values. Likewise, EKT processes the interaction sequence with a deep Bi-LSTM layer, and combines the output with attention weights given by the cosine similarity of exercise embeddings. UTMTI follows the same pattern of using interactions as attention keys/values and exercises as attention queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SSAKT</head><p>The SSAKT model consists of a single attention block that uses exercise embeddings as queries and interaction embeddings as keys/values. The authors report a decrease in AUC when the attention block is stacked multiple times. SSAKT resolves this issue by applying self-attention on exercises before supplying them as queries. The outputs of the exercise selfattention block and the exercise-interaction attention block enters the corresponding following blocks as inputs for their attention layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXPERIMENTS Dataset</head><p>We conduct experiments on a large scale knowledge tracing dataset, EdNet <ref type="bibr" target="#b1">[2]</ref>, collected by Santa, an active mobile application for English education <ref type="figure">(Figure 6</ref>). Santa is a selfstudy solution equipped with an artificial intelligence tutoring system that helps students prepare the  <ref type="figure">Figure 5</ref>. Description of EdNet dataset. For each user, the dataset contains his interaction history, which is a series of records each consisting of the following features: the absolute time when the user received each exercise (Timestamp), an ID unique to each exercise (Exercise ID), a category of the domain subject that each exercise belongs to (Exercise category), user response value (Response) and the time the user took to respond in seconds (Elapsed time). <ref type="figure">Figure 6</ref>. User interface of Santa. Santa is a self-study solution equipped with artificial intelligence tutoring system that aids students to prepare the Test of English for International Communication (TOEIC) Listening and Reading Test.</p><p>series of records, each consisting of the following features: the time at which the user received each exercise (Timestamp), an ID unique to each exercise (Exercise ID), a category of the domain subject that each exercise belongs to (Exercise category), the user response (Response), and the length of time taken by the user to respond (Elapsed time) ( <ref type="figure">Figure 5)</ref>. The dataset has a total of 16,175 exercises and 72,907,005 responses, with 627,347 users solving more than one exercise. We split the dataset into three parts per user basis: the train set (439,143 users, 51,117,443 responses), the validation set (62,734 users, 7,460,486 responses) and the test set (125,470 users, 14,329,076 responses) ( <ref type="table" target="#tab_4">Table 1)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Details</head><p>Hyper-parameters are determined by the ablation study in Section 4.4. We train the model from scratch, using the Xavier uniform <ref type="bibr" target="#b4">[5]</ref> distribution to initialize weights. The window size, dropout rate, and batch size are set to 100, 0.1, and 128 respectively. We use the Adam optimizer <ref type="bibr" target="#b9">[10]</ref> with lr = 0.001, ? 1 = 0.9, ? 2 = 0.999 and epsilon = 1e ? 8. We use the so-called Noam scheme to schedule the learning rate as in <ref type="bibr" target="#b21">[22]</ref> with warmup_steps set to 4000. We pick the model parameters that give the best results on the validation set and evaluate them with the test set.  <ref type="table">Table 2</ref>. Comparison of the current state-of-the-art models and SAINT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>We evaluate SAINT by comparing it with the current state-ofthe-art knowledge tracing based approaches and collaborative filtering based approaches: Multilayer Perceptron (MLP) <ref type="bibr" target="#b6">[7]</ref>, Neural Collaborative Filtering (NCF) <ref type="bibr" target="#b6">[7]</ref>, Neural Pedagogical Agent (NPA) <ref type="bibr" target="#b11">[12]</ref> and Self-Attentive Knowledge Tracing (SAKT) <ref type="bibr" target="#b13">[14]</ref>. In our experiments, we use two performance metrics commonly used in previous works: the area under the receiver operating characteristic curve (AUC) and accuracy (ACC). AUC shows sensitivity (recall) against 1 ? specificity. Sensitivity (resp. specificity) is the proportion of true positives (resp. negatives) that are correctly predicted to be positive (resp. negative). ACC is the proportion of predictions that were correct. <ref type="table">Table 2</ref> presents the overall results of our evaluation. It shows that our model outperforms the other models in both metrics. Compared to the current state-of-the-art models, the ACC of SAINT is higher by 1.1% and AUC is higher by  1.8%. Other Transformer-based variants of deep knowledge tracing models described in Section 3.4 also perform better than existing approaches.</p><p>We visualize the attention weights of the fully trained, bestperforming SAINT model to analyze the attention mechanism of SAINT. The self-attention weights of the last encoder and decoder block show different tendencies (see <ref type="figure" target="#fig_3">Figure 7</ref>). This shows that SAINT is capable of learning the different attention mechanisms appropriate for exercises and responses, and applying them separately. <ref type="figure" target="#fig_4">Figure 8</ref> shows that each head of the first decoder block's encoder-decoder attention layer attends the exercise sequence in varying patterns. As seen in <ref type="figure" target="#fig_5">Figure 9</ref>, the span of attention in later decoder blocks are more diverse. This can be interpreted as the attention mechanism capturing the increase in the complexity of the values that incorporate more complex relationships between the exercises and the responses as the values go through successive decoder blocks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>In this section, we present ablation studies for the suggested models. Firstly, we run ablation studies on each architecture with different hyper-parameters. <ref type="figure">Figure 10</ref> shows that SAINT, which applies deep attention layers separately to exercises and responses, gives the best result. The best performing model of SAINT has 4 layers and a latent space dimension of 512. It shows an ACC of 0.7368 and AUC of 0.7811. Next to SAINT, LTMTI models show high ACC and AUC overall. This shows that the data augmentation effects from lower-triangular masks in LTMTI boosts ACC and AUC.</p><p>Secondly, we evaluate the best-performing SAINT model when trained with inputs of different levels of detail. All models share the same embedding method for exercises. For response embeddings, Embedding A only uses positional information and the user response value to build the response  <ref type="table" target="#tab_4">4  2  3  4  2  3  4  2  3  4   512  256  512  256  512  256  512  256  512  256  512  256  512  256  512  256  512  256</ref>    <ref type="table" target="#tab_5">Table 3</ref> shows that using more information to construct embeddings did not improve results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSION</head><p>In this paper, we proposed SAINT, a state-of-the-art knowledge tracing model with deep attention networks. We empirically demonstrated a Transformer-based architecture that shows superior performance on knowledge tracing tasks. In addition, we showed through extensive experiments on the queries, keys, and values of attention networks that separately feeding exercises and responses to the encoder and decoder, respectively, is ideal for knowledge tracing tasks. Furthermore, by investigating the attention weights of SAINT, we found that the results of self-attention of encoder and decoder exhibit different patterns. We suggested that this supports our idea that the separation of exercises and responses in input allows the model to find attention mechanisms that are especially suited to the respective input values. Finally, Evaluation of SAINT on a large-scale knowledge tracing dataset showed that the model outperforms existing state-of-the-art knowledge tracing models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Feed</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>We use a total of five different attributes to construct the input embeddings throughout the experiments. The exercise embedding is the sum of the exercise ID, exercise category and position embeddings. The response embedding is the sum of the response value and position embeddings. Although interaction embeddings are not used in SAINT, Transformer-based variants of deep knowledge tracing models take them as an input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>We compare SAINT with three different architectures based on multiple stacked attention layers: Lower Triangular Masked Transformer with Interaction sequence (LTMTI), Upper Triangular Masked Transformer with Interaction sequence (UTMTI) and Stacked variant of SAKT (SSAKT) (from left to right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Self-attention of last encoder block (left) and last decoder (right) block. Each figure shows the attention matrix of each head in the selfattention layer. The attention matrices of the encoder are sparse -most exercises attend only a few relevant exercises. The attention matrices of the decoder are dense -the attention values are spread over a large number of responses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Visualization of the encoder-decoder attention values of the first decoder block. Some heads only have large weights on the diagonal, showing that each response attends only the corresponding exercise. Other heads show a series of vertical stripes, showing that only a few exercises are attended to by all responses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 .</head><label>9</label><figDesc>Comparison of decoder self-attention matrices at varying depths. The attention values in the first decoder block (left) are sparse, showing that each query only attends a few relevant response values. However, the attention values in the last decoder block (right) are more evenly distributed, showing that each query attends to the responses more thoroughly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>512 2 0Figure 10 .</head><label>210</label><figDesc>.7368 0.7339 0.7281 0.7317 0.7273 0.7333 0.7252 0.7323 0.7220 0.7301 0.7163 0.7323 0.7204 0.6744 0.6699 0.7322 0.7300 0.7340 0.7221 3 0.7811 0.7762 0.7673 0.7726 0.7656 0.7755 0.7642 0.7734 0.7577 0.7699 0.7493 0.7735 0.7565 0.6302 0.6057 0.7763 0.7722 0.7777 0Graph (above) and table (below) showing the ACC and AUC of our proposed methods at different model sizes. N is the number of stacked encoder and decoder blocks and d_model is the dimension of all sub-layer output of the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2002.07033v5 [cs.LG] 1 Feb 2021</figDesc><table><row><cell>Target</cell><cell cols="2">Source</cell><cell></cell><cell></cell><cell cols="2">Query</cell><cell>Key</cell><cell>Value</cell><cell></cell><cell></cell></row><row><cell></cell><cell>r k</cell><cell></cell><cell></cell><cell></cell><cell>r 1</cell><cell>r 2</cell><cell>r k</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>...</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Feed Forward^M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Additive Attention</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Feed Forward</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Bidirectional LSTM</cell><cell></cell><cell></cell><cell></cell><cell cols="3">ulti-head Attention with Upper Triangular Mask</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Start token</cell><cell>...</cell><cell></cell><cell></cell><cell>...</cell><cell>Start token</cell><cell>...</cell><cell></cell><cell></cell><cell>...</cell><cell></cell></row><row><cell>I 1</cell><cell>I 2</cell><cell>I k-1</cell><cell>E 1</cell><cell>E 2</cell><cell>E k</cell><cell>I 1</cell><cell>I k-1</cell><cell>E 1</cell><cell>E 2</cell><cell>E k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Forward ... ...</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>Query</cell><cell>Key</cell><cell>Value</cell></row><row><cell></cell><cell></cell><cell></cell><cell>r 1</cell><cell>r 2</cell><cell>r k</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Feed Forward</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Multi-head Attention</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>with Upper</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Triangular Mask</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>xN</cell></row><row><cell></cell><cell cols="2">Multi-head Attention</cell><cell cols="2">Multi-head Attention</cell></row><row><cell>Nx</cell><cell cols="2">with Upper</cell><cell></cell><cell>with Upper</cell></row><row><cell></cell><cell cols="2">Triangular Mask</cell><cell cols="2">Triangular Mask</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Start token</cell></row><row><cell></cell><cell>E 1 E 2</cell><cell>E k</cell><cell></cell><cell>R 1</cell><cell>R k-1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>Statistics of EdNet dataset</figDesc><table><row><cell>Statistics</cell><cell></cell><cell>EdNet dataset</cell></row><row><cell cols="2">total user count</cell><cell>627,347</cell></row><row><cell cols="2">train user count</cell><cell>439,143</cell></row><row><cell cols="2">validation user count</cell><cell>62,734</cell></row><row><cell cols="2">test user count</cell><cell>125,470</cell></row><row><cell cols="2">total response count</cell><cell>72,907,005</cell></row><row><cell cols="2">train response count</cell><cell>51,117,443</cell></row><row><cell cols="2">validation response count</cell><cell>7,460,486</cell></row><row><cell cols="2">test response count</cell><cell>14,329,076</cell></row><row><cell cols="2">correct response ratio</cell><cell>0.66</cell></row><row><cell cols="2">incorrect response ratio</cell><cell>0.34</cell></row><row><cell cols="3">mean interaction sequence length</cell><cell>116.21</cell></row><row><cell cols="3">median interaction sequence length</cell><cell>13</cell></row><row><cell cols="2">max interaction sequence length</cell><cell>41,644</cell></row><row><cell cols="2">number of categories</cell><cell>7</cell></row><row><cell cols="2">Methods ACC</cell><cell>AUC</cell></row><row><cell>MLP</cell><cell cols="2">0.7052 0.7363</cell></row><row><cell>NCF</cell><cell cols="2">0.7051 0.7341</cell></row><row><cell>NPA</cell><cell cols="2">0.7290 0.7656</cell></row><row><cell>SAKT</cell><cell cols="2">0.7271 0.7671</cell></row><row><cell cols="3">LTMTI 0.7339 0.7762</cell></row><row><cell cols="3">UTMTI 0.7323 0.7735</cell></row><row><cell cols="3">SSAKT 0.7340 0.7777</cell></row><row><cell cols="3">SAINT 0.7368 0.7811</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Ablation study for embedding embedding while Embedding B uses the exercise category, timestamp, and elapsed time in addition to the features used by Embedding A.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SAINT LTMTI UTMTI</head> <ref type="table">N  2  3  4  2  3  4  2  3  4   d_model  256  512  256  512  256  512  256  512  256  512  256  512  256  512  256  512  256</ref> </div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SAINT LTMTI UTMTI</head> <ref type="table">N  2  3  4  2  3  4  2  3  4   d_model  256  512  256  512  256  512  256  512  256  512  256  512  256  512  256  512  256</ref> </div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">EdNet: A Large-Scale Hierarchical Dataset in Education</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngduck</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngnam</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmin</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seoyon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seewoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jineon</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewe</forename><surname>Heo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Knowledge tracing: Modeling the acquisition of procedural knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John R</forename><surname>Corbett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">User modeling and user-adapted interaction</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="253" to="278" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">More accurate student modeling through contextual estimation of slip and guess probabilities in bayesian knowledge tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><forename type="middle">T</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Corbett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aleven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on intelligent tutoring systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="406" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th international conference on world wide web. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 26th international conference on world wide web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">EKT: Exercise-aware Knowledge Tracing for Student Performance Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenya</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Estimating programming knowledge with Bayesian knowledge tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jussi</forename><surname>Kasurinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uolevi</forename><surname>Nikula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCSE Bulletin</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="313" to="317" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Machine Learning Approaches for Learning Analytics: Collaborative Filtering Or Regression With Experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jichan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeongmin</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changho</forename><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngnam</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngduck</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">R</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunbin</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanyou</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongku</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.10910</idno>
		<title level="m">Creating A Neural Pedagogical Agent by Jointly Learning to Review and Assess</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<title level="m">Effective Approaches to Attention-based Neural Machine Translation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalini</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06837</idno>
		<title level="m">A Self-Attentive model for Knowledge Tracing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Navigating the parameter space of Bayesian Knowledge Tracing models: Visualizations of the convergence of the Expectation Maximization algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Pardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Heffernan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Educational Data Mining</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adapting Bayesian Knowledge Tracing to a Massive Open Online Course in edX</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Zachary A Pardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bergner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Seaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pritchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EDM</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="137" to="144" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling individualization in a bayesian networks implementation of knowledge tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zachary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil T</forename><surname>Pardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heffernan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on User Modeling, Adaptation, and Personalization</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="255" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Does Time Matter? Modeling the Effect of Time with Bayesian Knowledge Tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingmei</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">A</forename><surname>Pardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil T</forename><surname>Heffernan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDM</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="139" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Incorporating scaffolding and tutor context into bayesian knowledge tracing to predict inquiry skill acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Michael Sao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janice</forename><surname>Gobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Educational Data Mining</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recommender system for predicting student performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Nguyen Thai-Nghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artus</forename><surname>Drumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Krohn-Grimberghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2811" to="2819" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Properties of the bayesian knowledge tracing model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brett Van De Sande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Data Mining</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Individualized bayesian knowledge tracing models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael V Yudelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Kenneth R Koedinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial intelligence in education</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
