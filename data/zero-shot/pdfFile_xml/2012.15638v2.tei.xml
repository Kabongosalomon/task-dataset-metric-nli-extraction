<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CorrNet3D: Unsupervised End-to-end Learning of Dense Correspondence for 3D Point Clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">City University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Qian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">City University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Zhu</surname></persName>
							<email>zhiyuzhu2-c@my.cityu.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">City University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Hou</surname></persName>
							<email>jh.hou@cityu.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">City University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">Hui</forename><surname>Yuan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Shandong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>He</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CorrNet3D: Unsupervised End-to-end Learning of Dense Correspondence for 3D Point Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Motivated by the intuition that one can transform two aligned point clouds to each other more easily and meaningfully than a misaligned pair, we propose CorrNet3Dthe first unsupervised and end-to-end deep learning-based framework -to drive the learning of dense correspondence between 3D shapes by means of deformation-like reconstruction to overcome the need for annotated data. Specifically, CorrNet3D consists of a deep feature embedding module and two novel modules called correspondence indicator and symmetric deformer. Feeding a pair of raw point clouds, our model first learns the pointwise features and passes them into the indicator to generate a learnable correspondence matrix used to permute the input pair. The symmetric deformer, with an additional regularized loss, transforms the two permuted point clouds to each other to drive the unsupervised learning of the correspondence. The extensive experiments on both synthetic and real-world datasets of rigid and non-rigid 3D shapes show our CorrNet3D outperforms state-of-the-art methods to a large extent, including those taking meshes as input. CorrNet3D is a flexible framework in that it can be easily adapted to supervised learning if annotated data are available. The source code and pre-trained model will be available at https://github.com/ZENGYIMING-EAMON/CorrNet3D.git.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Owing to the flexibility and efficiency in representing 3D objects/scenes as well as the recent advances in 3D sensing technology, 3D point clouds have been widely adopted in various applications, e.g., immersive communication <ref type="bibr" target="#b1">[2]</ref>, autonomous driving <ref type="bibr" target="#b33">[33]</ref>, AR/VR <ref type="bibr" target="#b37">[37]</ref>, etc. Since each camera/scanner produces a point cloud in its own camera space rather than the object space, there is no correspondence between two point clouds (even they represent the same object), which poses great challenges for downstream processing and analysis, such as motion transfer <ref type="bibr" target="#b39">[39]</ref>, shape editing <ref type="bibr" target="#b24">[24]</ref>, dynamic point cloud compression <ref type="bibr" target="#b20">[20]</ref>, object recognition <ref type="bibr" target="#b2">[3]</ref>, shape retrieval <ref type="bibr" target="#b11">[11]</ref>, surface reconstruction <ref type="bibr" target="#b6">[7]</ref>, and many others.</p><p>Building dense shape correspondence is a fundamental and challenging problem in computer vision and digital geometry processing. There are a considerable number of methods proposed, which can be roughly classified into two categories: model-based <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b28">28]</ref> and datadriven <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8]</ref>. The model-based methods usually use handcrafted features to optimize pre-defined processes. The recent deep learning-based methods train their neural networks in a data-driven manner and improve the performance to a large extent. However, the existing methods either require a large amount of annotated data which are difficult to obtain or assume the connectivity information is available in the input data, i.e., polygonal meshes. This paper focuses on unsupervised learning of dense correspondence between non-rigid 3D shapes in the form of 3D point clouds, but the proposed method can also be used for rigid 3D shapes.</p><p>Motivation. Let A ? R n?3 and B ? R n?3 be the two point clouds to be corresponded * , where a i = {x i , y i , z i }, 1 ? i ? n and b j = {x j , y j , z j }, 1 ? j ? n are the i-th and j-th 3D points of A and B, respectively. <ref type="figure">Fig. 1</ref> illustrates our motivation: if A and B are well aligned, it is easier to transform one model to the other. More precisely, denote by A ? R n?3 and B ? R n?3 the re-ordered A and B via a designed permutation process, respectively. With a designed reconstruction process, it is expected that we can reconstruct A (resp. B) from B (resp. A) more easily and meaningfully than the manner of reconstructing A (resp. B) from B (resp. A) directly. Therefore, we can minimize the reconstruction error from B (resp. A) to A (resp. B) <ref type="figure">Figure 1</ref>. Illustration of the motivation of our unsupervised deep learning-based framework for computing dense correspondence between two point clouds.</p><p>to drive the learning of the permutation process, which implicitly encodes the dense correspondence between A and B.</p><p>Based on the above intuitive understanding, we propose the first unsupervised and end-to-end deep learning-based framework for point clouds. Technically, we propose a novel correspondence indicator and a deformation-like reconstruction module to achieve the permutation and reconstruction processes, respectively. To be specific, the correspondence indicator, fed with point-wise high-dimensional feature representations of the input point clouds learned by a hierarchical feature embedding module, generates a permutation matrix, which explicitly encodes the point-to-point correspondence. During training, the deformation-like reconstruction module receives the aligned point clouds and the global semantic features of inputs to reconstruct each other by optimizing the reconstruction error and additional regularization terms to drive the learning of the permutation matrix.</p><p>In summary, we make the following contributions. 1. We propose the first unsupervised deep learning framework for building dense correspondence between point clouds in an end-to-end manner. 2. We propose two novel modules, i.e., the correspondence indicator with the efficient DeSmooth module, the symmetric deformation module, as well as a novel loss function. 3. We show that CorrNet3D can be adapted to both unsupervised and supervised conditions, and handle both non-rigid and rigid shapes well. 4. We experimentally demonstrate the significant superiority of CorrNet3D over state-of-the-art methods. Especially, CorrNet3D even outperforms the method taking 3D meshes as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Deep Learning for Point Clouds</head><p>Unlike well-developed deep convolution neural network (CNN) techniques for 2D images/videos, deep learning based point cloud processing is more challenging and still in the infant stage, due to its irregular and unorder characteristics. PointNet <ref type="bibr" target="#b31">[31]</ref> and PointNet++ <ref type="bibr" target="#b32">[32]</ref> are the pioneering works and verify the effectiveness of multi-layer perceptrons (MLPs) in learning point cloud features. DGCNN <ref type="bibr" target="#b49">[48]</ref> uses a dynamic graph to aggregate neighborhood information in each layer, and the selection of neighbours is based on feature distances. DCG <ref type="bibr" target="#b46">[45]</ref> further boosts DGCNN by encoding additional local connections in coarse-to-fine manner. Volumetric-based methods <ref type="bibr" target="#b50">[49,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b47">46,</ref><ref type="bibr" target="#b22">22]</ref> apply 3D CNNs to process voxelized point clouds; however, they suffer from high computational costs and inevitable quantization errors. In the meantime, inspired by the Fold-ingNet <ref type="bibr" target="#b51">[50]</ref>, which learns to deform pre-defined 2D regular grids into 3D shapes, some deformation-based frameworks, such as AtlasNet <ref type="bibr" target="#b14">[14]</ref> and 3D-Coded <ref type="bibr" target="#b13">[13]</ref>, were proposed, which deform a fixed templates (e.g., 2D grid or 3D human mesh) to reconstruct the input point cloud or mesh. Please refer to <ref type="bibr" target="#b16">[16]</ref> for the comprehensive survey on deep learningbased point cloud processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Non-rigid Shape Correspondence</head><p>Non-rigid shape correspondence or matching aims to find the point-to-point correspondence of two deformable 3D shapes. As an active research area in computer vision and graphics, many methods have been proposed, and one may refer to the surveys <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b3">4]</ref> for a comprehensive understanding. Here we briefly introduce one stream, i.e., functional map (FM)-based methods <ref type="bibr" target="#b28">[28]</ref> which are compared in this paper. Specifically, this kind of methods first performs spectral analysis on 3D meshes to construct an FM, and then optimizes a least-squares problem to convert the resulting FM to point-to-point correspondence under the assumption of non-rigid but isometric deformation. To overcome the deficiency of solving the optimization problem, Litany et al. <ref type="bibr" target="#b23">[23]</ref> proposed deep FM, which integrates the module of computing FM into a deep neural network. However, both original FM-based methods and deep FM use the handcrafted SHOT descriptors <ref type="bibr" target="#b36">[36]</ref> , which may limit their performance. Based on deep FM, Halimi et al. <ref type="bibr" target="#b17">[17]</ref> proposed to minimize the surface distortion between meshes to drive the correspondence learning process. Instead of using handcrafted descriptors, the most recent deep learning-based method named deep geometric functional map (DeepGFM) <ref type="bibr" target="#b8">[9]</ref> employs KPConv <ref type="bibr" target="#b42">[42]</ref> to achieve a data-driven feature representation. Although DeepGFM can achieve state-of-the-art performance, it is only applicable for 3D meshes, and its training has to be supervised by ground-truth FMs, whose construction requires groundtruth correspondence. Moreover, additional post-processing is necessary to obtain the final correspondence.</p><p>Recently, FlowNet3D <ref type="bibr" target="#b25">[25]</ref> was designed to directly learn scene flows from two consecutive point clouds in the form <ref type="figure">Figure 2</ref>. The flowchart of CorrNet3D, an unsupervised and end-to-end deep learning framework, which aims to obtain a matrix P, which explicitly indicates the correspondence between any two points. We first represent A and B with high-dimensional point-wise features Fa and F b as well as the global features va and v b . Then the correspondence indicator with a novel DeSmooth module takes Fa and F b as input to regress P. To drive the unsupervised learning of P, two symmetric deformers with shared parameters takes the A, B, P and va, and v b as inputs to generate the reconstructed point clouds A and B in the deformation-like manner. CorrNet3D is trained with the reconstruction loss and additional regularization terms on P. of depth images. To some extent, it can also be used for indicating correspondence, i.e., adding the estimated flow to one point cloud, and then seeking the closest point in the other one. However, such a simple extension may result in serious many-to-one correspondence. Moreover, due to the specific application scenario, FlowNet3D only utilizes the neighborhood information based on the Euclidean distance in two frames, making it not applicable to 3D shapes with serious deformation. Groueix et al. <ref type="bibr" target="#b15">[15]</ref> proposed a self-supervised approach to achieve deep surface deformation for shapes in the same category, in which the semantic labels from a small set of segmented shapes are transferred to unlabeled data. This work has potential on shape matching.In our experiment, we slightly modified the loss function of FlowNet3D to produce an improved unsupervised model for correspondence prediction, which is adopted as a baseline method for comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Rigid Shape Matching</head><p>Rigid shape matching or registration aims to obtain a rotation matrix R ? R 3?3 and a translation vector t ? R 3?1 to align two rigid 3D shapes. Over the past decades, a considerable number of methods have been proposed. Please refer to <ref type="bibr" target="#b40">[40]</ref> for the comprehensive survey of traditional methods for rigid 3D shape registration. Recently, some deep learning-based methods were proposed. For example, PointNetLK <ref type="bibr" target="#b0">[1]</ref> utilizes PointNet <ref type="bibr" target="#b31">[31]</ref> to extract global features for two point clouds separately and then estimate R and t. DCP <ref type="bibr" target="#b48">[47]</ref> introduces a transformer <ref type="bibr" target="#b45">[44]</ref> to solve the seq-2-seq problem, where the point-wise correspondence and (R, t) are simultaneously estimated. The recent work RPMNet <ref type="bibr" target="#b52">[51]</ref> adopts the Sinkhorn layer <ref type="bibr" target="#b27">[27]</ref> to get the correspondence information and weighted SVD <ref type="bibr" target="#b12">[12]</ref> to compute R and t. Note that all these learning-based methods require ground-truth rotations and translations as supervision or even additional post-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>As illustrated in <ref type="figure">Fig. 2</ref>, our CorrNet3D mainly consists of three modules: feature embedding, correspondence indicator, and symmetric deformer. Specifically, we first pass paired input point clouds into the shared feature embedding module to generate point-wise high-dimensional feature embeddings F a ? R n?d and F b ? R n?d with d being the feature dimension, which encode their local geometric structures, respectively, and global feature vectors v a ? R d and v b ? R d , which encode their shape information, respectively. Then we predict a correspondence matrix P ? R n?n by feeding F a and F b into the correspondence indicator, where the the (i, j)-th element p ij = 1 indicates the point a i corresponds to b j . To drive the learning of P in an unsupervised manner, we propose the symmetric deformer in which we utilize v b (resp. v a ) and the permuted point cloud A (resp. B) to reconstruct B (resp. A). CorrNet3D is end-to-end trained by directly minimiz-</p><formula xml:id="formula_0">ing A ? A 2 F + B ? B 2 F + ?R(P), where A ? R n?3 and B ? R n?3</formula><p>are the reconstructed point clouds, ? F is the Frobenious norm of a matrix, ? &gt; 0 is the penalty parameter, and R(P) stands for the regularization on P.</p><p>Remark. The proposed CorrNet3D is fundamentally different from the existing works <ref type="bibr" target="#b52">[51]</ref>, <ref type="bibr" target="#b8">[9]</ref>, as the correspondence matrix is driven from the perspective of deformationlike reconstruction, rather than the ground-truth correspondence or the well-known functional maps. In addition, Cor-rNet3D is able to work as a supervised model by removing the deformation module and employing ground-truth corre- spondence to supervise the learning of P. In the experiment section, we demonstrate the significant advantage of Corr-Net3D under both unsupervised and supervised scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Embedding</head><p>We use a shared DNN-based feature learner, namely DGCNN <ref type="bibr" target="#b49">[48]</ref>, to embed A and B to a high-dimensional feature space in a hierarchical manner. Note that other advanced feature representation methods <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b4">[5]</ref> can also be used to further boost performance. To be specific, DGCNN consists of several layers named EdgeConv. For the i-th point, we first calculate the Euclidean distance between features to determine the set of its k nearest neighbours denoted by ? l i . Then, we apply an MLP <ref type="bibr" target="#b18">[18]</ref> followed by a max-pooling operator to obtain a new feature representa-</p><formula xml:id="formula_1">tion f l+1 i = f l j ?? l i M l (f l i , f l j ?f l i ), where f l i ? R 1?d be the feature representation of point i fed into the l-th EdgeConv. f l+1 i</formula><p>is capable of capturing the local geometry structure of point i. After L EdgeConv layers, we can obtain the final point-wise features F a ? R n?d for A and F b ? R n?d for B. By applying another max-avg-pooling operator, the global feature vectors for A and B could be accordingly obtained denoted as v a ? R d and v b ? R d , respectively. Please refer to <ref type="bibr" target="#b49">[48]</ref> for more details about DGCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Correspondence Indicator</head><p>Our correspondence indicator module aims to learn a correspondence matrix P ? R n?n to explicitly indicate the correspondence between any two points of A and B. Ideally, P should be a permutation matrix that is binary and orthogonal, and iff p ij = 1, point a i corresponds to point b j . However, such a permutation matrix is non-differentiable, making it hard to optimize in a deep learning framework. Alternatively, we regress an approximate doubly stochastic matrix instead, which is differentiable and close to binary. Moreover, there is only a single prominent element dominating each row and column. During inference, we quantize P to an exact binary matrix P.</p><p>As shown in <ref type="figure" target="#fig_0">Fig. 3</ref>, to learn P, we first measure the similarity between points in the high-dimensional feature space in the inverse distance sense, i.e.,</p><formula xml:id="formula_2">p ij = 1 f a,i ? f b,j 2 ,<label>(1)</label></formula><p>where p ij ? P ? R n?n , and f a,i and f b,j ? R 1?d are the i-th and j-th point-wise features corresponding to a i and b j , respectively. However, the resulting P is far away from realizing correspondence. To further enhance P, one can simply adopt Sinkhorn layers <ref type="bibr" target="#b27">[27]</ref>, which perform the softmax operation on in column-wise and row-wise iteratively and alternatively, as done in <ref type="bibr" target="#b52">[51]</ref>; however, the efficiency of Sinkhorn layers is low due to the iterative manner. To tackle this issue, we propose a novel DeSmooth module to improve P. DeSmooth Module. Assume that p ij generally obeys a series of Gaussian distributions ? : p ij ? N ? i , ? 2 i , where ? i and ? i are the mean and standard deviation of the ith row of P. We first normalize p ij in row-wise, i.e., z ij = pij ??i ?i . Accordingly, z ij follows a standard normal distribution z ij = pij ??i ?i ? N (0, 1). Give a prior ratio t to z ij , we have</p><formula xml:id="formula_3">z ij = t ? z ij ? N (0, t) .<label>(2)</label></formula><p>For i-th row of Z ? R n?n ( z ij ? Z), we compute the number of elements whose values are not less than a threshold ? , i.e.,</p><formula xml:id="formula_4">c i = # { z ij | z ij ?, j = 1, ? ? ? , n}<label>(3)</label></formula><p>where #{?} denotes the cardinality. We expect the value of c i to be close to 1, which means that in each row there's a high probability that only a single element dominates the row.</p><p>The n rows of Z could be thought of as n i.i.d events, and thus the set c = {c i |i = 1, ? ? ? , n} also follows a Gaussian distribution with the expectation ? c and variance ? c depending on the prior ratio t. Therefore, According to the three-sigma rule <ref type="bibr" target="#b30">[30]</ref>, we can set a proper t to control the bound [? c ? 3? c , ? c + 3? c ] to be centered around 1, such that the aforementioned expectation on a feasible correspondence matrix can be realized. Finally, we apply the softmax operation on z ij again and obtain the correspondence matrix P, i.e.,</p><formula xml:id="formula_5">p ij = e zij n j=1 e zij .<label>(4)</label></formula><p>The advantage of our DeSmooth over Sinkhorn layers is also experimentally demonstrated in Sec. 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Symmetric Deformer</head><p>Given ground-truth correspondence between A and B, the modules for learning P can be easily trained in a supervised manner. Annotating large amount of data is, ? As p ij is non-negative, it does not follow a strict Gaussian distribution. However, we found such an assumption still works well in practice however, costly and time-consuming. Inspired by recent deformation-based methods, such as FoldingNet <ref type="bibr" target="#b51">[50]</ref> and AtlasNet <ref type="bibr" target="#b14">[14]</ref>, which feed a pre-defined 2D grid appended with a global feature vector to a network to reconstruct 3D shapes, we propose symmetric deformer shown in <ref type="figure" target="#fig_1">Fig. 4</ref>, which reconstruct A and B in a deformation-like fashion to achieve the unsupervised learning of P.</p><p>We link the matrix P with the deformation module based on the aforementioned intuition, i.e., it is more easily and meaningfully to transform two aligned point clouds to each other than a misaligned pair. Specifically, we first permute the input point clouds using the learned P approximate to a permutation matrix, i.e.,</p><formula xml:id="formula_6">A = P T A, B = PB.<label>(5)</label></formula><p>The resulting A (resp. B) is approximately aligned with B (resp. A). Then, we deform A to B and B to A, by respectively utilizing their overall shape information encoded in the learned global feature vectors v a and v b . Technically, we concatenate v a (resp. v b ) to each point of B (resp. A) and pass the extended points to a network consisting of MLP layers, leading to reconstructed point clouds A (resp. B). See Sec.4.5 for the experimental validation towards the effectiveness and the deformation behavior of this module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Unsupervised Loss Function</head><p>To train the proposed CorrNet3D end-to-end, we promote A and B to be close to A and B, respectively, which is achieved by</p><formula xml:id="formula_7">L rec A, B = A ? A 2 F + B ? B 2 F .<label>(6)</label></formula><p>Benefiting from the alignment operation involved in Cor-rNet3D, we are allowed to use such a point-to-point reconstruction loss, which is easier to optimize than the commonly-used CD loss, thus producing better performance. See the ablation study. In addition to the reconstruction loss, we also propose another two terms to regularize the learning of the correspondence matrix P. The first regularization term is defined</p><formula xml:id="formula_8">as L perm (P) = PP T ? I n 2 F ,<label>(7)</label></formula><p>where I is the identity matrix of size n ? n. Such a term encourages P to be close to a permutation matrix to eliminate one-to-many correspondence. Second, we utilize the local geometry similarity between the input point cloud and permuted one to promote the learning of P, i.e., neighbouring points in A (resp. B) should also be neighbours in B (resp. A), which is mathematically expressed as</p><formula xml:id="formula_9">L mf d (P) = (8) n i=1 ? ? k?? a i p i B ? p k B 2 2 a i ? a k 2 2 + s?? b i p i A ? p s A 2 2 b i ? b s 2 2 ? ? ,</formula><p>where p i (resp. p i )is the i-th row (resp. column) of P, and ? a i (resp. ? b i ) is the index set of k nearest neighbours of point a i (resp. b i ).</p><p>Finally, the overall loss function for training CorrNet3D is written as</p><formula xml:id="formula_10">L A, B, P = (9) L rec A, B + ? 1 L perm (P) + ? 2 L mf d (P) ,</formula><p>where ? 1 and ? 2 &gt; 0 are the parameters to balance the three terms. See Sec. 4.5 for the experimental validation towards such an unsupervised loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Pseudo Clustering for Large-scale Point Clouds</head><p>As the size of predicted P depends on that of the input point cloud, directly inferring the correspondence of largescale point clouds may cause a memory issue. To this end, we propose pseudo clustering, a simple yet effective approach. Specifically, during inference, we first apply a typical sampling method such as farthest point sampling (FPS) on input point clouds to sample a fewer number of points called key points, which are thought of as cluster centers and fed into CorrNet3D, leading to the correspondence of the key points. Then the nearest neighboring points of each key point are found and sorted according to their Euclidean distances to center, and the correspondence of the neighbouring points of two corresponded key points are finally determined if two neighbouring points have the same rank in their own cluster. Such a pseudo clustering enables us to easily apply CorrNet3D to large-scale point clouds.</p><p>It is also worth pointing out that such a simple strategy would degrade the performance of our method when directly applied on large-scale point clouds under the condition with sufficient memory to some extent; however, the experiment shows CorrNet3D can still predict more accurate correspondence than the method even trained with 3D meshes, demonstrating the strong ability of our CorrNet3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we conducted extensive experiments and comparisons on real scanned non-rigid shapes and synthetic non-rigid and rigid shapes to demonstrate superiority of CorrNet3D in both supervised ? and unsupervised scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Setting</head><p>Datasets. For non-rigid shape correspondence, we adopted Surreal <ref type="bibr" target="#b13">[13]</ref> as the training dataset, consisting of 230K samples, which were randomly grouped into 115K training pairs. We conducted the test on the SHREC dataset <ref type="bibr" target="#b8">[9]</ref>, which has 430 pairs of non-rigid shapes. For rigid shape correspondence, we adopted the training and test dataset splits of the Surreal dataset <ref type="bibr" target="#b13">[13]</ref>, which contain 230K and 200 samples, respectively, and we randomly rotated and translated the samples to generate 230K pairs and 200 pairs for training and testing, respectively. Note that we chose these datasets in order to keep the same settings as the compared methods, including DeepGFM <ref type="bibr" target="#b8">[9]</ref>, DCP <ref type="bibr" target="#b48">[47]</ref> and RPMNet <ref type="bibr" target="#b52">[51]</ref>, for fair comparisons. For all the above training data, each point cloud contains 1024 points. The Surreal and SHREC datasets are both synthetic 3D meshes and we randomly picked 1024 vertices to form the point clouds.</p><p>Metrics. To fairly and quantitatively compare different methods, we define the corresponding percentage (Corr (%)) to measure correspondence accuracy, i.e.,</p><formula xml:id="formula_11">Corr = 1 n P P gt 1 ,<label>(10)</label></formula><p>where is the Hadamard product of matrices, ? 1 is the 1 norm of a matrix, and P gt encodes the ground-truth correspondence. Moreover, for a comprehensive comparison, we computed the corresponding percentage of different methods under various tolerant errors defined as r/dist max , where dist max := max{ a i ?a j 2 , ?i, j}, and r stands for the tolerant radius. It is worth pointing out that the above quantitative evaluation criteria are similar to those used for 3D mesh-based shape correspondence <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b23">23]</ref> which adopt the geodesic distance as the tolerant error requiring connectivity information. But such information is not available for point clouds, we directly compute the Euclidean distance.</p><p>Implementation details. For the parameters in Eq. (9), we empirically set ? 1 = 0.1 and ? 2 = 0.01. The shared symmetric deformer consists of a 3-layer MLP. We implemented it with the PyTorch framework <ref type="bibr" target="#b29">[29]</ref> on GeForce RTX 2080Ti.We trained the models with Adam <ref type="bibr" target="#b21">[21]</ref> optimizer with the learning rate equal to 1e-4 and the batch size equal to 10 for 300 epochs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on Non-rigid Shapes</head><p>In this scenario, we compared CorrNet3D and supervised CorrNet3D (S-CorrNet3D) with unsupervised F3D ? <ref type="bibr" target="#b25">[25]</ref> and supervised DeepGFM <ref type="bibr" target="#b8">[9]</ref> taking 3D meshes as input. <ref type="figure">Fig. 5</ref> shows the quantitative comparisons of different methods, where it can be observed that S-CorrNet3D always produces the best performance, and both CorrNet3D and S-CorrNet3D consistently outperform F3D and Deep-GFM. Especially, the performance advantage of our methods over DeepGFM is more obvious with the tolerant error increasing. <ref type="figure">Fig. 6</ref> shows visual comparisons of F3D, Cor-rNet3D and S-CorrNet3D on point clouds, where the predicted correspondence is visualized with colors and lines. From <ref type="figure">Fig. 6</ref>, we can see that CorrNet3D and S-CorrNet3D produce more accurate correspondence than F3D, espe- ? The supervised CorrNet3D (S-CorrNet3D) is achieved by removing the symmtric deformation module and training the remaining modules via minimizing the Euclidean distance between the predicted correspondence (i.e., P) and the ground-truth one. <ref type="bibr">?</ref> We obtained the unsupervised F3D by modifying the supervised FlowNet3D <ref type="bibr" target="#b25">[25]</ref>, i.e., we only replaced the loss function with the Chamfer Distance. cially at feet, hands and the right leg. <ref type="figure">Fig. 7</ref> shows the visual comparisons of CorrNet3D and DeepGFM on 3D meshes, which further demonstrates our method's advantage. That is, the predicted correspondence by our CorrNet3D is closer to the ground-truth one. However, DeepGFM results in patchy distributed wrong correspondence, although it utilizes additional connectivity information and ground-truth correspondence as supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on Rigid Shapes</head><p>In this scenario, we compared our CorrNet3D and S-CorrNet3D with DCP <ref type="bibr" target="#b48">[47]</ref> and RPMNet <ref type="bibr" target="#b52">[51]</ref>. Notice both DCP and RPMNet require ground-truth rigid transforma- tion as supervision during training ? . <ref type="figure" target="#fig_4">Fig. 9</ref> reports the quantitative comparisons of different methods, where it can be seen that CorrNet3D and S-CorrNet3D consistently outperform DCP an RPMNet. Interestingly but not surprisingly, the unsupervised Corr-Net3D performs even better than the other three supervised methods. The reason is that the freedom and searching space for the model in the supervised manner will be limited by the training dataset, making it harder to adapt the trained model to data with large transformation. <ref type="figure">Fig. 8</ref> visually compares the results of different methods, where it can be observed that DCP even fails to obtain correct matching, and RPMNet cannot predict correct matching for hands and the body part. In contrast, S-CorrNet3D and CorrNet3D are able to generate more accurate matching results, which are closer to ground-truth ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation on Real Scanned Data</head><p>We also examined the robustness of CorrNet3D on a real scanned dataset, i.e., 8iVFB <ref type="bibr" target="#b9">[10]</ref>, including the dynamic point cloud sequences of human motion with serious deformation. The test point clouds contain 1024 points each randomly picked from the original ones. As illustrated in <ref type="figure" target="#fig_5">Fig. 10</ref>, where the two corresponding points of two shapes predicted by CorrNet3D are visualized with the same color, we can observe CorrNet3D trained on the synthetic dataset still produces impressive performance on real data even with serious deformation, demonstrating the CorrNet3D's strong ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>In this section, we conducted extensive ablation studies for a comprehensive understanding towards our CorrNet3D. We carried out experiments on non-rigid Surreal dataset in the unsupervised scenario.   DeSmooth module. We compared our DeSmooth with the Sinkhorn layer. Specifically, we replaced the DeSmooth module of CorrNet3D with the Sinkhorn layer, while keep all the remaining settings the same. As shown in <ref type="figure" target="#fig_6">Fig. 11</ref>, it can be observed that under the same tolerant error, the accuracy of our DeSmooth is on par with that of the Sinkhorn layer, while our DeSmooth improves the efficiency up to 8?.</p><p>Deformation module. The results listed in <ref type="table" target="#tab_0">Table 1</ref> validate the effectiveness of our symmetric deformer design and its deformation-like behavior. That is, The correspondence accuracy of the other two settings, i.e., (a) replacing the symmetric deformer of CorrNet3D with fully connected (FC) layers and (b) training CorrNet3D without (w/o) feeding global features into the symmetric deformer (i.e., the absence of the shape information), decreases significantly. We also compared the symmetric deformer w/ and w/o shared parameters As listed in <ref type="table" target="#tab_0">Table 1</ref>, comparable performance is achieved under these two settings, but the deformer w/ shared parameters is more memory-efficient as the number of parameters reduces by half.</p><p>Loss function. In <ref type="table" target="#tab_1">Table 2</ref>, we compared the performance of CorrNet3D when trained with different loss functions. The effectiveness of the regularization terms can be validated by comparing the 2 nd and 4 th columns, and the advantage of our Euclidean distance-based reconstruction loss over the Chamfer Distance can be demonstrated by comparing the 3 rd and 4 th columns.</p><p>Failure cases. Here we present two failure cases, which occur for computing non-rigid shape correspondence on  highly symmetric and distorted shapes. As show in <ref type="figure" target="#fig_7">Fig. 12</ref>, we can see that the two unsupervised methods i.e., F3D and CorrNet3D, both generate wrong correspondence especially in the hands and feet. However, the supervised S-CorrNet3D can successfully obtain the correct correspondence. In future, solving such issues without using heavy annotations or simple data augmentation is a promising direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have presented the first unsupervised and end-to-end learning framework named CorrNet3D for building dense correspondence between 3D shapes in the form of point clouds. Unlike existing works, we addressed this challenging problem from the deformation-like reconstruction perspective. Note that CorrNet3D is a flexible framework in that it can be simplified to work in a supervised manner when annotated data are available. We demonstrated the significant advantages of our methods over state-ofthe-art ones by conducting extensive experiments on real scanned and synthetic data including rigid and non-rigid shapes in both unsupervised (CorrNet3D) and supervised (S-CorrNet3d) scenarios, as well as comprehensive ablation studies. We believe our methods will bring benefits to other tasks, such as point cloud sequence compression which needs correspondence for eliminating the inter-frame redundancy, and deep learning-based point cloud sequence analysis, which usually has to align points from different frames for feature aggregation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of the correspondence indicator, which predicts a correspondence matrix P by taking pointwise features Fa and F b as input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of one branch of the symmetric deformer. The global feature v b encoding the shape information of B is concatenated to the 3D coordinate of each point of A, which are fed into an MLP to reconstruct B from a deformation perspective.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Quantitative comparisons of different methods for nonrigid shape correspondence. Visual comparisons of different methods for non-rigid shape correspondence in the form of point clouds. Both lines and colors are used for illustrating the correspondence. (a) Groundtruth. (b) F3D. (c) CorrNet3D. (d) S-CorrNet3D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Visual comparisons of different methods on non-rigid shapes in the form of 3D meshes. Each mesh contains 5200 vertices. We render the two corresponded points of the two shapes in the same color.(a) and (d) show the ground-truth correspondence. (b) and (c) show the results of CorrNet3D and DeepGFM, respectively. Note that CorrNet3D takes only 1024 vertices as input and DeepGFM takes 3D meshes as input. We compute the correspondence of the remaining points is obtained via a pseudo clustering strategy. Visual comparisons of different methods for rigid shape correspondence. (a) and (f) show the ground-truth correspondence. (b),(c),(d) and (e) show the results of DCP, RPMNet, Corr-Net3D and S-CorrNet3D, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 .</head><label>9</label><figDesc>Quantitative comparisons of different methods for rigid shape correspondence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 .</head><label>10</label><figDesc>Visual results of CorrNet3D on 8iVFB [10] -a real scanned dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 .</head><label>11</label><figDesc>Comparisons between our DeSmooth and the Sinkhorn layer in terms of accuracy (a) and efficiency (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 12 .</head><label>12</label><figDesc>Visualization of two failure cases of non-rigid shape correspondence. (a) and (e) show the ground-truth correspondence. (b) and (e) show the results by F3D. (c) and (e) show the results by CorrNet3D. (d) and (e) show the results by S-CorrNet3D.The corresponding points are shown as the same color. Note that the two unsupervised methods F3D and CorrNet3D fail, while the supervised S-CorrNet3D can obtain correct results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Validation of the effectiveness and behavior of the symmetric deformer. Here the tolerance error is equal to 20%.</figDesc><table><row><cell>Module</cell><cell>(a) Fully</cell><cell cols="3">(b) Deformer (c) Deformer (d) Deformer</cell></row><row><cell></cell><cell>connected</cell><cell>(w/o va, v b )</cell><cell>(not shared)</cell><cell>(shared)</cell></row><row><cell>Corr(%)</cell><cell>26.21</cell><cell>25.24</cell><cell>95.97</cell><cell>95.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Validation of the effectiveness of loss setting in Eq. (9), Lreg = ?1Lperm + ?2L mf d . The tolerance error is 20%.LossL rec CD + L reg L rec + L reg</figDesc><table><row><cell>Corr(%)</cell><cell>31.49</cell><cell>25.74</cell><cell></cell><cell>95.61</cell></row><row><cell>( ) a</cell><cell>( ) b</cell><cell>( ) c</cell><cell>( ) d</cell><cell>( ) e</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? As DCP and RPMNet are able to generate soft correspondence by transformer and Sinkhorn, respectively, We output the soft correspondence matrix and set each row's max value as 1, and others as 0 to get a binary correspondence matrix for comparison.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pointnetlk: Robust &amp; efficient point cloud registration using pointnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhiro</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hunter</forename><surname>Goforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Rangaprasad Arun Srivatsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7163" to="7172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The road to immersive communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">A</forename><surname>Apostolopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ton</forename><surname>Culbertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susie</forename><surname>Trott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="974" to="990" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A survey on 3d object detection methods for autonomous driving applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Omar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Al-Jarrah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saber</forename><surname>Dianati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Fallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Oxtoby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mouzakitis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3782" to="3795" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recent trends, applications, and perspectives in 3d shape similarity assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Biasotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Cerri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="87" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convpoint: Continuous convolutions for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Boulch</surname></persName>
		</author>
		<idno>2020. 4</idno>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="24" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generalized multidimensional scaling: a framework for isometry-invariant partial surface matching. Proceedings of the National Academy of Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alexander M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kimmel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="1168" to="1172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Supervised descriptor learning for non-rigid shape matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?tienne</forename><surname>Corman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maks</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonin</forename><surname>Chambolle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="283" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep geometric functional maps: Robust feature learning for shape correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Donati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maks</forename><surname>Ovsjanikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="8592" to="8601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">8i voxelized full bodies-a voxelized point cloud dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Eugene D&amp;apos;eon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taos</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chou</surname></persName>
		</author>
		<idno>JTC1/SC29</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<title level="m">Joint WG11/WG1 (MPEG/JPEG) input document WG11M40059/WG1M74006, 2017</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A review on deep learning approaches for 3d data representations in retrieval and classifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Abubakar Sulaiman Gezawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qicong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yunqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57566" to="57593" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Singular value decomposition and least squares solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reinsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linear Algebra</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1971" />
			<biblScope unit="page" from="134" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3d-coded: 3d correspondences by deep deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A papier-m?ch? approach to learning 3d surface generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised cycleconsistent deformation for shape matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="123" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning for 3d point clouds: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised learning of dense shape correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oshri</forename><surname>Halimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4370" to="4379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The elements of statistical learning: data mining, inference, and prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Non-rigid registration under isometric deformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Xing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3d point cloud geometry compression on deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="890" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kd-networks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="863" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep functional maps: Structured prediction for dense shape correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Remez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5659" to="5667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning on point clouds and its application: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page">4188</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Flownet3d: Learning scene flow in 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning latent permutations with gumbelsinkhorn networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gonzalo</forename><surname>Mena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Linderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08665</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Functional maps: a flexible representation of maps between shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maks</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirela</forename><surname>Ben-Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Butscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The three sigma rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedrich</forename><surname>Pukelsheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="88" to="91" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An introduction of autonomous vehicles and a brief survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tirumalapudi</forename><surname>Raviteja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Critical Reviews</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="196" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Osman Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3577" to="3586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Recent advances in shape correspondence. The Visual Computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Sahillioglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1705" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Shot: Unique signatures of histograms for surface and texture description. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuele</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page" from="251" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Introduction to augmented reality. National laboratory for scientific computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jauvane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilson A</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Giraldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pointgrow: Autoregressively learned point cloud generation with self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Sarma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="61" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Human motion transfer with 3d constraints and detail enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang-Tian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Cheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue-Ren</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13510</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Registration of 3d point clouds and meshes: A survey from rigid to nonrigid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K L</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Langbein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1199" to="1217" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Intrinsic shape matching by planned landmark sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Art</forename><surname>Tevs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Ihrke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H-P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="543" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A survey on shape correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Van Kaick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Hamarneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1681" to="1707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiley Online Library</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep cascade generation on point sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">O-cnn: Octree-based convolutional neural networks for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep closest point: Learning representations for point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Foldingnet: Point cloud auto-encoder via deep grid deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rpm-net: Robust point matching using learned features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Zi Jian Yew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
