<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Loss for Active Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggeun</forename><surname>Yoo</surname></persName>
							<email>dgyoo@lunit.ioiskweon77@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Lunit Inc</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">KAIST</orgName>
								<address>
									<settlement>Daejeon</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">KAIST</orgName>
								<address>
									<settlement>Daejeon</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Loss for Active Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The performance of deep neural networks improves with more annotated data. The problem is that the budget for annotation is limited. One solution to this is active learning, where a model asks human to annotate data that it perceived as uncertain. A variety of recent methods have been proposed to apply active learning to deep networks but most of them are either designed specific for their target tasks or computationally inefficient for large networks. In this paper, we propose a novel active learning method that is simple but task-agnostic, and works efficiently with the deep networks. We attach a small parametric module, named "loss prediction module," to a target network, and learn it to predict target losses of unlabeled inputs. Then, this module can suggest data that the target model is likely to produce a wrong prediction. This method is task-agnostic as networks are learned from a single loss regardless of target tasks. We rigorously validate our method through image classification, object detection, and human pose estimation, with the recent network architectures. The results demonstrate that our method consistently outperforms the previous methods over the tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Data is flooding in, but deep neural networks are still data-hungry. The empirical analysis of <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b19">20]</ref> suggests that the performance of recent deep networks is not yet saturated with respect to the size of training data. For this reason, learning methods from semi-supervised learning <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b19">20]</ref> to unsupervised learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b37">38]</ref> are attracting attention along with weakly-labeled or unlabeled large-scale data.</p><p>However, given a fixed amount of data, the performance of the semi-supervised or unsupervised learning is still bound to that of fully-supervised learning. The experimen- tal results of semi-supervised learning in <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b44">45]</ref> demonstrate that the higher portion of annotated data ensures superior performance. This is why we are suffering from annotation labor and cost of time.</p><p>The cost of annotation varies widely depending on target tasks. In the natural image domain, it is relatively cheap to annotate class labels for classification, but detection requires expensive bounding boxes. For segmentation, it is more expensive to draw pixel-level masks. The situation gets much worse when we consider the bio-medical image domain. It requires board-citified specialists trained for several years (radiologists for radiography images <ref type="bibr" target="#b34">[35]</ref>, pathologists for slide images <ref type="bibr" target="#b23">[24]</ref>) to obtain annotations.</p><p>The budget for annotation is limited. What then is the most efficient use of the budget? <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref> first proposed ac-tive learning where a model actively selects data points that the model is uncertain of. For an example of binary classification <ref type="bibr" target="#b25">[26]</ref>, the data point whose posterior probability closest to 0.5 is selected, annotated, and added to a training set. The core idea of active learning is that the most informative data point would be more beneficial to model improvement than a randomly chosen data point.</p><p>Given a pool of unlabeled data, there have been three major approaches according to the selection criteria: an uncertainty-based approach, a diversity-based approach, and expected model change. The uncertainty approach <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b3">4]</ref> defines and measures the quantity of uncertainty to select uncertain data points, while the diversity approach <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b4">5]</ref> selects diverse data points that represent the whole distribution of the unlabeled pool. Expected model change <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b11">12]</ref> selects data points that would cause the greatest change to the current model parameters or outputs if we knew their labels. Readers can review most of classical studies for these approaches in <ref type="bibr" target="#b45">[46]</ref>.</p><p>The simplest method of the uncertainty approach is to utilize class posterior probabilities to define uncertainty. The probability of a predicted class <ref type="bibr" target="#b25">[26]</ref> or an entropy of class posterior probabilities <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b54">55]</ref> defines uncertainty of a data point. Despite its simplicity, this approach has performed remarkably well in various scenarios. For more complex recognition tasks, it is required to re-define taskspecific uncertainty such as object detection <ref type="bibr" target="#b53">[54]</ref>, semantic segmentation <ref type="bibr" target="#b28">[29]</ref>, and human pose estimation <ref type="bibr" target="#b7">[8]</ref>.</p><p>As a task-agnostic uncertainty approach, <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b3">4]</ref> train multiple models to construct a committee, and measure the consensus between the multiple predictions from the committee. However, constructing a committee is too expensive for current deep networks learned with large data. Recently, Gal et al. <ref type="bibr" target="#b13">[14]</ref> obtains uncertainty estimates from deep networks through multiple forward passes by Monte Carlo Dropout <ref type="bibr" target="#b12">[13]</ref>. It was shown to be effective for classification with small datasets, but according to <ref type="bibr" target="#b44">[45]</ref>, it does not scale to larger datasets.</p><p>The distribution approach could be task-agnostic as it depends on a feature space, not on predictions. However, extra engineering would be necessary to design a locationinvariant feature space for localization tasks such as object detection and segmentation. The method of expected model change has been successful for small models but it is computationally impractical for recent deep networks.</p><p>The majority of empirical results from previous researches suggest that active learning is actually reducing the annotation cost. The problem is that most of methods require task-specific design or are not efficient in the recent deep networks, resulting in another engineering cost. In this paper, we aim to propose a novel active learning method that is simple but task-agnostic, and performs well on deep networks.</p><p>A deep network is learned by minimizing a single loss, regardless of what a task is, how many tasks there are, and how complex an architecture is. This fact motivates our task-agnostic design for active learning. If we can predict the loss of a data point, it becomes possible to select data points that are expected to have high losses. The selected data points would be more informative to the current model.</p><p>To realize this scenario, we attach a "loss prediction module" to a deep network and learn the module to predict the loss of an input data point. The module is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>-(a). Once the module is learned, it can be utilized to active learning as shown in <ref type="figure" target="#fig_0">Figure 1</ref>-(b). We can apply this method to any task that uses a deep network.</p><p>We validate the proposed method through image classification, human pose estimation, and object detection. The human pose estimation is a typical regression task, and the object detection is a more complex problem combined with both regression and classification. The experimental results demonstrate that the proposed method consistently outperforms previous methods with a current network architecture for each recognition task. To the best of our knowledge, this is the first work verified with three different recognition tasks using the state-of-the-art deep network models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Contributions</head><p>In summary, our major contributions are 1. Proposing a simple but efficient active learning method with the loss prediction module, which is directly applicable to any tasks with recent deep networks.</p><p>2. Evaluating the proposed method with three learning tasks including classification, regression, and a hybrid of them, by using current network architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Research</head><p>Active learning has advanced for more than a couple of decades. First, we introduce classical active learning methods that use small-scale models <ref type="bibr" target="#b45">[46]</ref>. In the uncertainty approach, a naive way to define uncertainty is to use the posterior probability of a predicted class <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b24">25]</ref>, or the margin between posterior probabilities of a predicted class and the secondly predicted class <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b42">43]</ref>. The entropy <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b18">19]</ref> of class posterior probabilities generalizes the former definitions. For SVMs, distances <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b26">27]</ref> to the decision boundaries can be used to define uncertainty. Another approach is the query-by-committee <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b17">18]</ref>. This method constructs a committee comprising multiple independent models, and measures disagreement among them to define uncertainty.</p><p>The distribution approach chooses data points that represent the distribution of an unlabeled pool. The intuition is that learning over a representative subset would be competitive over the whole pool. To do so, <ref type="bibr" target="#b36">[37]</ref> applies a clustering algorithm to the pool, and <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15]</ref> formulate the subset selection as a discrete optimization problem. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32]</ref> consider how close a data point is to surrounding data points to choose one that could well propagate the knowledge. The method of expected model change is a more sophisticated and decision-theoretic approach for model improvement.</p><p>It utilizes the current model to estimate expected gradient length <ref type="bibr" target="#b47">[48]</ref>, expected future errors <ref type="bibr" target="#b43">[44]</ref>, or expected output changes <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21]</ref>, to all possible labels.</p><p>Do these methods, advanced with small models and data, well scale to large deep networks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b16">17]</ref> and data? Fortunately, the uncertainty approach <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b54">55]</ref> for classification tasks still performs well despite its simplicity. However, a task-specific design is necessary for other tasks since it utilizes network outputs. As a more generalized uncertainty approach, <ref type="bibr" target="#b13">[14]</ref> obtains uncertainty estimates through multiple forward passes with Monte Carlo Dropout, but it is computationally inefficient for recent large-scale learning as it requires dense dropout layers that drastically slow down the convergence speed. This method has been verified only with small-scale classification tasks. <ref type="bibr" target="#b3">[4]</ref> constructs a committee comprising 5 deep networks to measure disagreement as uncertainty. It has shown the state-of-the-art classification performance, but it is also inefficient in terms of memory and computation for large-scale problems.</p><p>Sener et al. <ref type="bibr" target="#b44">[45]</ref> propose a distribution approach on an intermediate feature space of a deep network. This method is directly applicable to any task and network architecture since it depends on intermediate features rather than the task-specific outputs. However, it is still questionable whether the intermediate feature representation is effective for localization tasks such as detection and segmentation. This method has also been verified only with classification tasks. As the two approaches based on uncertainty and distribution are differently motivated, they are complementary to each other. Thus, a variety of hybrid strategies have been proposed <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b55">56]</ref> for their specific tasks.</p><p>Our method can be categorized into the uncertainty approach but differs in that it predicts "loss" based on the input contents, rather than statistically estimating uncertainty from outputs. It is similar to a variety of hard example mining <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b10">11]</ref> since they regard training data points with high losses as being significant for model improvement. However, ours is distinct from theirs in that we do not have annotations of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we introduce the proposed active learning method. We start with an overview of the whole active learning system in Section 3.1, and provide in-depth descriptions of the loss prediction module in Section 3.2, and the method to learn this module in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>In this section, we formally define the active learning scenario with the proposed loss prediction module. In this scenario, we have a set of models composed of a target model ? target and a loss prediction module ? loss . The loss prediction module is attached to the target model as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>-(a). The target model conducts the target task as? = ? target (x), while the loss prediction module predicts the lossl = ? loss (h). Here, h is a feature set of x extracted from several hidden layers of ? target .</p><p>In most real-world learning problems, we can gather a large pool of unlabeled data U N at once. The subscript N denotes the number of data points. Then, we uniformly sample K data points at random from the unlabeled pool, and ask human oracles to annotate them to construct an initial labeled dataset L 0 K . The subscript 0 means it is the initial stage. This process reduces the size of the unlabeled pool as U 0 N ?K . Once the initially labeled dataset L 0 K is obtained, we jointly learn an initial target model ? 0 target and an initial loss prediction module ? 0 loss . After initial training, we evaluate all the data points in the unlabeled pool by the loss prediction module to obtain data-loss pairs {(x,l)|x ? U 0 N ?K }. Then, human oracles annotate the data points of the Khighest losses. The labeled dataset L 0 K is updated with them and becomes L 1 2K . After that, we learn the model set over</p><formula xml:id="formula_0">L 1 2K to obtain {? 1 target , ? 1 loss }. This cycle, illustrated in Fig- ure 1-(b)</formula><p>, repeats until we meet a satisfactory performance or until we have exhausted the budget for annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Loss Prediction Module</head><p>The loss prediction module is core to our task-agnostic active learning since it learns to imitate the loss defined in the target model. This section describes how we design it.</p><p>The loss prediction module aims to minimize the engineering cost of defining task-specific uncertainty for active learning. Moreover, we also want to minimize the computational cost of learning the loss prediction module, as we are already suffering from the computational cost of learning very deep networks. To this end, we design a loss prediction module that is (1) much smaller than the target model, and (2) jointly learned with the target model. There is no separated stage to learn this module. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the architecture of our loss prediction module. It takes multi-layer feature maps h as inputs that are extracted between the mid-level blocks of the target model. These multiple connections let the loss prediction module to choose necessary information between layers useful for loss prediction. Each feature map is reduced to a fixed dimensional feature vector through a global average pooling (GAP) layer and a fully-connected layer. Then, all features are concatenated and pass through another fully-  connected layer, resulting in a scalar valuel as a predicted loss. Learning this two-story module requires much less memory and computation than the target model. We have tried to make this module deeper and wider, but the performance does not change much.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning Loss</head><p>In this section, we provide an in-detail description of how to learn the loss prediction module defined before. Let us suppose we start the s-th active learning stage. We have a labeled dataset L s K?(s+1) and a model set composed of a target model ? target and a loss prediction module ? loss . Our objective is to learn the model set for this stage s to obtain {? s target , ? s loss }. Given a training data point x, we obtain a target prediction through the target model as? = ? target (x), and also a predicted loss through the loss prediction module a? l = ? loss (h). With the target annotation y of x, the target loss can be computed as l = L target (?, y) to learn the target model. Since this loss l is a ground-truth target of h for the loss prediction module, we can also compute the loss for the loss prediction module as L loss (l, l). Then, the final loss function to jointly learn both of the target model and the loss prediction module is defined as</p><formula xml:id="formula_1">L target (?, y) + ? ? L loss (l, l)<label>(1)</label></formula><p>where ? is a scaling constant. This procedure to define the final loss is illustrated in <ref type="figure">Figure 3</ref>. Perhaps the simplest way to define the loss-prediction loss function is the mean square error (MSE) L loss (l, l) = (l ? l) 2 . However, MSE is not a suitable choice for this problem since the scale of the real loss l changes (decreases in overall) as learning of the target model progresses. Minimizing MSE would let the loss prediction module adapt roughly to the scale changes of the loss l, rather than fitting to the exact value. We have tried to minimize MSE but failed to learn a good loss prediction module, and active learning with this module actually demonstrates performance worse than previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss prediction module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Target prediction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss prediction</head><p>Target GT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target loss</head><p>Loss-prediction loss <ref type="figure">Figure 3</ref>. Method to learn the loss. Given an input, the target model outputs a target prediction, and the loss prediction module outputs a predicted loss. The target prediction and the target annotation are used to compute a target loss to learn the target model. Then, the target loss is regarded as a ground-truth loss for the loss prediction module, and used to compute the loss-prediction loss.</p><p>It is necessary for the loss-prediction loss function to discard the overall scale of l. Our solution is to compare a pair of samples. Let us consider a training iteration with a mini-batch B s ? L s K?(s+1) . In the mini-batch whose size is B, we can make B/2 data pairs such as {x p = (x i , x j )}. The subscript p represents that it is a pair, and the minibatch size B should be an even number. Then, we can learn the loss prediction module by considering the difference between a pair of loss predictions, which completely make the loss prediction module discard the overall scale changes. To this end, the loss function for the loss prediction module is defined as</p><formula xml:id="formula_2">L loss (l p , l p ) = max 0, ?1(l i , l j ) ? (l i ?l j ) + ? s.t. 1(l i , l j ) = +1, if l i &gt; l j ?1, otherwise<label>(2)</label></formula><p>where ? is a pre-defined positive margin and the subscript p also represents the pair of (i, j). For instance when l i &gt; l j , this function states that no loss is given to the module only ifl i is larger thanl j + ?, but otherwise a loss is given to the module to force it to increasel i and decreasel j . Given a mini-batch B s in the active learning stage s, our final loss function to jointly learn the target model and the loss prediction module is</p><formula xml:id="formula_3">1 B (x,y)?B s L target (?, y) + ? 2 B ? (x p ,y p )?B s L loss (l p , l p ) s.t.? = ? target (x) l p = ? loss (h p ) l p = L target (? p , y p ).<label>(3)</label></formula><p>Minimizing this final loss give us ? s loss as well as ? s target without any separated learning procedure nor any taskspecific assumption. The learning process is efficient as the loss prediction module ? s loss has been designed to contain a small number of parameters but to utilize rich mid-level representations h of the target model. This loss prediction module will pick the most informative data points and ask human oracles to annotate them for the next active learning stage s + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head><p>In this section, we rigorously evaluate our method through three visual recognition tasks. To verify whether our method works efficiently regardless of tasks, we choose diverse target tasks including image classification as a classification task, object detection as a hybrid task of classification and regression, and human pose estimation as a typical regression problem. These three tasks are indeed important research topics for visual recognition in computer vision, and are very useful for many real-world applications.</p><p>We have implemented our method and all the recognition tasks with PyTorch <ref type="bibr" target="#b39">[40]</ref>. For all tasks, we initialize a labeled dataset L 0 K by randomly sampling K=1,000 data points from the entire dataset U N . In each active learning cycle, we continue to train the current model by adding K=1,000 labeled data points. The margin ? defined in the loss function (Equation 2) is set to 1. We design the fullyconnected layers (FCs) in <ref type="figure" target="#fig_1">Figure 2</ref> except for the last one to produce a 128-dimensional feature. For each active learning method, we repeat the same experiment multiple times with different initial labeled datasets, and report the performance mean and standard deviation. For each trial, our method and compared methods share the same random seed for a fair comparison. Other implementation details, datasets, and experimental results for each task are described in the following Sections 4.1, 4.2, 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image Classification</head><p>Image classification is a common problem that has been verified by most of the previous active learning methods. In this problem, a target model recognizes the category of a major object from an input image, so object category labels are required for supervised learning.</p><p>Dataset We choose CIFAR-10 dataset <ref type="bibr" target="#b21">[22]</ref> as it has been used for recent active learning methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b3">4]</ref>. CIFAR-10 consists of 60,000 images of 32?32?3 size, assigned with one of 10 object categories. The training and test sets contain 50,000 and 10,000 images respectively. We regard the training set as the initial unlabeled pool U 50,000 . As studied in <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>, selecting K-most uncertain samples from such a large pool U 50,000 often does not work well, because image contents among the K samples are overlapped. To address this, <ref type="bibr" target="#b3">[4]</ref> obtains a random subset S M ? U N for each active learning stage and choose K-most uncertain samples from S M . We adopt this simple yet efficient scheme and set the subset size to M =10,000. As an evaluation metric, we use the classification accuracy.</p><p>Target model We employ the 18-layer residual network (ResNet-18) <ref type="bibr" target="#b16">[17]</ref> as we aim to verify our method with current deep architectures. We have utilized an open source 1 in which this model specified for CIFAR showing 93.02% accuracy is implemented. ResNet-18 for CIFAR is identical to the original ResNet-18 except for the first convolution and pooling layers. The first convolution layer is changed to contain 3?3 kernels with the stride of 1 and the padding of 1, and the max pooling layer is dropped, to adapt to the small size images of CIFAR.</p><p>Loss prediction module ResNet-18 is composed of 4 basic blocks {convi 1, convi 2 | i=2, 3, 4, 5} following the first convolution layer. Each block comprises two convolution layers. We simply connect the loss prediction module to each of the basic blocks to utilize the 4 rich features from the blocks for estimating the loss.</p><p>Learning For training, we apply a standard augmentation scheme including 32?32 size random crop from 36?36 zero-padded images and random horizontal flip, and normalize images using the channel mean and standard deviation vectors estimated over the training set. For each of active learning cycle, we learn the model set {? s target , ? s loss } for 200 epochs with the mini-batch size of 128 and the initial learning rate of 0.1. After 160 epochs, we decrease the learning rate to 0.01. The momentum and the weight decay are 0.9 and 0.0005 respectively. After 120 epochs, we stop the gradient from the loss prediction module propagated to the target model. We set ? that scales the loss-prediction loss in Equation 3 to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison targets</head><p>We compare our method with random sampling, entropy-based sampling <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b30">31]</ref>, and coreset sampling <ref type="bibr" target="#b44">[45]</ref>, which is a recent distribution approach. For the entropy-based method, we compute the entropy from a softmax output vector. For core-set, we have implemented K-Center-Greedy algorithm in <ref type="bibr" target="#b44">[45]</ref> since it is simple to implement yet marginally worse than the mixed integer program. We also run the algorithm over the last feature space right before the classification layer as <ref type="bibr" target="#b44">[45]</ref> do. Note that we use exactly the same hyper-parameters to train target models for all methods including ours.</p><p>The results are shown in <ref type="figure" target="#fig_3">Figure 4</ref>. Each point is an average of 5 trials with different initial labeled datasets. Our implementations show that both entropy-based and coreset methods have better results than the random baseline. In the last active learning cycle, the entropy and core-set Accuracy (mean of 5 trials) random mean random mean?std entropy mean entropy mean?std core-set mean core-set mean?std learn loss mse mean learn loss mse mean?std learn loss mean learn loss mean?std methods show 0.9059 and 0.9010 respectively, while the random baseline shows 0.8764. The performance gaps between these methods are similar to those of <ref type="bibr" target="#b3">[4]</ref>. In particular, the simple entropy-based method works very effectively with the classification which is typically learned to minimize cross-entropy between predictions and target labels. Our method noted as "learn loss" shows the highest performance for all active learning cycles. In the last cycle, our method achieves an accuracy of 0.9101. This is 0.42% higher than the entropy method and 0.91% higher than the core-set method. Although the performance gap to the entropy-based method is marginal in classification, our method can be effectively applied to more complex and diverse target tasks.</p><p>We define an evaluation metric to measure the performance of the loss prediction module. For a pair of data points, we give a score 1 if the predicted ranking is true, and 0 for otherwise. These binary scores from every pair of test sets are averaged to a value named "ranking accuracy". <ref type="figure" target="#fig_4">Figure 5</ref> shows the ranking accuracy of the loss prediction module over the test set. As we add more labeled data, loss prediction module becomes more accurate and finally reaches 0.9074. The use of MSE for learning the loss prediction module with ?=0.1, noted by "learn loss mse", yields lower loss-prediction performance ( <ref type="figure" target="#fig_4">Figure 5</ref>) that results in less-efficient active learning <ref type="figure" target="#fig_3">(Figure 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Object Detection</head><p>Object detection localizes bounding boxes of semantic objects and recognizes the categories of the objects. It is a typical hybrid task as it combines a regression problem for bounding box estimation and a classification problem for category recognition. It requires both object bounding boxes and category labels for supervised learning. Following the recent use of VOC for object detection, we make a super-set trainval'07+12 by combining the two, and use it as the initial unlabeled pool U <ref type="bibr" target="#b15">16,</ref><ref type="bibr">551</ref> . The active learning method is evaluated over test'07 with mean average precision (mAP), which is a standard metric for object detection. We do not create a random subset S M since the size of the pool U 16,551 is not very large in contrast to CIFAR-10.</p><p>Target model We employ Single Shot Multibox Detector (SSD) <ref type="bibr" target="#b29">[30]</ref> as it is one of the popular models for recent object detection. It is a large network with a backbone of VGG-16 <ref type="bibr" target="#b50">[51]</ref>. We have utilized an open source 2 which shows 0.7743 (mAP) slightly higher than the original paper.</p><p>Loss prediction module SSD estimates bounding-boxes and their classes from 6-level feature maps extracted from {convi | i=4 3, 7, 8 2, 9 2, 10 2, 11 2} <ref type="bibr" target="#b29">[30]</ref>. Accordingly, we also connect the loss prediction module to each of them to utilize the 6 rich features for estimating the loss.</p><p>Learning We use exactly the same hyper-parameter values and the data augmentation scheme described in <ref type="bibr" target="#b29">[30]</ref>, except for the number of iterations since we use a smaller training set for each active learning cycle. We learn the model set for 300 epochs with the mini-batch size of 32.</p><p>After 240 epochs, we reduce the learning rate from 0.001 to 0.0001. We set the scaling constant ? in Equation 3 to 1.  Comparison targets For the entropy-based method, we compute the entropy of an image by averaging all entropy values from softmax outputs corresponding to detection boxes. For core-set, we also run K-Center-Greedy over conv7 (i.e., FC7 in VGG-16) features after applying the spatial average pooling. Note, we use exactly the same hyperparameters to train SSDs for all methods including ours. <ref type="figure" target="#fig_6">Figure 6</ref> shows the results. Each point is an average of 3 trials with different initial labeled datasets. In the last active learning cycle, our method achieves 0.7338 mAP which is 2.21% higher than 0.7117 of the random baseline. The entropy and core-set methods, showing 0.7222 and 0.7171 respectively, also perform better than the random baseline. However, our method outperforms these methods by margins of 1.15% and 1.63%. The entropy method cannot capture the uncertainty about bounding box regression, which is an important element of object detection, so need to design another uncertainty metric specified for regression. The core-set method also needs to design a feature space that well encodes object-centric information while being invariant to object locations. In contrast, our learning-based approach does not need specific designs since it predicts the final loss value, regardless of tasks. Even if it is much difficult to predict the final loss come from regression and classification, our loss prediction module yields about 70% ranking accuracy as shown in <ref type="figure" target="#fig_4">Figure 5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Human Pose Estimation</head><p>Human pose estimation is to localize all the body parts from an image. The point annotations of all the body parts are required for supervised learning. It is often approached by a regression problem as the target is a set of points.</p><p>Dataset We choose MPII dataset <ref type="bibr" target="#b1">[2]</ref> which is commonly used for the majority of recent works. We follow the same splits used in <ref type="bibr" target="#b35">[36]</ref> where a training set consists of 22,246 poses from 14,679 images and a test set consists of 2,958 poses from 2,729 images. We use the training set as the initial unlabeled pool U <ref type="bibr" target="#b21">22,</ref><ref type="bibr">246</ref> . For each cycle, we obtain a random sub-pool S 5,000 from U 22,246 , following the similar portion of the sub-pool to the entire pool in CIFAR-10. The standard evaluation metric for this problem is Percentage of Correct Key-points (PCK) which measures the percentage of predicted key-points falling within a distance threshold to the ground truth. Following <ref type="bibr" target="#b35">[36]</ref>, we use PCKh@0.5 in which the distance is normalized by a fraction of the head size and the threshold is 0.5.</p><p>Target model We adopt Stacked Hourglass Networks <ref type="bibr" target="#b35">[36]</ref>, in which an hourglass network consists of down-scale pooling and subsequent up-sampling processes to allow bottom-up, top-down inference across scales. The network produces heatmaps corresponding to the body parts and they are compared to ground-truth heatmaps by applying an MSE loss. We have utilized an open source 3 yielding 88.78% (PCK@0.5), which is similar to <ref type="bibr" target="#b35">[36]</ref> with 8 hourglass networks. Since learning 8 hourglass networks on a single GPU with the original mini-batch size of 6 is too slow for our active learning experiments, we have tried multi-GPU learning with larger mini-batch sizes. However, the performance has significantly decreased as the mini-batch size increases, even without the loss prediction module. Thus, we have inevitably stacked two hourglass networks which show 86.95%.</p><p>Loss prediction module For each hourglass network, the body part heatmaps are driven from the last feature map of (H,W,C)=(64,64,256). We choose this feature map to estimate the loss. As we stack two hourglass networks, the two feature maps are given to our loss prediction module.</p><p>Learning We use exactly the same hyper-parameter values and data augmentation scheme described in <ref type="bibr" target="#b35">[36]</ref>, except the number of training iterations. We learn the model set for 125 epochs with the mini-batch size of 6. After 100 epochs, we reduce the learning rate from 0.00025 to 0.000025. After 75 epochs, the gradient from the loss prediction module is not propagated to the target model. We set the scaling constant ? in Equation 3 to 0.0001 since the scale of MSE is very small (around 0.001 after several epochs).</p><p>Comparison targets Stacked Hourglass Networks do not produce softmax outputs but body part heatmaps. Thus, we apply the softmax to each heatmap and estimate an entropy PCKh@0.5 (mean of 3 trials) random mean random mean?std entropy mean entropy mean?std core-set mean core-set mean?std learn loss mean learn loss mean?std for each body part. We then average all of the entropy values. For core-set, we run K-Center-Greedy over the last feature maps after applying the spatial average pooling. Note, we use exactly the same hyper-parameters to train the target models for all methods including ours. Experiment results are given in <ref type="figure" target="#fig_9">Figure 7</ref>. Each point is also an average of 3 trials with different initial labeled datasets. The results show that our method outperforms other methods as the active learning cycle progresses. At the end of the cycles, our method attains 0.8046 PCKh@0.5 while the entropy and core-set methods reach 0.7899 and 0.7985, respectively. The performance gaps to these methods are 1.47% and 0.61%. The random baseline shows the lowest of 0.7862. In human pose estimation, the entropy method is not as effective as the classification problem. While this method is advantageous to classification in which a cross-entropy loss is directly minimized, this task minimizes an MSE to estimate body part heatmaps. The core-set method also requires a novel feature space that is invariant to the body part location while preserving the local body part features.</p><p>Our loss prediction module predicts the regression loss with about 75% of ranking accuracy ( <ref type="figure" target="#fig_4">Figure 5</ref>), which enables efficient active learning in this problem. We visualize how the predicted loss correlates with the real loss in <ref type="figure">Fig</ref> loss. The blue color means 20% data points selected from the population according to the predicted loss or entropy. The points chosen by our method actually have high loss values, while the entropy method chooses many points with low loss values. This visualization demonstrates that our method is effective for selecting informative data points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitations and Future Work</head><p>We have introduced a novel active learning method that is applicable to current deep networks with a wide range of tasks. The method has been verified with three major visual recognition tasks with popular network architectures. Although the uncertainty score provided by this method has been effective, the diversity or density of data was not considered. Also, the loss prediction accuracy was relatively low in complex tasks such as object detection and human pose estimation. We will continue this research to take data distribution into consideration and design a better architecture and objective function to increase the accuracy of the loss prediction module.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Active learning with a loss prediction module A novel active learning method with a loss prediction module. (a) A loss prediction module attached to a target model predicts the loss value from an input without its label. (b) All data points in an unlabeled pool are evaluated by the loss prediction module. The data points with the top-K predicted losses are labeled and added to a labeled training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The architecture of the loss prediction module. This module is connected to several layers of the target model to take multi-level knowledge into consideration for loss prediction. The multi-level features are fused and map to a scalar value as the loss prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Active learning results of image classification over CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Loss-prediction accuracy of the loss prediction module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>2 https://github.com/amdegroot/ssd.pytorch 1k 2k 3k 4k 5k 6k 7k 8k 9k 10k Number of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Active learning results of object detection over PASCAL VOC 2007+2012.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>3 https://github.com/bearpaw/pytorch-pose 1k 2k 3k 4k 5k 6k 7k 8k 9k 10k Number of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 .</head><label>7</label><figDesc>Active learning results of human pose estimation over MPII.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>- ure 8 .Figure 8 .</head><label>88</label><figDesc>At the top of the figure, the data points of the MPII test set are scattered to the axes of predicted loss and real loss. Overall, the two values are correlated, and the correlation coefficient<ref type="bibr" target="#b5">[6]</ref> (0 for no relation, 1 for strong relation) is 0.68. At the bottom of the figure, the data points are scattered to the axes of entropy and real loss. The correlation coefficient is 0.45, which is much lower than our predicted Data visualization of (top) our method and (bottom) entropy-based method. We use the model set from the last active learning cycle to obtain the loss, predicted loss and entropy of a human pose. 2,000 poses randomly chosen from the MPII test set are shown.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/kuangliu/pytorch-cifar</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to see by moving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Training connectionist networks with queries and selective sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Atlas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Ladner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="566" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The power of ensembles for active learning in image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Beluch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>N?rnberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>K?hler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9368" to="9377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Link-based active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Analyzing Networks and Learning with Graphs</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Statistical methods in practice: for scientists and technologists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Boddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Active image segmentation propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dutt Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2864" to="2873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A convex optimization framework for active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sasrty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Selecting influential examples: Active learning with expected model output changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Freytag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="562" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep bayesian active learning with image data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1183" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Active instance sampling via matrix partition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Context aware active learning of activity recognition models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4543" to="4551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Combining generative and discriminative models for semantic segmentation of ct scans via active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Iglesias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biennial International Conference on Information Processing in Medical Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-class active learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2372" to="2379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning visual features from large weakly supervised data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasilache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="67" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>K?ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Freytag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06129</idno>
		<title level="m">Active and continuous exploration with deep neural networks and expected model output changes</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A robust and effective approach towards accurate metastasis detection and pn-stage classification in breast cancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Paeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference On Medical Image Computing &amp; Computer Assisted Intervention (MICCAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Heterogeneous uncertainty sampling for supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Catlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Proceedings 1994</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A sequential algorithm for training text classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Gale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 17th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-level adaptive active learning for scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="234" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Active self-paced learning for cost-effective and progressive face identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="7" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Active learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4363" to="4372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Latent structured active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="728" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical subquery evaluation for active learning on a graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>University of Bath</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Employing em and poolbased active learning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallumzy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nigamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="359" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Development and validation of deep learning-based automatic detection algorithm for malignant pulmonary nodules on chest radiographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-N</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Goo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="page">180237</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Active learning using preclustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">79</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Representation learning by learning to count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1742" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Nonuniform subset selection for active learning in structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Bappy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="830" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with ladder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3546" to="3554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Margin-based active learning for structured output spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Small</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="413" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Toward optimal active learning through monte carlo estimation of error reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page" from="441" to="448" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Active learning for convolutional neural networks: A core-set approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="114" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An analysis of active learning strategies for sequence labeling tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Craven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1070" to="1079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multiple-instance active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1289" to="1296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Query by committee</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sompolinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth annual workshop on Computational learning theory</title>
		<meeting>the fifth annual workshop on Computational learning theory</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="287" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Training regionbased object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Support vector machine active learning with applications to text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2001-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Large-scale live active learning: Training object detectors with crawled data and crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="97" to="114" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Towards human-machine cooperation: Self-supervised sample mining for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09867</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Costeffective active learning for deep image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2591" to="2600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Suggestive annotation: A deep active learning framework for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="399" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multi-class active learning by uncertainty sampling with diversity maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="127" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Fine-tuning convolutional neural networks for biomedical image analysis: Actively and incrementally</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gurudu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gotway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4761" to="4772" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
