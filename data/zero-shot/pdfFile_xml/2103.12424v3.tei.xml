<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BossNAS: Exploring Hybrid CNN-transformers with Block-wisely Self-supervised Neural Architecture Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changlin</forename><surname>Li</surname></persName>
							<email>changlin.li@monash.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of DSAI</orgName>
								<orgName type="laboratory">GORSE Lab</orgName>
								<orgName type="institution">Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Tang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">DarkMatter AI Research</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Peng</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">DarkMatter AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Wang</surname></persName>
							<affiliation key="aff4">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
							<email>xdliang328@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
							<email>xiaojun.chang@rmit.edu.au</email>
							<affiliation key="aff5">
								<orgName type="institution">RMIT University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BossNAS: Exploring Hybrid CNN-transformers with Block-wisely Self-supervised Neural Architecture Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A myriad of recent breakthroughs in hand-crafted neural architectures for visual recognition have highlighted the urgent need to explore hybrid architectures consisting of diversified building blocks. Meanwhile, neural architecture search methods are surging with an expectation to reduce human efforts. However, whether NAS methods can efficiently and effectively handle diversified search spaces with disparate candidates (e.g. CNNs and transformers) is still an open question. In this work, we present Block-wisely Self-supervised Neural Architecture Search (BossNAS), an unsupervised NAS method that addresses the problem of inaccurate architecture rating caused by large weight-sharing space and biased supervision in previous methods. More specifically, we factorize the search space into blocks and utilize a novel self-supervised training scheme, named ensemble bootstrapping, to train each block separately before searching them as a whole towards the population center. Additionally, we present HyTra search space, a fabric-like hybrid CNN-transformer search space with searchable downsampling positions. On this challenging search space, our searched model, BossNet-T, achieves up to 82.5% accuracy on ImageNet, surpassing EfficientNet by 2.4% with comparable compute time. Moreover, our method achieves superior architecture rating accuracy with 0.78 and 0.76 Spearman correlation on the canonical MBConv search space with Im-ageNet and on NATS-Bench size search space with CIFAR-100, respectively, surpassing state-of-the-art NAS methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The development of neural network architectures has brought about significant progress in a wide range of visual recognition tasks over the past several years. Representative examples of such models include ResNet <ref type="bibr" target="#b24">[25]</ref>, SENet <ref type="bibr" target="#b30">[31]</ref>, MobileNet <ref type="bibr" target="#b29">[30]</ref> and EfficientNet <ref type="bibr" target="#b63">[64]</ref>. Recently, the newly emerging attention-based architectures are coming to the forefront in the vision field, challenging the dominance of convolutional neural networks (CNNs). This exciting breakthrough in vision transformers led by ViT <ref type="bibr" target="#b19">[20]</ref> and DETR <ref type="bibr" target="#b7">[8]</ref>, are achieving competitive performance on various vision tasks, such as image classification <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b8">9]</ref>, object detection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b61">62]</ref>, semantic segmentation <ref type="bibr" target="#b87">[88]</ref>, and others <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b32">33]</ref>. As suggested by prior works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b2">3]</ref>, hybrids of CNNs and transformers can outperform both pure transformers and pure CNNs.</p><p>Despite the large advances brought about by network design, manually finding well-optimized hybrid architectures can be challenging, especially as the number of design choices increases. Neural Architecture Search (NAS) is a popular approach to reducing the human effort in network architecture design by automatically searching for optimal architectures in a predefined search space. Representative success in performing NAS on manually designed building blocks include MobileNetV3 <ref type="bibr" target="#b28">[29]</ref>, EfficientNet <ref type="bibr" target="#b63">[64]</ref>, etc. These works are searched by multi-trial NAS methods <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b46">47]</ref>, which are computationally prohibitive (costing thousands of GPU days). Recent weight-sharing NAS methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b42">43]</ref> encode the entire search space as a weight-sharing supernet to avoid repetitive training of candidate networks, thus largely reducing the search cost.</p><p>However, as shown in <ref type="figure" target="#fig_0">Fig. 1a</ref>, architecture search spaces with layer-level granularity grow exponentially with increased network depth, which has been identified (in <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b38">39]</ref>) as the main culprit of inaccurate architecture rating 2 in weight-sharing NAS methods. To reduce the size of the large weight-sharing space, previous works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b45">46]</ref> factorize the search space into blocks and use a pretrained teacher model to provide block-wise supervision <ref type="figure" target="#fig_0">(Fig. 1b)</ref>. Despite their high ranking correlation and high efficiency, we find (in Sec. 5) their results to be highly correlated with the teacher architecture. As illustrated in <ref type="figure" target="#fig_0">Fig. 1b</ref>, when training by a teacher with blue nodes, candidate architectures with more blue nodes tend to get higher ranks in these methods. This limits its application on diversified search spaces with disparate candidates, such as CNNs and transformers.</p><p>On the other hand, unsupervised NAS <ref type="bibr" target="#b40">[41]</ref> has recently emerged as an interesting research topic. Without access to any human-annotated labels, unsupervised NAS methods (optimized with pretext tasks <ref type="bibr" target="#b40">[41]</ref> or random labels <ref type="bibr" target="#b86">[87]</ref>) have been proven capable of achieving comparable performance to supervised NAS methods. Accordingly, we propose to use an unsupervised learning method as an alternative to supervised distillation in the aforementioned block-wise NAS scheme ( <ref type="figure" target="#fig_0">Fig. 1c</ref>), aiming to address the problem of architectural bias caused by the use of the teacher model.</p><p>In this work, we propose a novel unsupervised NAS method, Block-wisely Self-supervised Neural Architecture Search (BossNAS), which aims to address the problem of inaccurate predictive architecture ranking caused by a large weight-sharing space while avoiding possible architectural bias caused by the use of the teacher model. As opposed to the block-wise solutions discussed above, which utilize distillation as intermediate supervision, we propose a selfsupervised representation learning scheme named ensemble bootstrapping to optimize each block of our supernet. To be more specific, each sampled sub-networks are trained to predict the probability ensemble of all the sampled ones in the target network, between different augmented views of the same image. In the searching stage, an unsupervised evaluation metric, is proposed to ensure fairness by searching towards the architecture population center. More specifically, the probability ensemble of all the architectures in the population is used as the evaluation target to measure the 2 In this work, architecture rating accuracy refers to the correlation of the predicted architecture ranking and the ground truth architecture ranking. performance of the sampled models.</p><p>Additionally, we design a fabric-like hybrid CNNtransformer search space (HyTra) with searchable downsampling positions and use it as a case study for hybrid architectures to evaluate our method. In each layer of HyTra search space, CNN building blocks and transformer building blocks of different resolutions are in parallel and can be chosen flexibly. This diversified search space covers pure transformers with fixed content length and normal CNNs with progressively reduced spatial scales.</p><p>We prove that our NAS method can generalize well on three different search spaces and three datasets. On HyTra search space, our searched models outperforms the ones searched by our supervised NAS counterpart <ref type="bibr" target="#b36">[37]</ref>, proving that our method successfully avoids possible architecture bias brought by supervised distillation. Our method achieves superior architecture rating accuracy with 0.78 and 0.76 Spearman correlation on the canonical MBConv search space with ImageNet and on NATS-Bench size search space S S <ref type="bibr" target="#b16">[17]</ref> with CIFAR-100, respectively, surpassing state-ofthe-art NAS methods, proving that our method successfully suppressed the problem of inaccurate architecture rating caused by large weight-sharing space.</p><p>Our searched models on HyTra search space achieves 82.5% accuracy on ImageNet, surpassing EfficientNet <ref type="bibr" target="#b63">[64]</ref> by 2.4%, with comparable compute time <ref type="bibr" target="#b2">3</ref> . By providing strong results through BossNet-T, we hope that this diversified HyTra search space with disparate candidates and high-performance architectures can serve as a new arena for future NAS works. We also hope that our BossNAS can serve as a widely used tool for hybrid architecture design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Block-wise weight-sharing NAS <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b84">85]</ref> approaches factorize the supernet into independently optimized blocks and thus reduce the weight-sharing space, resolving the issue of inaccurate architecture ratings caused by weightsharing. DNA <ref type="bibr" target="#b36">[37]</ref> first introduced the block-wisely supervised architecture rating scheme with knowledge distillation. Based on this scheme, DONNA <ref type="bibr" target="#b45">[46]</ref> further propose to predict an architecture rating using a linear combination of its blockwise ratings rather than a simplistic sum. SP <ref type="bibr" target="#b83">[84]</ref> were the first to apply this scheme to network pruning. However, all of the aforementioned methods rely on a supervised distillation scheme, which inevitably introduces architectural bias from the teacher. We accordingly propose a block-wisely self-supervised scheme, which completely casts off the yoke of the teacher architecture. Unsupervised NAS <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b86">87]</ref> methods perform architecture search without access to any human-annotated labels. Un-NAS <ref type="bibr" target="#b40">[41]</ref> introduced unsupervised pretext tasks <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b85">86]</ref> to weight-sharing NAS for supernet training and architecture rating. RLNAS <ref type="bibr" target="#b86">[87]</ref> optimized the supernet using random labels <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b44">45]</ref> and further rated architectures by means of a convergence-based angle metric <ref type="bibr" target="#b31">[32]</ref>. Another line of NAS methods <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b50">51]</ref> belonging to the category of supervised NAS perform unsupervised pretraining of network accuracy predictor or supernet before supervised finetuning or evaluation. Differing from aforementioned works in motivation and methodology, we explore self-supervised contrastive learning methods in our unsupervised NAS scheme to avoid the supervision bias in block-wise NAS. Self-supervised contrastive learning methods <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b90">91,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b9">10]</ref> have significantly advanced the unsupervised learning of visual representations. These approaches learn visual representations in a discriminative fashion by gathering the representations of different views from the same image and spreading those from different images. Recently, the innovative BYOL <ref type="bibr" target="#b20">[21]</ref> and SimSiam <ref type="bibr" target="#b10">[11]</ref> learned visual representations without the use of negative examples. These works directly predict the representation of one view from another using a pair of Siamese networks with the same architectures and shared weights <ref type="bibr" target="#b10">[11]</ref>, or with one of the Siamese network branches being a momentum encoder, thereby forming a bootstrapping scheme <ref type="bibr" target="#b20">[21]</ref>. Our work introduces a novel bootstrapping scheme with probability ensemble to Siamese supernets. Architecture Search Spaces. Cell-based search spaces, first proposed in <ref type="bibr" target="#b92">[93]</ref>, are generally used in previous NAS methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b51">52]</ref> and benchmarks <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b16">17]</ref>. They search for a repeatable cell-level architecture, while keeping a manually designed network-level architecture. By contrast, network-level search spaces with layer-level granularity <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b86">87]</ref> and block-level granularity <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b63">64]</ref> search for the macro network-level structure using manually designed building blocks (e.g. MBConv <ref type="bibr" target="#b55">[56]</ref>). Auto-DeepLab <ref type="bibr" target="#b39">[40]</ref> presents a hierarchical search space for semantic segmentation, with repeatable cells and a fabriclike <ref type="bibr" target="#b56">[57]</ref> network-level structure. Our HyTra search space also has a fabric-like network-level structure, albeit with layer-level granularity rather than repeated cells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Block-wisely Self-supervised NAS</head><p>In this section, we first briefly introduce the dilemma of NAS and its block-wise solutions <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b84">85]</ref>, then present our proposed BossNAS in detail, along with its two key elements: i) unsupervised supernet training phase with ensemble bootstrapping; ii) unsupervised architecture rating and searching phase towards architecture population center. Notations. We denote scalars, tensors and sets of tensors using lower case, bold lower case and upper case calligraphic letters respectively (e.g., n, x and X ). For simplicity, we use {xn} to denote the set {xn} |n| n=1 with cardinality |n|. Here L train (?) denotes the training loss function, while x and y denote the input data and the labels, respectively. Subsequently, architectures ? are searched based on the ranking of their ratings with these shared network weights. Without loss of generality, we choose the evaluation loss function L val as the rating metric; the searching phase can be formulated as: ? * = arg min ???A Lval(W * , ?; x, y). However, the architecture ranking based on the shared weights W * does not necessarily represents the correct ranking of the architectures, as the weights inherited from the supernet are highly entangled and are not fully and fairly optimized. As pointed out in the literature <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b79">80]</ref>, weight-sharing methods suffer from low architecture rating accuracy. Block-wisely supervised NAS. As proven theoretically and experimentally by <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b45">46]</ref>, reducing the weight-sharing space (i.e. total number of weight-sharing architectures) can effectively improve the accuracy of architecture rating. In practice, block-wise solutions <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b84">85]</ref> find a way out of this dilemma of NAS by block-wisely factorizing the search space in the depth dimension, thus reducing the weight-sharing space while maintaining the original size of the search space. Given a supernet consisting of |k| blocks S(W, A) = {S k (W k , A k )}, with W = {W k } and A = {A k } denoting its weights and architecture that are block-wisely separable in the depth dimension, each block of the supernet is trained separately before searching among all blocks in combination by the sum <ref type="bibr" target="#b36">[37]</ref>, or a linear combination <ref type="bibr" target="#b45">[46]</ref> (with weights {? k }), of each block's evaluation loss L val :</p><formula xml:id="formula_0">? * = {? k } * = arg min ?{? k }?A |k| k=1 ? k Lval W * k , ? k ; x k , y k s.t. W * k = arg min W k Ltrain(W k , A k ; x k , y k ) .<label>(1)</label></formula><p>To isolate the training of each supernet block, given an input x, the intermediate input and target {x k , y k } of the k-th block is generated by a fixed teacher network T (with architecture ? T and ground-truth</p><formula xml:id="formula_1">weights W T ): {x1, y 1 } = {x, T1(x)}, and {x k , y k } = {T k?1 (x), T k (x)}, k &gt; 1,</formula><p>where T k represents the teacher network truncated after the k-th block. As the data used for both training and searching phase are generated by the teacher model T (W T , ? T ), the architecture ratings are likely to be highly correlated with the teacher architecture. For instance, a convolutional teacher have a limited receptive field and distinctive architectural inductive biases like translation equivariance. With such a biased supervision, candidate architectures are likely to be trained and rated unfairly. We observes two phenomenons that can be attrbute to the biased supervision, i.e. candidate  preference and teacher preference. Detailed experimental analysis of these two phenomenons is provided in Sec. 5. To break these restrictions of current block-wise NAS solutions, we explore a scheme without using a teacher model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training with Ensemble Bootstrapping</head><p>Starting from the dual network scheme with studentteacher pair {S(W, A), T (W T , ? T )}, the first step to cast off the yoke of the teacher architecture is to assign ? T = A, thus forming a pair of Siamese supernets. Bootstrapping with Siamese Supernets. To optimize such Siamese networks block-wisely, we adopt a self-supervised contrastive learning scheme. More specifically, these two supernets receive a pair of augmented views {x 1 , x 2 } of the same training sample x and generate the outputs {S(W, A; x1), T (W T , A; x2)}, respectively. Analogous to previous teacher-student settings, the Siamese supernets are optimized by minimizing the distance between their outputs. In previous Siamese networks and self-supervised contrastive learning methods, the two networks either share their weights <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> (i.e. W T = W) or form a mean teacher scheme with Exponential Moving Average (EMA) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b20">21]</ref> </p><formula xml:id="formula_2">(i.e. W T = W ? , where W ? t = ? W ? t?1 + (1 ? ? )W t represents</formula><p>the temporal average of W, with t being a training timestamp, and ? denoting the momentum factor that controls the updating speed of W ? ). By learning representation from the mean teacher, analogous to the simple yet powerful BYOL <ref type="bibr" target="#b20">[21]</ref>, our supernet can be optimized in an unsupervised manner without relying on a fully supervised teacher network:</p><formula xml:id="formula_3">W * k = arg min W k Ltrain {W k , W ? k }, A k ; x k .<label>(2)</label></formula><p>To eliminate the influence of pixel-wise differences between two intermediate representations caused by augmentations (e.g. random crop), as well as to ensure better generalization on candidate architectures with different reception fields or even different resolutions, we project the representations to the latent space before calculating the element-wise distance. Ensemble Bootstrapping. However, unlike single networks, supernets are typically optimized by path sampling strategies, e.g. single path <ref type="bibr" target="#b21">[22]</ref> or fair path <ref type="bibr" target="#b14">[15]</ref>. When naively adopting bootstrapping, each sub-network learns from the moving average of itself. In the absence of a common objective, the weights shared by different sub-networks suffer from convergence hardship, leading to training in-stability and inaccurate architecture ratings. To address this problem, we propose an unsupervised supernet training scheme, named ensemble bootstrapping.</p><p>Considering |p| sub-networks {?p} ? A k sampled from the k-th block of the search space A in the t-th training iteration, and given a training sample x, |p| pairs of augmented views {xp} ? p aug (?|x), {x p } ? p aug (?|x) are generated for each sampled sub-network of the Siamese supernets. To form a common objective for all paths, we can use a scheme analogous to ensemble distillation <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60]</ref> in supervised learning. As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, each sampled sub-network of the online supernet learns to predict the probability ensemble of all sampled sub-networks in the EMA supernet:</p><formula xml:id="formula_4">T k {?p}; {x p } = 1 |p| |p| p=1 T k (W ? , ?p; x p ).<label>(3)</label></formula><p>In summary, the block-wisely self-supervised training process of the Siamese supernets is formulated as follows:</p><formula xml:id="formula_5">W * k = arg min W k |p| p=1 Ltrain {W k , W ? k }, {?p}; x , where Ltrain {W k , W ? k }, {?p}; x = S k (W k , ?p; xp) ? T k W ? k , {?p}; {x p } 2 2 .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Searching Towards the Population Center</head><p>After the convergence of the Siamese supernets is complete, the architectures can be ranked and searched by the rating determined based on the weight of the supernets, as in Eqn. 1. In this section, we design a fair and effective unsupervised rating metric L val for searching phase.</p><p>To evaluate the performance of a network trained with contrastive self-supervision, previous works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b10">11]</ref> have utilized supervised metrics, such as accuracies of linear evaluation or few-shot classification. To develop an unsupervised NAS method, we aim to avoid schemes that depend on human-annotated labels and instead pursue a completely unsupervised evaluation metric. Previous unsupervised NAS methods <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b86">87]</ref> utilize either the accuracy of pretext tasks or convergence measurement with angle-based metrics to rate candidate architectures. Unfortunately, the losses of self-supervised contrastive learning do not necessarily represent either the architecture performance or the architecture convergence, as the input views and target networks are both randomly sampled. Moreover, the target networks are somewhat biased and cannot serve as ground truth targets.</p><p>To avoid these concerns, we propose a fair and effective unsupervised evaluation metric for architecture search. Without loss of generality, we consider searching with an evolutionary algorithm <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b53">54]</ref>, where architectures are optimized by evolving an architecture population {?p}. Analogous to the optimization of the weights, we propose to use probability ensemble among the population {?p} as the common target to provide a fair rating for each architecture ?p. Additionally, one pair of views {x 1 , x 2 } for each validation samples x are generated and fixed to avoid the bias introduced by variable augmentation. In parallel to Eqn. 3, we have the probability ensemble of the architecture population:</p><formula xml:id="formula_6">S k {?p}; x2 = 1 |p| |p| p=1 S k (?p; x2).<label>(5)</label></formula><p>In practice, by dividing the supernet into medium-sized blocks (e.g. 4 layers of 4 candidates, 4 4 = 256 architectures), traversal evaluation of all the candidate architectures are affordable. In this case, the architecture population {?p} is expanded to the whole block-wise search space A k , and the whole searching process is finished in a single step:</p><formula xml:id="formula_7">? * = arg min ???A |k| k=1 ? k Lval(?; x k ) where Lval(?; x) = S k (?; x1) ? S k (A k ; x2) 2 2 . (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Hybrid CNN-transformer Search Space</head><p>In this section, we present a fabric-like hybrid CNNtransformer search space, named HyTra, with disparate candidate building blocks and flexible down-sampling positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">CNN and Transformer Candidate Blocks</head><p>The first step in designing a hybrid CNN-transformer search space is to include the proper CNN and transformer building blocks. These two types of building blocks should be able to perform well either when simply aggregated in sequence or when combined freely. We choose the classical and robust residual bottleneck (ResConv) in ResNet <ref type="bibr" target="#b24">[25]</ref> as the CNN candidate building block. In parallel, we design a lightweight and robust transformer building block ResAtt based on the pluggable BoTBlock <ref type="bibr" target="#b61">[62]</ref> and NLBlock <ref type="bibr" target="#b67">[68]</ref>. Computation Balancing with Implicit Position Encodings. To facilitate fair and meaningful competition, can-  <ref type="figure">Figure 4</ref>: Transformer blocks in CPVT <ref type="bibr" target="#b13">[14]</ref> and BoT <ref type="bibr" target="#b61">[62]</ref>.</p><p>didate building blocks should have similar computation complexities. The original BoTBlock is slower than ResConv, as its relative position encodings are computed separately through multiplication with the query. Simply removing the content-position branch from BoTBlocks, resembling to NLBlocks, could reduce their compute time to make them comparable to ResConv. However, position encodings are crucial for vision transformers to achieve good performance.</p><p>In CPVT <ref type="bibr" target="#b13">[14]</ref>, the authors uses single convolutions in between transformer encoder blocks as the position encoding generator. Similarly, we replace the relative position encoding branch in BoTBlock with a light depthwise separable convolution as an implicit position encoding module, forming our ResAtt. By this simple modification, we reduce the computation complexity of position encoding module from the original O(CW 3 ) to O(CW 2 ), with C denoting number of channels and W denoting the width or height. In contrast to CPVT and BoT <ref type="figure">(Fig. 4)</ref>, our position encoding modules ( <ref type="figure" target="#fig_2">Fig. 3 right)</ref> are placed between the input projection layer and the self-attention module. In addition, our implicit position encoding modules are also responsible for downsampling. This modification is also applied to ResConv, which enables weight sharing between candidate blocks with different down-sampling rates (i.e. 1 or 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Fabric of Hybrid CNN-transformers</head><p>Beyond the building blocks, CNNs and transformers differ considerably in terms of their macro architectures. Unlike CNNs, which process images in stages with various spatial sizes, transformers typically do not change sequence length (image patches) and retains the same scale at each layer. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref> left, to cover both the CNNs and transformers, our search space is designed with flexible down-sampling positions, forming a fabric <ref type="bibr" target="#b56">[57]</ref> of Hybrid CNN-transformers. At each choice block layer of the fabric, the spatial resolution can either stay unchanged or be reduced to half of its scale, until reaching the smallest scale. This fabric-like search space contains architectures resembling the popular vision transformers <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b13">14]</ref>, CNNs <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31]</ref> and hybrid CNN-transformers <ref type="bibr" target="#b61">[62]</ref> at different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAdds</head><p>Steptime  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Setups. We evaluate our method on three search spaces, including our proposed HyTra search space and other two existing search spaces, i.e. MBConv search space <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">37]</ref> and NATS-Bench size search space S S <ref type="bibr" target="#b16">[17]</ref>. The datasets we use to evaluate and analyze our method are ImageNet <ref type="bibr" target="#b15">[16]</ref>, CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b35">[36]</ref>. We train each block of the supernet for 20 epochs, including one linear warm-up epoch. We randomly sample four paths in each training step. See Appendix A.2 for more implementation details. (b) Architecture of DNA-T. <ref type="figure">Figure 5</ref>: Visualization of architectures searched by Boss-NAS and DNA <ref type="bibr" target="#b36">[37]</ref> in HyTra search space. Blue nodes denotes ResConv and red nodes denotes ResAtt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Searching for Hybrid CNN-transformer</head><p>Firstly, BossNet-T0 outperforms a wide range of state-ofthe-art models. For instance, BossNet-T0 without SE module achieves 80.5% top-1 accuracy, surpassing the humandesigned hybrid CNN-transformer, BoTNet50, by 2.2% while being 1.19? faster in terms of compute time; when equipped with SE and SiLU activation, BossNet-T0 further achieves 80.8% top-1 accuracy, surpassing the NAS searched EfficientNet-B1 by 1.7% while being 1.14? faster.</p><p>Secondly, our searched model demonstrates absolute superiority over manually and randomly selected models from search space HyTra. In particular, BossNet-T0 achieves up to 6.0% improvement over manually selected models, proving the effectiveness of our architecture search.</p><p>Thirdly, BossNet-T0 outperforms other recent NAS methods on search space HyTra. BossNet-T0 achieves 0.5% accuracy gains over DNA-T, which is searched by our supervised NAS counterpart <ref type="bibr" target="#b36">[37]</ref>.</p><p>Finally, when extended to larger model size and input size, the family of BossNet-T models maintain their superiority. By removing the downsampling in the last stage of BossNet-T0 (same scheme as BoTNet-S1 [62]), we have BossNet-T1, which achieves 82.2% accuracy, surpassing EfficientNet-B2 by 2.1%. By directly testing on larger input resolutions without finetuning, BossNet-T0? (on 288?288 input size) achieves 81.6% top-1 accuracy, and outperforms BoTNet50 + SE by 2.0% with similar runtime; BossNet-T1? (on 256?256 input size) achieves 82.5% top-1 accuracy, surpassing T2T-ViT-19 and EfficientNet-B2 by 0.6% and 2.4% with comparable steptime, respectively. Architecture visualization and analysis. We visualize the architecture of DNA-T and BossNet-T0 in <ref type="figure">Fig. 5</ref>. DNA-T clearly prefers convolutions, as it contains 13 ResConv blocks and only three ResAtt blocks. By contrast, BossNet-T0 has similar numbers of convolutions and attentions and eventually achieves a higher accuracy. We refer this to Phenomenon I: candidate preference, and attribute it to architectural bias from the teacher supervision. Without using the teacher model, our method successfully avoids this bias. BossNAS, = 0.65 <ref type="bibr" target="#b73">74</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results on MBConv Search Space</head><p>To further prove the effectiveness and generalization ability of BossNAS, we compare it with a wide range of NAS methods on MBConv search space. Performance of searched models. As shown in Tab. 2, our searched models, BossNet-M, achieve competitive results in search spaces with and without SE module. In the search space without SE, BossNet-M1, searched under constraint of 475M MAdds, outperforms SPOS <ref type="bibr" target="#b21">[22]</ref> and another recent unsupervised NAS method, RLNAS [87] by 1.4% and 0.6%, respectively. In the search space with SE, BossNet-M2, under constraint of 405M MAdds, outperforms the popular EfficientNet [64] by 1.1%, and is also competitive with our supervised counterpart, DNA <ref type="bibr" target="#b36">[37]</ref>. Note that candidate building blocks in MBConv search space are quite similar, concealing the candidate preference phenomenon in <ref type="bibr" target="#b36">[37]</ref>. Architecture rating accuracy. As BossNAS performs traversal search (i.e. accuracy of searching phase is 100%), the architecture rating accuracy directly represents its effectiveness. We use the 23 open-sourced architectures in MBConv search space and their corresponding ground truth accuracies provided by <ref type="bibr" target="#b36">[37]</ref> to calculate the architecture rating accuracy, i.e. the ranking correlation between the  <ref type="table">Table 4</ref>: Comparison of searched model accuracy and architecture rating accuracy of different NAS methods on NATS-Bench S S (C-10: CIFAR-10, C-100: CIFAR-100).</p><p>predicted architecture ranking and the ground truth model ranking. We use three different ranking correlation metrics: Kendall Tau (? ) <ref type="bibr" target="#b33">[34]</ref>, Spearman Rho (?) and Pearson R (R). All three metrics range from -1 to 1, with "-1" representing a completely reversed ranking, "1" meaning an entirely correct ranking, and "0" representing no correlation between rankings. As shown in Tab. 3 and <ref type="figure">Fig. 6</ref> left, our BossNAS obtains the highest rating accuracies with 0.65 ? among sota NAS methods, while addressing two problems. First, classic weight sharing methods, SPOS <ref type="bibr" target="#b21">[22]</ref> and DARTS <ref type="bibr" target="#b42">[43]</ref>, fails to achieve reasonable ranking correlation despite their lower search costs, while the multi-trial method, MnasNet <ref type="bibr" target="#b62">[63]</ref>, achieves high rating accuracies with massive search cost. BossNAS successfully addressed such dilemma of NAS by achieving even higher rating accuracies than MnasNet (e.g. 0.07 R) with 28.8? acceleration.</p><p>Second, supervised block-wise NAS method, DNA <ref type="bibr" target="#b36">[37]</ref>, fails to achieve high rating accuracies when using a teacher largely different from the candidates (MobileNetV1 <ref type="bibr" target="#b29">[30]</ref> vs. EfficientNet-based candidates <ref type="bibr" target="#b63">[64]</ref>), which we refer to as Phenomenon II: teacher preference. Our unsupervised BossNAS achieves higher rating accuracies than DNA (0.03 ? ), successfully casting off the yoke of the teacher network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results on NATS-Bench S S</head><p>For NATS-Bench size search space S S , experiments are conducted on two datasets: CIFAR-10 and CIFAR-100. Candidates of different channel numbers in our supernet share the weights in a slimmable manner <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b8">9]</ref>. Performance of searched models. After searching on our supernet, we look up the performance of searched models in NATS-Bench S S for fair comparision. The results are shown in Tab. 4. Our BossNAS outperforms recent NAS methods <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b4">5]</ref> designed particularly for network size search spaces, proving the generalization ability of our method on specified search spaces and relatively small datasets. Architecture rating accuracy. We rate all the 32768 architectures in the search space to compare with their ground  truth accuracies in the benchmark on CIFAR-10 dataset. As shown in <ref type="figure">Fig. 6</ref> right, all the architectures in the search space forms a dense, spindle-shaped pattern, proving the effectiveness of our BossNAS. In addition, the architecture rating accuracies on CIFAR-100 dataset are shown in Tab. 4. Our method, without access to the ground truth architecture accuracies and even without access to any human-annotated labels, outperforms a predictor-based NAS method <ref type="bibr" target="#b26">[27]</ref>, which is trained with ground truth architecture accuracies, by a large gap (i.e. 0.16 ? and 0.19 R). More analysis on NATS-Bench S S could be found in Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Study</head><p>In this section, we perform extensive ablation studies on MBConv search space and ImageNet to analyze our proposed training and evaluation methods separately. training methods. We compared several training methods for the block-wise supernet: (1) Supervised distillation method (Supv. distill.), using a pre-trained teacher model to provide block-wise supervision, i.e. the training scheme used in DNA <ref type="bibr" target="#b36">[37]</ref> (2) Supervised classification (Supv. class.), using real labels directly as the block-wise supervision. (3) Unsupervised bootstrapping (Unsupv. bootstrap.), where the Siamese supernets are optimized by bootstrapping the corresponding paths in the two networks. (4) Our unsupervised ensemble bootstrapping method (Unsupv. EB), where each sampled paths are optimized by learning to predict the probability ensemble of sampled paths from the mean teacher. As shown in Tab. 5, our training method surpasses all others, achieving the best results in architecture rating accuracy. In particular, by comparing the 3-rd and 5-th line, we can see that replacing our proposed Unsupv. EB with the naive Unsupv. bootstrap. scheme, the architecture rating accuracy drops sharply by 0.53 ? . Without the probability ensemble, bootstrapping fails to reach a reasonable rating accuracy, proving that the proposed ensemble bootstrapping is indispensable for our BossNAS. Evaluation methods. Slimilar to the ablation analysis of training methods, we also compare our evaluation methods with (1) Supervised distillation method (Supv. distill.) and (2) Supervised classification (Supv. class.). Additionally, to perform ablation analysis of evaluation without changing the training method, we also compare with weight sharing linear classifier to evaluate each architecture. (4) Our unsupervised evaluation metric (Unsupv. eval) rate architectures by its distance to the ensemble probability center of the whole search space. From the last two rows of Tab. 5, we suprisingly found that our Unsupv. eval outperforms supervised linear evaluation scheme in architecture rating by a remarkable gap (0.1 ? ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Convergence Behavior</head><p>To further demonstrate the effectiveness of BossNAS, we investigate the architecture rating accuracy during the supernet training process on MBConv search space with ImageNet. The three ranking correlation metrics of our BossNAS during its 20 training epochs are shown in <ref type="figure" target="#fig_5">Fig. 7</ref>. The architecture rating accuracy increases rapidly in the early stage and continues to grow with minor fluctuation. The rating accuracy converges at the 12-th epoch and continues to be stable till the end of the training phase. The stably increasing architecture rating ability proves the stability of our BossNAS. In addition, the fast converging ranking correlation demonstrates that our method is easy to optimize and do not require longer training. Please refer to Appendix A.3 for analysis of convergence behavior on NATS-Bench S S .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we present BossNAS, a general, unsupervised NAS method with the ensemble bootstrapping training technique and an unsupervised evaluation metric. Experiments on three search spaces prove that our method successfully addressed the problem of inaccurate architecture rating caused by large weight-sharing space while avoiding the architectural bias brought by supervised distillation. Ablation analysis proved that the two components, ensemble bootstrapping scheme and unsupervised evaluation metric, are both crucial for our method. Additionally, we present a fabric-like search space named HyTra. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. A brief review of NAS</head><p>NAS methods aim to automatically optimize neural network architectures by exploring search spaces with search algorithms and evaluating architectures by means of rating schemes. NAS methods can be divided into two categories depending on the rating scheme utilized, i.e. multi-trial NAS and weight-sharing NAS. Multi-trial NAS methods <ref type="bibr" target="#b91">[92,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b82">83]</ref> rate all sampled architectures by training them from scratch, making this process computationally prohibitive and difficult to deploy on large datasets. They either perform architecture rating by training on relatively small datasets (e.g. CIFAR-10) <ref type="bibr" target="#b91">[92,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b53">54]</ref> or by training for the first few epochs (e.g. 5 epochs) <ref type="bibr" target="#b62">[63]</ref> on ImageNet. To avoid repeated training of candidate networks, weightsharing NAS methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b81">82</ref>] optimize a supernet that encodes the whole search space, then rate each candidate architecture according to its weights inherited from the supernet. Among them, gradient-based approaches <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b69">70]</ref> and sampler-based approaches <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b60">61]</ref> jointly optimize the weight of the supernet and the factors (or agent) used to choose the architecture; for their part, one-shot approaches 1 <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b50">51]</ref> optimize the supernet before performing a search with the frozen supernet weights. We refer to <ref type="bibr" target="#b54">[55]</ref> for a more comprehensive NAS review. * Corresponding Author. <ref type="bibr" target="#b0">1</ref> In this paper, following the pioneering works SMASH <ref type="bibr" target="#b5">[6]</ref> and Oneshot <ref type="bibr" target="#b3">[4]</ref>, when we refer to one-shot NAS methods, we are discussing those incorporating two-stage (i.e., a supernet training stage and a searching stage) weight-sharing methods rather than the general weight-sharing NAS discussed in <ref type="bibr" target="#b79">[80]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Implementation Details</head><p>Search spaces. We evaluate our method on three search spaces:</p><p>? HyTra search space. The beginning of the networks in this search space is the classic ResNet stem that reduces the spatial resolution by a factor of 4 with a strided 7?7 convolution layer and a max-pooling layer. It contains L = 16 choice block layers in total, as the same to ResNet50. Before the first choice block layer, the input can be further down-sampled to different scales.  <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b8">9]</ref>. We divide the supernet into 3 blocks, according to spatial size.</p><p>Datasets. The datasets we use to evaluate and analyze our method include ImageNet <ref type="bibr" target="#b15">[16]</ref>, CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b35">[36]</ref>. ImageNet is a large-scale dataset containing 1.  <ref type="bibr" target="#b20">[21]</ref>, we use the LARS optimizer <ref type="bibr" target="#b74">[75]</ref> with a cosine decay learning rate schedule <ref type="bibr" target="#b43">[44]</ref>. The base learning rate is set to 4.8 for a total batchsize of 4096.</p><p>For ImageNet retraining of BossNet-T models, we follow similar with DeiT [66], as we found it robust for both CNNs and transformers. More specifically, we use AdamW optimizer with 1e-3 initial learning rate and cosine learning rate scheduler, for a total batch size of 1024. Weight decay is set to 0.05. We use model EMA with decay rate 0.99996 following <ref type="bibr" target="#b78">[79]</ref>. Please refer to DeiT <ref type="bibr" target="#b65">[66]</ref> for more details on data-augmentation and regularization.</p><p>For ImageNet retraining of BossNet-M models, we follow closely to EfficientNet <ref type="bibr" target="#b63">[64]</ref>. We use batchsize 4096, RMSprop optimizer with momentum 0.9 and initial learning rate of 0.256 which decays by 0.97 every 2.4 epochs. Please refer to EfficientNet <ref type="bibr" target="#b63">[64]</ref> for more details of other settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Re-implementation of other NAS methods on HyTra.</head><p>For DNA <ref type="bibr" target="#b36">[37]</ref>, we use ResNet-50 <ref type="bibr" target="#b24">[25]</ref> as the teacher model. We divide the supernet into four blocks, with four layers in each block, and train each block for 20 epochs. The intermediate features of every block of the student supernet <ref type="bibr" target="#b81">82</ref>     <ref type="table">Table 6</ref>: Architecture rating accuracy on NATS-Bench S S with CIFAR datasets. and the teacher are all downsampled with global pooling and projected with one fully-connected layer before calculating distillation loss, as the scale of different candidate block is not the same in HyTra search space. Other settings follow closely to DNA <ref type="bibr" target="#b36">[37]</ref>.</p><p>For UnNAS <ref type="bibr" target="#b40">[41]</ref>, we adopt rotation prediction <ref type="bibr" target="#b34">[35]</ref> (Rot) pretext task, for its simplicity. Following <ref type="bibr" target="#b40">[41]</ref>, we use three extra stride-2 convolution layers at the beginning of the supernet to reduce spatial resolution. The supernet is trained for 2 epochs as in <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Additional Analysis on NATS-Bench S S</head><p>Architecture rating comparison. We compare with the predictor-based NAS method CE <ref type="bibr" target="#b26">[27]</ref> by architecture rating accuracy on CIFAR-10 and CIFAR-100. As shown in <ref type="figure" target="#fig_7">Fig. 8</ref>, we compare the two NAS methods by plotting the correlation of the architecture rating and the true accuracy of 3000 randomly sampled architectures from NATS-Bench size search space S S <ref type="bibr" target="#b16">[17]</ref>. Architectures with BossNAS form denser and more spindly scatter pattern than CE on both of the two datasets. Moreover, as measured quantitatively in Tab. 6, BossNAS outperforms CE by a large margin (0.11 and 0.16 ? ) in both datasets.     Convergence Behavior. We illustrate the architecture rating accuracy of BossNAS during its 30 epoch supernet training phase on CIFAR datasets in <ref type="figure" target="#fig_9">Fig. 9</ref>. The architecture rating accuracy increases quickly and steadily with minor fluctuations, in a similar manner with that on MBConv search space <ref type="figure" target="#fig_5">(Fig. 7)</ref>. In particular, architecture rating accuracy of our BossNAS converges to a satisfactory result, 0.76 ?, smoothly and quickly within only 20 epochs on CIFAR-100, and continues to be stable for the subsequent 10 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Visualization of Human-designed Architectures in HyTra</head><p>The architectures of ResNet50-T, ViT-T/16 and BoTNet50-T from our HyTra search space are illustrated in <ref type="figure" target="#fig_0">Fig. 10</ref>. Their architectures follow as closely as possible to the architectures of their prototypes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparision of three NAS schemes. Red arrows represent the supervision during training and searching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the Siamese supernets training with ensemble bootstrapping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the fabric-like Hybrid CNN-transformer Search Space with flexible down-sampling positions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>(3) supervised linear evaluation (Supv. linear eval), where architectures are rated by fixing the weights of the supernet and finetuning a Ranking correlations during supernet training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>CE [27] on CIFAR-100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Comparison of architecture rating and its true accuracy of our BossNAS and CE<ref type="bibr" target="#b26">[27]</ref> on NATS-Bench S S with CIFAR datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Ranking correlations during supernet training on CIFAR-10. Ranking correlations during supernet training with CIFAR-100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Convergence behavior of BossNAS on NATS-Bench S S and CIFAR datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Architecture of ViT-T/16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Visualization of Human-designed Architectures in HyTra. Blue nodes denotes ResConv and red nodes denotes ResAtt.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>are also computationally prohibitive. Weightsharing rating scheme in one-shot NAS methods has brought about a tremendous reduction of search cost by encoding the entire search space A into a weight-sharing supernet, with the weights W shared by all the candidate architectures and optimized concurrently as: W * = arg min</figDesc><table><row><cell>ratings, they W</cell><cell>Ltrain(W, A; x, y).</cell></row><row><cell>3.1. Dilemma of NAS and the Block-wise Solutions</cell><cell></cell></row><row><cell>Dilemma of NAS: efficiency or accuracy. While classical</cell><cell></cell></row><row><cell>sample-based NAS methods produce accurate architecture</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>ImageNet results of state-of-the-art models and our searched hybrid CNN-transformers. Compute steptime is measured on a single GeForce RTX 3090 GPU with batch size 32. Purple is used to denote manually selected architec- tures from search space HyTra.?: Directly tested on larger input size without finetuning (i.e. 288 for BossNet-T0? and 256 for BossNet-T1?).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Left: Ranking correlations of 6 different NAS methods on MBConv Search Space. Right: Architecture ranking of BossNAS on NATS-Bench S S . In all the diagrams, x-axis denotes ground truth accuracy; y-axis denotes evaluation metrics.</figDesc><table><row><cell>70.6 70.7 70.8 70.9 71.0</cell><cell>SPOS, = 0.18</cell><cell>2 4 6 8</cell><cell cols="2">DARTS, = 0.08</cell><cell>43.5 44.0 44.5 45.0 45.5</cell><cell cols="2">MnasNet, = 0.61</cell><cell>0.5 0.4 0.3</cell><cell cols="2">DNA (EffNet), = 0.62</cell><cell>0.400 0.375 0.350 0.325</cell><cell>DNA (MBNet), = 0.23</cell><cell>1.12 1.11 1.10 1.09 1.08 1.07 1.06 1.05</cell></row><row><cell></cell><cell>75</cell><cell></cell><cell>74</cell><cell>75</cell><cell></cell><cell>74</cell><cell>75</cell><cell></cell><cell>74</cell><cell>75</cell><cell>74</cell><cell>75</cell><cell>1.13</cell><cell>80 82 84 86 88 90 92 94</cell></row><row><cell>Figure 6: Method</cell><cell>MAdds (M)</cell><cell cols="2">Top-1 (%)</cell><cell cols="2">Top-5 (%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FairNAS-A [15]</cell><cell>388M</cell><cell cols="2">75.3</cell><cell>92.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ProxylessNAS [7]</cell><cell>465M</cell><cell cols="2">75.1</cell><cell>92.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FBNet-C [70]</cell><cell>375M</cell><cell cols="2">74.9</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SPOS [22]</cell><cell>472M</cell><cell cols="2">74.8</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RLNAS [87]</cell><cell>473M</cell><cell cols="2">75.6</cell><cell>92.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BossNet-M1 w/o SE</cell><cell>475M</cell><cell cols="2">76.2</cell><cell>93.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MobileNetV3 [29]</cell><cell>219M</cell><cell cols="2">75.2</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MnasNet-A3 [63]</cell><cell>403M</cell><cell cols="2">76.7</cell><cell>93.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EfficientNet-B0 [64]</cell><cell>399M</cell><cell cols="2">76.3</cell><cell>93.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DNA-b [37]</cell><cell>406M</cell><cell cols="2">77.5</cell><cell>93.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BossNet-M2</cell><cell>403M</cell><cell cols="2">77.4</cell><cell>93.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>ImageNet results of state-of-the-art NAS models on MBConv search space.</figDesc><table><row><cell>Method</cell><cell>Search Cost</cell><cell>?</cell><cell>?</cell><cell>R</cell></row><row><cell>SPOS [22]</cell><cell>8.5 Gds</cell><cell>-0.18</cell><cell>-0.27</cell><cell>-0.29</cell></row><row><cell>DARTS [43]</cell><cell>50 Gds</cell><cell>0.08</cell><cell>0.14</cell><cell>0.06</cell></row><row><cell>MnasNet [63]</cell><cell>288 Tds</cell><cell>0.61</cell><cell>0.77</cell><cell>0.78</cell></row><row><cell>DNA [37] (EffNetB0)</cell><cell>8.5 Gds</cell><cell>0.62</cell><cell>0.77</cell><cell>0.83</cell></row><row><cell>DNA [37] (MBNetV1)</cell><cell>8.5 Gds</cell><cell>0.23</cell><cell>0.27</cell><cell>0.37</cell></row><row><cell>BossNAS</cell><cell>10 Gds</cell><cell>0.65</cell><cell>0.78</cell><cell>0.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Comparison of the effectiveness and efficiency of different NAS methods on MBConv search space and ImageNet dataset. (Gds: GPU days; Tds: TPU days)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Ablation analysis of training methods and evaluation methods on MBConv Search Space.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>On this challenging search space, our searched hybrid CNN-transformer model, achieves 82.5% accuracy on ImageNet, surpassing Efficient-Net by 2.4% with comparable compute time. BossNAS: Exploring Hybrid CNN-transformers with Block-wisely Self-supervised Neural Architecture Search Supplementary Material Changlin Li 1 , Tao Tang 2 , Guangrun Wang 3,4 , Jiefeng Peng 3 , Bing Wang 5 , Xiaodan Liang 2 * , Xiaojun Chang 6 1 GORSE Lab, Dept. of DSAI, Monash University 2 Sun Yat-sen University 3 DarkMatter AI Research 4 University of Oxford 5 Alibaba Group 6 RMIT University changlin.li@monash.edu, {trent.tangtao,wanggrun,jiefengpeng,xdliang328}@gmail.com, fengquan.wb@alibaba-inc.com, xiaojun.chang@rmit.edu.au</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>The downsampling module consists of multiple 3?3 convolutions with stride of 2. At each choice block layer, the spatial resolution can either stay unchanged or be reduced to half of its scale, unless reaching the smallest scale 1/32. As introduced in Sec. 4, this search space contains two disparate candidate choices: {ResConv, ResAtt}. As transformer blocks are expensive in the first scales, we only enable the choice of ResAtt in the last two scales (i.e. 1/16 and 1/32). The total size of this challenging hybrid search space is roughly 2.8?10 6 .?MBConv search space. MobileNet-like search space and its variations are generally used as benchmarks for recent NAS methods [63, 29, 64, 7, 70, 15, 37, 46, 87]. Following Li et. al. [37], we use a search space with 18 layers and each layer contains 4 candidate MobileNet blocks (combination of kernel size {3, 5} and reduction rate {3, 6}). This results in a large search space containing about 4 18 ? 6.9?10 10 architectures. [17] is a channel configuration search space built upon a fixed cell-based architecture with 5 layers, where the 2-nd and 4-th layers have a down-sample rate of 2. Number of channels in each layer is chosen from {8, 16, 24, 32, 40, 48, 56, 64}. S S has 8 5 = 32768 architecture candidates in total. Candidates of different channel numbers in our supernet share the weights in a slimmable manner</figDesc><table /><note>? NATS-Bench S S . The NATS-Bench size search space SS</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>2 M train set images and 50 K val set images in 1000 classes. We randomly samples 50 K images from the original train set to form a NAS-val set for architecture rating and use the remainder as the NAS-train set for supernet training. No labels are used during training and searching of our NAS method. Finally, our searched architectures are retrained from scratch on train set and evaluated on val set. For CIFAR-10 and CIFAR-100<ref type="bibr" target="#b35">[36]</ref>, we use the splits proposed in NATS-Bench<ref type="bibr" target="#b16">[17]</ref>. CIFAR-10 is divided into 25 K train set, 25 K val set, and 10 K test set. CIFAR-100 is devided into 50 K train set, 5 K val set, and 5 K test set. The final accuracies of searched architectures are queried from NATS-Bench S S<ref type="bibr" target="#b16">[17]</ref>. We train each block of the BossNAS supernet for 20 epochs including 1 linear warm-up epoch on ImageNet. For the relatively smaller CIFAR datasets, we extend it to 30 epochs. In each training step, we randomly sample 4 paths for the ensemble bootstrapping. Other hyperparameters for self-supervised training of the supernet follow closely to BYOL</figDesc><table><row><cell>Training details.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>(b) CE<ref type="bibr" target="#b26">[27]</ref> on CIFAR-10.</figDesc><table><row><cell>2.84</cell><cell></cell><cell></cell><cell>88.4</cell></row><row><cell>2.86</cell><cell></cell><cell></cell><cell>88.0 88.2</cell></row><row><cell>2.90 2.88</cell><cell></cell><cell></cell><cell>87.4 87.6 87.8</cell></row><row><cell>2.92</cell><cell></cell><cell></cell><cell>87.0 87.2</cell></row><row><cell cols="4">84 86 88 90 92 94</cell><cell>80 82 84 86 88 90</cell></row><row><cell cols="4">(a) BossNAS on CIFAR-10.</cell></row><row><cell>2.875</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2.900</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2.925</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2.950</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2.975</cell><cell></cell><cell></cell><cell></cell></row><row><cell>3.000</cell><cell></cell><cell></cell><cell></cell></row><row><cell>3.025</cell><cell></cell><cell></cell><cell></cell></row><row><cell>3.050</cell><cell></cell><cell></cell><cell></cell></row><row><cell>40</cell><cell>50</cell><cell>60</cell><cell>70</cell></row><row><cell cols="4">(c) BossNAS on CIFAR-100.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Following<ref type="bibr" target="#b61">[62]</ref>, compute time refers to the time spent for forward and backward passes.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adaptive stochastic natural gradient method for one-shot neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youhei</forename><surname>Akimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinichi</forename><surname>Shirakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nozomu</forename><surname>Yoshinari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kento</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shota</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kouhei</forename><surname>Nishida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Designing neural network architectures using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lambdanetworks: Modeling long-range interactions without attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Can weight sharing outperform random architecture search? an investigation with tunas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SMASH: one-shot model architecture search through hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">AutoFormer: Searching transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021. 1</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">RENAS: reinforced evolutionary neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiming</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisen</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Hierarchical neural architecture search for deep stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10882</idno>
		<title level="m">Conditional positional encodings for vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">FairNAS: Rethinking evaluation fairness of weight sharing neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021. 3, 4</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">NATS-Bench: Benchmarking nas algorithms for architecture topology and size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katarzyna</forename><surname>Musial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Gabrys</surname></persName>
		</author>
		<idno type="DOI">file:/localhost/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/zero-shot/arxiv_pdf/10.1109/TPAMI.2021.3054824</idno>
		<idno>doi:10.1109/ TPAMI.2021.3054824. 2</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four GPU hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">NAS-Bench-201: Extending the scope of reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno>ICLR, 2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021. 1</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>In NeurIPS, 2020. 3, 4</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Single path oneshot neural architecture search with uniform sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020. 4</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Transformer in transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00112</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">TransReID: Transformer-based object reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuting</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Poli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04208</idno>
		<title level="m">Contrastive embeddings for neural architectures</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Searching for MobileNetV3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Angle-based search space shrinking for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruosi</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Transgan: Two transformers can make one strong gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07074</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A new measure of rank correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Maurice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="1938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Blockwisely supervised neural architecture search with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuchun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dynamic Slimmable Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improving one-shot nas by suppressing the posterior fading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Are labels necessary for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">DARTS: differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">What do neural networks learn when trained with random labels? In NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartmut</forename><surname>Maennel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ibrahim M Alabdulmohsin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Baldock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keysers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Distilling optimal neural networks: Rapid search in diverse spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>Moons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parham</forename><surname>Noorzad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrii</forename><surname>Skliar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Mariani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Lott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Blankevoort</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.08859</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><surname>Negrinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08792</idno>
		<title level="m">Deeparchitect: Automatically designing and training deep architectures</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pi-NAS: Improving neural architecture search by reducing supernet training consistency shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameters sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melody</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melody</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">A comprehensive survey of neural architecture search: Challenges and solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengzhen</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ACM Computing Surveys</publisher>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreyas</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Convolutional neural fabrics. In NeurIPS</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Evaluating the search phase of neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Sciuto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaicheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudiu</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<idno>ICLR, 2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Meal: Multi-model ensemble via adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhankui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.08453,2020.4</idno>
		<title level="m">Meal v2: Boosting vanilla resnet-50 to 80%+ top-1 accuracy on imagenet without tricks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Bridging the gap between sample-based and one-shot neural architecture search with bonas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Fbnetv2: Differentiable neural architecture search for spatial and channel dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Self-supervised representation learning for evolutionary neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haihong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.00186,2020.3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Does unsupervised architecture representation learning help neural architecture search? In NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M</forename><surname>Esperan?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Maria</forename><surname>Carlucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NAS evaluation is frustratingly hard. In ICLR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Nas-bench-101: Towards reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Scaling sgd batch size to 32k for imagenet training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Autoslim: Towards one-shot architecture search for channel numbers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11728</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Universally slimmable networks and improved training techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Slimmable neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Tokensto-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021. 1</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Nas-bench-1shot1: Benchmarking and dissecting one-shot neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arber</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Siems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Differentiable neural architecture search in equivalent space with exploration enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">W</forename><surname>Su</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Overcoming multi-model forgetting in one-shot NAS with diversity maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">W</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Stage-wise channel pruning for model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Ou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.04908</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Semi-supervised blockwisely architecture search for efficient lightweight generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizheng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Neural architecture search with random labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Practical block-wise neural network architecture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Deformable DETR: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Lin</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
