<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Convolutional Tensor-Train LSTM for Spatio-Temporal Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Su</surname></persName>
							<email>jiahaosu@umd.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonmin</forename><surname>Byeon</surname></persName>
							<email>wbyeon@nvidia.com</email>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA Research</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Kossaifi</surname></persName>
							<email>jkossaifi@nvidia.com</email>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA Research</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furong</forename><surname>Huang</surname></persName>
							<email>furongh@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
							<email>jkautz@nvidia.com</email>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA Research</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
							<email>aanandkumar@nvidia.com</email>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA Research</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Convolutional Tensor-Train LSTM for Spatio-Temporal Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning from spatio-temporal data has numerous applications such as humanbehavior analysis, object tracking, video compression, and physics simulation. However, existing methods still perform poorly on challenging video tasks such as long-term forecasting. This is because these kinds of challenging tasks require learning long-term spatio-temporal correlations in the video sequence. In this paper, we propose a higher-order convolutional LSTM model that can efficiently learn these correlations, along with a succinct representations of the history. This is accomplished through a novel tensor-train module that performs prediction by combining convolutional features across time. To make this feasible in terms of computation and memory requirements, we propose a novel convolutional tensortrain decomposition of the higher-order model. This decomposition reduces the model complexity by jointly approximating a sequence of convolutional kernels as a low-rank tensor-train factorization. As a result, our model outperforms existing approaches, but uses only a fraction of parameters, including the baseline models. Our results achieve state-of-the-art performance in a wide range of applications and datasets, including the multi-steps video prediction on the Moving-MNIST-2 and KTH action datasets as well as early activity recognition on the Something-Something V2 dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>models in nature, meaning that the hidden states are updated using information from the previous time step only, resulting in an intrinsic difficulty in capturing long-range temporal correlations.</p><p>Incorporating higher-order correlations. For one-dimensional sequence modeling, higher-order generalizations of RNNs have previously been proposed for long-term forecasting problems <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Higher-order RNNs explicitly incorporate a longer history of previous states in each update. This requires higher-order tensors to characterize the transition function (instead of a transition matrix as in first-order RNNs). However, this typically leads to an exponential blow-up in the complexity of the transition function. This problem is further compounded when trying to generalize ConvLSTM to higher-orders and these generalizations have not been explored.</p><p>Scaling up with tensor methods. To avoid the exponential blow-up in the complexity of transition function, tensor decompositions <ref type="bibr" target="#b9">[10]</ref> have been investigated within higher-order RNNs <ref type="bibr" target="#b8">[9]</ref>. Tensor decomposition not only avoids the exponential growth of model complexity, but also introduces an information bottleneck that facilitates effective representation learning. This restricts how much information can be passed on from one sub-system to another in a learning system <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. Previously, low-rank tensor factorization has been used to improve a variety of deep network architectures <ref type="bibr">[13? , 14, 16]</ref>. However, it has not been analyzed in the context of spatio-temporal LSTMs. The only approach that leveraged tensor factorization for compact higher-order LSTMs <ref type="bibr" target="#b8">[9]</ref> considers exclusively sequence forecasting and cannot be directly extended to general spatio-temporal data.</p><p>Generalizing ConvLSTM to higher-orders. When extending to higher-orders, we aim to design a transition function that is able to leverage all previous hidden states and satisfies three properties: (i) the spatial structure in the hidden states is preserved; (ii) the receptive field increases with time. In other words, the longer the temporal correlation captured, the larger the spatial context should be. (iii) Finally, space and time complexities grow at most linearly with the number of times steps. Because previous transition functions in higher-order RNNs were designed specifically for one-dimensional sequence, when directly extended to spatio-temporal data they do not satisfy all three properties. A direct extension fails to preserves the spatial stricture or increases the complexity exponentially.</p><p>Contributions. In this paper, we propose a higher-order Convolutional LSTM model for complex spatio-temporal data satisfying all three properties. Our model incorporates a long history of states in each update, while preserving their spatial structure using convolutional operations. Directly constructing such a model leads to an exponential growth of parameters in both spatial and temporal dimensions. Instead, our model is made computationally tractable via a novel convolutional tensortrain decomposition, which recursively performs a convolutional factorization of the kernels across time. In addition to the parameter reduction, this low-rank factorization introduces an information bottleneck that helps to learn better representations. As a result, it achieves better results than previous works with only a fraction of parameters.</p><p>We empirically demonstrate the performance of our model on several challenging tasks, including early activity recognition and video prediction. We report an absolute increase of 8% in accuracy over the state-of-the-art <ref type="bibr" target="#b6">[7]</ref> for early activity recognition on the Something-Something v2 dataset. Our model outperforms both 3D-CNN and ConvLSTM by a large margin. We also report a new state-of-the-art for multi-step video prediction on both Moving-MNIST-2 and KTH datasets.</p><p>Finally, we propose a principled procedure to train higher-order models: we design a preprocessing module to incorporate longer temporal context and highlight the importance of appropriate gradient clipping and learning scheduling to improve training of higher-order models. We train all models with these strategies and report consistent improvements in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: Convolutional LSTM and Higher-order LSTM</head><p>In this section, we briefly review Long Short-Term Memory (LSTM), and its generalizations Convolutional LSTM for sptio-temporal modeling, and higher-order LSTM for learning long-term dynamics.</p><p>Long Short-Term Memory (LSTM) <ref type="bibr" target="#b16">[17]</ref> is a first-order Markovian model widely used in 1D sequence learning. At each time step, an LSTM cell updates its states {h(t), c(t)} using the immediate previous states {h(t ? 1), c(t ? 1)} and the current input x(t) as</p><formula xml:id="formula_0">[i(t); f (t);c(t); o(t)] = ?(W x(t) + Kh(t ? 1)); (1a) c(t) = c(t ? 1) ? f (t) +c(t) ? i(t); h(t) = o(t) ? ?(c(t)),<label>(1b)</label></formula><p>where ?(?) denotes a sigmoid(?) applied to the input gate i(t), forget gate f (t) and output gate o(t), and a tanh(?) applied to the memory cellc(t) and cell state c(t). ? denotes element-wise product. LSTMs have two major restrictions: (a) only 1D-sequences can be modeled, not spatio-temporal data such as videos; (b) they are difficult to capture long-term dynamics as first-order models.</p><p>Convolutional LSTM (ConvLSTM) <ref type="bibr" target="#b3">[4]</ref> addresses the limitation (a) by extending LSTM to model spatio-temporal structures within each cell, i.e. the states, cell memory, gates and parameters are all encoded as high-dimensional tensors. Furthermore, Eq. (1a) is replaced by</p><formula xml:id="formula_1">[I(t); F(t);C(t); O(t)] = ?(W * X (t) + K * H(t ? 1)),<label>(2)</label></formula><p>where * defines convolution between states and parameters as in convolutional neural networks.</p><p>Higher-order LSTM (HO-LSTM) is a higher-order Markovian generalization of the basic LSTM, which partially addresses the limitation (b) in modeling long-term dynamics. Specifically, HO-LSTM explicitly incorporates more previous states in each update, replacing the first step in LSTM by</p><formula xml:id="formula_2">[i(t); f (t);c(t); o(t)] = ? (W x(t) + ? (h(t ? 1), ? ? ? , h(t ? N ))) ,<label>(3)</label></formula><p>where ? combines N previous states {h(t ? 1), ? ? ? , h(t ? N )} and N is the order of the HO-LSTM. Two realizations of ? have been proposed: a linear function <ref type="bibr" target="#b7">[8]</ref> and a polynomial one <ref type="bibr" target="#b8">[9]</ref>:</p><formula xml:id="formula_3">Linear: ? (h(t ? 1), ? ? ? , h(t ? M ); T (1), ? ? ? , T (N )) = N i=1 T (i)h(t ? i).<label>(4)</label></formula><formula xml:id="formula_4">Polynomial: ? (h(t ? 1), ? ? ? , h(t ? N ); T ) = T , h(t ? 1) ? ? ? ? ? h(t ? N ) .<label>(5)</label></formula><p>While a linear function requires the numbers of parameters and operations growing linearly in N , a polynomial function has space/computational complexity exponential in N if implemented naively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology: Convolutional Tensor-Train LSTM</head><p>Here, we detail the challenges and requirements for designing a higher-order ConvLSTM. We then introduce our model, and motivate the design of each module by these requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Extending ConvLSTM to Higher-orders</head><p>We can express a general higher-order ConvLSTM by combining several previous states when computing the gates for each step:</p><formula xml:id="formula_5">I(t); F(t);C(t); O(t) = ? (W * X (t) + ? (H(t ? 1), ? ? ? , H(t ? N ))) .<label>(6)</label></formula><p>The choice of a suitable function ? for a spatio-temporal learning problem, however, is difficult, as it should satisfy the following properties:</p><p>(1) The spatial structure in the hidden states H(t)'s is preserved by the operations in ?.</p><p>(2) The size of the receptive field for H(t ? i) increases with i, the time gap from the current step (i = 1, 2, ? ? ? , N ). In other words, the longer temporal correlation captured, the larger the considered spatial context should be. Limitations of previous approaches. While it is possible to construct a function ? by extending the linear function in Eq.(4) or the polynomial function in Eq.(5) to the tensor case, none of these extensions satisfy the all three properties. While the polynomial function with tensor-train decomposition <ref type="bibr" target="#b8">[9]</ref> meets requirement (3), the operations do not preserve the spatial structures in the hidden states. On the other hand, augmenting the linear function with convolutions leads to a function:</p><formula xml:id="formula_6">? (H(t ? 1), ? ? ? , H(t ? N ); K(1), ? ? ? , K(N )) = N i=1 K(i) * H(t ? i)<label>(7)</label></formula><p>which does not satisfy requirement (2) if all K(i) contain filters of the same size K. An immediate remedy is to expand K(i) such that its filter size K(i) grows linearly in i. However, the resulting function requires O(N 3 ) space/computational complexity, violating the requirement (3).  <ref type="figure" target="#fig_3">Figure 1</ref>: Convolutional Tensor-Train LSTM. The preprocessing module first groups the previous hidden states into overlapping sets with a sliding window, and reduces the number of channels in each group using a convolutional layer. The convolutional tensor-train module takes the results, aggregates their spatio-temporal information, and computes the gates for the LSTM update. The diagram visualizes a Conv-TT-LSTM with one channel. When Conv-TT-LSTM has multiple channels, the addition also accumulates the results from multiple channels.</p><formula xml:id="formula_7">H(t?1:t?3) H(t?3:t?5) * * * * * K (3) C(t?1) X(t ) W * H (1) H (2) H (3) H(t ) C(t ) LSTM</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Designing an Effective and Efficient Higher-order ConvLSTM</head><p>In order to satisfy all three requirements (1)-(3) introduced above, and enable efficient learning/inference, we propose a novel convolutional tensor-train decomposition (CTTD) that leverages a tensor-train structure <ref type="bibr" target="#b17">[18]</ref> to jointly express the convolutional kernels {K(1), ? ? ? , K(N )} in Eq. <ref type="bibr" target="#b6">(7)</ref> as a series of smaller factors {G(1), ? ? ? , G(N )} while maintaining their spatial structures.</p><p>Convolutional Tensor-Train module. Concretely, let K(i) be the i-th kernel in Eq.(7), of size [K(i) ? K(i) ? C(i) ? C(0)], where K(i) = i[K(1) ? 1] + 1 is the filter size that increases linearly with i; K(1) is the initial filter size; C(i) is the number of channels in H(t ? i); and C(0) is the number of channels for the output of the function ? (thus C(0) = 4 ? C out , where C out is the number of channels of the higher-order ConvLSTM). The CTTD factorizes K(i) using a subset of factors {G(1), ? ? ? , G(i)} up to index i such that</p><formula xml:id="formula_8">K(i):,:,c i ,c 0 CTTD {G(j)} i j=1 = C(i?1) c i?1 =1 ? ? ? C(1) c 1 =1</formula><p>G(i):,:,c i ,c i?1 * ? ? ? * G(2):,:,c 2 ,c 1 * G(1):,:,c 1 ,c 0 , <ref type="bibr" target="#b7">(8)</ref> where G(i) has size [K(1) ? K(1) ? C(i) ? C(i ? 1)]. The number of factors N is known as the order of the decomposition, and the ranks of the decomposition {C(1), ? ? ? , C(N ? 1)} are the channels of the convolutional kernels.</p><p>Notice that the same set of factors {G(1), ? ? ? , G(N )} is reused to construct all convolutional kernels {K(1), ? ? ? , K(N )}, such that the number of total parameters grows linearly in N . In fact, the convolutional kernel K(i + 1) can be recursively constructed as K(i) = G(i) * K(i ? 1) with K(1) = G(1) and K(i) :,:,ci,c0 = ci?1 G(i) :,:,ci,ci?1 * K(i ? 1) :,:,ci?1,c0 for i ? 2.</p><p>This results into in a convolutional tensor-train module that we use for function ? in Eq. <ref type="formula" target="#formula_6">(7)</ref>:</p><formula xml:id="formula_9">? = CTT(H(t ? 1), ? ? ? , H(t ? N ); G(1), ? ? ? , G(N )) = N i=1 CTTD {G(j)} i j=1 * H(t ? i) (9)</formula><p>In Appendix A, we show that the computation of Eq. Preprocessing module. In Eq.(9), we use the raw hidden states H(t) as inputs to CTT. This design has two limitations: (a) The number of past steps in CTT (i.e. the order of the higher-order ConvLSTM) is equal to the number of factors in CTTD (i.e. the order of the tensor decomposition), which both equal to N . It is prohibitive to use a long history, as a large tensor order leads to gradient vanishing/exploding problem in computing Eq.(9); (b) All the ranks C(i) are equal to the number of channels in H(t), which prevents the use of lower-ranks to further reduce the model complexity.</p><p>To address both issues, we develop a preprocessing module to reduce both the number of steps and channels in previous hidden states before they are fed into CTT. Suppose the number of steps M is no less than the tensor order N (i.e. M ? N ), the preprocessing collects the neighboring steps with a sliding window and reduce it into an intermediate result with C(i) channels:</p><formula xml:id="formula_10">H(i) = P(i) * [H(t ? i); ? ? ? ; H(t ? i + N ? M )]<label>(10)</label></formula><p>where P(i) represents a convolutional layer that maps the concatenation [?] intoH(i).</p><p>Convolutional Tensor-Train LSTM. By combining all the above modules, we obtain our proposed Conv-TT-LSTM, illustrated in <ref type="figure" target="#fig_3">Figure 1</ref> and expressed as:</p><formula xml:id="formula_11">I(t); F(t);C(t); O(t) = ? W * X (t) + CTT H (1), ? ? ? ,H(N ); G(1), ? ? ? , G(N )<label>(11)</label></formula><p>This final implementation has several advantages: it drastically reduces the number of parameters and makes the higher-order ConvLSTM even more compact than first-order ConvLSTM. The lowrank constraint acts as an implicit regularizer, leading to more generalizabled models. Finally, the tensor-train structure inherently encodes the correlations resulting from the natural flow of time <ref type="bibr" target="#b8">[9]</ref>. The full procedure can be found in Appendix A (algorithm 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Here, we quantitatively and empirically evaluate our approach on several datasets, for two different tasks, video prediction and early activity recognition and find that it outperforms existing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>Effective training strategy of higher-order prediction models. To facilitate training, we argue for a careful choice of the learning scheduling and gradient clipping. Specifically, various learning scheduling techniques including learning rate decay, scheduled sampling <ref type="bibr" target="#b18">[19]</ref> and curriculum learning with varying weighting factor are added during training. For video prediction, learning rate decay is used along with scheduled sampling, where scheduled sampling starts if the model does not improve for a few epochs in terms of validation loss; For early activity recognition, learning rate decay is combined with weighting factor decay, where the weighting factor is decreased linearly ? := max(?? , 0) on plateau. We also found gradient clipping essential for higher-order models. All models are trained with ADAM optimizer <ref type="bibr" target="#b19">[20]</ref>. In the initial experiments, we found that our models are unstable at a high learning rate 1e ?3 , but learn poorly at a low learning rate 1e ?4 . Consequently, we use gradient clipping with learning rate 1e ?3 , with clipping value 1 for all experiments.</p><p>Evaluation. For video prediction, the model predicts every pixel in the frame. We test our proposed models on the KTH human action dataset <ref type="bibr" target="#b20">[21]</ref> with resolution 128?128 and on the Moving-MNIST-2 dataset <ref type="bibr" target="#b1">[2]</ref> with resolution 64 ? 64. All models are trained to predict 10 future frames given 10 input frames, and tested to predict 10 ? 40 frames recursively. For early activity recognition, we evaluate our approach on the Something-Something V2 dataset. Following <ref type="bibr" target="#b6">[7]</ref>, we used the subset of 41 categories defined by Goyal et al. <ref type="bibr" target="#b21">[22]</ref> ( <ref type="table" target="#tab_11">Table 7</ref>). The prediction model is trained to predict the next 10 frames given 25% ? 50% of frames, and jointly classify the activity using the learned representations of the prediction model.</p><p>Model architecture. In all video prediction experiments, we use 12 RNN layers. For early activity recognition, we follow the base framework of <ref type="bibr" target="#b6">[7]</ref>. The prediction model consists of two layers of 2D-CNN encoder and decoder with eight RNN layers in between. The output of the RNN layer is fed to the classifier that contains two 2D convolutional layers and one fully-connected layer. We explain the detailed architecture in Appendix B.</p><p>Loss function. For video prediction, we optimize an 1 + 2 loss L prediction = X ?X 2 F + X ?X 1 , where X andX are the ground-truth and predicted frames. For early activity recognition, we combine the prediction loss above with an additional cross entropy for classification L recognition = ? ? L prediction + L ce (y,?), where y and? are the ground-truth and predicted labels. The weighting factor ? balances the learning representation and exploiting the representation for activity recognition.</p><p>Hyper-parameter selection. We validate the hyper-parameters of our Conv-TT-LSTM on though a wide grid search on the validation set. Specifically, we consider a base filter size S = 3, 5, order of the decomposition N = 1, 2, 3, 5, tensor ranks C(i) = 4, 8, 16, and number of hidden states M = 1, 3, 5. Appendix B contains the details of our hyper-parameter search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head><p>Multi-frame Video prediction: KTH action dataset. First, we test our model on human actions videos. In <ref type="table" target="#tab_5">Table 4</ref>, we report the evaluation on both 20 and 40 frames prediction. <ref type="figure">Figure 2</ref> (right) shows the model comparisons with SSIM vs LPIPS and the model size.</p><p>(1) Our model is consistently better than the ConvLSTM baseline for both 20 and 40 frames prediction. (2) While our proposed Conv-TT-LSTMs achieve lower SSIM value compared to the state-of-the-art models in 20 frames prediction, they outperform all previous models in LPIPS for both 20 and 40 frames prediction. <ref type="figure" target="#fig_0">Figure 3</ref> (right) shows a visual comparison of our model, ConvLSTM baseline, PredRNN++ <ref type="bibr" target="#b5">[6]</ref>, and E3D-LSTM <ref type="bibr" target="#b6">[7]</ref>. More examples of visual results are presented in Appendix C. Overall, our model produces sharper frames and better preserves the details of the human silhouettes, although there exist slight artifacts over time (shifting). We believe this artifact can be resolved by adding a different loss or an additional technique that help per-pixel motion prediction.</p><p>Early activity recognition: Something-Something V2 dataset. To demonstrate that our Conv-TT-LSTM-based prediction model can learn effective representations from videos, we evaluate the models on early activity recognition on the Something-Something V2 dataset. In this task, a model only observes a small fraction (25% ? 50%) of frames, and learns to predict future frames. Based on the learned representations of the beginning frames, the model predicts the overall activity of the full video. Intuitively, the learned representation encodes the future information for frame prediction, and the better the representations quality, the higher the classification accuracy. As shown in <ref type="table" target="#tab_2">Table 1</ref> and <ref type="table" target="#tab_3">Table 2</ref> our Conv-TT-LSTM model consistently outperforms the baseline ConvLSTM and 3D-CNN models as well as E3D-LSTM <ref type="bibr" target="#b6">[7]</ref> under different ratio of input frames. Our experimental setup and architecture follow <ref type="bibr" target="#b6">[7]</ref>.</p><p>Multi-frame video prediction: Moving-MNIST-2 dataset. We additionally evaluate our model on the Moving-MNIST-2 dataset and show that our model can predict the digits almost perfectly in terms of structure and motion (See <ref type="figure" target="#fig_0">Figure 3)</ref>. <ref type="table" target="#tab_5">Table 4</ref> reports the average statistics for 10 and 30 frames prediction, and <ref type="figure">Figure 2</ref> (left) shows the comparisons of SSIM vs LPIPS and the model size. Our Conv-TT-LSTM models (1) consistently outperform the ConvLSTM baseline for both 10 and 30 frames prediction with fewer parameters; (2) outperform previous approaches in terms of SSIM and LPIPS (especially on 30 frames prediction), with less than one fifth of the model parameters.</p><p>We reproduce the PredRNN++ <ref type="bibr" target="#b5">[6]</ref> and E3D-LSTM <ref type="bibr" target="#b6">[7]</ref> from the source code <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. We find that (1) PredRNN++ and E3D-LSTM output vague and blurry digits in long-term prediction (especially after 20 steps); (2) our Conv-TT-LSTM produces sharp and realistic digits over all steps. An example of visual comparison is shown in <ref type="figure" target="#fig_0">Figure 3</ref>, and more visualizations can be found in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ConvLSTM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv-TT-LSTM [Ours]</head><p>PredRNN++ <ref type="bibr" target="#b17">[18]</ref> E3D-LSTM <ref type="bibr">[</ref>  <ref type="figure">Figure 2</ref>: SSIM vs LPIPS scores on Moving MNIST-s2 (left) and KTH action datasets (right). The bubble size is the model size. Higher SSIM score and lower LPIPS score are better. On both datasets and for both metrics, our approach reaches a significantly better performance than other methods while having only a fraction of the parameters.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In this section, we further justify the importance of the proposed modules, convolutional tensor-train decomposition (CTTD) and the preprocessing module. We also explain the computational complexity of our model, and the difficulties of spatio-temporal learning with Transformer <ref type="bibr" target="#b26">[27]</ref>.</p><p>Importance of encoding higher-order correlations in a convolutional manner. Two key differences between CTTD and existing low-rank decompositions are higher-order decomposition and convolutional operations. To verify their impact, we compare the performance of two ablated models against our CTTD-base model in <ref type="table" target="#tab_4">Table 3</ref>. The single order means that the higher-order model is replaced with a first-order model (tensor order = 1). By replacing 5 ? 5 filters to 1 ? 1, the convolutions are removed, and the CTTD reduces to a standard tensor-train decomposition. The results show a decrease in performance: the ablated models at best achieve similar performance of ConvLSTM baseline, demonstrating that both higher-order model and convolutional operations are necessary.</p><p>Importance of the preprocessing module. There could be other ways to incorporate previous hidden states into the CTT module. One is to reduce the number of channels while keeping the number of steps; the other is to reuse the concatenation of all previous states for each input to CTT. The former fails due to gradient vanishing/exploding problem, while the latter has a tube-shaped receptive field that fails to distinguish more recent steps and the ones from the remote history.</p><p>Computational complexity. The number of FLOPS for all models are compared in <ref type="table" target="#tab_5">Table 4</ref>. Our Conv-TT-LSTM model has a lower computational complexity and fewer parameters than other models under comparison. This efficiency is made possible by a linear algorithm for the convolutional tensor-train module in Eq <ref type="bibr" target="#b8">(9)</ref>, which is derived in Appendix A.</p><p>Trade-off between FLOPs and latency. Notice that a lower number of FLOPS does not necessarily lead to faster computation due to the sequential nature of convolutional tensor-train module. In Appendix A, we introduce two algorithms. While Alg. 2 significantly decreases the complexity in FLOPs, it also lowers the degree of parallelism. However, Alg. 1 shows how our model can be parallelized. Ideally, these two algorithms can be combined using CUDA multi-streams (execute multiple kernels in parallel): use Alg. 1 for the beginning iterations of i and Alg. 2 for the later ones (the beginning ones have smaller kernel sizes). In our current implementation, we use Alg. 2 to reduce the GPU memory requirement, and the run-time is 27.3 mins (37.83 GFLOPs) for Conv-TT-LSTM verse 26.2 mins (55.83 GFLOPs) for ConvLSTM (per epoch on KTH), as shown in <ref type="table" target="#tab_5">Table 4</ref>.</p><p>Classic Tensor-Train Decomposition for RNN compression. Classic tensor-train decomposition (TTD) <ref type="bibr" target="#b17">[18]</ref> is traditionally used to compress fully-connected and convolutional layer in a feed-forward network <ref type="bibr">[34?</ref> ], where the parameters in each layer are reshaped into a higher-order tensor and stored in a factorized form. Yang et al. <ref type="bibr" target="#b39">[40]</ref> applies this idea to RNNs, and proposes to compress the parameters in the input-hidden transition matrix <ref type="bibr" target="#b33">[34]</ref>.</p><p>There are three major differences between our work and Yang et al. <ref type="bibr" target="#b39">[40]</ref>:</p><p>? Single-order LSTM v.s. Higher-order ConvLSTM. Yang et al. <ref type="bibr" target="#b39">[40]</ref> consider a first-order fullyconnected LSTM <ref type="bibr" target="#b16">[17]</ref> for compression, while our method aims to compress a higher-order convolutional LSTM model. ? Classic decomposition v.s. Convolutional decomposition. Yang et al. <ref type="bibr" target="#b39">[40]</ref> relies on the classic TTD, while our proposed convolutional tensor-train decomposition (CTTD) factorizes the tensor with convolutions in addition to inner products; our decomposition is designed to preserve spatial structures in spatio-temporal data. ? Compression of input-hidden matrix v.s. hidden-to-hidden convolutional kernels. Yang et al. <ref type="bibr" target="#b39">[40]</ref> only compresses input-hidden transition W in LSTM, but our CTTD compresses a sequence of convolutional kernels {K(1), ? ? ? , K(N )} for different time steps simultaneously (see Eq. <ref type="formula" target="#formula_6">(7)</ref>).</p><p>To understand the necessity of our design for long-term spatio-temporal dynamics, we develop a compressed ConvLSTM following the same idea in <ref type="bibr">[34? , 40]</ref>, which stores the parameters for input-hidden transition W in a tensor-train format W = TT({G(i)} N ?1 i=0 ) (where N denotes the order of the decomposition, i.e. the number of factors):</p><formula xml:id="formula_12">[I(t); F(t);C(t); O(t)] = ?(TT({G(i)} N ?1 i=0 ) * X (t) + K * H(t ? 1))<label>(12)</label></formula><p>Since the transition in ConvLSTM is characterized as a convolutional layer, we follow the approach by ? ] and represent W with size</p><formula xml:id="formula_13">[K ? K ? C out ? C in ] using N factors: (1) The 4-th order tensor W is reshaped to an 2M -th order tensor W with size [K ? K ? T 1 ? ? ? ? T N ?1 ? S 1 ? ? ? ? S N ?1 ] and C out = N ?1 i=1 T i , C in = N ?1 i=1 S i ; (2)</formula><p>The tensor W is factorized using TTD as</p><formula xml:id="formula_14">W i,j,t1,??? ,t N ?1 ,s1,??? ,s N ?1 r0,??? ,r N ?1 G(0) i,j,r0 G(1) t1,s1,r0,r1 ? ? ? G(N ? 1) i,j,r N ?1 ,<label>(13)</label></formula><p>where</p><formula xml:id="formula_15">G(0) has size [K ? K ? R 0 ], G(i) has [T i ? S i ? R i?1 ? R i ] for 0 &lt; i &lt; N ? 1, and G(N ? 1) has [T N ?1 ? S N ?1 ? R N ?1 ]</formula><p>. A comparison against the uncompressed ConvLSTM and our Conv-TT-LSTM is presented in <ref type="table" target="#tab_5">Table 4</ref>. We observe that our model outperforms this method on MNIST and KTH (except LPIPS on KTH) with similar number of parameters.</p><p>Transformer for spatio-temporal learning. Transformer <ref type="bibr" target="#b26">[27]</ref> is a popular predictive model based on attention mechanism, which is very successful in natural language processing <ref type="bibr" target="#b27">[28]</ref>. However, Transformer has prohibitive limitations when it comes to video understanding, due to excessive needs for both memory and computation. While language modeling only involves temporal attention, video understanding requires attention on spatial dimensions as well <ref type="bibr" target="#b28">[29]</ref>. Moreover, since attention mechanism is not designed to preserve the spatial structures, Transformer additionally requires auxiliary components including autoregressive module and multi-resolution upscaling when applied on spatial data <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b28">29]</ref>. Our Conv-TT-LSTM incorporates a large spatio-temporal context, but with a compact, efficient and structure-preserving operator without additional components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Tensor decompositions. Tensor decompositions such as CP, Tucker or Tensor-Train <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b17">18]</ref>, are widely used for dimensionality reduction <ref type="bibr" target="#b32">[33]</ref> and learning probabilistic models <ref type="bibr" target="#b9">[10]</ref>. These tensor factorization techniques have also been widely used in deep learning, to improve performance, speed-up computation and compress the deep neural networks <ref type="bibr">[13, 14, 34? , 35, 37, 38, 16]</ref>, recurrent networks <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref> and Transformers <ref type="bibr" target="#b40">[41]</ref>. Yang et al. <ref type="bibr" target="#b39">[40]</ref> has proposed tensor-train RNNs to compress both inputs-states and states-states matrices within each cell with TTD by reshaping the matrices into tensors, and showed improvement for video classification.</p><p>Departing from prior works that rely on existing, well-established tensor decompositions, here we propose a novel convolutional tensor-train decomposition (CTTD) that is designed to enable efficient and compact higher-order convolutional recurrent networks. Unlike Yang et al. <ref type="bibr" target="#b39">[40]</ref>, we aim to compress higher-order ConvLSTM, rather than first-order fully-connected LSTM. We further propose Convolutional Tensor-Train decomposition to preserve spatial structure after compression.</p><p>Spatio-temporal prediction models. Prior prediction models have focused on predicting short-term video <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref> or decomposing motion and contents <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref>. Many of these works use ConvLSTM as a base module, which deploys 2D convolutional operations in LSTM to efficiently exploit spatio-temporal information. Some works modified the standard ConvLSTM to better capture spatio-temporal correlations <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. Byeon et al. <ref type="bibr" target="#b42">[43]</ref> demonstrated strong performance using a deep ConvLSTM network as a baseline, which is used as the base architecture in the present paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we proposed a fully-convolutional higher-order LSTM model for spatio-temporal data.</p><p>To make the approach computationally and memory feasible, we proposed a novel convolutional tensor-train decomposition that jointly parameterizes the convolutions and naturally encodes temporal dependencies. The result is a compact model that outperforms prior work on video prediction, including something-something V2, moving-MNIST-2 and the KTH action datasets. Going forward, we plan to investigate our CTT module in a framework that spans not only higher-order RNNs but also Transformer-like architectures for efficient spatio-temporal learning.</p><p>[ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Convolutional Tensor-Train LSTM for Spatio-temporal Learning</head><p>In the supplementary material, we first provide a constructive proof that our approach can be computed in linear time. We then provide thorough implementation details for all experiments, and perform extra ablation studies of our model, demonstrating that our Conv-TT-LSTM model is general and outperforms regular ConvLSTM regardless of the architecture or setting used. Finally, we provide additional visualizations of our experimental results.</p><p>To facilitate the reading of our paper, we provide a Table of notation in <ref type="table" target="#tab_7">Table 5</ref>. </p><formula xml:id="formula_16">Symbol Meaning Value or Size H Height of feature map - W Width of feature map C in # of input channels C out # of output channels t Current time step - W Weights for X (t) [K ? K ? 4C out ? C in ] X (t) Input features [H ? W ? C in ] H(t) Hidden state [H ? W ? C out ] C(t) Cell state I(t) Input gate F(t) Forget gat? C(t) Cell memory O(t) Output gate ? Mapping function for higher-order RNN - M order of higher-order RNN M ? N N Order of CTTD K Initial filter size K(0) = K K(i) Filter size inK(i) C(i) # channels inH(i) C(0) = 4C out G(i) Factors in the CTTD [K(0) ? K(0) ? C(i) ? C(i ? 1)] D Size of sliding window D = M ? N + 1 P(i) Preprocessing kernel [D ? K ? K ? C out ? C(i)] H(i) Pre-processed hidden state [H ? W ? C(i)] K(i) Weights forH(i) [K(i) ? K(i) ? C(i) ? C(0)]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A An Efficient Algorithm for Convolutional Tensor-Train Module</head><p>In this section, we prove that our convolutional tensor-train module, CTT (Eq.7 in main paper), can be evaluated with linear computational complexity. Our proof is constructive and readily provides an algorithm for computing CTT in linear time.</p><p>First, let's recall the formulation of the CTT function:</p><formula xml:id="formula_17">? = CTT(H(t ? 1), ? ? ? , H(t ? N ); G(1), ? ? ? , G(N )) = N i=1 K(i) * H(t ? i).<label>(14)</label></formula><p>where each kernel K(i) is factorized by convolutional tensor-train decomposition (CTTD):</p><formula xml:id="formula_18">K(i):,:,c i ,c 0 CTTD {G(j)} i j=1 = C(i?1) c i?1 =1 ? ? ? C(1) c 1 =1</formula><p>G(i):,:,c i ,c i?1 * ? ? ? * G(1):,:,c 1 ,c 0 , ?i ? [N ] <ref type="bibr" target="#b14">(15)</ref> However, a naive algorithm that first reconstruct all the kernels K(i), then applies Eq.(14) results in a computational complexity of O(N 3 ), as illustrated in algorithm 1. To scale our approach to higher-order models (i.e. larger N ), we need a more efficient implementation of the function CTT. </p><formula xml:id="formula_19">C(t) = C(t ? 1) +C(t) ? I(t); H(t) = O(t) ? ?(C(t)) return H(t), C(t)</formula><p>Recursive evaluation. We will prove that CTT can be evaluated backward recursively using</p><formula xml:id="formula_20">V(i ? 1):,:,c i?1 = C(i) c i =1 G(i):,:,c i ,c i?1 * (V(i):,:,c i + H(i):,:,c i ) , i = N, N ? 1, ? ? ? , 0<label>(16)</label></formula><p>where V(N ) is initialized as zeros, and the final output of CTT is equal to V(0).</p><p>Proof. First, we note that K(i) can be represented recursively in terms of K(i ? 1) and G(i): </p><p>and therefore it holds for n = 1, ? :,:,c0 = C(1) c1=1 G(1) :,:,c1,c0 * (V(1) :,:,c1 + H(1) :,:,c1 ) = V(0) :,:,c0 . Notice that the case n = N is obvious by the definition of CTT and the zero initialization of V(N ). Therefore the remaining of this proof is to induce the case n = N ? 1 from n = N .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experimental Details</head><p>In this section, we provide the detailed setup of all experiments (datasets, model architectures, learning strategies and evaluation metrics) for both video prediction and early activity recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Preprocessing Module</head><p>In the main paper, we use a sliding window to group consecutive states in the proprocessing module (Section 3). In the Discussion (Section 5), we argued that other possible approaches are less effective in preserving spatio-temporal structure compared to our sliding window approach. Here, we discuss an alternative approach that was previously proposed for non-convolutional higher-order RNN <ref type="bibr" target="#b8">[9]</ref>, which we name as fixed window approach. We will compare these two approaches in terms of computational complexity, ability to preserve temporal structure and predictive performance.</p><p>Fixed window approach. With fixed window approach, M previous steps {H(t ? 1), ? ? ? , H(t ? M )} are first concatenated into a single tensor, which is then repeatedly mapped to N inputs {H(1), ? ? ? ,H(N )} to the CTT module. </p><p>For comparison, we list both equations for fixed window approach and sliding window approach. These two approaches are also illustrated in <ref type="figure" target="#fig_8">Figure 5</ref>.</p><p>Drawbacks of fixed window approach. (a) The fixed window approach has a larger window size than the sliding window approach, thus requires more parameters in the preprocessing kernels and higher computational complexity. (b) More importantly, the fixed window approach does not preserve the chronological order of the preprocessed states; unlike sliding window approach, the index i forH(i) in fixed window approach cannot reflect the time for the compressed states. Actually, all preprocessed statesH(1), ? ? ? ,H(M ) are equivalent, which violates the property (2) in designing our convolutional tensor-train module (Section 3.1). (c) In <ref type="table" target="#tab_12">Table 8</ref>, we compare these two approaches on Moving-MNIST-2 under the same experimental setting, and we find that the sliding window approach performs slightly better than fixed window. For all aforementioned reasons, we choose sliding window approach in our implementation of the preprocessing module. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Model Architectures</head><p>Video prediction. All experiments use a stack of 12-layers of ConvLSTM or Conv-TT-LSTM with 32 channels for the first and last 3 layers, and 48 channels for the 6 layers in the middle. A convolutional layer is applied on top of all recurrent layers to compute the predicted frames, followed by an extra sigmoid layer for KTH action dataset. Following Byeon et al. <ref type="bibr" target="#b42">[43]</ref>, two skip connections performing concatenation over channels are added between <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b8">9)</ref> and <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b11">12)</ref> layers. An illustration of the network architecture is included in <ref type="figure" target="#fig_10">Figure 6a</ref>. All convolutional kernels were initialized by Xavier's normalized initializer <ref type="bibr" target="#b46">[47]</ref> and initial hidden/cell states in ConvLSTM or Conv-TT-LSTM were initialized as zeros.</p><p>Early activity recognition. Following <ref type="bibr" target="#b6">[7]</ref>, the network architecture consists of four modules: a 2D-CNN encoder, a video prediction network, a 2D-CNN decoder and a 3D-CNN classifier, as illustrated in <ref type="figure" target="#fig_10">Figure 6b.</ref> (1) The 2D-CNN encoder has two 2-strided 2D-convolutional layers with 64 channels, which reduce the resolution from 224 ? 224 to 56 ? 56, and (2) the 2D-CNN decoder contains two 2-strided transposed 2D-convolutional layers with 64 channels, which restore the resolution from 56 ? 56 to 224 ? 224.</p><p>(3) The video prediction network is miniature version of <ref type="figure" target="#fig_10">Figure 6a</ref>, where the number of layers in each block is reduced to 2. In the experiments, we evaluate three realizations of each layer: ConvLSTM, Conv-TT-LSTM or causal 3D-convolutional layer. (4) The 3D-CNN classifier takes the last 16 frames from the input, and predicts a label for the 41 categories. The classifier contains two 2-strided 3D-convolutional layers with 128 channels, each of which is followed by a 3D-pooling layer. These layers reduce the resolution from 56 ? 56 to 7 ? 7, and the output feature is fed into a two-layers perceptron with 512 hidden units to compute the label.</p><p>B.3 Hyper-parameters selection.   (2) For learning rate, we found that our models are unstable at a high learning rate such as 10 ?3 , but learn poorly at a low learning rate 10 ?4 . Consequently, we use gradient clipping with learning rate 10 ?3 , with clipping value 1 for all experiments.</p><p>(3) While the performance typically increases as the order grows, the model suffers gradient instability in training with a high order, e.g. N = 5. Therefore, we choose the order N = 3 for all Conv-TT-LSTM models. (4)(5) For small ranks C(i) and steps M , the performance increases monotonically with C(i) and M . But the performance stays on plateau when we further increase them, therefore we settle down at C(i) = 8, ?i and M = 5 for all experiments.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Datasets</head><p>Moving-MNIST-2 dataset. The Moving-MNIST-2 dataset is generated by moving two digits of size 28 ? 28 in MNIST dataset within a 64 ? 64 black canvas. These digits are placed at a random initial location, and move with constant velocity in the canvas and bounce when they reach the boundary. Following Wang et al. <ref type="bibr" target="#b5">[6]</ref>, we generate 10,000 videos for training, 3,000 for validation, and 5,000 for test with default parameters in the generator <ref type="bibr" target="#b2">3</ref> .</p><p>Similarly, we summarize the search values KTH action dataset. The KTH action dataset <ref type="bibr" target="#b20">[21]</ref> contains videos of 25 individuals performing 6 types of actions on a simple background. Our experimental setup follows Wang et al. <ref type="bibr" target="#b5">[6]</ref>, which uses persons 1-16 for training and 17-25 for testing, and each frame is resized to 128 ? 128 pixels. All our models are trained to predict 10 frames given 10 input frames. During training, we randomly select 20 contiguous frames from the training videos as a sample and group every 10,000 samples into one epoch to apply the learning strategy as explained at the beginning of this section.</p><p>Something-Something V2 dataset. The Something-Something V2 dataset <ref type="bibr" target="#b21">[22]</ref> is a benchmark for activity recognition, which can be download online <ref type="bibr" target="#b3">4</ref> . Following Wang et al. <ref type="bibr" target="#b6">[7]</ref>, we use the official subset with 41 categories that contains 55111 training videos and 7518 test videos. The video length ranges between 2 and 6 seconds with 24 frames per second (fps). We reserve 10% of the training videos for validation, and use the remaining 90% for optimizing the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Evaluation Metrics</head><p>We use two traditional metrics MSE (or PSNR) and SSIM <ref type="bibr">[48]</ref>, and a recently proposed deep-learning based metric LPIPS <ref type="bibr">[49]</ref>, which measures the similarity between deep features. Since MSE (or PSNR) is based on pixel-wise difference, it favors vague and blurry predictions, which is not a proper measurement of perceptual similarity. While SSIM was originally proposed to address the problem, Zhang et al. <ref type="bibr">[49]</ref> shows that their proposed LPIPS metric aligns better to human perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 Ablation Studies</head><p>Here, we show that our proposed Conv-TT-LSTM consistently improves the performance of Con-vLSTM, regardless of the architecture, loss function and learning schedule used. Specifically, we perform three ablation studies on our experimental setting, by (1) Reducing the number of layers from 12 layers to 4 layers (same as <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b5">[6]</ref>); (2) Changing the loss function from L 1 + L 2 to L 1 only; and (3) Disabling the scheduled sampling and use teacher forcing during training process. We compare the performance of our proposed Conv-TT-LSTM against the ConvLSTM baseline in these ablated settings,  (2) Our current learning approach is optimal in the search space; (3) The sliding window approach outperforms the fixed window one under the optimal experimental setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Experimental Results</head><p>Per-frame evaluations. The per-frame metrics are illustrated in <ref type="figure">Figure 7</ref> for Moving-MNIST-2 dataset, and <ref type="figure">Figure 8</ref> for KTH action dataset. (1) In Moving-MNIST-2 dataset, PredRNN++ performs comparably with our Conv-TT-LSTM on early frames, but drops significantly for longterm prediction. E3D-LSTM performs similarly to ConvLSTM baseline, and our Conv-TT-LSTM consistently outperforms E3D-LSTM and ConvLSTM over all frames.</p><p>(2) In KTH action dataset, PredRNN++ consistently perform worse than our Conv-TT-LSTM model over all frames; E3D-LSTM performs well on early frames in MSE and SSIM, but quickly deteriorates for long-term prediction. <ref type="figure">Figure 7</ref>: Frame-wise comparison in MSE, SSIM and PIPS on Moving-MNIST-2 dataset. For MSE and LPIPS, lower curves denote higher quality; while for SSIM, higher curves imply better quality. Our Conv-TT-LSTM performs better than ConvLSTM baseline, PredRNN++ <ref type="bibr" target="#b5">[6]</ref> and E3D-LSTM <ref type="bibr" target="#b6">[7]</ref> in all metrics (except for PredRNN++ in term of MSE). <ref type="figure">Figure 8</ref>: Frame-wise comparison in PSNR, SSIM and PIPS on KTH action dataset. For LPIPS, lower curves denote higher quality; For PSNR and SSIM, higher curves imply better quality. Our Conv-TT-LSTM outperforms ConvLSTM, PredRNN++ <ref type="bibr" target="#b5">[6]</ref> and E3D-LSTM <ref type="bibr" target="#b6">[7]</ref> in SSIM and LPIPS.</p><p>Additional visual results: Video prediction. <ref type="figure" target="#fig_2">Figure 9</ref>, 10, 11, 12, 13, and 14 show additional visual comparisons. We also attach two video clips (KTH and MNIST) as supplementary material.</p><p>Additional visual results: Early activity recognition. We attach two video clips (video 1 and 2) as supplementary material. The videos show the comparisons among 3D-CNN, Conv-LSTM and our Conv-TT-LSTM when the input frames are partially seen. The time-frame of the video corresponds to an amount of video frames seen by the models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E3D-LSTM</head><p>Conv-TT-LSTM <ref type="figure" target="#fig_2">Figure 9</ref>: 20 frames prediction on KTH given 10 input frames. Every 2 frames are shown. input ground truth (top) / predictions <ref type="table" target="#tab_2">t = 1  4  6  8  11  13  15  17  19  21  23  25  27</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( 3 )</head><label>3</label><figDesc>Both space and time complexities grow at most linearly with times steps N , i.e. O(N ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 9 )</head><label>9</label><figDesc>can be done in linear time O(N ), thus the construction of CTT satisfies all requirements (1)-(3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 :</head><label>1</label><figDesc>Convolutional Tensor-Train LSTM (Original: T (N ) = O(N 3 )). Input: current input X (t), previous cell state C(t ? 1), M previous hidden states {H(t ? 1), ? ? ? , H(t ? M )} Output: new hidden state H(t), new cell state C(t) Initialization: K(0) = 1; V = 0 /* Convolutional Tensor-Train (CTT) module */ for i = 1 to N do /* preprocessing module */ // compress the states from a sliding window H(i) = P(i) * [H(t ? i); ? ? ? ; H(t ? i + N ? M )] // recursively construct the kernel K(i) = G(i) * K(i ? 1) // accumulate the output V = V + K(i) * H(i) end for /* Long-Short Term Memory (LSTM) */ I(t); F(t);C(t); O(t) = ?(W * X (t) + V)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>K</head><label></label><figDesc>(i):,:,c i ,c 0 = C(i?1) c i?1 =1 G(i):,:,c i ,c i?1 * K(i ? 1):,:,c i?1 ,c 0 (17) with K(1) = G(1). Next, we aim to inductively prove the following holds for any n ? [N ]: ):,:,c i ,c 0 * H(t ? i):,:,c i + C(n) cn=1 K(n):,:,c n,c0 * V(n):,:,c n ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>):,:,c i ,c 0 * H(t ? i):,:,c i + C(N ) c N =1 K(N ):,:,c N ,c 0 * V(N ):,:,c N (19)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>==C N ? 1 c N ? 1 1 . 2 :</head><label>1112</label><figDesc>):,:,c i ,c 0 * H(t ? i):,:,c i + C(N ) c N =1 K(N ):,:,c N ,c 0 * (H(N ):,:,c N + H(N ):,:,c N )<ref type="bibr" target="#b19">(20)</ref> Notice that the second term can be rearranged asC(N ) c N =1 K(N ):,:,c N ,c 0 * (H(N ):,:,c N + H(N ):,:,c N ) (21) = C(N ) c N =1 C(N ?1) c N ?1 =1 G(N ? 1):,:,c N ,c N ?1 * K(N ? 1):,:,c N ?1 ,c 0 K(N ):,:,c N ,c 0 , by Eq.(17) * (H(N ):,:,c N + H(N ):,:,c N ) (22) =1 K(N ? 1):,:,c N ?1 ,c 0 * C(N ) c N =1 G(N ? 1):,:,c N ,c N ?1 * (H(N ):,:,c N + H(N ):,:,c N ) V(N ?1) :,:,N ?1 , by Eq.(16) 1):,:,c N ?1 ,c 0 * V(N ? 1):,:,N?1 (24) where Eq.(22) uses the recursive formula in Eq.(17), and Eq.(23) is by definition of V(N ? 1) in Eq.(16). Therefore, we show that the case n = N ? 1 also holds ?:,:,c 0 = ):,:,c i ,c 0 * H(t ? i):,:,c i + C(N ?1) c N ?1 =1 K(N ? 1):,:,c N ?1 ,c 0 * V(N ? 1):,:,N?1 (25) which completes the induction from n = N to n = N ? Algorithm Convolutional Tensor-Train LSTM (Accelerated: T (N ) = O(N )). Input: current input X (t), previous cell state C(t ? 1), M previous hidden states {H(t ? 1), ? ? ? , H(t ? M )} Output: new hidden state H(t), new cell state C(t) Initialization: K(0) = 1; V(N ) = 0 /* Convolutional Tensor-Train (CTT) module */ for i = N to 1 do /* preprocessing module */ // compress the states from a sliding window H(i) = P(i) * [H(t ? i); ? ? ? ; H(t ? i + N ? M )] // recursively compute the intermediate results V(i ? 1) = G(i) * V(i) +H(i) ; end for /* Long-Short Term Memory (LSTM) */ I(t); F(t);C(t); O(t) = ? (W * X (t) + V(0)) C(t) = C(t ? 1) +C(t) ? I(t); H(t) = O(t) ? ?(C(t)) return H(t), C(t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fixed</head><label></label><figDesc>Window (FW):H(i) = P(i) * [H(t ? 1); ? ? ? ; H(t ? N )] (26a) Sliding Window (SW):H(i) = P(i) * [H(t ? i); ? ? ? ; H(t ? i + N ? M )]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Variations of proprocessing modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Network architecture for video prediction and early activity recognition tasks. K = 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :Figure 11 :Figure 12 :Figure 13 :Figure 14 :</head><label>1011121314</label><figDesc>20 frames prediction on KTH given 10 input frames. Every 2 frames are shown. 20 frames prediction on KTH given 10 input frames. Every 2 frames are shown. 20 frames prediction on KTH given 10 input frames. Every 2 frames are shown. input ground truth (top) / predictions 30 frames prediction on Moving-MNIST given 10 input frames. Every 3 frames are shown. The first frames (t = 1 and 11) are animations. To view the animation, Adobe reader is required. input ground truth (top) / predictions 30 frames prediction on Moving-MNIST given 10 input frames. Every 3 frames are shown. The first frames (t = 1 and 11) are animations. To view the animation, Adobe reader is required.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>30 frames prediction on Moving-MNIST (left), and 20 frame prediction on KTH action datasets (right) given 10 input frames. The first frames (t = 1, 11) are animations. Adobe reader is required to view the animation. Our method generates both semantically plausible and visually crisp images, compared to other approaches. Examples of Early Activity Recognition Results given 25% and 50% of frames on the Something-Something V2 dataset, and (?) is the confidence for Correct/Wrong prediction.</figDesc><table><row><cell>input</cell><cell></cell><cell cols="6">ground truth (top) / predictions</cell><cell cols="2">input</cell><cell cols="5">ground truth (top) / predictions</cell></row><row><cell>t = 1</cell><cell>6</cell><cell>11</cell><cell>17</cell><cell></cell><cell>23</cell><cell>29</cell><cell>35</cell><cell>t = 1</cell><cell>6</cell><cell>11</cell><cell>15</cell><cell>19</cell><cell>23</cell><cell>27</cell></row><row><cell cols="2">PredRNN++</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PredRNN++</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">E3D-LSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">E3D-LSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ConvLSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ConvLSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Conv-TT-LSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Conv-TT-LSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Figure 3: Front 25%</cell><cell></cell><cell></cell><cell>Front 50%</cell><cell></cell><cell cols="2">100%</cell><cell cols="2">Front 25%</cell><cell>Front 50%</cell><cell></cell><cell>100%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>...</cell><cell></cell><cell></cell><cell>....</cell><cell></cell><cell></cell><cell></cell><cell>...</cell><cell></cell><cell>....</cell><cell></cell></row><row><cell>3D-CNN ConvLSTM</cell><cell cols="3">Wrong (100%) Wrong (84%)</cell><cell cols="3">Wrong (100%) Wrong (46%)</cell><cell cols="2">Pushing [something]</cell><cell cols="4">Wrong (67%) Wrong (84%) Correct (61%) Wrong (100%)</cell><cell cols="2">Pulling [something]</cell></row><row><cell cols="4">Conv-TT-LSTM Wrong (18%)</cell><cell cols="3">Correct (100%)</cell><cell cols="2">from right to left</cell><cell cols="4">Wrong (18%) Correct (98%)</cell><cell cols="2">from right to left</cell></row><row><cell>Figure 4: Model</cell><cell></cell><cell cols="13">Input Dropping Holding MovingLR MovingRL Picking Poking Pouring Putting Showing Tearing</cell></row><row><cell>3D-CNN</cell><cell></cell><cell></cell><cell cols="2">8.5</cell><cell>4.7</cell><cell cols="2">25.8</cell><cell>32.6</cell><cell>7.5</cell><cell>2.9</cell><cell>1.9</cell><cell>10.3</cell><cell>14.0</cell><cell>14.5</cell></row><row><cell>ConvLSTM</cell><cell></cell><cell>25%</cell><cell cols="2">8.5</cell><cell>7.0</cell><cell cols="2">27.4</cell><cell>38.8</cell><cell>16.8</cell><cell>5.9</cell><cell>1.9</cell><cell>12.0</cell><cell>7.0</cell><cell>21.2</cell></row><row><cell cols="2">Conv-TT-LSTM</cell><cell></cell><cell cols="2">11.5</cell><cell>4.7</cell><cell cols="2">33.9</cell><cell>40.8</cell><cell>16.8</cell><cell>5.9</cell><cell>5.7</cell><cell>13.6</cell><cell>20.9</cell><cell>26.0</cell></row><row><cell>3D-CNN</cell><cell></cell><cell></cell><cell cols="2">14.6</cell><cell>11.6</cell><cell cols="2">45.2</cell><cell>57.1</cell><cell>16.8</cell><cell>8.8</cell><cell>11.3</cell><cell>17.4</cell><cell>16.3</cell><cell>26.0</cell></row><row><cell>ConvLSTM</cell><cell></cell><cell>50%</cell><cell cols="2">21.5</cell><cell>7.0</cell><cell cols="2">43.5</cell><cell>47.0</cell><cell>15.9</cell><cell>14.7</cell><cell>5.7</cell><cell>20.7</cell><cell>16.3</cell><cell>30.8</cell></row><row><cell cols="2">Conv-TT-LSTM</cell><cell></cell><cell cols="2">24.6</cell><cell>11.6</cell><cell cols="2">56.5</cell><cell>57.1</cell><cell>27.6</cell><cell>5.9</cell><cell>13.2</cell><cell>25.5</cell><cell>37.2</cell><cell>46.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Per-activity accuracy of early activity recognition on the Something-Something V2 dataset. We used 41 categories for training. For per-activity evaluation, the 41 categories are grouped into 10 similar activities. The activity mapping are described in<ref type="bibr" target="#b21">[22]</ref>. Our model substantially outperforms 3D-CNN and ConvLSTM on long-term dynamics such as Moving or Tearing, while achieves marginal improvement on static activities such as Holding or Pouring.</figDesc><table><row><cell>Model</cell><cell cols="2">Input Ratio</cell></row><row><cell></cell><cell cols="2">Front 25% Front 50%</cell></row><row><cell>3D-CNN*</cell><cell>9.11</cell><cell>10.30</cell></row><row><cell>E3D-LSTM* [7]</cell><cell>14.59</cell><cell>22.73</cell></row><row><cell>3D-CNN</cell><cell>13.26</cell><cell>20.72</cell></row><row><cell>ConvLSTM</cell><cell>15.46</cell><cell>21.97</cell></row><row><cell cols="2">Conv-TT-LSTM (ours) 19.53</cell><cell>30.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">MSE(?10 ?3 ) SSIM</cell><cell>LPIPS</cell></row><row><cell cols="4">CTTD with 1 ? 1 filters (similar to standard TTD)</cell></row><row><cell>single order</cell><cell>31.52</cell><cell>0.810</cell><cell>148.7</cell></row><row><cell>order 3</cell><cell>34.84</cell><cell>0.800</cell><cell>151.2</cell></row><row><cell></cell><cell cols="2">CTTD with 5 ? 5 filters</cell><cell></cell></row><row><cell>single order</cell><cell>33.08</cell><cell>0.806</cell><cell>140.1</cell></row><row><cell>order 3</cell><cell>28.88</cell><cell>0.831</cell><cell>104.1</cell></row></table><note>Early activity recognition on the Something-Something V2 dataset using 41 categories as [7]. (*) indicates the result by [7].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>] (retrained [23]) 28.62 0.888 228.9 26.94 0.865 279.0 15.</figDesc><table><row><cell></cell><cell>Method</cell><cell>(10 -&gt; 20)</cell><cell></cell><cell>(10 -&gt; 40)</cell><cell></cell><cell cols="2">Complexities</cell></row><row><cell></cell><cell></cell><cell cols="7">PSNR SSIM LPIPS PSNR SSIM LPIPS # Params. # FLOPS Time(m)</cell></row><row><cell></cell><cell>ConvLSTM [4]</cell><cell>23.58 0.712</cell><cell>-</cell><cell>22.85 0.639</cell><cell>-</cell><cell>7.58M</cell><cell>-</cell><cell>-</cell></row><row><cell>KTH action</cell><cell cols="6">MCNET [25] PredRNN++ [605M 25.95 0.804 -----E3D-LSTM [7] (retrained [24]) 27.92 0.893 298.4 26.55 0.878 328.8 41.94M ConvLSTM (baseline) 28.21 0.903 137.1 26.01 0.876 201.3 3.97M ConvLSTM (classic TTD [40? ]) 27.70 0.897 141.5 25.89 0.872 191.7 2.21M</cell><cell>---55.83G -</cell><cell>---26.2 -</cell></row><row><cell></cell><cell>Conv-TT-LSTM (Ours)</cell><cell cols="5">28.36 0.907 133.4 26.11 0.882 191.2 2.69M</cell><cell>37.83G</cell><cell>27.3</cell></row><row><cell></cell><cell>Method</cell><cell>(10 -&gt; 10)</cell><cell></cell><cell>(10 -&gt; 30)</cell><cell></cell><cell cols="2">Complexities</cell></row><row><cell></cell><cell></cell><cell cols="7">MSE SSIM LPIPS MSE SSIM LPIPS # Params. # FLOPS Time(m)</cell></row><row><cell>Moving-MNIST</cell><cell cols="6">ConvLSTM [4] VPN [26] PredRNN++ [6] (retrained [23]) 10.29 0.913 59.51 20.53 0.834 139.9 15.05M 25.22 0.713 -38.13 0.595 -7.58M 15.65 0.870 -31.64 0.620 --E3D-LSTM [7] (pretrained [24]) 20.23 0.869 76.12 32.37 0.803 150.3 41.94M ConvLSTM (baseline) 18.17 0.882 67.13 33.08 0.806 140.1 3.97M ConvLSTM (classic TTD [40? ]) 16.78 0.890 57.90 29.07 0.815 126.4 2.20M</cell><cell>----15.88G -</cell><cell>----6.35 -</cell></row><row><cell></cell><cell>Conv-TT-LSTM (Ours)</cell><cell cols="5">12.96 0.915 40.54 25.81 0.840 90.38 2.69M</cell><cell>10.76G</cell><cell>7.40</cell></row></table><note>Ablation studies of higher-order Conv- TT-LSTM on Moving-MNIST-2 dataset. The models are tested for 10 to 30 frames prediction.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Evaluation of multi-steps prediction on the KTH action (top) and Moving-MNIST-2 (bottom) datasets. Higher PSNR/SSIM and lower MSE/LPIPS values indicate better predictive results. # of FLOPs denotes the multiplications for one-step prediction per sample, and Time(m) represents the clock time (in minutes) required by training the model for one epoch (10,000 samples)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>48] Zhou Wang, Alan C Bovik, Hamid R Sheikh, Eero P Simoncelli, et al. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4): 600-612, 2004. [49] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 586-595, 2018.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Table of notations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 summarizes</head><label>6</label><figDesc></figDesc><table /><note>our search values for different hyper-parameters for Conv-TT-LSTM. (1) For filter size K, we found models with larger filter size K = 5 consistently outperform the ones with</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Hyper-parameters search values for Conv-TT-LSTM experiments.Similarly,Table 7summarize the hyper-parameters search for tensor-train compression of ConvL-STM [? ]. (1) Since the best ConvLSTM baseline has filter size K = 5, we only consider K = 5 in the compression experiments. (2) We observe that the compressed ConvLSTM models consistently achieve better performance with learning rate 10 ?3 . (3)(4) The compressed ConvLSTMs are robust to different order and ranks, and N = 2, R = 8 wins by a small margin.</figDesc><table><row><cell>Filter size K</cell><cell>Learning rate</cell><cell cols="2">Order of TTD N Ranks of TTD R</cell></row><row><cell>5</cell><cell>{10 ?4 , 10 ?3 }</cell><cell>{2, 3}</cell><cell>{8, 16, 32}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Hyper-parameters search values for Tensor-Train compression of ConvLSTM.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 .</head><label>8</label><figDesc>The results show that our proposed Conv-TT-LSTM consistently outperforms ConvLSTM in all settings, i.e. the Conv-TT-LSTM model improves upon ConvLSTM in a board range of setups, which is not limited to the certain setting used in our paper. These ablation studies further show that our setup is optimal for predictive learning in Moving-MNIST-2 dataset.</figDesc><table><row><cell>Model</cell><cell></cell><cell>Layers Sched. 4 12 TF SS 1 1 + 2 MSE SSIM LPIPS Loss (10 -&gt; 30)</cell><cell>Params.</cell></row><row><cell>ConvLSTM</cell><cell>-</cell><cell cols="2">37.19 0.791 184.2 11.48M</cell></row><row><cell cols="2">Conv-TT-LSTM FW</cell><cell cols="2">31.46 0.819 112.5 5.65M</cell></row><row><cell>ConvLSTM</cell><cell>-</cell><cell cols="2">33.96 0.805 184.4 3.97M</cell></row><row><cell cols="2">Conv-TT-LSTM FW</cell><cell cols="2">30.27 0.827 118.2 2.65M</cell></row><row><cell>ConvLSTM</cell><cell>-</cell><cell cols="2">36.95 0.802 135.1 3.97M</cell></row><row><cell cols="2">Conv-TT-LSTM FW</cell><cell cols="2">34.84 0.807 128.4 2.65M</cell></row><row><cell>ConvLSTM</cell><cell>-</cell><cell cols="2">33.08 0.806 140.1 3.97M</cell></row><row><cell cols="2">Conv-TT-LSTM FW</cell><cell cols="2">28.88 0.831 104.1 2.65M</cell></row><row><cell cols="2">Conv-TT-LSTM SW</cell><cell cols="2">25.81 0.840 90.38 2.69M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Evaluation of ConvLSTM and our Conv-TT-LSTM under ablated settings. In this table, FW stands for fixed window approach, SW stands for sliding window approach; For learning scheduling, TF denotes teaching forcing and SS denotes scheduled sampling. The experiments show that (1) our Conv-TT-LSTM is able to improve upon ConvLSTM under all settings;</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://20bn.com/datasets/something-something</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep visual foresight for planning robot motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2786" to="2793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Social lstm: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="961" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Shi Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Predrnn: Recurrent neural networks for predictive learning using spatiotemporal lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Yu</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="879" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Predrnn++: Towards a resolution of the deep-in-time dilemma in spatiotemporal predictive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06300</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Eidetic 3d lstm: A model for video prediction and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohollah</forename><surname>Soltani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.00064</idno>
		<title level="m">Higher order recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rose</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00073</idno>
		<title level="m">Long-term forecasting using tensor-train rnns</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tensor decompositions for learning latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matus</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Telgarsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2773" to="2832" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep learning and the information bottleneck principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noga</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Information Theory Workshop (ITW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Emergence of invariance and disentanglement in deep representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1947" to="1980" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Speeding-up convolutional neural networks using fine-tuned cp-decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Rakhuba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Oseledets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6553</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Deok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunhyeok</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjoo</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taelim</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjun</forename><surname>Shin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06530</idno>
		<title level="m">Compression of deep convolutional neural networks for fast and low power mobile applications</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">T-net: Parametrizing fully convolutional nets with a single high-order tensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Yannis Panagakis, Timothy Hospedales, and Maja Pantic. Factorized higher-order cnns with an application to spatio-temporal emotion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Toisoul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tensor-train decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oseledets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2295" to="2317" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Recognizing human actions: a local svm approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="32" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Github</forename><surname>Repo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Github</surname></persName>
		</author>
		<ptr target="https://github.com/Yunbo426/predrnn-pp" />
		<title level="m">Online; accessed 05</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Github</forename><surname>Repo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Github</surname></persName>
		</author>
		<ptr target="https://github.com/google/e3d_lstm" />
		<title level="m">Online; accessed 05</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Decomposing motion and content for natural video sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08033</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video pixel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1771" to="1779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Scaling autoregressive video models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>T?ckstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02634</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Generating high fidelity images with subscale pixel networks and multidimensional upscaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01608</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Tensor networks for dimensionality reduction and large-scale optimization: Part 1 low-rank tensor decompositions. Foundations and Trends? in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namgil</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Oseledets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh-Huy</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><forename type="middle">P</forename><surname>Mandic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Tensorizing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Osokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry P</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="442" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aran</forename><surname>Khanna</surname></persName>
		</author>
		<title level="m">Tommaso Furlanello, and Anima Anandkumar. Tensor regression networks. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep multi-task representation learning: A tensor factorisation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Tensorized spectrum preserving compression for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bobby</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furong</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10352</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arinbj?rn</forename><surname>Kolbeinsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioanna</forename><surname>Tzoulaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Matthews</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10758</idno>
		<title level="m">Stochastically rank-regularized tensor regression networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Compressing recurrent neural network with tensor train</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andros</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sakriani</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4451" to="4458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Tensor-train recurrent neural networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinchong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Krompass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3891" to="3900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A tensorized transformer for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xindian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexian</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.09777</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08104</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Contextvp: Fully context-aware video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonmin</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Koumoutsakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="753" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="64" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled representations from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Emily L Denton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4414" to="4423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning to decompose and disentangle representations for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Ting</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><forename type="middle">F</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
