<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Boosting Semantic Human Matting with Coarse Annotations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlin</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendi</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Cui</surname></persName>
							<email>miaomiao.cmm@alibaba-inc.comxingtong.xxs@taobao.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuansong</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Boosting Semantic Human Matting with Coarse Annotations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic human matting aims to estimate the per-pixel opacity of the foreground human regions. It is quite challenging and usually requires user interactive trimaps and plenty of high quality annotated data. Annotating such kind of data is labor intensive and requires great skills beyond normal users, especially considering the very detailed hair part of humans. In contrast, coarse annotated human dataset is much easier to acquire and collect from the public dataset. In this paper, we propose to use coarse annotated data coupled with fine annotated data to boost endto-end semantic human matting without trimaps as extra input. Specifically, we train a mask prediction network to estimate the coarse semantic mask using the hybrid data, and then propose a quality unification network to unify the quality of the previous coarse mask outputs. A matting refinement network takes in the unified mask and the input image to predict the final alpha matte. The collected coarse annotated dataset enriches our dataset significantly, allows generating high quality alpha matte for real images. Experimental results show that the proposed method performs comparably against state-of-the-art methods. Moreover, the proposed method can be used for refining coarse annotated public dataset, as well as semantic segmentation methods, which reduces the cost of annotating high quality human data to a great extent.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human matting is an important image editing task which enables accurate separation of humans from their backgrounds. It aims to estimate the per-pixel opacity of the foreground regions, making it valuable to use the extracted human image in some recomposition scenarios, including digital image and video production. One may refer this task as semantic segmentation problem <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b23">24]</ref>, which  <ref type="figure">Figure 1</ref>. The user interactive method could catch precise semantics and details under the guidance of trimaps. Without the trimap and enough training dataset, one may get inaccurate semantic estimation, which inevitably leads to wrong matting results. Our methods achieve comparable matting results by leveraging coarse annotated data while do not need trimaps as inputs.</p><p>achieves fine-grained inference for enclosing objects. However, segmentation techniques focus on pixel-wise binary classification towards scene understanding, although semantic information is well labelled, it could not catch complicated semantic details like human hair. The matting problem can be formulated in a general manner. Given an input image I, matting is modeled as the weighted combination of foreground image F and background image B as follows <ref type="bibr" target="#b29">[30]</ref>:</p><formula xml:id="formula_0">I z = ? z F z + (1 ? ? z )B z , ? z ? [0, 1] .<label>(1)</label></formula><p>where z represents any pixel in image I. The known information in Eq. 1 are the three dimensional RGB color I z , while the RGB color F z and B z , and the alpha matte estimation ? z are unknown. Matting is thus to solve the 7 unknown variables from 3 known values, which is highly under-constrained. Therefore, most existing matting methods take a carefully specified trimap as constraint to reduce the solution space. However, a dilemma in terms of quality and efficiency for trimaps still exists.</p><p>The key factor that affecting the performance of matting algorithm is the accuracy of trimap. The trimap divides the image into three regions, including the definite foreground, the definite background and the unknown region. Intuitively, the smaller regions around foreground boundary that the trimap contains, the less unknown variables would be estimated, leading to a more precise alpha matte result. However, designing such an accurate trimap requires a lot of human efforts with low efficiency. The labeling quality should be unified among all the data, either large or small size of unknown regions will degrade the final alpha matte effects. One possible solution to solve the dilemma is adaptively learn a trimap from coarse to fine <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b5">6]</ref>. In contrast, another solution discards the trimap from the input and employs it as an implicit constraint to a deep matting network <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b32">33]</ref>. However, these methods still rely on the quality of the generated trimap, unable to retain both the semantic information and high quality details when implicit trimap is inaccurate.</p><p>Another limitation comes from the data for human matting. It is important to have high quality annotation data for image matting task. Since humans in natural images possess a variety of colors, poses, head positions, clothes, accessories, etc. The semantically meaningful structure around the foreground like human hair, furs are the challenging regions for human matting. Annotating such accurate alpha matte is labor intensive and requires great skills beyond normal users. Shen et al. <ref type="bibr" target="#b27">[28]</ref> proposed a human portrait dataset with 2000 images, but it has strict constraint on position of human upper body. The widely used DIM dataset <ref type="bibr" target="#b31">[32]</ref> is limited in human data, with only 213 human images. Although Chen et al. <ref type="bibr" target="#b7">[8]</ref> created a large human matting dataset, it is only for commercial use. Unfortunately, collecting the dataset in <ref type="bibr" target="#b7">[8]</ref> with 35,311 images takes more than 1,200 hours, which is undesirable in practice. Therefore, we argue that there is a solution by combining the limited fine annotated image with easily collected coarse annotated image for human matting.</p><p>To address the aforementioned problems, we propose a novel framework to utilize both coarse and fine annotated data for human matting. Our method could predict accurate alpha matte with high quality details and sufficient semantic information without trimap as constraint, as shown in <ref type="figure">Figure 1</ref>. We achieve this goal by proposing a coupled pipeline with three subnetworks. The mask prediction network (MPN) aims to predict low resolution coarse mask, which contains semantic human information. MPN is trained using both fine and coarse annotated data for better performance on various real images. However, the output of MPN may vary and are not consistent with respect to different input images. Therefore, a quality unification net-work (QUN) trained on hybrid annotated data is introduced to rectify the quality level of MPN output to the same level. A matting refinement network (MRN) is proposed to predict the final accurate alpha matte, taking in both the origin image and its unified coarse mask as input. Different with MPN and QUN, the matting refinement network is trained using only the fine annotated data.</p><p>We also constructed a hybrid annotated dataset for human matting task. The dataset consists of both high quality (fine) annotated human images and low quality (coarse) annotated human images. We first collect 9526 images/alpha pairs with fine annotations. In comparison with previous dataset, we diversity the distribution of human images with carefully annotated alpha matte <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b31">32]</ref>, within a labor rational volume size <ref type="bibr" target="#b7">[8]</ref>. We further collect 10597 coarse annotated data to better capture accurate semantics within our framework. We follow <ref type="bibr" target="#b31">[32]</ref> to composite both data onto 10 background images in MS COCO <ref type="bibr" target="#b22">[23]</ref> and Pascal VOC <ref type="bibr" target="#b11">[12]</ref> to form our dataset. Comprehensive experiments have been conducted on this dataset to demonstrate the effectiveness of our method, and our model is able to refine coarse annotated public dataset as well as semantic segmentation methods, which further verifies the generalization of our method. The main contributions of this work are:</p><p>? To our best knowledge, this is the first method that uses coarse annotated data to enhance the performance of end-to-end human matting. Previous methods either take trimap as constraint or use sufficient fine annotated dataset only.</p><p>? We propose a quality unification network to rectify the mask quality during the training process so as to utilize both coarse and fine annotations, allowing accurate semantic information as well as structural details.</p><p>? The proposed method can be used to refine coarse annotated public dataset as well as semantic segmentation methods, which makes it easy to create fine annotated data from coarse masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Natural Image Matting. Natural image matting tries to estimate the the unknown area with known foreground and background in the trimap. The traditional methods can be summarized to sampling based methods and affinity based methods <ref type="bibr" target="#b29">[30]</ref>. The sampling based methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27]</ref> leverage the nearby known foreground and background colors to infer the alpha values of the pixels in the undefined area. Assuming that alpha values for two pixels have strong correlations if the corresponding colors are similar. Following the assumption, various sampling methods are proposed including Bayesian matting <ref type="bibr" target="#b10">[11]</ref>, sparse coding <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref>, global sampling <ref type="bibr" target="#b16">[17]</ref> and KL-divergence approaches <ref type="bibr" target="#b19">[20]</ref>. Com- <ref type="figure">Figure 2</ref>. An overview of our network architecture. The proposed method is composed of three parts. The first part is mask prediction network (MPN), to predict low resolution coarse semantic mask. MPN is trained using both coarse and fine data. The second part is quality unification network (QUN). QUN aims to rectify the quality of the output from the mask prediction network to the same level. The rectified coarse mask is then unified and enables consistent input for training the following alpha matte prediction stage. The third part is matting refinement network (MRN), taking in the input image and the unified coarse mask to predict the final accurate alpha matte. pared with sampling based methods, Affinity based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29]</ref> define different affinities between neighboring pixels, trying to model the matte gradient instead of the per-pixel alpha value.</p><p>Deep learning based method is able to learn a mapping between the image and corresponding alpha matte in an end-to-end manner. Cho et al. <ref type="bibr" target="#b9">[10]</ref> take the advantage of close-form matting <ref type="bibr" target="#b20">[21]</ref> and KNN matting <ref type="bibr" target="#b8">[9]</ref> for alpha mattes reconstruction. Xu et al. <ref type="bibr" target="#b31">[32]</ref> integrate the encoderdecoder structure with a following refinement network to predict alpha matte. Lutz et al. <ref type="bibr" target="#b24">[25]</ref> further employ the generative adversarial network for image matting task. Cai et al. <ref type="bibr" target="#b5">[6]</ref> argue the limitation of directly estimating the alpha matte from a coarse trimap, and propose to disentangle the matting into trimap adaptation and alpha estimation tasks. Compared with the above methods, our method simply use RGB images as input without the constraint of designated trimaps. Human image Matting. As a specific type of image matting, human matting aims to estimate the accurate alpha matte corresponding to the human in the input image, which involves semantically meaningful structures like hair. Recently, several deep learning based human matting methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref> have been proposed. Shen et al. <ref type="bibr" target="#b27">[28]</ref> propose a deep neural network to generate the trimap of a portrait image and add a matting layer <ref type="bibr" target="#b20">[21]</ref> for network optimization using the forward and backward propagation strategy. Zhu et al. <ref type="bibr" target="#b33">[34]</ref> use a similar pipeline and design a light dense network for portrait segmentation and a feature block to learn the guided filter <ref type="bibr" target="#b17">[18]</ref> for alpha matte prediction. Chen et al. <ref type="bibr" target="#b7">[8]</ref> introduce an automatic human matting al-gorithm without feeding trimaps. It combines a segmentation module with a matting module for end-to-end matting. The late fusion CNN structure in <ref type="bibr" target="#b32">[33]</ref> integrates the foreground and background classification presents its capacity for human image matting. However, these models require carefully collected image/alpha pairs, which may also suffer from inaccurate semantics due to lack of fine annotated human data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>We develop three subnetworks as a sequential pipeline. The first one is mask prediction network (MPN), to predict coarse semantic masks using data at different annotation quality level. The second one is quality unification network (QUN). QUN rectifies the quality of the output coarse mask from MPN to the same level. The third part is matting refinement network (MRN), to predict the final accurate alpha matte. The flowchart and the network structure is displayed in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Mask Prediction Network</head><p>As no trimap is required as input, the first stage of the proposed method is to predict a coarse semantic mask. The network we use is encoder-decoder structure with skip connection, and we predict the foreground mask and the background mask at the same time. At this stage, we aim to estimate a coarse mask, and therefore the network is not trained at a high resolution. We resize all training data to resolution 192 ? 160 so as to train the mask prediction network (MPN) efficiently. In addition, the mask predic-tion network is trained using all training data, including low quality and high quality annotated data. The loss function to train LRPN is L 1 loss,</p><formula xml:id="formula_1">L M P N = ? L |? c p ? ? c g | 1 + (1 ? ? L )|? c p ? ? c g | 1 , (2)</formula><p>where the output is a 2-channel mask, ? c p denotes the first channel of the output, i.e., the predicted foreground mask, ? c g denotes the ground truth foreground mask, ? c p denotes the second channel of the output, i.e., the predicted background mask, and ? c g denotes the ground truth background mask. We set ? L = 0.5 in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Quality Unification Network</head><p>Due to the high cost of annotating high quality matting data, we propose to use hybrid data from different data source. Some of the data is annotated at high quality , even hairs are very well separated from the background <ref type="figure" target="#fig_2">(Figure 3(a)</ref>). Whereas, majority of other data are annotated at a relatively low quality <ref type="figure" target="#fig_2">(Figure 3(b)</ref>). Mask prediction network is trained with both fine annotated data and coarse annotated data. Thus, the quality of the predicted mask may vary significantly. As the alpha matte prediction network can only be trained on the high quality annotated data, the variation of the coarse mask quality will inevitably lead to inconsistent matting results during the inference stage. As illustrated in <ref type="figure">Figure 6</ref>(c), if the coarse mask is relatively accurate, the refinement network will work well to output accurate alpha matte. On the contrary, the refinement network will fail if the coarse mask lacks important details.</p><p>We proposed to eliminate the data bias for training matting refinement network by introducing a quality unification network (QUN). The quality unification network aims to rectify the output quality of the mask prediction network to the same level, by improving the quality of coarse masks and lowering the quality of fine masks simultaneously. The output of the mask prediction network and the original image are feed into the quality unification network to unify the quality level. The rectified coarse mask is unified and enables consistent input for training the following accurate alpha matte prediction stage.</p><p>The loss function of training QUN network contains two parts, identity loss and consistence loss. Identity loss forces the output of QUN not to change much from the original input,</p><formula xml:id="formula_2">L identity = |Q(x) ? x| 1 + |Q(x ) ? x | 1 ,<label>(3)</label></formula><p>where Q(?) represent the quality unification network. x denotes the concatenation of the input image and the accurate mask, x denotes the concatenation of the input image and the inaccurate mask. The second part is consistence loss. Consistence loss forces the output of QUN corresponding to accurate mask and inaccurate mask to be close.  </p><formula xml:id="formula_3">L consist = |Q(x) ? Q(x )| .<label>(4)</label></formula><p>Thus, the loss function of training QUN is the weighted sum of identity loss and consistence loss,</p><formula xml:id="formula_4">L QU N = ? 1 L identity + ? 2 L consist .<label>(5)</label></formula><p>During the training, we set ? 1 = 0.25 and ? 2 = 0.5.</p><p>In <ref type="figure" target="#fig_2">Figure 3</ref>, we illustrate the results of QUN. Fine mask <ref type="figure" target="#fig_2">(Figure 3(a)</ref>) and coarse mask <ref type="figure" target="#fig_2">(Figure 3(b)</ref>) are unified by QUN to <ref type="figure" target="#fig_2">Figure 3(d)</ref> and (e) respectively. The difference maps are also calculated. We can observe that the unified high quality mask become relatively coarser and low quality mask becomes relatively finer. As a result, the unified masks are much closer to each other than the original fine and coarse masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Matting Refinement Network</head><p>Matting refinement network (MRN) aims to predict accurate alpha matte. Therefore, we train MRN at a higher resolution (768 * 640 in all experiments). Note that the coarse mask from MPN and QUN is at low resolution (192 ? 160). The coarse mask is integrated to MRN as external input feature maps, where the input is downscaled 4 times after several convolution operations. The output of  MRN are 4-channel maps, including three foreground RGB channels and one alpha matte channel. Predicting the foreground RGB channels coupled with alpha matte is able to increase the robustness, which plays a similar role of the compositional loss used in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b7">8]</ref>. The loss function we used to train MRN is L 1 loss,</p><formula xml:id="formula_5">L M RN = ? H |RGB p ? RGB g | 1 + (1 ? ? H )|? p ? ? g | 1 ,<label>(6)</label></formula><p>where RGB p and RGB g denote the predicted RGB foreground channels and ground truth foreground channels respectively. ? p and ? g denote the predicted alpha matte and ground truth alpha matte respectively. We set ? H = 0.5 in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation details</head><p>We implement our method with Tensorflow [1] framework. We perform training for our three networks sequentially. Before feeding into the mask prediction network, we conduct a down-sampling operation on images at 192 ? 160 resolution, including both fine and coarse annotated data. Flipping is performed randomly on each training pair. We first train the mask prediction network for 20 epochs and fix the parameters. Then we concatenate the low resolution image and the output foreground mask as input to train quality unification network. When training QUN, random filters(filter size set as 3 or 5), binarization and morphology operations(dilate and erode) are exerted to fine annotated data to generate paired high and low quality mask data. After training quality unification network, all parameters are fixed. We finally train the matting refinement network with only the fine annotated data. The entire data pairs (image, alpha matte) are randomly cropped to 768 ? 640. The learning rate for training all networks is 1e ? 3. MPN and QUN are trained using batch size 16 and MRN is trained using batch size 1, as MRN is trained using only high resolution data. When testing, a feed-forward pass of our pipeline is performed to output the alpha matte prediction with only the image as input. The average testing time on multiple 800?800 images is 0.08 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Human matting dataset</head><p>A main challenge for human matting is the lack of data. Xu et al. <ref type="bibr" target="#b31">[32]</ref> proposed a general matting dataset by compositing foreground objects from natural images to differents backgrounds, which has been widely used in the following matting works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33]</ref>. However, the diversity of human images is severely limited, including only 202 human images in training set and 11 human images in testing set. For human matting dataset, Shen et al. <ref type="bibr" target="#b27">[28]</ref> collected a portrait dataset with 2000 images, it assumes that the upper body appears at similar positions in human images and the images are annotated by Closed From <ref type="bibr" target="#b20">[21]</ref>, KNN <ref type="bibr" target="#b8">[9]</ref> methods, which can be inevitably biased. Although a large human fashion dataset is created by <ref type="bibr" target="#b7">[8]</ref> for matting, it is only for commercial use. To this end, we create a human matting dataset with high-quality for research. We carefully collected 9449 diverse human images with simple background from the Internet (i.e., white or transparent background in PNG format), each human image acquires a well annotated alpha matte after simple processing. The human images are split to training/testing set, with 9324 and 125 respectively. Following Xu et al. <ref type="bibr" target="#b31">[32]</ref>, we first add the human images in DIM dataset <ref type="bibr" target="#b31">[32]</ref> into our training/testing set, forming a total of 9526 and 136 human foregrounds respectively. We then randomly sample 10 background images in MS COCO <ref type="bibr" target="#b22">[23]</ref> and Pascal VOC <ref type="bibr" target="#b11">[12]</ref> and composite the human images onto those background images. During composition, we ensure that the background images are not containing humans.</p><p>Another issue should be addressed for human matting dataset is the quality of annotations. Image matting task requires user designated annotations for objects, i.e., the high quality alpha matte. Besides, the user interaction methods require carefully prepared trimaps and scribbles as constraints, which is labor intensive and less scalable. Method without user provided trimaps is to predict the alpha matte by first generating implicit trimaps for further guidance, thus lead to some artifacts as well as losing some semantics  <ref type="figure">Figure 5</ref>. The qualitative comparison on our proposed dataset. The first column and the last column show the input image and the ground truth alpha matte, and the rest columns present the estimation results by DeepLab <ref type="bibr" target="#b6">[7]</ref>, Closed-form matting <ref type="bibr" target="#b20">[21]</ref>, DIM <ref type="bibr" target="#b31">[32]</ref>, SHM <ref type="bibr" target="#b7">[8]</ref>, our method trained using fine annotated data only and our method trained using hybrid annotated data.</p><p>for complex structures. We integrate the coarse annotation data to tackle this problem as they are much easier to obtain. We collect another 10597 human data from <ref type="bibr" target="#b30">[31]</ref> and Supervisely Person Dataset, and follow the above setup to generate 105970 image with coarse annotations. <ref type="table" target="#tab_0">Table 1</ref> shows the configuration of the existing human matting dataset. Our dataset consists of both fine and coarse annotated data, with nearly the same amount. Compared with user interactive methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b31">32]</ref>, our dataset covers diverse high quality human images, making it more robust for human matting models. Although sacrifice the number of high quality annotations than automatic method <ref type="bibr" target="#b7">[8]</ref>, we introduce coarse annotated data to enhance the capacity for extracting both semantic and matting details at a lower cost. The data for both annotations are shown in <ref type="figure" target="#fig_4">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation results.</head><p>Evaluation metrics. We adopt four widely used metrics for matting evaluations following the previous works <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b7">8]</ref>. The metrics are MSE (mean square error), SAD (sum of the absolution difference), the gradient error and the connectivity error. The gradient error and connectivity error proposed in <ref type="bibr" target="#b25">[26]</ref> are used to reflect the human perception towards visual quality of the alpha matte. Lower values of these metrics correspond to better estimated alpha matte. We normalize the estimated alpha matte and true alpha matte to [0, 1] to calculate these evaluation metrics. Since no trimap is required, we calculate over the entire images and average by the pixel number.  <ref type="bibr" target="#b6">[7]</ref> 0.028 0.023 0.012 0.028 Trimap+CF <ref type="bibr" target="#b20">[21]</ref> 0.0083 0.0049 0.0035 0.080 Trimap+DIM <ref type="bibr" target="#b31">[32]</ref> 0.0045 0.0017 0.0013 0.0043 SHM <ref type="bibr" target="#b7">[8]</ref> 0.011 0.0078 0.0032 0.011 ours(w/o coarse data) 0.0099 0.0067 0.0029 0.0095 ours(w/o QUN) 0.0076 0.0042 0.0024 0.0072 ours 0.0058 0.0026 0.0016 0.0054</p><p>Baselines. We select the most typical method from semantic segmentation methods, traditional matting methods, user interactive methods and automatic methods respectively as our baselines. These methods are DeepLab <ref type="bibr" target="#b6">[7]</ref>, Closed-form matting <ref type="bibr" target="#b20">[21]</ref>, DIM <ref type="bibr" target="#b2">[3]</ref> and SHM <ref type="bibr" target="#b7">[8]</ref>. Note that the Closed-form matting and DIM need extra trimap as input. DIM and SHM can only be trained using the fine annotated data. DeepLab and the proposed method are trained using the proposed hybrid annotated dataset.</p><p>Performance comparison. In <ref type="table" target="#tab_1">Table 2</ref>, we list the quantitative results over 1360 testing images. The semantic segmentation method DeepLab <ref type="bibr" target="#b6">[7]</ref> only predict coarse mask and lack fine details ( <ref type="figure">Figure 5(b)</ref>), resulting in the worst quantitative metrics. SHM <ref type="bibr" target="#b7">[8]</ref> does not perform well as the volume of our high quality training dataset is limited, and fails to predict accurate semantic information for some images ( <ref type="figure">Figure 5(d)</ref>). In contrast, the interactive method closeform matting <ref type="bibr" target="#b20">[21]</ref> and DIM <ref type="bibr" target="#b31">[32]</ref> performs well, benefiting from the input semantic information provided by trimaps. These two methods only need to estimate the uncertain part in trimaps. The proposed method using hybrid training dataset outperforms most methods and is comparable with state-of-the-art methods. DIM <ref type="bibr" target="#b31">[32]</ref> is slightly better than the proposed method. Note that the proposed method only take in input images, DIM requires high informative trimaps as extra input. Even though, the visual quality of the proposed method ( <ref type="figure">Figure 5</ref>(g)) and DIM ( <ref type="figure">Figure 5(d)</ref>) looks very close.</p><p>Self-comparisons. Our method can achieve high quality alpha matte estimation by incorporating coarse annotated human data. Coarse annotated data promote the proposed network to estimate semantic information accurately. To verify the importance of the these data, we separately train the same network with fine annotated dataset only. The quantitative results are listed in <ref type="table" target="#tab_1">Table 2</ref>. Without using the coarse data, the performance is obviously worse. From Figure 5(f) and (g), we can also observe that method trained only with fine annotated data suffers from inaccurate semantic estimation and presents incomplete alpha matte. The mask quality unification network make it possible for the final matting refinement network to adapt to different kinds of coarse mask input. Without QUN, inputs to the  <ref type="figure">Figure 6</ref>. Self-comparisons. Without quality unification network (QUN), the quality of coarse mask sent to the matting refinement network (MRN) may vary significantly. When the coarse mask is relatively accurate, MRN predicts alpha matte well. When the coarse mask lacks most hair details, the estimated alpha matte is accurate. Equipped with QUN, the mask quality is unified before feeding into MRN. The estimated alpha matte is more consistent against different kinds of coarse masks. matting refinement network may vary significantly, which is hard to deal with at inference stage. We list the quantitative metrics without QUN being used in <ref type="table" target="#tab_0">Table 1</ref>. Both fine and coarse annotated dataset are used in this comparison. The results are obviously worse when QUN is removed. For a better visual comparison, we display the results in <ref type="figure">Figure 6</ref>. The predicted alpha matte is fine if the coarse mask is relatively accurate. When the coarse mask lacks most hair details, the estimated alpha matte is not good. With QUN, the mask quality is unified before feeding into MRN. The estimated alpha matte is more accurate and robust to different kinds of coarse masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Applying to real images</head><p>We further apply the proposed method to real images from the Internet. Matting on real images is challenging as the foreground is smoothly fused with the background. In <ref type="figure" target="#fig_7">Figure 7</ref>, we display our testing results on real images. Benefiting from the sufficient training on our hybrid dataset, the  proposed method captures the semantic information very well for different kinds of input images and predicts accurate alpha matte at a detailed level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Applications</head><p>The mask prediction network in the proposed method aims to capture coarse semantic information requiring by the subsequent networks. The semantic mask from this network can be coarse or accurate. The following quality unification network will unify the mask quality for the final matting refinement network. Therefore, if the semantic mask is arrange in some way, the proposed method is still able to work seamlessly and generate accurate alpha matte.</p><p>Thus we can apply our framework to refine coarse annotated public dataset, such as the PASCAL <ref type="bibr" target="#b12">[13]</ref>  <ref type="figure" target="#fig_9">(Figure 8(ac)</ref>) and COCO dataset <ref type="bibr" target="#b22">[23]</ref>  <ref type="figure" target="#fig_9">(Figure 8(d-f)</ref>). The annotated human mask are resized and used as input for our QUN and MRN. Even though the annotations are not accurate, especially the annotations from COCO dataset, the proposed method manages to generate accurate refinement results.</p><p>We can also use the proposed method to refine semantic segmentation methods <ref type="figure" target="#fig_9">(Figure 8(g-i)</ref>). Semantic segmentation methods are usually trained on coarse annotated public dataset, and the output mask is not precise. We feed the coarse mask obtained from DeepLab <ref type="bibr" target="#b6">[7]</ref> to our QUN and MRN. The proposed method generates surprisingly good alpha matte. Details that are missing from the coarse mask are well recovered, even for the very detailed hair parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we propose to use coarse annotated data coupled with fine annotated data to enhance the performance of end-to-end semantic human matting. We propose to use MPN to estimate coarse semantic masks using the hybrid annotated dataset, and then use QUN to unify the quality of the coarse masks. The unified mask and the input images are fed into MRN to predict the final alpha matte. The collected coarse annotated dataset enriches our dataset significantly, and makes it possible to generate high quality alpha matte for real images. Experimental results show that the proposed method performs comparably against state-ofthe-art methods. In addition, the proposed method can be used for refining coarse annotated public dataset, as well as semantic segmentation methods, which potentially brings a new method to annotate high quality human data with much less effort.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Diff map of (a,b) (d) Unified (a) (e) Unified (b) (f) Diff map of (d,e) (g) Diff map of (a,d) (h) Diff map of (b,e) (i) Input image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Different quality of masks are unified by QUN. (a) High quality mask. (b) low quality mask. (c) Difference map of high and low quality mask. (d) Unified result of high quality mask by QUN. (e) Unified result of low quality mask by QUN. (f) Difference map of the unified high quality mask and the low quality mask. (g) Difference map of the unified high quality mask and the original high quality mask. (h) Difference map of the unified low quality mask and the original low quality mask. (i) Input image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Input images and the corresponding annotations in our dataset. Our dataset consists of both coarse annotated images (a) and fine annotated images (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Alpha-GT (f) Ours (fine data)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Real image matting results. The collected coarse annotated dataset enriches our dataset significantly and enables the proposed method to capture the semantic information well and predicts accurate alpha matte for different kinds of input images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(d) Input image (e) COCO annotation (f) Our refinement (g) Input image (h) DeepLab output (i) Our refinement (a) Input image (b) Pascal annotation (c) Our refinement</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Using the proposed method to refine coarse human mask from public dataset annotations or semantic segmentation methods. Feed the coarse human mask from Pascal (b) or Coco (e) dataset annotation or DeepLab (h) to our quality unification network, and then use the matting refinement network to generate the accurate human alpha matte.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The configurations of human matting datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Train Set Human</cell><cell>image</cell><cell>Test Set Human</cell><cell>image</cell></row><row><cell>Shen et al. [28]</cell><cell>1700</cell><cell></cell><cell>1700</cell><cell>300</cell><cell>300</cell></row><row><cell>TrimapDIM [32]</cell><cell>202</cell><cell></cell><cell>20200</cell><cell>11</cell><cell>220</cell></row><row><cell>SHM [8]</cell><cell>34493</cell><cell></cell><cell>34493</cell><cell>1020</cell><cell>1020</cell></row><row><cell>Ours(coarse) Ours(fine)</cell><cell>10597 9324(+202)</cell><cell cols="2">105970 95260</cell><cell>125 (+11)</cell><cell>1360</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The quantitative results.</figDesc><table><row><cell>Method</cell><cell>SAD</cell><cell>MSE</cell><cell>Gradient</cell><cell>Connectivity</cell></row><row><cell>DeepLab</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic soft segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yagiz</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Designing effective inter-pixel information flow for natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yagiz</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Tunc Ozan Aydin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A geodesic framework for fast interactive image and video segmentation and matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Disentangled image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofan</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic human matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiezheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM international conference on Multimedia</title>
		<meeting>the 26th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knn matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingzeyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Natural image matting using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inso</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="626" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A bayesian approach to digital matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="264" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html.8" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A cluster sampling method for image matting via sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxue</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zili</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="204" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Shared sampling for real-time alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Eduardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel M</forename><surname>Gastal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="575" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Random walks for interactive alpha-matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schiwietz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aharon</forename><surname>Shmuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?diger</forename><surname>Westermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of VIIP</title>
		<meeting>VIIP</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="423" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A global sampling method for alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2049" to="2056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Guided image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sparse coding for alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jubin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ehsan Shahrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Varnousfaderani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepu</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3032" to="3043" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image matting with kl-divergence based sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Karacan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aykut</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkut</forename><surname>Erdem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<title level="m">Spectral matting. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1699" to="1712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Lutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Amplianitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Smolic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alphagan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.10088</idno>
		<title level="m">Generative adversarial networks for natural image matting</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A perceptually motivated online benchmark for image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margrit</forename><surname>Gelautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Rott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1826" to="1833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Alpha estimation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Ruzon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep automatic portrait matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="92" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Poisson matting. In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="315" to="321" />
			<date type="published" when="2004" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image and video matting: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael F Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Computer Graphics and Vision</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="175" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Early hierarchical contexts learned by convolutional networks for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1538" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2970" to="2979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A late fusion cnn for digital matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunke</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiran</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast deep matting for portrait animation on mobile phone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="297" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
