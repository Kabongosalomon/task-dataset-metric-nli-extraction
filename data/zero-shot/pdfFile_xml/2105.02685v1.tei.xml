<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Novel Estimator of Mutual Information for Learning to Disentangle Textual Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Colombo</surname></persName>
							<email>pierre.colombo@ibm.comchloe.clavel@telecom-paris.frpablo.piantanida@centralesupelec.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chlo?</forename><surname>Clavel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Piantanida</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Laboratoire des Signaux et Syst?mes (L2S)</orgName>
								<orgName type="institution">CNRS Universite Paris-Saclay</orgName>
								<address>
									<country>CentraleSupelec</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">T?l?com ParisTech</orgName>
								<orgName type="institution" key="instit2">Universit? Paris Saclay ? IBM GBS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Novel Estimator of Mutual Information for Learning to Disentangle Textual Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning disentangled representations of textual data is essential for many natural language tasks such as fair classification, style transfer and sentence generation, among others. The existent dominant approaches in the context of text data either rely on training an adversary (discriminator) that aims at making attribute values difficult to be inferred from the latent code or rely on minimising variational bounds of the mutual information between latent code and the value attribute. However, the available methods suffer of the impossibility to provide a fine-grained control of the degree (or force) of disentanglement. In contrast to adversarial methods, which are remarkably simple, although the adversary seems to be performing perfectly well during the training phase, after it is completed a fair amount of information about the undesired attribute still remains. This paper introduces a novel variational upper bound to the mutual information between an attribute and the latent code of an encoder. Our bound aims at controlling the approximation error via the Renyi's divergence, leading to both better disentangled representations and in particular, a precise control of the desirable degree of disentanglement than state-of-the-art methods proposed for textual data. Furthermore, it does not suffer from the degeneracy of other losses in multi-class scenarios. We show the superiority of this method on fair classification and on textual style transfer tasks. Additionally, we provide new insights illustrating various trade-offs in style transfer when attempting to learn disentangled representations and quality of the generated sentence.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning disentangled representations hold a central place to build rich embeddings of highdimensional data. For a representation to be disentangled implies that it factorizes some latent cause or causes of variation as formulated by <ref type="bibr">(Bengio et al., 2013)</ref>. For example, if there are two causes for the transformations in the data that do not generally happen together and are statistically distinguishable (e.g., factors occur independently), a maximally disentangled representation is expected to present a sparse structure that separates those causes. Disentangled representations have been shown to be useful for a large variety of data, such as video <ref type="bibr">(Hsieh et al., 2018)</ref>, image <ref type="bibr">(Sanchez et al., 2019)</ref>, text <ref type="bibr">(John et al., 2018)</ref>, audio <ref type="bibr">(Hung et al., 2018)</ref>, among others, and applied to many different tasks, e.g., robust and fair classification <ref type="bibr">(Elazar and Goldberg, 2018)</ref>, visual reasoning <ref type="bibr">(van Steenkiste et al., 2019)</ref>, style transfer <ref type="bibr">(Fu et al., 2017)</ref>, conditional generation <ref type="bibr">(Denton et al., 2017;</ref><ref type="bibr">Burgess et al., 2018)</ref>, few shot learning <ref type="bibr">(Kumar Verma et al., 2018)</ref>, among others.</p><p>In this work, we focus our attention on learning disentangled representations for text, as it remains overlooked by <ref type="bibr">(John et al., 2018)</ref>. Perhaps, one of the most popular applications of disentanglement in textual data is fair classification <ref type="bibr">(Elazar and Goldberg, 2018;</ref><ref type="bibr">Barrett et al., 2019)</ref> and sentence generation tasks such as style transfer <ref type="bibr">(John et al., 2018)</ref> or conditional sentence generation <ref type="bibr">(Cheng et al., 2020b)</ref>. For fair classification, perfectly disentangled latent representations can be used to ensure fairness as the decisions are taken based on representations which are statistically independent from-or at least carrying limited information about-the protected attributes. However, there exists a trade-offs between full disentangled representations and performances on the target task, as shown by <ref type="bibr">(Feutry et al., 2018)</ref>, among others. For sequence generation and in particular, for style transfer, learning disentangled representations aim at allowing an easier transfer of the desired style. To the best of our knowledge, a depth study of the relationship between disentangled representa-tions based either on adversarial losses solely or on vCLU B ? S and quality of the generated sentences remains overlooked. Most of the previous studies have been focusing on either trade-offs between metrics computed on the generated sentences <ref type="bibr">(Tikhonov et al., 2019)</ref> or performance evaluation of the disentanglement as part of (or convoluted with) more complex modules. This enhances the need to provide a fair evaluation of disentanglement methods by isolating their individual contributions <ref type="bibr">(Yamshchikov et al., 2019;</ref><ref type="bibr">Cheng et al., 2020b)</ref>. Methods to enforce disentangled representations can be grouped into two different categories. The first category relies on an adversarial term in the training objective that aims at ensuring that sensitive attribute values (e.g. race, sex, style) as statistically independent as possible from the encoded latent representation. Interestingly enough, several works <ref type="bibr">(John et al., 2018;</ref><ref type="bibr">Elazar and Goldberg, 2018;</ref><ref type="bibr">Bao et al., 2019;</ref><ref type="bibr">Yi et al., 2020;</ref><ref type="bibr">Jain et al., 2019;</ref><ref type="bibr">Zhang et al., 2018;</ref><ref type="bibr">Hu et al., 2017)</ref>, <ref type="bibr">Elazar and Goldberg (2018)</ref> have recently shown that even though the adversary teacher seems to be performing remarkably well during training, after the training phase, a fair amount of information about the sensitive attributes still remains, and can be extracted from the encoded representation. The second category aim at minimising Mutual Information (MI) between encoded latent representation and the sensitive attribute values, i.e., without resorting to an adversarial discriminator. MI acts as an universal measure of dependence since it captures non-linear and statistical dependencies of high orders between the involved quantities <ref type="bibr">(Kinney and Atwal, 2014)</ref>. However, estimating MI has been a long-standing challenge, in particular when dealing with high-dimensional data <ref type="bibr">(Paninski, 2003;</ref><ref type="bibr">Pichler et al., 2020)</ref>. Recent methods rely on variational upper bounds. For instance, <ref type="bibr">(Cheng et al., 2020b</ref>) study vCLUB-S (Cheng et al., 2020a) for sentence generation tasks. Although this approach improves on previous state-of-the-art methods, it does not allow to fine-tuning of the desired degree of disentanglement, i.e., it enforces light or strong levels of disentanglement where only few features relevant to the input sentence remain (see <ref type="bibr">Feutry et al. (2018)</ref> for further discussion).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Our Contributions</head><p>We develop new tools to build disentangled textual representations and evaluate them on fair classifi-cation and two sentence generation tasks, namely, style transfer and conditional sentence generation. Our main contributions are summarized below:</p><p>? A novel objective to train disentangled representations from attributes. To overcome some of the limitations of both adversarial losses and vCLUB-S we derive a novel upper bound to the MI which aims at correcting the approximation error via either the <ref type="bibr">Kullback-Leibler (Ali and Silvey, 1966)</ref> or <ref type="bibr">Renyi (R?nyi et al., 1961)</ref> divergences. This correction terms appears to be a key feature to fine-tuning the degree of disentanglement compared to vCLUB-S.</p><p>? Applications and numerical results. First, we demonstrate that the aforementioned surrogate is better suited than the widely used adversarial losses as well as vCLUB-S as it can provide better disentangled textual representations while allowing fine-tuning of the desired degree of disentanglement. In particular, we show that our method offers a better accuracy versus disentanglement trade-offs for fair classification tasks. We additionally demonstrate that our surrogate outperforms both methods when learning disentangled representations for style transfer and conditional sentence generation while not suffering (or degenerating) when the number of classes is greater than two, which is an apparent limitation of adversarial training. By isolating the disentanglement module, we identify and report existing tradeoffs between different degree of disentanglement and quality of generated sentences. The later includes content preservation between input and generated sentences and accuracy on the generated style.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Main Definitions and Related Works</head><p>We introduce notations, tasks, and closely related work. Consider a training set</p><formula xml:id="formula_0">D = {(x i , y i )} n i=1</formula><p>of n sentences x i ? X paired with attribute values y i ? Y ? {1, . . . , |Y|} which indicates a discrete attribute to be disentangled from the resulting representations. We study the following scenarios: Disentangled representations. Learning disentangled representations consists in learning a model M : X ? R d that maps feature inputs X to a vector of dimension d that retains as much as possible information of the original content from the input sentence but as little as possible about the undesired attribute Y . In this framework, content is defined as any relevant information present in X that does not depend on Y .</p><p>Applications to binary fair classification. The task of fair classification through disentangled representations aims at building representations that are independent of selective discrete (sensitive) attributes (e.g., gender or race). This task consists in learning a model M : X ? {0, 1} that maps any input x to a label l ? {0, 1}. The goal of the learner is to build a predictor that assigns each x to either 0 or 1 "oblivious" of the protected attribute y. Recently, much progress has been made on devising appropriate means of fairness, e.g., <ref type="bibr">(Zemel et al., 2013;</ref><ref type="bibr">Zafar et al., 2017;</ref><ref type="bibr">Mohri et al., 2019)</ref>. In particular, <ref type="bibr">(Xie et al., 2017;</ref><ref type="bibr">Barrett et al., 2019;</ref><ref type="bibr">Elazar and Goldberg, 2018)</ref> approach the problem based on adversarial losses. More precisely, these approaches consist in learning an encoder that maps x into a representation vector h x , a critic C ?c which attempts to predict y, and an output classifier f ? d used to predict l based on the observed h x . The classifier is said to be fair if there is no statistical information about y that is present in h x <ref type="bibr">(Xie et al., 2017;</ref><ref type="bibr">Elazar and Goldberg, 2018)</ref>.</p><p>Applications to conditional sentence generation. The task of conditional sentence generation consists in taking an input text containing specific stylistic properties to then generate a realistic (synthetic) text containing potentially different stylistic properties. It requests to learn a model M : X ? Y ? X that maps a pair of inputs (x, y t ) to a sentence x g , where the outcome sentence should retain as much as possible of the original content from the input sentence while having (potentially a new) attribute y g . Proposed approaches to tackle textual style transfer <ref type="bibr">(Zhang et al., 2020;</ref><ref type="bibr">Xu et al., 2019)</ref> can be divided into two main categories. The first category <ref type="bibr">(Prabhumoye et al., 2018;</ref><ref type="bibr">Lample et al., 2018)</ref> uses cycle losses based on back translation <ref type="bibr">(Wieting et al., 2017)</ref> to ensure that the content is preserved during the transformation. Whereas, the second category look to explicitly separate attributes from the content. This constraint is enforced using either adversarial training <ref type="bibr">(Fu et al., 2017;</ref><ref type="bibr">Hu et al., 2017;</ref><ref type="bibr">Zhang et al., 2018;</ref><ref type="bibr">Yamshchikov et al., 2019)</ref> or MI minimisation using vCLUB-S <ref type="bibr">(Cheng et al., 2020b)</ref>. Traditional adversarial training is based on an encoder that aims to fool the adversary discriminator by removing attribute information from the content embedding <ref type="bibr">(Elazar and Goldberg, 2018)</ref>. As we will observe, the more the representations are disentangled the easier is to transfer the style but at the same time the less the content is preserved. In order to approach the sequence generation tasks, we build on the Style-embedding Model by <ref type="bibr">(John et al., 2018)</ref> (StyleEmb) which uses adversarial losses introduced in prior work for these dedicated tasks. During the training phase, the input sentence is fed to a sentence encoder, namely f ?e , while the input style is fed to a separated style encoder, namely f s ?e . During the inference phase, the desired style-potentially different from the input style-is provided as input along with the input sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model and Training Objective</head><p>This section describes the proposed approach to learn disentangled representations. We first review MI along with the model overview and then, we derive the variational bound we will use, and discuss connections with adversarial losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Overview</head><p>The MI is a key concept in information theory for measuring high-order statistical dependencies between random quantities. Given two random variables Z and Y , the MI is defined by</p><formula xml:id="formula_1">I(Z; Y ) = E ZY log p ZY (Z, Y ) p Z (Z)p Y (Y ) ,<label>(1)</label></formula><p>where p ZY is the joint probability density function (pdf) of the random variables (Z, Y ), with p Z and p Y representing the respective marginal pdfs. MI is related to entropy h(Y ) and conditional entropy h(Y |Z) as follows:</p><formula xml:id="formula_2">I(Z; Y ) = h(Y ) ? h(Y |Z).<label>(2)</label></formula><p>Our models for fair classification and sequence generation share a similar structure. These rely on an encoder that takes as input a random sentence X and maps it to a random representation Z using a deep encoder denoted by f ?e . Then, classification and sentence generation are performed using either a classifier or an auto-regressive decoder denoted by f ? d . We aim at minimizing MI between the latent code represented by the Random Variable (RV) Z = f ?e (X) and the desired attribute represented by the RV Y . The objective of interest L(f ?e ) is defined as:</p><formula xml:id="formula_3">L(f ?e ) ? L down. (f ?e ) downstream task +? ? I(f ?e (X); Y ) disentangled ,<label>(3)</label></formula><p>where L down. represents a downstream specific (target task) loss and ? is a meta-parameter that controls the sensitive trade-off between disentanglement (i.e., minimizing MI) and success in the downstream task (i.e., minimizing the target loss). In Sec. 5, we illustrate theses different trade-offs. Applications to fair classification and sentence generation. For fair classification, we follow standard practices and optimize the cross-entropy between prediction and ground-truth labels. In the sentence generation task L down. represents the negative log-likelihood between individual tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A Novel Upper Bound on MI</head><p>Estimating the MI is a long-standing challenge as the exact computation <ref type="bibr">(Paninski, 2003)</ref> is only tractable for discrete variables, or for a limited family of problems where the underlying datadistribution satisfies smoothing properties, see recent work by <ref type="bibr">(Pichler et al., 2020)</ref>. Different from previous approaches leading to variational lower bounds <ref type="bibr">(Belghazi et al., 2018;</ref><ref type="bibr">Hjelm et al., 2018;</ref><ref type="bibr">Oord et al., 2018)</ref>, in this paper we derive an estimator based on a variational upper bound to the MI which control the approximation error based on the Kullback-Leibler and the Renyi divergences <ref type="bibr">(Daudel et al., 2020)</ref>.</p><p>Theorem 1 (Variational upper bound on MI) Let (Z, Y ) be an arbitrary pair of RVs with (Z, Y ) ? p ZY according to some underlying pdf, and let q Y |Z be a conditional variational distribution on the attributes satisfying p ZY p Z ? q Y |Z , i.e., absolutely continuous. Then, we have that</p><formula xml:id="formula_4">I(Z; Y ) ? E Y ? log q Y |Z (Y |z)p Z (z)dz + E Y Z log q Y |Z (Y |Z) + KL p ZY p Z ? q Y |Z ,<label>(4)</label></formula><p>where KL p ZY p Z ? q Y |Z denotes the KL divergence. Similarly, we have that for any ? &gt; 1,</p><formula xml:id="formula_5">I(Z; Y ) ? E Y ? log q Y |Z (Y |z)p Z (z)dz + E Y Z log q Y |Z (Y |Z) + D ? p ZY p Z ? q Y |Z ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_6">(? ? 1)D ? p ZY p Z ? q Y |Z = log E ZY [ R ??1 (Z, Y )] denotes the Renyi divergence and R(z, y) = p Y |Z (y|z) q Y |Z (y|z) , for (z, y) ? Supp(p ZY ).</formula><p>Proof: The upper bound on H(Y ) is a direct application of the the <ref type="bibr">(Donsker and Varadhan, 1985)</ref> representation of KL divergence while the lower bound on H(Y |Z) follows from the monotonicity property of the function: ? ? D ? p ZY p Z ?q Y |Z . Further details are relegated to Appendix A.</p><p>Remark: It is worth to emphasise that the KL divergence in equation 4 and Renyi divergence in equation 5 control the approximation error between the exact entropy and its corresponding bound.</p><p>From theoretical bounds to trainable surrogates to minimize MI: It is easy to check that the inequalities in (Eq. 4) and (Eq. 5) are tight provided that p ZY ? p Z ? q Y |Z almost surely for some adequate choice of the variational distribution. However, the evaluation of these bounds requires to obtain an estimate of the density-ratio R(z, y). Density-ratio estimation has been widely studied in the literature (see <ref type="bibr">(Sugiyama et al., 2012)</ref> and references therein) and confidence bounds has been reported by <ref type="bibr">(Kpotufe, 2017)</ref> under some smoothing assumption on underlying data-distribution p ZY . In this work, we will estimate this ratio by using a critic C ? R which is trained to differentiate between a balanced dataset of positive i.i.d samples coming from p ZY and negative i.i.d samples coming from q Y |Z ? p Z . Then, for any pair (z, y), the densityratio can be estimated by R(z, y) ? ?(C ? R (z,y)) 1??(C ? R (z,y)) , where ?(?) indicates the sigmoid function and C ? R (z, y) is the unnormalized output of the critic. It is worth to mention that after estimating this ratio, the previous upper bounds may not be strict bounds so we will refer them as surrogates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison to existing methods</head><p>Adversarial approaches: In order to enhance our understanding of why the proposed approach based on the minimization of the MI using our variational upper bound in Th. 1 may lead to a better training objective than previous adversarial losses, we discuss below the explicit relationship between MI and cross-entropy loss. Let Y ? Y denote a random attribute and let Z be a possibly highdimensional representation that needs to be disentangled from Y . Then,</p><formula xml:id="formula_7">I(Z; Y ) ? H(Y ) ? E Y Z log q Y |Z (Y |Z) = Const ? CE( Y |Z),<label>(6)</label></formula><p>where CE( Y |Z) denotes the cross-entropy corresponding to the adversarial discriminator q Y |Z , not-ing that Y comes from an unknown distribution on which we have no influence H(Y ) is an unknown constant, and using that the approximation error:</p><formula xml:id="formula_8">KL q ZY q Y |Z ?p Z = CE( Y |Z)?H(Y |Z).</formula><p>Eq. 6 shows that the cross-entropy loss leads to a lower bound (up to a constant) on the MI. Although the cross-entropy can lead to good estimates of the conditional entropy, the adversarial approaches for classification and sequence generation by <ref type="bibr">(Barrett et al., 2019;</ref><ref type="bibr">John et al., 2018)</ref> which consists in maximizing the cross-entropy, induces a degeneracy (unbounded loss) as ? increases in the underlying optimization problem. As we will observe in next section, our variational upper bound in Th. 1 can overcome this issue, in particular for |Y| &gt; 2.</p><p>vCLUB-S: Different from our method, Cheng et al. (2020a) introduce I vCLUB which is an upper bound on MI defined by</p><formula xml:id="formula_9">I vCLUB (Y ; Z) =E Y Z [log p Y |Z (Y |Z)] ? E Y E Z [log p Y |Z (Y |Z)].<label>(7)</label></formula><p>It would be worth to mention that this bound follows a similar approach to the previously introduced bound in <ref type="bibr">(Feutry et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Fair classification task. We follow the experimental protocol of (Elazar and Goldberg, 2018). The main task consists in predicting a binary label representing either the sentiment (positive/negative) or the mention. The mention task aims at predicting if a tweet is conversational. Here the considered protected attribute is the race. The dataset has been automatically constructed from DIAL corpus (Blodgett et al., 2016) which contained race annotations over 50 Million of tweets. Sentiment tweets are extracted using a list of predefined emojis and mentions are identified using @mentions tokens. The final dataset contains 160k tweets for the training and two splits of 10K tweets for validation and testing. Splits are balanced such that the random estimator is likely to achieve 50% accuracy. Style Transfer For our sentence generation task, we conduct experiments on three different datasets extracted from restaurant reviews in Yelp. The first dataset, referred to as SYelp, contains 444101, 63483, and 126670 labelled short reviews (at most 20 words) for train, validation, and test, respectively. For each review a binary label is assigned depending on its polarity. Following <ref type="bibr">(Lample et al., 2018)</ref>, we use a second version of Yelp, referred to as FYelp, with longer reviews (at most 70 words). It contains five coarse-grained restaurant category labels (e.g., Asian, American, Mexican, Bars and Dessert). The multi-category FYelp is used to access the generalization capabilities of our methods to a multi-class scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Metrics for Performance Evaluation</head><p>Efficiency measure of the disentanglement methods. (Barrett et al., 2019) report that offline classifiers (post training) outperform clearly adversarial discriminators. We will re-training a classifier on the latent representation learnt by the model and we will report its accuracy. Measure of performance within the fair classification task. In the fair classification task we aim at maximizing accuracy on the target task and so we will report the corresponding accuracy.</p><p>Measure of performance within sentence generation tasks. Sentences generated by the model are expected to be fluent, to preserve the input content and to contain the desired style. For style transfer, the desired style is different from the input style while for conditional sentence generation, both input and output styles should be similar. Nevertheless, automatic evaluation of generative models for text is still an open problem. We measure the style of the output sentence by using a fastText classifier <ref type="bibr">(Joulin et al., 2016b)</ref>. For content preservation, we follow (John et al., 2018) and compute both: (i) the cosine measure between source and generated sentence embeddings, which are the concatenation of min, max, and mean of word embedding (sentiment words removed), and (ii) the BLEU score between generated text and the input using SACRE-BLEU from <ref type="bibr">(Post, 2018)</ref>. Motivated by previous work, we evaluate the fluency of the language with the perplexity given by a GPT-2 (Radford et al., 2019) pretrained model performing fine-tuning on the training corpus. We choose to report the logperplexity since we believe it can better reflects the uncertainty of the language model (a small variation in the model loss would induce a large change in the perplexity due to the exponential term). Besides the automatic evaluation, we further test our disentangled representation effectiveness by human evaluation results are presented in Tab. 1. Conventions and abbreviations. Adv refers to a model trained using the adversarial loss; vCLUB-S, KL refers to a model trained using the vCLUB-S and KL surrogate (see Eq. 14) respectively; and D ? refers to a model trained based on the ?-Renyi surrogate (Eq. 15), for ? ? {1.3, 1.5, 1.8}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Numerical Results</head><p>In this section, we present our results on the fair classification and binary sequence generation tasks, see Ssec. 5.1 and Ssec. 5.2, respectively. We additionally show that our variational surrogates to the MI-contrarily to adversarial losses-do not suffer in multi-class scenarios (see Ssec. 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Applications to Fairness</head><p>Upper bound on performances. We first examine how much of the protected attribute we can be recovered from an unfair classifier (i.e., trained without adversarial loss) and how well does such classifier perform. Results are reported in <ref type="figure">Fig. 1</ref>. We observe that we achieve similar scores than the ones reported in previous studies <ref type="bibr">(Barrett et al., 2019;</ref><ref type="bibr">Elazar and Goldberg, 2018)</ref>. This experiment shows that, when training to solve the main task, the classifier learns information about the protected attribute, i.e., the attacker's accuracy is better than random guessing. In the following, we compare the different proposed methods to disentangle representations and obtain a fairer classifier.</p><p>Methods comparisons. <ref type="figure">Fig. 1</ref> shows the results of the different models and illustrates the trade-offs between disentangled representations and the target task accuracy. Results are reported on the testset for both sentiment and mention tasks when race is the protected. We observe that the classifier trained with an adversarial loss degenerates for ? &gt; 5 since the adversarial term in Eq. 3 is influencing much the global gradient than the downstream term (i.e., cross-entropy loss between predicted and golden distribution). Remarkably, both models trained to minimize either the KL or the Renyi surrogate do not suffer much from the aforementioned multiclass problem. For both tasks, we observe that the KL and the Renyi surrogates can offer better disentangled representations than those induced by adversarial approaches. In this task, both the KL and Renyi achieve perfect disentangled representations (i.e., random guessing accuracy on protected attributes) with a 5% drop in the accuracy of the target task, when perfectly masking the protected attributes. As a matter of fact, we ob-serve that vCLUB-S provides only two regimes: either a "light" protection (attacker accuracy around 60%), with almost no loss in task accuracy (? &lt; 1), or a strong protection (attacker accuracy around 50%), where a few features relevant to the target task remain. 1 On the sentiment task, we can draw similar conclusions. However, the Renyi's surrogate achieves slightly better-disentangled representations. Overall, we can observe that our proposed surrogate enables good control of the degree of disentangling. Additionally, we do not observe a degenerated behaviour-as it is the case with adversarial losses-when ? increases. Furthermore, our surrogate allows simultaneously better disentangled representations while preserving the accuracy of the target task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Applications to binary polarity transfer</head><p>In the previous section, we have shown that the proposed surrogates do not suffer from limitations of adversarial losses and allow to achieve better disentangled representations than existing methods relying on vCLUB-S. Disentanglement modules are a core block for a large number of both style transfer and conditional sentence generation algorithms <ref type="bibr">(Tikhonov et al., 2019;</ref><ref type="bibr">Yamshchikov et al., 2019;</ref><ref type="bibr">Fu et al., 2017)</ref> that place explicit constraints to force disentangled representations. First, we assess the disentanglement quality and the control over desired level of disentanglement while changing the downstream term, which for the sentence generation task is the cross-entropy loss on individual token. Then, we exhibit the existing trade-offs between quality of generated sentences, measured by the metric introduced in Ssec. 4.2, and the resulting degree of disentanglement. The results are presented for SYelp <ref type="figure" target="#fig_1">Fig. 2a</ref> shows the adversary accuracy of the different methods as a function of ?. Similarly to the fair classification task, a fair amount of information can be recovered from the embedding learnt with adversarial loss. In addition, we observe a clear degradation of its performance for values ? &gt; 1. In this setting, the Renyi surrogates achieves consistently better results in terms of disentanglement than the one minimizing the KL surrogate. The curve for Renyi's surrogates shows that exploring different values of ? allows good control of the  <ref type="figure">Figure 1</ref>: Numerical results on fair classification. Trade-offs between target task and attacker accuracy are reported in <ref type="figure">Fig. 1a, Fig. 1b</ref> for mention task, and <ref type="figure">Fig. 1c, Fig. 1d</ref> for sentiment task. For low values of ? some points coincide.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Evaluating disentanglement</head><p>As ? increases the level of disentanglement increases and the proposed methods using both KL (KL) and Reny divergences (D ? ) clearly offer better control than existing methods. disentanglement degree. Renyi surrogate generalizes well for sentence generation. Similarly to the fairness task vCLUB-S only offers two regimes: "light" disentanglement with very little polarity transfer and "strong" disentanglement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Disentanglement in Polarity Transfer</head><p>The quality of generated sentences are evaluated using the fluency (see <ref type="figure">Fig. 3c</ref> ), the content preservation (see <ref type="figure">Fig. 3a</ref>), additional results using a cosine similarity are given in Appendix D, and polarity accuracy (see <ref type="figure">Fig. 3b</ref> ). For style transfer, and for all models, we observe trade-offs between disentanglement and content preservation (measured by BLEU) and between fluency and disentanglement. Learning disentangled representations leads to poorer content preservation. As a matter of fact, similar conclusions can be drawn while measuring content with the cosine similarity (see Appendix D). For polarity accuracy, in non-degenerated cases (see below), we observe that the model is able to better transfer the sentiment in presence of disentangled representations. Transferring style is easier with disentangled representations, however there is no free lunch here since disentangling also re-moves important information about the content. It is worth noting that even in the "strong" disentanglement regime vCLUB-S struggles to transfer the polarity (accuracy of 40% for ? ? {1, 2, 10, 15}) where other models reach 80%. It is worth noting that similar conclusions hold for two different sentence generation tasks: style transfer and conditional generation, which tends to validate the current line of work that formulates text generation as generic text-to-text <ref type="bibr">(Raffel et al., 2019)</ref>. Quality of generated sentences. Examples of generated sentences are given in Tab. 2 , providing qualitative examples that illustrate the previously observed trade-offs. The adversarial loss degenerates for values ? ? 5 and a stuttering phenomenon appears <ref type="bibr">(Holtzman et al., 2019)</ref>. Tab. 1 gathers results of human evaluation and show that our surrogates can better disentangle style while preserving more content than available methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Adversarial Loss Fails to Disentangle when |Y| ? 3</head><p>In <ref type="figure" target="#fig_1">Fig. 2b</ref> we report the adversary accuracy of our different methods for the values of ? using FYelp   <ref type="figure">Figure 3</ref>: Numerical experiments on binary style transfer. Quality of generated sentences are evaluated using BLEU <ref type="figure">(Fig. 3a)</ref>; style transfer accuracy <ref type="figure">(Fig. 3a)</ref>; sentence fluency ( <ref type="figure">Fig. 3c</ref>). We report existing trade-offs between disentanglement and sentence generation quality. Human evaluation is reported in Tab. 1.</p><p>dataset with category label. In the binary setting for ? ? 1, models using adversarial loss can learn disentangled representations while in the multi-class setting, the adversarial loss degenerates for small values of ? (i.e sentences are no longer fluent as shown by the increase in perplexity in <ref type="figure">Fig. 4c</ref>).</p><p>Minimizing MI based on our surrogates seems to mitigate the problem and offer a better control of the disentanglement degree for various values of ? than vCLU B ? S. Further results are gathered in Appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summary and Concluding Remarks</head><p>We devised a new alternative method to adversarial losses capable of learning disentangled textual representation. Our method does not require adversarial training and hence, it does not suffer in presence of multi-class setups. A key feature of this method is to account for the approximation error incurred when bounding the mutual information. Experiments show better trade-offs than both adversarial training and vCLUB-S on two fair classification tasks and demonstrate the efficiency to learn disentangled representations for sequence generation.</p><p>As a matter of fact, there is no free-lunch for sentence generation tasks: although transferring style is easier with disentangled representations, it also removes important information about the content.</p><p>The proposed method can replace the adversary in any kind of algorithms <ref type="bibr">(Tikhonov et al., 2019;</ref><ref type="bibr">Fu et al., 2017)</ref>  In this section, we provide a formal proof of the Eq. 6. Let (Z, Y ) be an arbitrary pair of RVs with (Z, Y ) ? p ZY according to some underlying pdf, and let q Y |Z be a conditional variational probability distribution on the discrete attributes satisfying p ZY p Z ? q Y |Z , i.e., absolutely continuous.</p><formula xml:id="formula_10">I(Z; Y ) ? H(Y ) ? CE( Y |Z).<label>(8)</label></formula><p>Proof: We start by the definition of the MI and use the fact that the maximum entropy distribution is reached for the uniform law in the case of a discrete variable (see <ref type="bibr">(Cover and Thomas, 2006)</ref>).</p><formula xml:id="formula_11">I(Z; Y ) = H(Y ) ? H(Y |Z) (9) = Const ? H(Y |Z).<label>(10)</label></formula><p>We then need to find the relationship between the cross-entropy and the conditional entropy.</p><formula xml:id="formula_12">KL(p Y Z q Y Z ) = E Y Z log p Y |Z (Y |Z) q Y |Z (Y |Z) = E Y Z log p Y |Z (Y |Z) ? E Y Z log q Y |Z (Y |Z) = ?H(Y |Z) + CE( Y |Z).<label>(11)</label></formula><p>We know that KL(p Y Z q Y Z ) ? 0, thus CE( Y |Z) ? H(Y |Z) which gives the result. The underlying hypothesis made by approximating the MI with an adversarial loss is that the contribution of gradient from KL(p Y Z q Y Z ) to the bound is negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Th. 1</head><p>Let (Z, Y ) be an arbitrary pair of RVs with (Z, Y ) ? p ZY according to some underlying pdf, and let q Y |Z be a conditional variational probability distribution satisfying p ZY p Z ? q Y |Z , i.e., absolutely continuous. To obtain an upper bound on the MI we need to upper bound the entropy H(Y ) and to lower bound the conditional entropy H(Y |Z).</p><p>Upper bound on H(Y ). Since the KL divergence is non-negative, we have</p><formula xml:id="formula_13">H(Y ) ? E Y [? log q Y (Y )]<label>(12)</label></formula><formula xml:id="formula_14">= E Y ? log q Y |Z (Y |z)p Z (z)dz . (13)</formula><p>Lower bounds on H(Y |Z). We have the following inequalities:</p><formula xml:id="formula_15">H(Y |Z) = E Y Z ? log q Y |Z (Y |Z) ? KL(p Y Z p Z ? q Y |Z ),<label>(14)</label></formula><p>where KL(p Y Z p Z ? q Y |Z ) denotes the KL divergence. Furthermore, for arbitrary values ? &gt; 1,</p><formula xml:id="formula_16">H(Y |Z) ?E Y Z ? log q Y |Z (Y |Z) ? D ? (p Y Z p Z ? q Y |Z ),<label>(15)</label></formula><formula xml:id="formula_17">where D ? (p Y Z p Z ? q Y |Z ) = 1 ? ? 1 log E ZY R ??1 (Z, Y )</formula><p>is the Renyi divergence with</p><formula xml:id="formula_18">R(y, z) = p Y |Z (y|z) q Y |Z (y|z) .</formula><p>The proof of Eq. 14 is given in Ssec. A.1. In order to show Eq. 15, we remark that Renyi divergence is non-decreasing function ? ? D ? (p ZY p Z ? q Y |Z ) in ? ? [0, +?) (the reader is refereed to (Van Erven and Harremos, 2014) for a detailed proof). Thus, we have ?? &gt; 1,</p><formula xml:id="formula_19">KL(p ZY p Z ?q Y |Z ) ? D ? (p ZY p Z ?q Y |Z ). (16)</formula><p>Therefore, from expression Eq. 14 we obtain the desired result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Optimization of the Surrogates on MI</head><p>In this section, we give details to facilitate the practical implementation of our methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 Computing the entropy H(Y )</head><formula xml:id="formula_20">H(Y ) ? E Y ? log q Y |Z (Y |z)p Z (z)dz ? E Y ? log n i=1 q Y |Z (Y |z i ) + const. ? ? 1 |Y| |Y| j=1 log n i=1 C ?c (z i ) y j + const.</formula><p>(17) where C ?c (z i ) y j is the y j -th component of the normalised output of the classifier C ?c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2 Computing the lower bound on H(Y |Z)</head><p>The upper bound helds for ? &gt; 1,</p><formula xml:id="formula_21">H(Y |Z) ? CE(Y |Z) ? D ? (p ZY p Z ? q Y |Z ) ? ? 1 n n i=1 log q Y |Z (y i |z i )? 1 ? ? 1 log n i=1 R ??1 (z i , y i ).<label>(18)</label></formula><p>Estimating the density-ratio R(z, y) In what follows we apply the so-called density-ratio trick to our specific setup. Suppose we have a bal-</p><formula xml:id="formula_22">anced dataset {(y p i , z p i )} ? p Y Z and {(y q i , z q i )} ? q Y |Z p Z with i ? [1, K].</formula><p>The density-ratio trick consists in training a classifier C ? R to distinguish between theses two distribution. Samples coming from p are labelled u = 1, samples coming from q are labelled u = 0. Thus, we can rewrite R(z, y) as</p><formula xml:id="formula_23">R(z, y) = p Y |Z (y, z) q Y |Z (y, z) (19) = p Y Z|U (y, z|u = 0) p Y Z|U (y, z|u = 1) (20) = p U |Y Z (u = 0|y, z) p U |Y Z (u = 1|y, z) p U (u = 1) p U (u = 0) (21) = p U |Y Z (u = 0|y, z) p U |Y Z (u = 1|y, z) (22) = p U |Y Z (u = 0|y, z) 1 ? p U |Y Z (u = 0|y, z) .<label>(23)</label></formula><p>Obviously, the true posterior distribution p U |Y Z is unknown. However, if C ? R is well trained, then p U |Y Z (u = 0|y, z) ? ?(C ? R (y, z)), where ?(?) denotes the sigmoid function. A detailled procedure for training is given in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Details on the Model B.1 Baseline Schemas</head><p>We report in <ref type="figure">Fig. 7</ref> the schema of the proposed approach as well as the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Architecture Hyerparameters</head><p>We use an encoder parameterized by a 2-layer bidirectional GRU <ref type="bibr">(Chung et al., 2014)</ref> and a 2-layer decoder GRU. Both GRU and our word embedding lookup tables, trained from scratch, and have  <ref type="figure">Figure 4</ref>: Numerical experiments on multiclass style transfer using categorical labels. Results include: BLEU <ref type="figure">(Fig. 4a)</ref>); style transfer accuracy <ref type="figure">(Fig. 4b)</ref>; sentence fluency <ref type="figure">(Fig. 4c</ref>).</p><p>Algorithm 1 Our method for the fair classification task INPUT: training dataset for the encoder D n = {(x 1 , y 1 , l 1 ), . . . , (x n , y n , l n )}, batch size m, training dataset for the classifiers and decoder D n = {(x 1 , y 1 , l 1 ), . . . , (x n , y n , l n )}. Update ? e with B using Eq. 3 with ? d . 10: end while OUTPUT: f ?e , f ? d a dimension of 128 (as already reported by <ref type="bibr">(Garcia et al., 2019)</ref>, building experiments on higher dimensions produces marginal improvement). The style embedding is set to a dimension of 8. The attribute classifier are MLP and are composed of 3 layer MLP with 128 hidden units and LeakyReLU <ref type="bibr">(Xu et al., 2015)</ref> activations, the dropout <ref type="bibr">(Srivastava et al., 2014)</ref> rate is set to 0.1. All models are optimised with <ref type="bibr">AdamW (Kingma and Ba, 2014;</ref><ref type="bibr">Loshchilov and Hutter, 2017)</ref> with a learning rate of 10 ?3 and the norm is clipped to 1.0. Our model's hyperparameters have been set by a preliminary training on each downstream task: a simple classifier for the fair classification and a vanilla seq2seq <ref type="bibr">(Sutskever et al., 2014;</ref><ref type="bibr">Colombo et al., 2020</ref>) for the conditional generation task. The models re-  </p><formula xml:id="formula_24">Initialization: parameters (? e , ? R , ? c , ? d ) of the encoder f ?e , classifiers C ? R , C ?c , f ? d Optimization: 1: while (? e , ? R , ? c , ? d ) not converged do 2: for i ? [1, U nroll] do Train C ?c , C ? R , f ? d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Details on the experimental Setup</head><p>In this section, we provide additional details on the metric used for evaluating the different models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Content Preservation: BLEU &amp; Cosine Similarity</head><p>Content preservation is an important aspect of both conditional sentence generation and style transfer. We provide here the implementation details regarding the implemented metrics. BLEU. For computing the BLEU score we choose to use the corpus level method provided in python sacrebleu (Post, 2018) library https:// github.com/mjpost/sacrebleu.git. It produces the official WMT scores while working with plain text.</p><p>Cosine Similarity. For the cosine similarity, we follow the definition of <ref type="bibr">John et al. (2018)</ref> by taking the cosinus between source and generated sentence embedding. For computing the embedding we rely on the bag of word model  </p><formula xml:id="formula_25">C ? R f ? e R(z, y) C ? c (z,?) ? q Z? (z, y) ? p ZY f ? d (z, y, l) ? p ZY Ll z (a) Classifier with our MI surrogate C ? R f ? e R(z, y) C ? c (z,?) ? q Z? (z, y) ? p ZY f ? d x ? p Xx z y ? p Y s f ? s e (b)</formula><formula xml:id="formula_26">f ? e C ? c? f ? d (z, y, l) ? p ZY Ll z L down.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H(Y |Z)</head><p>(a) Classifier with adversarial loss from <ref type="bibr">(Elazar and Goldberg, 2018)</ref>  <ref type="bibr">(John et al., 2018)</ref> Figure 7: Baselines methods, theses models use an adversarial loss for disentanglement. f ?e represents the input sentence encoder; f s ?e denotes the style encoder (only used for sentence generation tasks); C ?c represents the adversarial classifier; f ? d represents the decoder that can be either a classifier ( <ref type="figure">Fig. 7a</ref> or a sequence decoder <ref type="figure">(Fig. 7b</ref>). Schemes of our proposed models are given in ??</p><formula xml:id="formula_27">f ? e? C ? c L down. H(Y |Z) f ? d x ? p Xx z y ? p Y s f ? s e (b) StyleEmb model from</formula><p>and take the mean pooling of word embedding. We choose to use the pre-trained word vectors provided in https://fasttext.cc/docs/ en/pretrained-vectors.html. They are trained on Wikipedia using fastText. These vectors in dimension 300 were obtained using the skip- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Fluency: Perplexity</head><p>To evaluate fluency we rely on the perplexity (Jalalzai et al., 2020), we use <ref type="bibr">GPT-2 (Radford et al., 2019)</ref> fine-tuned on the training corpus. GPT-2 is pre-trained on the BookCorpus dataset (Zhu et al., 2015) (around 800M words). The model has been taken from the HuggingFace Library <ref type="bibr">(Wolf et al., 2019)</ref>. Default hyperparameters have been used for the finetuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Style Conservation/Transfer</head><p>For style conservation (Colombo et al., 2019) (e.g., polarity, gender or category) we train a fasttext <ref type="bibr">(Bojanowski et al., 2017;</ref><ref type="bibr">Joulin et al., 2016a,b)</ref> classifier https://fasttext.cc/docs/ en/supervised-tutorial.html. We use the validation corpus to select the best model. Preliminary comparisons with deep classifiers (based on either convolutionnal layers or recurrent layers) show that fasttext obtains similar result while being litter and faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Disentanglement</head><p>For disentanglement, we follow common practice <ref type="bibr">(Lample et al., 2018)</ref> and implement a two layers perceptron <ref type="bibr">(Rosenblatt, 1958)</ref>. We use LeakyRelu <ref type="bibr">(Xu et al., 2015)</ref> as activation functions and set the dropout (Srivastava et al., 2014) rate to 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Results on Sentiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Binary Sentence Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1.1 Human Evaluation</head><p>In Tab. 1, we report the performances of systems when evaluated by humans on the polarity transfer task. 100 sentences are generated by each system and 3 english native speakers are asked to annotate each sentence along 3 dimensions (i.e fluency, sentiment and content preservation). Turkers assign binary labels to fluency and sentiment (following the protocol introduced in Jalalzai et al. <ref type="formula" target="#formula_2">(2020)</ref>) while content is evaluated on a likert scale from 1-5. For content preservation, both the input sentence and the generated sentence are provided to the turker. The annotator agreement is measure by  the Krippendorff Alpha 2 <ref type="bibr">(Krippendorff, 2018)</ref>. The Krippendorff Alpha is: ? = 0.54 on the sentiment classification, ? = 0.20 for fluency and ? = 0.18 for content preservation. <ref type="figure" target="#fig_9">Fig. 8</ref> measures the content preservation measured using cosine similarity for the sentence generation task using sentiment labels. As with the BLEU score, we observe that as the learnt representation becomes more entangled (? increases) less content is preserved. Similarly to BLEU the model using the KL bound conserves outperforms other models 2 Krippendorff Alpha measures of inter-rater reliability in [0, 1]: 0 is perfect disagreement and 1 is perfect agreement. in terms of content preservation for ? &gt; 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Content preservation using Cosine Similarity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Example of generated sentences</head><p>Tab. 2 gathers some sentences generated by the different sentences for different values of ?.</p><p>Style transfert. From Tab. 2, we can observe that the impact of disentanglement on a qualitative point of view. For small values of ? the models struggle to do the style transfer (see example 2 for instance). As ? increases disentanglement becomes easier, however, the content becomes more generic which is a known problem (see <ref type="bibr">(Li et al., 2015)</ref> for instance).</p><p>Example of "degeneracy" for large values of ?. For sentences generated with the baseline model a repetition phenomenon appears for greater values of ?. For certain sentences, models ignore the style token (i.e., the sentence generated with a positive sentiment is the same as the one generated with the negative sentiment). We attribute this degeneracy to the fact that the model is only trained with (x i , y i ) sharing the same sentiment which appears to be an intrinsic limitation of the model introduced by <ref type="bibr">(John et al., 2018)</ref>.</p><p>Analysis of performances of vCLUB-S Similarly to what can be observed with automatic evaluation Tab. 2 shows that the system based on vCLUB-S has only two regimes: "light" disentanglement and strong disentanglement. With light disentanglement the decoder fail at transferring the polarity and for strong disentanglement few content features remain and the system tends to output generic sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Results on Multi class Sentence Generation</head><p>Results on the multi-class style transfer and on are reported in <ref type="figure">Fig. 4b</ref> Similarly than in the binary case there exists a trade-off between content preservation and style transfer accuracy. We observe that the BLEU score in this task is in a similar range than the one in the gender task, which is expected because data come from the same dataset where only the labels changed. it's not good, but the prices are good. D ?=1.5 it's not very good, and the service was terrible. D ?=1. <ref type="bibr">8</ref> it was a very disappointing experience and the food was awful. it was a little overpriced and not very good. D ?=1.5 it's a shame, and the service is horrible. D ?=1.8 it's not worth the $ NUM.  In <ref type="figure" target="#fig_11">Fig. 9</ref>, we report the adversary accuracy of the different methods for the values of ?. It is worth noting that gender labels are noisier than sentiment labels <ref type="bibr">(Lample et al., 2018)</ref>. We observe that the adversarial loss saturates at 55% where a model trained on MI bounds can achieve a better disentanglement. Additionally, the models trained with MI bounds allow better control of the desired degree of disentanglement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Quality of Generated Sentences</head><p>Results on the sentence generation tasks are reported in <ref type="figure">Fig. 10</ref> and in <ref type="figure">Fig. 11</ref>. We observe that for ? &gt; 1 the adversarial loss degenerates as observe in the sentiment experiments.Compared to sentiment score we observe a lower score of BLEU which can be explained by the length of the review in the FYelp dataset. On the other hand, we observe a similar trade-off between style transfer accuracy and content preservation in the non degenerated case: as style transfer accuracy increases, content preservation decreases. Overall, we remark a behaviour similar to the one we observe in sentiment experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Additional Results on Multi class Sentence Generation</head><p>Results on the multi-class style transfer and on conditional sentence generation are reported in <ref type="figure">Fig. 4b</ref> and ??. Similarly than in the binary case there exists a trade-off between content preservation and style transfer accuracy. We observe that the BLEU score in this task is in a similar range than the one in the gender task, which is expected because data come from the same dataset where only the labels changed.  <ref type="figure">Figure 10</ref>: Numerical experiments on binary style transfer using gender labels. Results include: BLEU <ref type="figure">(Fig. 10a)</ref>; cosine similarity <ref type="figure">(Fig. 10d)</ref>; style transfer accuracy <ref type="figure">(Fig. 10b)</ref>; sentence fluency ( <ref type="figure">Fig. 10c</ref>).  <ref type="figure">Figure 11</ref>: Numerical experiments on conditional sentence generation using gender labels. Results includes: BLEU <ref type="figure">(Fig. 11a)</ref>; cosine similarity <ref type="figure">(Fig. 11d)</ref>; style transfer accuracy <ref type="figure">(Fig. 11b)</ref>; sentence fluency <ref type="figure">(Fig. 11c)</ref>.  <ref type="figure" target="#fig_1">Figure 12</ref>: Numerical experiments on the multi-class conditionnal sentence generation. Results include: BLEU <ref type="figure" target="#fig_1">(Fig. 12a)</ref>; cosine similarity <ref type="figure" target="#fig_1">(Fig. 12d)</ref>; style transfer accuracy <ref type="figure" target="#fig_1">(Fig. 12b)</ref>; sentence fluency ( <ref type="figure" target="#fig_1">Fig. 12c</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Disentanglement of representation learnt by f ?e in the binary (left) and multi-class (i.e., |Y| = 5) (right) sentence generation scenario. In the multi-class scenario the Adv degenerates for ? ? 0.01 and offer no finedgrained control over the degree of disentanglement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Results of cosine similarity on multiclass style transfer using categorical labels quested for the classification task are trained during 100k steps while 300k steps are used for the generation task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>StyleEmb model from (John et al., 2018) with our MI surrogate</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Proposed methods. As described in Th. 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>gram model described in Bojanowski et al. (2017); Joulin et al. (2016b) with default parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Content preservation measured by the cosine similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>freshly made, very soft and flavorful. Adv it's crispy and too nice and very flavor. vCLUB-S It's freshly made, and great. KL it's a huge, crispy and flavorful. D ?=1.3 it's hard, and the flavor was flavorless. D ?=1.5 it's very dry and not very flavorful either. D ?=1.8 it's a good place for lunch or dinner. 1 Input it's freshly made, very soft and flavorful. Adv it's not crispy and not very flavorful flavor. vCLUB-S It's bad. KL it's very fresh, and very flavorful and flavor. D ?=1.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Disentanglement of the learnt embedding when training an off-line adversarial classifier for the sentence generation with gender data.F Binary Sentence Generation:Application to Gender Data F.1 Quality of the Disentanglement</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Bao, Hao Zhou, Shujian Huang, Lei Li, Lili Mou, Olga Vechtomova, Xinyu Dai, and Jiajun Chen. 2019. Generating sentences from disentangled syntactic and semantic spaces. arXiv preprint arXiv:1907.05789. Kpotufe. 2017. Lipschitz Density-Ratios, Structured Data, and Data-driven Tuning. volume 54 of Proceedings of Machine Learning Research, pages 1320-1328, Fort Lauderdale, FL, USA. PMLR. Klaus Krippendorff. 2018. Content analysis: An introduction to its methodology. Sage publications. Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683. Jonathan Mallinson, and Kevin Gimpel. 2017. Learning paraphrastic sentence embeddings from back-translated bitext. arXiv preprint arXiv:1706.01847. Nan Ding, and Radu Soricut. 2018. Shaped: Shared-private encoder-decoder for text style adaptation. arXiv preprint arXiv:1804.04093. Yi Zhang, Tao Ge, and Xu Sun. 2020. Parallel data augmentation for formality style transfer. arXiv preprint arXiv:2005.07522.</figDesc><table><row><cell>Maria Barrett, Yova Kementchedjhieva, Yanai Elazar, Desmond Elliott, and Anders S?gaard. 2019. Adver-sarial removal of demographic attributes revisited. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-guage Processing (EMNLP-IJCNLP), pages 6331-6336. arXiv:1811.03271. tional Linguistics. timber and pitch in music audio. arXiv preprint 1725, Berlin, Germany. Association for Computa-2018. Learning disentangled representations for Linguistics (Volume 1: Long Papers), pages 1715-Yun-Ning Hung, Yi-An Chen, and Yi-Hsuan Yang. nual Meeting of the Association for Computational controlled generation of text. with subword units. In Proceedings of the 54th An-arXiv:1703.00955. 2016. Neural machine translation of rare words arXiv preprint Rico Sennrich, Barry Haddow, and Alexandra Birch. Salakhutdinov, and Eric P Xing. 2017. Toward Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan preprint arXiv:1912.03915. the token to the review: A hierarchical multi-modal approach to opinion mining. arXiv preprint arXiv:1908.11216. R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. 2018. Learn-ing deep representations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751. Jun-Ting Hsieh, Bingbin Liu, De-An Huang, Li F Fei-Fei, and Juan Carlos Niebles. 2018. Learning to de-compose and disentangle representations for video tations via mutual information estimation. arXiv cessing Systems, pages 517-526. ias Ortner. 2019. Learning disentangled represen-prediction. In Advances in Neural Information Pro-Eduardo Hugo Sanchez, Mathieu Serrurier, and Math-in the brain. Psychological review, 65(6):386. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Alfr?d R?nyi et al. 1961. On measures of entropy and information. In Proceedings of the Fourth Berke-ley Symposium on Mathematical Statistics and Prob-ability, Volume 1: Contributions to the Theory of Statistics. The Regents of the University of Califor-nia. tic model for information storage and organization 27. Frank Rosenblatt. 1958. The perceptron: a probabilis-national conference on computer vision, pages 19-and reading books. In Proceedings of the IEEE inter-story-like visual explanations by watching movies Fidler. 2015. Aligning books and movies: Towards dinov, Raquel Urtasun, Antonio Torralba, and Sanja Ye Zhang, Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-</cell><cell>Pengyu Cheng, Martin Renqiang Min, Dinghan Shen, Christopher Malon, Yizhe Zhang, Yi-tong Li, and Lawrence Carin. 2020b. Improv-ing disentangled text representation learning with information-theoretic guidance. arXiv preprint arXiv:2006.00693. Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence model-ing. arXiv preprint arXiv:1412.3555. Pierre Colombo, Emile Chapuis, Matteo Manica, Em-manuel Vignon, Giovanna Varni, and Chloe Clavel. 2020. Guiding attention in sequence-to-sequence models for dialogue act prediction. In AAAI, pages 7594-7601. Pierre Colombo, Wojciech Witon, Ashutosh Modi, James Kennedy, and Mubbasir Kapadia. 2019. arXiv:1904.02793. Thomas M. Cover and Joy A. Thomas. 2006. Elements of Information Theory (Wiley Series in Telecommuni-USA. 2020. Infinite-dimensional gradient-based descent Juncen Li, Robin Jia, He He, and Percy Liang. 596. Kam?lia Daudel, Randal Douc, and Fran?ois Portier. Neural Information Processing Systems, pages 585-preprint arXiv:1510.03055. through adversarial feature learning. In Advances in tive function for neural conversation models. arXiv and Bill Dolan. 2015. A diversity-promoting objec-Graham Neubig. 2017. Controllable invariance cations and Signal Processing). Wiley-Interscience, Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, and resentations. language processing. ArXiv, abs/1910.03771. ing. In International Conference on Learning Rep-Huggingface's transformers: State-of-the-art natural Lan Boureau. 2018. Multiple-attribute text rewrit-Quentin Lhoest, and Alexander M. Rush. 2019. Affect-driven dialog generation. arXiv preprint Ludovic Denoyer, Marc'Aurelio Ranzato, and Y-Teven Le Scao, Sylvain Gugger, Mariama Drame, Guillaume Lample, Sandeep Subramanian, Eric Smith, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Samory Taku Kudo. 2018. Subword regularization: Improving neural network translation models with multiple sub-word candidates. arXiv preprint arXiv:1804.10959. Vinay Kumar Verma, Gundeep Arora, Ashish Mishra, and Piyush Rai. 2018. Generalized zero-shot learn-icz, Joe Davison, Sam Shleifer, Patrick von Platen, recognition, pages 4281-4289. ric Cistac, Tim Rault, R?mi Louf, Morgan Funtow-the IEEE conference on computer vision and pattern Chaumond, Clement Delangue, Anthony Moi, Pier-ing via synthesized examples. In Proceedings of Thomas Wolf, Lysandre Debut, Victor Sanh, Julien John Wieting, Wojciech Witon, Pierre Colombo, Ashutosh Modi, and Mubbasir Kapadia. 2018. Disney at IEST 2018: Pre-dicting emotions using an ensemble. In Proceedings tional Linguistics. 31, 2018, pages 248-253. Association for Computa-WASSA@EMNLP 2018, Brussels, Belgium, October to Subjectivity, Sentiment and Social Media Analysis, of the 9th Workshop on Computational Approaches A Additional Details On the Surrogates A.1 Proof of Inequality Eq. 6</cell></row><row><cell>Parag Jain, Abhijit Mishra, Amar Prakash Azad, and Karthik Sankaranarayanan. 2019. Unsupervised controllable text formalization. In Proceedings of Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks the AAAI Conference on Artificial Intelligence, vol-ume 33, pages 6554-6561. Hamid Jalalzai, Pierre Colombo, Chlo? Clavel, Eric from overfitting. The journal of machine learning research, 15(1):1929-1958. Sjoerd van Steenkiste, Francesco Locatello, J?rgen</cell><cell>for alpha-divergence minimisation. Working paper or preprint. 2018. Delete, retrieve, generate: A simple approach Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. to sentiment and style transfer. arXiv preprint 2015. Empirical evaluation of rectified activa-arXiv:1804.06437. tions in convolutional network. arXiv preprint Emily L Denton et al. 2017. Unsupervised learning of disentangled representations from video. In Ad-vances in neural information processing systems, pages 4414-4423. arXiv:1505.00853. Ilya Loshchilov and Frank Hutter. 2017. Decou-pled weight decay regularization. arXiv preprint Ruochen Xu, Tao Ge, and Furu Wei. 2019. Formality arXiv:1711.05101. style transfer with hybrid textual annotations. arXiv</cell></row><row><cell>Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, and Alexander Lerchner. 2018. Understanding disentan-gling in ?-vae. arXiv preprint arXiv:1804.03599. Emile Chapuis, Pierre Colombo, Matteo Manica, Matthieu Labeau, and Chlo? Clavel. 2020. Hierar-chical pre-training for sequence labelling in spoken dialog. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-ing: Findings, EMNLP 2020, Online Event, 16-20 November 2020, pages 2636-2648. Association for Computational Linguistics. Gaussier, Giovanna Varni, Emmanuel Vignon, and Anne Sabourin. 2020. Heavy-tailed representa-tions, text polarity classification &amp; data augmenta-tion. arXiv preprint arXiv:2003.11593. Schmidhuber, and Olivier Bachem. 2019. Are dis-entangled representations helpful for abstract visual reasoning? In Advances in Neural Information Pro-cessing Systems, pages 14245-14258. Vineet John, Lili Mou, Hareesh Bahuleyan, and Olga Masashi Sugiyama, Taiji Suzuki, and Takafumi Vechtomova. 2018. Disentangled representation Kanamori. 2012. Density Ratio Estimation in Ma-learning for non-parallel text style transfer. arXiv chine Learning, 1st edition. Cambridge University preprint arXiv:1808.04339. Press, USA. Armand Joulin, Edouard Grave, Piotr Bojanowski, Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Matthijs Douze, H?rve J?gou, and Tomas Mikolov. 2016a. Fasttext.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651. Sequence to sequence learning with neural networks. In Advances in neural information processing sys-tems, pages 3104-3112. Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016b. Bag of tricks for efficient text classification. arXiv preprint Alexey Tikhonov, Viacheslav Shibaev, Aleksander Na-gaev, Aigul Nugmanova, and Ivan P Yamshchikov. 2019. Style transfer for texts: Retrain, report er-arXiv:1607.01759. Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. rors, compare with rewrites. In Proceedings of the 2019 Conference on Empirical Methods in Natu-Joint Conference on Natural Language Processing ral Language Processing and the 9th International</cell><cell>Tanvi Dinkar, Pierre Colombo, Matthieu Labeau, and Chlo? Clavel. 2020. The importance of fillers for text representations of speech transcripts. In Pro-preprint arXiv:1903.06353. Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. 2019. Agnostic federated learning. arXiv Ivan P Yamshchikov, Viacheslav Shibaev, Aleksander preprint arXiv:1902.00146. Nagaev, J?rgen Jost, and Alexey Tikhonov. 2019. ceedings of the 2020 Conference on Empirical Meth-ods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 7985-7993. Association for Computational Linguistics. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Decomposing textual information for style transfer. 2018. Representation learning with contrastive pre-arXiv preprint arXiv:1909.12928. dictive coding. arXiv preprint arXiv:1807.03748. Xiaoyuan Yi, Zhenghao Liu, Wenhao Li, and Maosong Liam Paninski. 2003. Estimation of entropy and mu-Sun. 2020. Text style transfer via learning style in-MD Donsker and SRS Varadhan. 1985. Large devia-tions for stationary gaussian processes. Communi-cations in Mathematical Physics, 97(1-2):187-210. Yanai Elazar and Yoav Goldberg. 2018. Adversarial removal of demographic attributes from text data. arXiv preprint arXiv:1808.06640. Cl?ment Feutry, Pablo Piantanida, Yoshua Bengio, and Pierre Duhamel. 2018. Learning anonymized repre-sentations with adversarial neural networks. Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao, and Rui Yan. 2017. Style transfer in tual information. Neural computation, 15(6):1191-stance supported latent space. In Proceedings of 1253. the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pages 3801-3807. Georg Pichler, Pablo Piantanida, and G?nther Kolian-International Joint Conferences on Artificial Intelli-der. 2020. On the estimation of information mea-gence Organization. sures of continuous distributions. Muhammad Bilal Zafar, Isabel Valera, Manuel Matt Post. 2018. A call for clarity in reporting bleu Gomez Rodriguez, and Krishna P Gummadi. 2017. scores. arXiv preprint arXiv:1804.08771. Fairness beyond disparate treatment &amp; disparate im-pact: Learning classification without disparate mis-Shrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhut-treatment. In Proceedings of the 26th international dinov, and Alan W Black. 2018. Style trans-arXiv:1804.09000. fer through back-translation. arXiv preprint conference on world wide web, pages 1171-1180.</cell></row><row><cell>(EMNLP-IJCNLP), pages 3927-3936.</cell><cell>text: Exploration and evaluation. arXiv preprint Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and</cell></row><row><cell></cell><cell>arXiv:1711.06861. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Cynthia Dwork. 2013. Learning fair representations.</cell></row><row><cell>Tim Van Erven and Peter Harremos. 2014. R?nyi diver-</cell><cell>Dario Amodei, and Ilya Sutskever. 2019. Language volume 28 of Proceedings of Machine Learning</cell></row><row><cell>gence and kullback-leibler divergence. IEEE Trans-</cell><cell>Alexandre Garcia, Pierre Colombo, Slim Essid, Flo-models are unsupervised multitask learners. OpenAI Research, pages 325-333, Atlanta, Georgia, USA.</cell></row><row><cell>actions on Information Theory, 60(7):3797-3820.</cell><cell>rence d'Alch? Buc, and Chlo? Clavel. 2019. From Blog, 1(8):9. PMLR.</cell></row></table><note>with no modifications. Future work includes testing with other type of labels such as dialog act (Chapuis et al., 2020; Colombo et al., 2020), emotions (Witon et al., 2018), opinion (Gar- cia et al., 2019) or speaker's stance and confidence (Dinkar et al., 2020). Since it allows more fine- grained control over the amount of disentangle- ment, we expect it to be easier to tune when com- bined with more complex models.YuMohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and R Devon Hjelm. 2018. Mine: mu- tual information neural estimation. arXiv preprint arXiv:1801.04062.Y. Bengio, A. Courville, and P. Vincent. 2013. Rep- resentation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798-1828. Su Lin Blodgett, Lisa Green, and Brendan O'Connor. 2016. Demographic dialectal variation in social media: A case study of African-American English. In Proceedings of the 2016 Conference on Empiri- cal Methods in Natural Language Processing, pages 1119-1130, Austin, Texas. Association for Compu- tational Linguistics. Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. Transactions of the Associa- tion for Computational Linguistics, 5:135-146.Pengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, Zhe Gan, and Lawrence Carin. 2020a. Club: A contrastive log-ratio upper bound of mutual in- formation. In International Conference on Machine Learning, pages 1779-1788. PMLR.Justin B Kinney and Gurinder S Atwal. 2014. Eq- uitability, mutual information, and the maximal in- formation coefficient. Proceedings of the National Academy of Sciences, 111(9):3354-3359.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>very fresh, flavorful and flavorful. D ?=1.3it's not worth the money, but it was wrong. D ?=1.5 it's not worth the price, but not worth it. D ?=1.8 it's hard to find, and this place is horrible.</figDesc><table><row><cell>Input</cell><cell>it's freshly made, very soft and flavorful.</cell></row><row><cell>Adv</cell><cell>i hate this place.</cell></row><row><cell cols="2">vCLUB-S i hate it.</cell></row><row><cell cols="2">5 it's 10 KL Input it's freshly made, very soft and flavorful. Adv i hate this place. vCLUB-S i hate it. KL it's a little warm and very flavorful flavor.</cell></row><row><cell>D ?=1.3</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Sequences generated by the different models on the binary sentiment transfer task.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This phenomenon is also reported in (Feutry et al., 2018) on a picture anonymization task.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>The authors would like to thanks Georg Pichler for the thorough reading. The work of Prof. Pablo Piantanida was supported by the European Commission's Marie Sklodowska-Curie Actions (MSCA), through the Marie Sklodowska-Curie IF (H2020-MSCAIF-2017-EF-797805).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A general class of coefficients of divergence of one distribution from another</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Syed Mumtaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="142" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
