<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Key.Net: Keypoint Detection by Handcrafted and Learned CNN Filters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Barroso-Laguna</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Riba</surname></persName>
							<email>eriba@cvc.uab.es</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Vision Center</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Arraiy, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ponsa</surname></persName>
							<email>daniel@cvc.uab.es</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Vision Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
							<email>k.mikolajczyk@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Key.Net: Keypoint Detection by Handcrafted and Learned CNN Filters</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a novel approach for keypoint detection task that combines handcrafted and learned CNN filters within a shallow multi-scale architecture. Handcrafted filters provide anchor structures for learned filters, which localize, score and rank repeatable features. Scale-space representation is used within the network to extract keypoints at different levels. We design a loss function to detect robust features that exist across a range of scales and to maximize the repeatability score. Our Key.Net model is trained on data synthetically created from ImageNet and evaluated on HPatches benchmark. Results show that our approach outperforms state-of-the-art detectors in terms of repeatability, matching performance and complexity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Research advances in local feature detectors and descriptors led to remarkable improvements in areas such as image matching, object recognition, self-guided navigation or 3D reconstruction. Although the general direction of image matching methods is moving towards learned based systems, the advantage of learning methods over handcrafted ones has not been clearly demonstrated in keypoint detection <ref type="bibr" target="#b0">[1]</ref>. In particular, Convolutional Neural Networks (CNNs) were able to significantly reduce matching error in local descriptors <ref type="bibr" target="#b1">[2]</ref>, despite the impractical inefficiency of the initial techniques <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. These works stimulated further research efforts and resulted in improved efficiency of CNN based descriptors, on the contrary, on top of the limited success of learned detectors, a general trend towards dense rather than sparse representation and matching put aside local feature detectors. However, the growing popularity of augmented reality (AR) headsets, as well as AR smartphone apps, has drawn more attention to reliable and efficient local feature detectors that could be used for surface estimation, sparse 3D reconstruction, 3D model acquisition or objects alignment, among others.</p><p>Traditionally, local feature detectors were based on engineered filters. For instance, approaches such as Difference of Gaussians <ref type="bibr" target="#b4">[5]</ref>, Harris-Laplace or Hessian-Affine <ref type="bibr" target="#b5">[6]</ref> use combinations of image derivatives to compute feature maps, which is remarkably similar to the operations in trained CNN's layers. Intuitively, with just a few layers, a network could mimic the behavior of traditional detectors by learning the appropriate values in its convolutional filters. However, unlike the success with CNNs based local image descriptors, the improvements upon handcrafted detectors offered by recently proposed fully CNN based methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> are limited in terms of widely accepted metrics such as repeatability. One of the reasons is their low accuracy when estimating the affine parameters of the feature regions. Robustness to scale variations seems particularly problematic while other parameters such as dominant orientation can be regressed well by CNNs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b6">7]</ref>. This motivates our novel architecture, termed Key.Net, that makes use of handcrafted and learned filters as well as a multiscale representation. The Key.Net architecture is illustrated in figure 1. Introducing handcrafted filters, which act as soft anchors, makes possible to reduce the number of parameters used by state-of-the-art detectors while maintaining the per-formance in terms of repeatability. The model operates on multi-scale representation of full-size images and returns a response map containing the keypoint score for every pixel. The multi-scale input allows the network to propose stable keypoints across scales thus providing robustness to scale changes.</p><p>Ideally, a robust detector is able to propose the same features for images that undergo different geometric or photometric transformations. A number of related works have focused their objective function to address this issue, although they were based either on local patches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> or global map regression loss <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b10">11]</ref>. In contrast, we extend the covariant constraint loss to a new objective function that combines local and global information. We design a fully differentiable operator, Multi-scale Index Proposal, that proposes keypoints at multi-scale regions. We extensively evaluate the method in recently introduced HPatches benchmark <ref type="bibr" target="#b1">[2]</ref> in terms of accuracy and repeatability according to the protocol from <ref type="bibr" target="#b14">[15]</ref>.</p><p>In summary, our contributions are the following: a) a keypoint detector that combines handcrafted and learned CNN features, b) a novel multi-scale loss and operator for detecting and ranking stable keypoints across scales, c) a multi-scale feature detection with shallow architecture.</p><p>The rest of the paper is organized as follows. We review the related work in section 2. Section 3 presents our proposed hybrid Key.Net architecture of handcrafted and learned CNNs filters and section 4 introduces the loss. Implementation and experimental details are given in section 5 and the results are presented in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There are many surveys that extensively discuss feature detection methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref>. We present related works in two main categories: handcrafted and learned based.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Handcrafted Detectors</head><p>Traditional feature detectors localize geometric structures through engineered algorithms, which are often referred to as handcrafted. Harris <ref type="bibr" target="#b16">[17]</ref> and Hessian <ref type="bibr" target="#b17">[18]</ref> detectors used first and second order image derivatives to find corners or blobs in images. Those detectors were further extended to handle multi-scale and affine transformations <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19]</ref>. Later, SURF <ref type="bibr" target="#b19">[20]</ref> accelerated the detection process by using integral images and an approximation of the Hessian matrix. Multi-scale improvements were proposed in KAZE <ref type="bibr" target="#b20">[21]</ref> and its extension, A-KAZE <ref type="bibr" target="#b21">[22]</ref>, where Hessian detector was applied to a non-linear diffusion scale space in contrast to widely used Gaussian pyramid. Although corner detectors proved to be robust and efficient, other methods seek alternative structures within images. SIFT <ref type="bibr" target="#b4">[5]</ref> looked for blobs over multiple scale levels, and MSER <ref type="bibr" target="#b22">[23]</ref> segmented and selected stable regions as keypoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Learned Detectors</head><p>The success of learned methods in general object detection and feature descriptors motivated the research community to explore similar techniques for feature detectors. FAST <ref type="bibr" target="#b23">[24]</ref> was one of the first attempts to use machine learning to derive a corner keypoint detector. Further works extended FAST by optimizing it <ref type="bibr" target="#b24">[25]</ref>, adding a descriptor <ref type="bibr" target="#b25">[26]</ref> or orientation estimation <ref type="bibr" target="#b26">[27]</ref>.</p><p>Latest advances in CNNs also made an impact on feature detection. TILDE <ref type="bibr" target="#b13">[14]</ref> trained multiple piece-wise linear regression models to identify interest points that are robust under severe weather and illumination changes. <ref type="bibr" target="#b8">[9]</ref> introduced a new formulation to train a CNN based on feature covariant constraints. Previous detector was extended in <ref type="bibr" target="#b9">[10]</ref> by adding predefined detector anchors, showing improved stability in training. <ref type="bibr" target="#b7">[8]</ref> presented two networks, MagicPoint, and MagicWarp, which first extracted salient points and then a parameterized transformation between pairs of images. MagicPoint was extended in <ref type="bibr" target="#b12">[13]</ref> to Su-perPoint, which included a salient detector and descriptor. LIFT <ref type="bibr" target="#b6">[7]</ref> implemented an end-to-end feature detection and description pipeline, including the orientation estimation for every feature. Quadruple image patches and a ranking scheme of point responses as cost function were used in <ref type="bibr" target="#b27">[28]</ref> to train a neural network. In <ref type="bibr" target="#b28">[29]</ref>, authors proposed a pipeline to automatically sample positive and negative pairs of patches from a region proposal network to optimize jointly point detections and their representations. Recently, LF-Net <ref type="bibr" target="#b10">[11]</ref> estimated position, scale and orientation of features by optimizing jointly the detector and descriptor.</p><p>In addition to the above presented learned detectors, CNN architectures also were deployed to optimize the matching stage. <ref type="bibr" target="#b29">[30]</ref> learned to predict which features and descriptors were matchable. More recently, <ref type="bibr" target="#b30">[31]</ref> introduced a network to learn to find good correspondences for wide-baseline stereo. Furthermore, other CNNs also studied to perform tasks beyond detection or matching. In <ref type="bibr" target="#b11">[12]</ref>, the architecture assigned orientations to interest points and AffNet <ref type="bibr" target="#b31">[32]</ref> used the descriptor loss to learn to predict the affine parameters of a local feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Key.Net Architecture</head><p>Key.Net architecture combines successful ideas from handcrafted and learned methods namely gradient-based feature extraction, learned combinations of low-level features and multi-scale pyramid representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Handcrafted and Learned Filters</head><p>The design of the handcrafted filters is inspired by the success of Harris <ref type="bibr" target="#b16">[17]</ref> and Hessian <ref type="bibr" target="#b17">[18]</ref> detectors, which used first and second order derivatives to compute the salient corner responses. A complete set of derivatives is called LocalJet <ref type="bibr" target="#b32">[33]</ref> and they approximate the signal in the local neighborhood as known from Taylor expansion:</p><formula xml:id="formula_0">I i1,...,in = I 0 * ? i1,...,in g ? ( x),<label>(1)</label></formula><p>where g ? denotes the Gaussian of width ? centered at x = 0, and i n denotes the direction. Higher order derivatives i.e., n &gt; 2 are sensitive to noise and require large kernels, we, therefore, include derivatives and their combinations up to the second order only:</p><p>? First Order. From image I we derive 1st order gradients I x and I y . In addition, we compute I x * I y , I x 2 and I y 2 as in the second moment matrix of Harris detector <ref type="bibr" target="#b16">[17]</ref>.</p><p>? Second Order. From image I, 2nd order derivatives I xx , I yy and I xy are also included as in the Hessian matrix used in Hessian and DoG detectors <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>Since Hessian detector uses the determinant of the Hessian matrix we add I xx * I yy and I 2 xy . ? Learned. A convolutional layer with M filters, a batch normalization layer and a ReLU activation function form a learned block.</p><p>The hardcoded filters reduce the number of total learnable parameters to train the architecture, improving the stability and convergence during backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-scale Pyramid</head><p>We design our architecture to be robust to small scale changes without the need for computing several forward passes. As illustrated in <ref type="figure" target="#fig_0">figure 1</ref>, the network includes three scale levels of the input image which is blurred and downsampled by a factor of 1.2. All the feature maps resulting from the handcrafted filters are concatenated to feed the stack of learned filters in each of the scale levels. All three streams share the weights, such that the same type of anchors result from different levels and form the set of candidates for final keypoints. Feature maps from all scale levels are then upsampled, concatenated and fed to the last convolutional filter to obtain the final response map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Loss Functions</head><p>In supervised training, the loss function relies on the ground truth. In the case of keypoints, ground truth is not well defined as keypoint locations are useful as long as they can be accurately detected regardless of geometric or photometric image transformation. Some learned detectors <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b10">11]</ref> train the network to identify keypoints without constraining their locations, where only the homography transformation between images is used as ground truth to calculate the loss as a function of keypoints repeatability.</p><p>Other works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b9">10]</ref> show the benefits of using anchors to guide their training. Although anchors make the training more stable and lead to better results, they prevent the network from proposing new keypoints in case there is no anchor in the proximity.</p><p>In contrast, the handcrafted filters in Key.Net provide a weak constraint with the benefit of the anchor-based methods while allowing the detector to propose new stable keypoints. In our approach, only the geometric transformation between images is required to guide the loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Index Proposal Layer</head><p>This section introduces the Index Proposal (IP) layer, which is extended to its multi-scale version in section 4.2.</p><p>Extracting coordinates for training keypoint detectors has been widely studied and showed great improvements: <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> extracted coordinates in a patch level, SuperPoint <ref type="bibr" target="#b12">[13]</ref> used a channel-wise softmax to get maxima belonging to fix grids of 8x8, and <ref type="bibr" target="#b34">[35]</ref> used a spatial softmax layer to compute the global maxima of a feature map, obtaining one keypoint candidate per feature map. In contrast to previous methods, the IP layer is able to return multiple global keypoint coordinates centered on local maxima from a single image without constraining the number of keypoints to the depth of the feature map <ref type="bibr" target="#b34">[35]</ref> or the size of the grid <ref type="bibr" target="#b12">[13]</ref>.</p><p>Similarly to handcrafted techniques, keypoint locations are indicated by local maxima of the filter response map R output by Key.Net. Spatial softmax operator is an effective method for extracting the location of a soft maximum within a window <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref>. Therefore, to ensure that the IP layer is fully differentiable, we rely on spatial softmax operator to obtain the coordinates of a single keypoint per window. Consider a window w i of size N ? N in R, with the score value at each coordinate [u, v] within the window, exponentially scaled and normalized:</p><formula xml:id="formula_1">m i (u, v) = e wi(u,v) N j,k e wi(j,k)</formula><p>.</p><p>(</p><p>Due to exponential scaling the maximum dominates and the expected location calculated as the weighted average</p><formula xml:id="formula_3">[? i ,v i ]</formula><p>gives an approximation of the maximum coordinates:</p><formula xml:id="formula_4">[x i , y i ] T = [? i ,v i ] T = N u,v [W m i , W T m i ] T +c w ,<label>(3)</label></formula><p>where W is a kernel of size N ? N with index values j = 1 : N along its columns, pointwise product , and c w is the top-left corner coordinates of window w i . This is similar to non-maxima suppression (NMS) but unlike NMS, the IP layer is differentiable and it is a weighted average of the global maximum of the window rather than the exact location of it. Depending on the base of the power expression in equation 2, multiple local maxima may have a more or less significant effect on the resulting coordinates. A detector is covariant if same features are detected under varying image transformations. Covariant constraint was formulated as a regression problem in <ref type="bibr" target="#b8">[9]</ref>. Given images I a and I b , and ground truth homography H b,a between them, the loss L is based on the squared difference between points extracted by IP layer and actual maximum coordinates (NMS) in corresponding windows from I a and I b :</p><formula xml:id="formula_5">L IP (I a , I b , H a,b , N ) = i ? i [x i , y i ] T a ?H b,a [x i ,? i ] T b 2 , and ? i = R a (x i , y i ) a + R b (x i ,? i ) b ,<label>(4)</label></formula><p>where R a and R b are the response map of I a and I b with coordinates related by the homography H b,a . We skip homogeneous coordinates for simplicity. Parameter ? i controls the contribution of each location based on its score value, thus computing the loss for significant features only.</p><p>As NMS is non-differentiable, gradients are only backpropagated where IP layer is applied, therefore, we switch I a and I b and combine both losses to enforce consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Multi-scale Index Proposal Layer</head><p>IP layer returns one location per window, therefore, the number of keypoints per image strongly depends on the predefined window size N , in particular, with an increasing size only a few dominant keypoints survive in the image. In <ref type="bibr" target="#b35">[36]</ref>, authors demonstrated improved performance of local features by accumulating image features not only within a spatial window but also within the neighboring scales. We propose to extend IP layer loss by incorporating multi-scale representation of a local neighborhood. Multiple window sizes encourage the network to find keypoints that exist across a range of scales. The additional benefit of including larger windows is that other keypoints within the window can act as anchors for the estimated location of the dominant keypoint. Similar idea proved successful in <ref type="bibr" target="#b36">[37]</ref>, where stable region boundaries are used.</p><p>We, therefore, propose the Multi-Scale Index Proposal (M-SIP) layer. M-SIP splits multiple times the response map into grids, each with a window size of N s ? N s and computes the candidate keypoint position for each window as shown in figure 2. Our proposed loss function is the average of covariant constraint losses from all scale levels:</p><formula xml:id="formula_6">L M SIP (I a , I b , H a,b ) = s ? s L IP (I a , I b , H a,b , N s ),<label>(5)</label></formula><p>where s is the index of the scale level with N s as window size, L IP is the covariant constraint loss and ? s is the control parameter at scale level s, that decreases proportionally to the increasing window area as larger windows lead to a larger loss, which is somewhat similar to the scale-space normalisation <ref type="bibr" target="#b5">[6]</ref>.</p><p>The combination of different scales imposes an intrinsic process of simultaneous scoring and ranking of keypoints within the network. In order to minimize the loss, the network must learn to give higher scores to robust features that remain dominant across a range of scales. <ref type="figure" target="#fig_2">Figure 3</ref> shows different response maps for increasing window size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Settings</head><p>In this section, we present implementation details, metrics and the dataset used for evaluating the method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Training Data</head><p>We generate a synthetic training set from ImageNet ILSVRC 2012 dataset. We apply random geometric trans- Textureless regions are not discriminative, therefore, we discard them by checking if the response of any of the handcrafted filters is lower than a threshold. We modify the contrast, brightness and hue value in HSV space to one of the images to improve network's robustness against illumination changes. In addition, for each pair, we generate binary masks that indicate the common area between images. Those masks are used in training to avoid regressing indexes of keypoints that are not present in the common region. There are 12,000 image pairs of size 192 ? 192. We use 9,000 of them as the training data and 3,000 as validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation Metrics</head><p>We follow the evaluation protocol proposed in <ref type="bibr" target="#b14">[15]</ref> and improved in the follow up works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b0">1]</ref>. Repeatability score for a pair of images is computed as the ratio between the number of corresponding keypoints and the lower number of keypoints detected in one of the two images. We fix the number of extracted keypoints to compare across methods and allow each keypoint to match only once as in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b13">14]</ref>. In addition, as exposed by <ref type="bibr" target="#b0">[1]</ref>, we address the bias from the magnification factor that was applied to accelerate the computation of the overlap error between multi-scale keypoints. Keypoints are identified by spatial coordinates and scales at which the features were detected. To identify corresponding keypoints we compute the Intersectionover-Union error, IoU , between the areas of the two candidates. To evaluate the accuracy of keypoint location and scale independently, we perform two sets of experiments. One is based on the detected scales and the other assumes the scales are correctly detected by using the ground truth parameters. In our benchmark, we use top 1,000 interest points that belong to the common region between images and a match is considered correct when IoU is smaller than 0.4 i.e., the overlap between corresponding regions is more than 60%. The scales are normalized as in <ref type="bibr" target="#b0">[1]</ref>, which sets the larger size in a pair of points to 30 pixels, and rescales the other one accordingly. Non-maxima suppression of 15 ? 15 is performed at inference time during evaluation. HPatches <ref type="bibr" target="#b1">[2]</ref> dataset is used for testing. HPatches contains 116 sequences, which are split between viewpoint and illumination transformations, 59 and 57 sequences respectively. HPatches offers predefined image patches for evaluating descriptors, instead, we use full images for evaluating keypoint detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Implementation Notes</head><p>Training is performed in a siamese pipeline, with two instances of Key.Net that share the weights and are updated at the same time. Each convolutional layer has M = 8 filters of size 5 ? 5, with He weights initialization and L2 kernel regularizer. We compute the covariant constraint loss L M -SIP for five scale levels, with the size of the M-SIP windows N s ? <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr">40]</ref> and loss term ? s ? [256, 64, <ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1]</ref>, that were determined by performing a hyperparameter search on the validation set. Larger candidate window sizes have greater mean errors between coordinate points since the maximum distance is proportional to the window size. Thus, ? s has the largest value for the smallest window. We use a batch size of 32, an Adam Optimizer with a learning rate of 10 ?3 and a decay factor of 0.5 after 20 epochs. On average, the architecture converges in 30 epochs, 2h on a machine with an i7-7700 CPU running at 3.60GHz and a NVIDIA GeForce GTX 1080 Ti. Evaluation benchmark, synthetic data generator, Key.Net network, and loss are implemented using TensorFlow and are available on GitHub 1 .   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M-SIP Region Sizes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>In this section, we present the experiments and discuss the results. We first show results on validation data for several variants of the proposed architecture. Next, Key.Net repeatability scores in single-scale and multi-scale are presented along with the state-of-the-art detectors on HPatches. Moreover, we evaluate the matching performance, the number of learnable parameters and inference time of our proposed detector and compare to other techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Preliminary Analysis</head><p>We study several combinations of loss terms, different handcrafted filters and the effects of the number of learnable layers or pyramid levels within the architecture. M-SIP Levels are investigated in <ref type="figure" target="#fig_5">figure 5 (Left)</ref> showing increasing repeatability with more scale levels within M-SIP operator. In addition, we show how the loss with smaller window size N improves repeatability. However, the best result is obtained when all levels are combined. Filter Combinations are analyzed in figure 5 (Right). We show results for 1 st and 2 nd order filters as well as their combination. All networks have the same number of filters, however, we either freeze first layer of 10 filters with handcrafted kernels (c.f. section 3.1) or learn them depending on the variant of our network, e.g, in Fully Learnable Key.Net there are no handcrafted filters as all are randomly initialized and learned. The results show that the information provided by handcrafted filters is essential when the number of learnable layers is small. Handcrafted filters act as soft constraints, which directly discard areas without gradients, i.e. non-discriminative with low repeatability. However, as we add more learnable blocks, repeatability scores for combined and fully learnable networks become comparable. Naturally, gradient-based handcrafted filters are simple, and architectures with enough complexity could learn them if they were required. However, the use of engineered features leads to a smaller architecture while maintaining the performance, which is often critical for real-time applications. In summary, combining both types of filters allows to significantly reduce the number of learnable layers. We use Key.Net architecture with three learnable blocks in the next experiments. Multiple Pyramid Levels at the input to the network also affect the detection performance as shown in table 1a. For a single pyramid level, only the original image is used as input. Adding pyramid levels is similar to increasing the size of the receptive fields in the architecture. Our experiment suggests that using more than three levels does not lead to significantly improved results. On the validation set, we obtain a repeatability score of 72.5% for one level, an increase of 6.6% for three, and 7.0% for five levels. We, therefore, use three levels, which achieve good performance while keeping the computational cost low. Spatial Softmax Base in equation 2 defines how soft the estimation of keypoint coordinates is. High values return the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Keypoint Detection</head><p>This section presents the results for state-of-the-art local feature detectors along with our proposed method. <ref type="table" target="#tab_3">Table  2</ref> shows the repeatability score, average intersection-overunion error? IoU and scale range S range , which is the ratio between the maximum and minimum scale values of the extracted interest points. Suffixes -TI and -SI, refer to translation (detection at a single scale only) and scale invariance (detection at multiple scales), respectively. Keypoint location is only evaluated under L by assuming correct scale detection, while scale and location (SL) use the actual detected scale and location for computing the repeatability and overlap error.</p><p>In addition to Key.Net, we propose Tiny-Key.Net, which is a reduced size architecture with all handcrafted filters but only one learnable layer with one filter (M = 1) and a single scale input. The idea behind Tiny-Key.Net is to demonstrate how far the complexity can be reduced while keeping good performance. Key.Net and Tiny-Key.Net are extended to scale invariance by evaluating the detector on several scaled images, similar to <ref type="bibr" target="#b9">[10]</ref>. We also show results on single scale input Key.Net-TI, to compare it directly with other TI detectors such as SuperPoint or TILDE. We set the thresholds of algorithms such that they return at least 1,000 points per image. As MSER proposes regions without scoring or ranking, we randomly pick 1,000 points to compute the results. We repeat this experiment ten times and average the results for MSER. Key.Net has the best results on viewpoint sequences, in terms of both, location and scale. Tiny-Key.Net does not perform as well as Key.Net but it is within the top three repeatability scores, after Key.Net-TI and Key.Net-SI. On illumination sequences, Key.Net-TI performs the best among TI detectors, not being affected by scale estimation errors. TCDET, which uses points detected by TILDE as anchors, is the most accurate in location estimation compared to other SI detectors. Note that TILDE based detectors were specifically designed and trained for illumination sequences. LF-Net is the best SI detector according to SL overlap, not suffering much from incorrect scale estimations. However, its repeatability decreases the most from L to SL among all SI detectors on viewpoint sequences. Key.Net-SI addresses the scale changes better than the other methods but the errors in multi-scale sampling affect it  when there is no scale change between images i.e. illumination sequences. This has often been observed for detectors with more invariance than required by the data. Handcrafted detectors have the lowest average overlap error? IoU among all detectors. A wide range of scales S range is detected by MSER, which has a great capability of extracting local features from different scales due to its feature segmentation nature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Keypoint Matching</head><p>Moreover, in order to demonstrate that the detected features are useful for matching, table 3 shows matching scores for detectors combined with HardNet descriptor <ref type="bibr" target="#b37">[38]</ref>. As our method only focuses on the detection part, and for a fair comparison, we used the same descriptor and discard the orientation for all methods that provide it. In addition, we include in the table LIFT <ref type="bibr" target="#b6">[7]</ref>, SuperPoint <ref type="bibr" target="#b12">[13]</ref> and LF-Net <ref type="bibr" target="#b10">[11]</ref> with their descriptors, but ignoring their orientation estimation. SuperPoint and LF-Net have 256 descriptor dimension, while dimension of HardNet <ref type="bibr" target="#b37">[38]</ref> and LIFT is 128. Matching score is computed as the ratio between features matched and detected (top 1,000). Top matching scores is obtained by Key.Net on viewpoint, and LF-Net+HardNet on illumination. Feature detectors that were optimized jointly with a descriptor <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11]</ref> have better matching score than regular learned detectors on il-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Learnable Parameters</head><p>TCDET SuperPoint LF-Net Tiny-Key.Net Key.Net 548k 940k 39k 280 5.9k <ref type="table">Table 4</ref>: Comparison of the number of learnable parameters for state-of-the-art architectures. Tiny-Key.Net has only one learnable block with one filter. lumination sequences, but not on viewpoint. Handcrafted AKAZE performs close to the top learned methods for both viewpoint and illumination sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Efficiency</head><p>We also compare the number of learnable parameters, indicating then the complexity of the predictor, which leads to an increasing risk of overfitting and need for a large amount of training data. <ref type="table">Table 4</ref> shows the approximate number of parameters for different architectures. Learnable parameters that are not used during inference in the detector part are not counted for SuperPoint and LF-Net detectors. The highest complexity is from SuperPoint with 940k learnable parameters. Key.Net has nearly 160 times fewer parameters and Tiny-Key.Net has 3,100 times fewer parameters than SuperPoint with better repeatability for viewpoint scenes. The inference time of an image of 600 ? 600 is 5.7ms (175 FPS) and 31ms (32.25 FPS) for Tiny-Key.Net and Key.Net, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>We have introduced a novel approach to detect local features that combines handcrafted and learned CNN filters. We have proposed a multi-scale index proposal layer that finds keypoints across a range of scales, with a loss function that optimizes the robustness and discriminating properties of the detections. We demonstrated how to compute and combine differentiable keypoint detection loss for multiscale representation. Evaluation results on large benchmark show that combining handcrafted and learned features as well as multi-scale analysis at different stages of the network improves the repeatability scores compared to other state-of-the-art keypoint detection methods.</p><p>We further show that excessively increasing network's complexity does not lead to improved results. In contrast, using handcrafted filters allows to significantly reduce the complexity of the architecture leading to a detector with 280 learnable parameters and inference of 175 frames per second. Proposed detectors lead to state-of-the-art matching performance when used with a descriptor on viewpoint.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The proposed Key.Net architecture combines handcrafted and learned filters to extract features at different scale levels. Feature maps are upsampled and concatenated. Last learned filter combines the Scale Space Volume to obtain the final response map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Siamese training process. Image I a and I b go through Key.Net to generate their response maps, R a and R b . M-SIP proposes interest point coordinates for each one of the windows at multi-scale regions. The final loss function is computed as a regression of coordinate indexes from I a and local maximum coordinates from I b . Better visualize in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Keypoints obtained after adding larger context windows to M-SIP operator. The points that are more stable remain as the M-SIP operator increases its window size. Feature maps in the middle row contain points around edges or non discriminative areas, while bottom row shows detections that are more robust under geometric transformations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>formations to images and extract pairs of corresponding regions as our training set. The process is illustrated in figure 4. The parameters of the transformations are: scale [0.5, 3.5], skew [?0.8, 0.8] and rotation [?60 ? , 60 ? ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>We apply random geometric and photometric transformations to images and extract pairs of corresponding regions as the training set. Red crop is discarded by checking the response of the handcrafted filters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Left: Comparison of repeatability results for several levels in the M-SIP operator. We show different combinations of context losses as the final loss, from smaller to larger regions. The best result is obtained when using five window sizes from 8 ? 8 up to 40 ? 40. Right: Repeatability results for different combinations of handcrafted filters and a number of learnable layers (M = 8 filters each). A higher number of layers leads to better results. All repeatability scores are computed on synthetic validation set from ImageNet. .5 74.6 79.1 79.4 79.5 78.6 (a) Number of input scale levels in Key.Net. Spatial softmax base used in equation 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>W 8x8 W 16x16 W 24x24 W 32x32 W 40x40</figDesc><table><row><cell>---</cell><cell>----</cell><cell>----</cell><cell>----</cell><cell>----</cell><cell>Repeatability 70.5 74.6 76.8 77.6 65.7 71.4 73.2</cell><cell>Number Learnable Blocks</cell><cell>1 2 3 4 5</cell><cell cols="2">Full Learnable 1st Order 2nd Order 1st and 2nd Order</cell></row><row><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>74.9 79.1</cell><cell></cell><cell>0.72</cell><cell>0.74</cell><cell>0.76 Repeatability</cell><cell>0.78</cell><cell>0.80</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Repeatability results for different design choices on synthetic validation set from ImageNet.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Repeatability results (%) for translation (TI) and scale (SI) invariant detectors on HPatches. We also report average overlap error? IoU and ratio of maximum to minimum extracted scale S Range . In SL, scales and locations are used to compute overlap error, meanwhile, in L, only locations are used and scales are assumed to be correctly estimated. Key.Net and Tiny-Key.Net are the best algorithms on viewpoint, for both L and SL. On illumination sequences, translation invariant Key.Net-TI obtains the best accuracy. Among scale invariant SI detectors, TCDET is the best in L and LF-Net in SL.</figDesc><table><row><cell>location of the global maximum within the window, while</cell></row><row><cell>low values average local maxima. The base is varied in ta-</cell></row><row><cell>ble 1b. Optimum scores are obtained when using the base</cell></row><row><cell>in equation 2 close to the e value, which is in line with the</cell></row><row><cell>setting used in [35].</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Matching score (%) of best detectors together with HardNet and state-of-the-art detector/descriptors. Results on HPatches sequences, both viewpoint, and illumination. Key.Net architecture gets the best matching score for viewpoint, while LF-Net+HardNet for illumination sequences.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/axelBarroso/Key.Net</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Large scale evaluation of local image feature detectors on homography datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Hpatches: A benchmark and evaluation of handcrafted and learned local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassileios</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Matchnet: Unifying feature and metric learning for patch-based matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xufeng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Distinctive image features from scaleinvariant keypoints. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Scale &amp; affine invariant interest point detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Lift: Learned invariant feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Kwang Moo Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07410</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Toward geometric deep slam. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning covariant feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning discriminative and transformation covariant local feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svebor</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang Moo</forename><surname>Yi</surname></persName>
		</author>
		<title level="m">LF-Net: Learning Local Features from Images. NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning to assign orientations to feature points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Kwang Moo Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Verdie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lepetit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Superpoint: Self-supervised interest point detection and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Tilde: a temporally invariant learned detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Verdie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang Moo</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A performance evaluation of local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Local invariant feature detectors: a survey. Foundations and Trends in Computer Graphics and Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Stephens</surname></persName>
		</author>
		<title level="m">Alvey Vision Conference</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Beaudet</surname></persName>
		</author>
		<title level="m">Rotationally invariant image operators. ICPR</title>
		<imprint>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A comparison of affine region detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Schaffalitzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timor</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Speeded-up robust features (surf)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Fern?ndez Alcantarilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Bartoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<title level="m">Kaze features. ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Jes?s Nuevo, and Adrien Bartoli. Fast explicit diffusion for accelerated features in nonlinear scale spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alcantarilla</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Robust wide-baseline stereo from maximally stable extremal regions. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chum</forename><surname>Ondrej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urban</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pajdla</forename><surname>Toms</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Machine learning for high-speed corner detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Rosten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Drummond</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Faster and better: A machine learning approach to corner detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Rosten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Drummond</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chli</forename><surname>Margarita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siegwart</forename><surname>Roland</surname></persName>
		</author>
		<title level="m">Brisk: Binary robust invariant scalable keypoints. ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Orb: An efficient alternative to sift or surf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Quad-networks: unsupervised learning to rank for interest point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihito</forename><surname>Seki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubor</forename><surname>Ladicky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Torsten Sattler, and Marc Pollefeys</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">End-to-end learning of keypoint detector and descriptor for pose invariant 3d matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Georgakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikrishna</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilfried</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Havlena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<title level="m">Predicting matchability. CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning to find good correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Kwang Moo Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<title level="m">Repeatability is not enough: Learning affine regions via discriminability. ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The gaussian scale-space paradigm and the multiscale local jet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Florack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Ter Haar Romeny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koenderink</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Indexing based on scale invariant interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Discovery of latent 3d keypoints via end-to-end geometric reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supasorn</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Domain-size pooling in local descriptors: Dsp-sift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingming</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Object recognition using local affine frames on distinguished regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stepan</forename><surname>Obdrzalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Working hard to know your neighbor&apos;s margins: Local descriptor learning loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasiya</forename><surname>Mishchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
