<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Emotion Recognition In Persian Speech Using Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Yazdani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of computer science and engineering Shahid</orgName>
								<orgName type="institution">Beheshti University Tehran</orgName>
								<address>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Simchi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of computer science and engineering Shahid</orgName>
								<orgName type="institution">Beheshti University Tehran</orgName>
								<address>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Shekofteh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of computer science and engineering Shahid</orgName>
								<orgName type="institution">Beheshti University Tehran</orgName>
								<address>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Emotion Recognition In Persian Speech Using Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Speech Emotion Recognition</term>
					<term>Deep Learning</term>
					<term>Neural Network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speech Emotion Recognition (SER) is of great importance in Human-Computer Interaction (HCI), as it provides a deeper understanding of the situation and results in better interaction. In recent years, various machine learning and deep learning algorithms have been developed to improve SER techniques. Recognition of emotions depends on the type of expression that varies between different languages. In this article, to further study this important factor in Farsi, we examine various deep learning techniques on the SheEMO dataset. Using signal features in low-and high-level descriptions and different deep networks and machine learning techniques, Unweighted</head><p>Average Recall (UAR) of 65.20 is achieved with an accuracy of 78.29.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Human emotional state is an important factor in their interactions and affects most communication ways such as facial expressions, voice characteristics, and linguistic content of verbal communication. Speech is one of the main ways to express emotions. To obtain a natural human-computer interaction (HCI), it is highly important to recognize, interpret, and respond to the emotions expressed in speech.</p><p>Speech emotion recognition (SER) currently has several applications such as car-human interactions naturally like web videos, computer videos and training programs, car driver safety, diagnostic tools to treat the disease, as a tool for an automatic translation system and mobile communications.</p><p>Deep learning has been considered as an emerging research field in machine learning and has received more attention in recent years. Deep learning techniques for SER have several advantages over traditional methods, including their ability to recognize complex structures and features without need to manually extraction, to extract low-level features from raw data and the ability to deal with unlabeled data. The rest of the structure of this article is as follows: In section 2 , we will review the related works. In section 3, we introduce our contribution which includes reviewing the ShEMO data set in Persian and the two common methods of extracting features as LLDs (Low-Level Descriptors) and functionals from audio signals, as well as testing different neural network models on these features. In section 4 we will review the results and finally in section 5, we will have the conclusion.</p><p>II. RELATED WORKS speech emotion recognition (SER) approaches consist of two steps known as feature extraction and feature classification. At the first step of speech processing, researchers have acquired several characteristics including prosodic and vocal tract features. The second step is about using traditional classifiers such as Support Vector Machine (SVM) or neural networks. Deep neural networks (DNNs) and Convolutional Neural Networks (CNN) provide efficient results for signal processing. On the other hand, recursive networks such as Recursive Neural Networks (RNNs) and Long Short-Term Memory (LSTM) are very effective in SER.</p><p>First of all, it should be said that emotion recognition systems are classified into two levels, frame and utterance. In fact, the utterance is divided into smaller parts with sound areas called frames. Using BLSTM (Bidirectional LSTM) network which effectively preserves the characteristics of temporal dynamic, in <ref type="bibr" target="#b0">[1]</ref> on the IEMOCAP database shows that the system will perform better than DNN. They obtained low-level acoustic and also extracted the global features using statistical functions are applied to the output and then the acoustic features are given to the Extreme Learning Machine (ELM) network. Finally, UA (Unweighted Accuracy) is achieved of 63.89 with WA (Weighted Accuracy) of 62.85.</p><p>In <ref type="bibr" target="#b1">[2]</ref> the spectrogram is used directly to train a CNN-LSTM network. At the first step, utterances that are longer than 3 seconds are divided into small equal parts. Also, They claculated spectrogram for each frame by applying Hemming windows to each signal with a frame size of 10 milliseconds, a window size of 20 milliseconds and a Fourier series with a length of 800. In the CNN-LSTM architecture, it is assumed that CNN extracts specific patterns that contain emotional information in the utterance, and LSTM pays attention to the temporal behavior during the utterance. The system without having a separate feature extraction step is achieved 67.3 of WA and 62 of UA.</p><p>In <ref type="bibr" target="#b4">[5]</ref>, a BLSTM recursive neural network is used along with the attention mechanism which ignores the silent parts of the utterance as well as the parts that do not have emotional content. In fact, both frame features and time aggregation can be learned over longer periods of time. Therefore, silent frames receive small weights and the rest of the frames receive appropriate weights based on the amount of emotional content. Finally, they achieved WA of 63.5 and UA of 58.8 on the IEMOCAP database. In <ref type="bibr" target="#b2">[3]</ref> the FCN (Fully Connected Network) is introduced which works with variable length of speech as well as regardless of segmentation. In this paper, network with convolutional layers with the attention mechanism is used and it works without needing speech segmentation. Using the attention mechanism, the weights of silent frames are completely reduced and are actually ignored to detect emotions. This model is achieved WA of 70.4 and UA of 63.9.</p><p>In <ref type="bibr" target="#b3">[4]</ref>, after converting the signal into spectrogram, the data is transmitted directly to the network. A CNN network is used to extract high-level features and then they have used a recursive network. Finally, UA of 65.3 and WA of 66.9 were obtained on the IEMOCAP database.</p><p>Also as we mentioned before, by Choosing the right features is an important factor in the SER task. In <ref type="bibr" target="#b5">[6]</ref>, using the openSMILE tool and emobase2010 features, they have extracted a 1582-dimensional vector for each utterance. Using GAN architecture and mixup data augmentation techniques, better results in a cross-corpus task have been obtained than previous studies. Also <ref type="bibr" target="#b6">[7]</ref> uses the IS09 features which includes 384 features per utterance, along with SVM and logistic regression. MFCC features are also extracted from a maximum of 120 frames of utterances and a matrix (120, 13) is obtained for each utterance which is used in a stacked LSTM architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTAL SETUP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>The ShEMO dataset contains 3,000 semi-natural speech files, equivalent to 3 hours and 25 minutes of speech samples collected from online radio broadcasts. These files are in .wav, 16-bit, 44.1 kHz and single-channel formats. In this data set, 87 people (including 31 women and 56 men), whose mother tongue is Persian used to examine the 5 main feelings of anger, fear, happiness, sadness, surprise and normal state without feeling. Twelve people, including 6 men and 6 women, tag these speech files as labels and are used by voting to determine the final label.</p><p>The average duration of utterances is 4.11 seconds with a standard deviation of 3.41. It should be noted that due to the small number of files labeled with fear (a total of 38 files), these files have been removed from the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Features</head><p>In general, there are two steps to extract features from audio files. Initially, the audio file is split into smaller frames and the low-level descriptions (LLDs) are extracted from each frame. Then different statistical functions (such as mean, max, variance, linear regression coefficients, etc.) are applied to the LLDs to obtain a high-level or functional feature vector for each utterance. Different machine learning models such as SVM or decision tree can be used on Functional feature vectors or neural networks can be used directly using LLD models such as recursive or convolutional networks.</p><p>We extracted various LLDs from speech files. For this purpose, we sampled each speech file at a sampling rate of 16KHz and we used it for 7.52 seconds (average total file duration and standard deviation). Also, we crop longer files and shorter files with zero padding in 7.52 seconds. Then we divided the obtained time series into 32ms and 100ms frames with 50% overlap. Finally, 52 features including various spectral features such as centroid, contrast, roll off and bandwidth along with features such as zero crossing rate and MFFC features are extracted from each frame as Hand Crafted LLDs. In addition, we tested two different features for extracting LLDs using the openSMILE tool. It should be noted that these features are extracted from 20ms long frames. The first feature is called the extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS), which extracts 25 LLD features (such as MFCC features, frequency-related features, bandwidth, etc.) from each frame. The second feature is the Interspeech 2016 Computational Paralinguistics Challenge (ComParE_2016), which extracts 65 LLDs per frame.</p><p>The openSMILE tool provides various features for extracting speech files. To extract these features, it splits the speech files into 20ms long frames with 10ms overlap, extracts the various LLDs from the frames, and then using several statistical functions on them to achieve a ddimensional feature vector as functional features for each utterance. We used 3 different features for our experiments:</p><p>? The extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS): In addition to the LLD features described in the previous section, this feature set includes 88 functional features such as spectral, frequency, amplitude and energy. ? The large openSMILE emotion feature set (emo_large): It extracts the largest number of functional features from each audio file, contains 6552 features obtained by applying more functions to more LLDs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DEEP LEARNING MODELS</head><p>In recent years, deep learning has been used effectively by researchers due to its multi layers structure and the presentation of efficient results in a variety of fields, including emotion recognition in speech. Deep neural networks (DNNs) are based on forward structures, consisting of one or more hidden layers between input and output. Convolutional Neural Network (CNN) is another type of deep learning technique that is used exclusively with forward-looking architecture for classification. CNNs are commonly used to identify patterns and provide better classification. Recursive Neural Network (RNN) is a branch of sequential information neural network, in which outputs and inputs are interdependent, and its dependence is usually useful in predicting the next state of input. RNNs, like CNNs, require memory to store general information obtained in a sequential deep learning modeling process, and usually only work efficiently for a few generations. The main problem that affects the overall performance of RNN is its sensitivity to the disappearance of gradients, which leads to forgetting the initial input. To prevent this, Long Short-Term Memory (LSTM) is used to create a block between frequent connections. These networks can also be used Bi-directional (BLSTM).</p><p>The combination of CNN and LSTM networks has received much attention in recent years. In the SER task, it is assumed that CNN extracts specific patterns that contain emotional information in the utterance and LSTM pays attention to the temporal behavior during the utterance. Therefore, using the CNN-LSTM architecture can be effective in categorizing Functional features. The use of attention mechanisms in neural networks has shown widespread success in a wide range of tasks, such as question answering, machine translation, natural language understanding and speech recognition. The main idea of the attention mechanism is to focus on a few related parts while ignoring the rest. There are many variations on this mechanism (global vs. local, soft vs. hard) but its main use is to reinforce different LSTM models such as encoder-decoder architecture (for example in machine translation), avoiding the use of a fixed context vector as the only output of the decoder. Specifically, this is the last hidden LSTM layer that carries all the information extracted by the LSTM encoder. Thus, in the classical structure, all information is compressed into a context vector, which can act as a bottleneck, while all hidden middle layers of the encoder are ignored. This vector is then passed to subsequent layers such as the LSTM or dense decoder. In later steps we rely only on this type of summary given by the encoder, and by increasing the length of the time sequence analyzed, the performance of the model can be reduced. To deal with this problem, the attention mechanism is very effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>The results of our experiments are divided into 3 general sections:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1)</head><p>We first compare the results of applying different deep learning models to the features of Hand Crafted LLDs. In this section, we extract the features of Hand Crafted LLDs in high resolution (32ms frames) and low resolution (100ms frames). All models of deep learning in this section receive inputs (batch size, number of frames, number of features per frame) and after passing different layers, at the end, by the softmax function, the class that is most likely will be determined. The DNN used in the experiment is a Fully connected network consisting of dense layers with batch normalization and drop out to prevent overfitting. The use of the BLSTM and BLSTM networks along with the attention mechanism are two other models that were tested, and finally a CNN-BLSTM network that uses the attention mechanism was also tested. The architecture of this network is such that it consists of 1dimensional convolution layers with 1-dimensional max pooling, followed by a bidirectional LSTM layer that uses the attention mechanism. The results show that the use of LSTM recursive network on LLD features that are sequential has improved the model performance over DNN. It is also much more effective than LSTM alone in recognizing emotion in spoken utterances using the attention mechanism associated with BLSTM. Finally, using convolution layers before BLSTM attention, more prominent components in LLDs were extracted first, which improved the performance of the model. It should be noted that this network in low resolution (100ms frames) can not have the same performance as high resolution because at low resolutions, convolutional layers cannot create effective representations of LLD properties. The results of this section are given in <ref type="table">Table 1</ref>.</p><p>2) In previous experiments, we showed that a neural network with a CNN-BLSTM architecture with an attention mechanism can perform well on Hand Crafted LLDs. In this section, we test this model on the eGeMAPS (25 LLDs) and ComParE_2016 (65 LLDs) feature lists extracted by openSMILE tool. As shown in <ref type="table">Table 2</ref>, this model still performs better on Hand Crafted LLDs than other features.</p><p>3) At this section, we test different models of deep learning on functional features. The neural networks used in this section receive the inputs (batch size, number of features per utterance) and after passing through different layers, like the previous models, the softmax function of the class with the most Specifies the probability. A DNN network consisting of fully connected layers is a CNN network with 1-dimensional convolutional layers. In addition, an SVM model was tested for comparison according to the settings used in the ShEMO paper. The feature lists used, along with the number of their functional features, are listed in <ref type="table">Table 3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this study, we used different deep and machine learning techniques to recognize emotions. Also, due to the novelty of using different deep learning techniques in Farsi, we introduced the best model for recognizing emotions in Farsi and finally we were able to obtain higher accuracy than traditional methods. We also observed how the use of different techniques and tools to extract features and also the use of attention mechanism helps to increase the accuracy..</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>The INTERSPEECH 2010 Paralinguistic Challenge feature set (IS10_Paraling): It uses 21 different statistical functions (such as standard deviation, arithmetic mean, skewness, kurtosis, etc.) to 34 LLD features (including MFCC features, logarithmic power of Mel-frequency bands and the loudness as the normalized intensity, etc.) is obtained along with their delta. In addition, there are a number of pitch-related features that ultimately result in a 1582-dimensional vector feature for each utterance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Example of a figure caption.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I .</head><label>I</label><figDesc>TABLE STYLES</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE III.</cell><cell>TABLE STYLES</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Feature Set</cell><cell>Method</cell><cell>Functionals UA</cell><cell>WA</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SVM</cell><cell>65.02</cell><cell>78.29</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>IS10_Paraling</cell><cell>DNN</cell><cell>64.78</cell><cell>76.63</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CNN</cell><cell>63.77</cell><cell>76.70</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SVM</cell><cell>58.01</cell><cell>73.90</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>eGeMAPS</cell><cell>DNN</cell><cell>56.05</cell><cell>72.45</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CNN</cell><cell>57.62</cell><cell>73.63</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>DNN</cell><cell>64.33</cell><cell>76.80</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>emo_large</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CNN</cell><cell>65.20</cell><cell>77.61</cell></row><row><cell>Frame</cell><cell cols="3">Hand Crafted LLDs</cell></row><row><cell>Length</cell><cell>Method</cell><cell></cell><cell>UA</cell><cell>WA</cell></row><row><cell>BLSTM</cell><cell></cell><cell></cell><cell>54.13</cell><cell>69.75</cell></row><row><cell cols="2">Attention-BLSTM</cell><cell></cell><cell>60.94</cell><cell>74.61</cell></row><row><cell>100ms</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">CNN + Attention-BLSTM</cell><cell>59.75</cell><cell>73.59</cell></row><row><cell>DNN</cell><cell></cell><cell></cell><cell>51.78</cell><cell>64.38</cell></row><row><cell>BLSTM</cell><cell></cell><cell></cell><cell>51.04</cell><cell>66.91</cell></row><row><cell cols="2">Attention-BLSTM</cell><cell></cell><cell>58.06</cell><cell>72.21</cell></row><row><cell>32ms</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">CNN + Attention-BLSTM</cell><cell>63.52</cell><cell>75.32</cell></row><row><cell>DNN</cell><cell></cell><cell></cell><cell>49.77</cell><cell>64.18</cell></row><row><cell cols="2">TABLE II.</cell><cell cols="2">TABLE STYLES</cell></row><row><cell>Feature Set</cell><cell></cell><cell cols="3">CNN + Attention-BLSTM UA WA</cell></row><row><cell>ComParE_2016</cell><cell></cell><cell>61.00</cell><cell></cell><cell>74.40</cell></row><row><cell>eGeMAPS</cell><cell></cell><cell>60.45</cell><cell></cell><cell>73.63</cell></row><row><cell>Hand Crafted Features</cell><cell></cell><cell>63.52</cell><cell></cell><cell>75.32</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On certain integrals of Lipschitz-Hankel type involving products of Bessel functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Eason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">N</forename><surname>Sneddon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phil. Trans. Roy. Soc. London</title>
		<imprint>
			<biblScope unit="volume">247</biblScope>
			<biblScope unit="page" from="529" to="551" />
			<date type="published" when="1955-04" />
		</imprint>
	</monogr>
	<note>references</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Oxford: Clarendon, 1892</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J. Clerk</forename><surname>Maxwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A Treatise on Electricity and Magnetism</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="68" to="73" />
		</imprint>
	</monogr>
	<note>3rd ed.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fine particles, thin films and exchange anisotropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Bean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Magnetism</title>
		<editor>III, G.T. Rado and H. Suhl</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1963" />
			<biblScope unit="page" from="271" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Title of paper if known</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Elissa</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>unpublished</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Title of paper with only first word capitalized</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nicole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Name Stand. Abbrev</title>
		<imprint/>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Electron spectroscopy studies on magneto-optical media and plastic substrate interface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yorozu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digests 9th Annual Conf. Magnetics Japan</title>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">301</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The Technical Writer&apos;s Handbook. Mill Valley</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Young</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
		<respStmt>
			<orgName>CA: University Science</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
