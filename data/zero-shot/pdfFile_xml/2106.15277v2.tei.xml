<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Perception-Aware Multi-Sensor Fusion for 3D LiDAR Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuangwei</forename><surname>Zhuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Li</surname></persName>
							<email>selirong@mail.scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Pazhou Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
							<email>kuijia@scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Pazhou Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qicheng</forename><surname>Wang</surname></persName>
							<email>wangqicheng@minieye.cc</email>
							<affiliation key="aff2">
								<orgName type="institution">Shenzhen Youjia Innov Tech Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Pazhou Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
							<email>mingkuitan@scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Pazhou Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Perception-Aware Multi-Sensor Fusion for 3D LiDAR Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D LiDAR (light detection and ranging) semantic segmentation is important in scene understanding for many applications, such as auto-driving and robotics. For example, for autonomous cars equipped with RGB cameras and LiDAR, it is crucial to fuse complementary information from different sensors for robust and accurate segmentation. Existing fusion-based methods, however, may not achieve promising performance due to the vast difference between the two modalities. In this work, we investigate a collaborative fusion scheme called perception-aware multi-sensor fusion (PMF) to exploit perceptual information from two modalities, namely, appearance information from RGB images and spatio-depth information from point clouds. To this end, we first project point clouds to the camera coordinates to provide spatio-depth information for RGB images. Then, we propose a two-stream network to extract features from the two modalities, separately, and fuse the features by effective residual-based fusion modules. Moreover, we propose additional perception-aware losses to measure the perceptual difference between the two modalities. Extensive experiments on two benchmark data sets show the superiority of our method. For example, on nuScenes, our PMF outperforms the state-of-the-art method by 0.8% in mIoU.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic scene understanding is a fundamental task for many applications, such as auto-driving and robotics <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref>. Specifically, in the scenes of auto-driving, it provides fine-grained environmental information for high-level motion planning and improves the safety of autonomous cars <ref type="bibr">[3,</ref><ref type="bibr" target="#b17">18]</ref>. One of the important tasks in semantic scene understanding is semantic segmentation, which assigns a class label to each data point in the input data, and helps autonomous cars to better understand the environment.</p><p>According to the sensors used by semantic segmenta- ? Corresponding authors.  <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b53">54]</ref> and perspective projection. With spherical projection, most of the appearance information from RGB images is lost. Instead, we preserve the information of images with perspective projection. To distinguish different classes, we colorize the point clouds using semantic labels from SemanticKITTI.</p><p>tion methods, recent studies can be divided into three categories: camera-only methods <ref type="bibr">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b57">58]</ref>, LiDARonly methods <ref type="bibr">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b61">62]</ref> and multi-sensor fusion methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b58">59]</ref>. Camera-only methods have achieved great progress with the help of a massive amount of open-access data sets <ref type="bibr">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14]</ref>. Since images obtained by a camera are rich in appearance information (e.g., texture and color), camera-only methods can provide fine-grained and accurate semantic segmentation results. However, as passive sensors, cameras are susceptible to changes in lighting conditions and are thus unreliable <ref type="bibr" target="#b49">[50]</ref>. <ref type="bibr">1</ref> To address this problem, researchers conduct semantic segmentation on point clouds from LiDAR. Compared with camera-only approaches, LiDAR-only methods are more robust to different light conditions, as LiDAR provides reliable and accurate spatio-depth information on the physical world. Unfortunately, LiDAR-only semantic segmentation is challenging due to the sparse and irregular distribution of point clouds. In addition, point clouds lack texture and color information, resulting in high classification error in the fine-grained segmentation task of LiDAR-only methods. A straightforward solution for addressing both drawbacks of camera-only and LiDAR-only methods is to fuse the multimodal data from both sensors, i.e., multi-sensor fusion methods. Nevertheless, due to the large domain gap between RGB cameras and LiDAR, multi-sensor fusion is still a nontrivial task.</p><p>In multi-sensor fusion methods, fusing multimodal data from different sensors is an important problem. Existing fusion-based methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b51">52]</ref> mainly project dense image features to the LiDAR coordinates using spherical projection <ref type="bibr" target="#b39">[40]</ref> and conduct feature fusion in the sparse LiDAR domain. However, these methods suffer from a critical limitation: as the point clouds are very sparse, most of the appearance information from the RGB images is missing after projecting it to the LiDAR coordinates. For example, as shown in <ref type="figure" target="#fig_2">Figure 1 (c)</ref>, the car and motorcycle in the image become distorted with spherical projection. As a result, existing fusion-based methods have difficulty capturing the appearance information from the projected RGB images.</p><p>In this paper, we aim to exploit an effective multi-sensor fusion method. Unlike existing methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b51">52]</ref>, we assume and highlight that the perceptual information from both RGB images and point clouds, i.e., appearance information from images and spatio-depth information from point clouds, is important in fusion-based semantic segmentation. Based on this intuition, we propose a perception-aware multi-sensor fusion (PMF) scheme that conducts collaborative fusion of perceptual information from two modalities of data in three aspects. First, we propose a perspective projection to project the point clouds to the camera coordinate system to obtain additional spatio-depth information for RGB images. Second, we propose a two-stream network (TSNet) that contains a camera stream and a LiDAR stream to extract perceptual features from multimodal sensors separately. Considering that the information from images is unreliable in an outdoor environment, we fuse the image features to the LiDAR stream by effective residual-based fusion (RF) modules, which are designed to learn the complementary features of the original LiDAR modules. Third, we propose perception-aware losses to measure the vast perceptual difference between the two data modalities and boost the fusion of different perceptual information. Specifically, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the perceptual features captured by the camera stream and LiDAR stream are different. Therefore, we use the predictions with higher confidence to supervise those with lower confidence.</p><p>Our contributions are summarized as follows. First, we propose a perception-aware multi-sensor fusion (PMF) scheme to effectively fuse the perceptual information from RGB images and point clouds. Second, by fusing the spatiodepth information from point clouds and appearance information from RGB images, PMF is able to address segmentation with undesired light conditions and sparse point clouds. More critically, PMF is robust to adversarial samples of RGB images by integrating the information from point clouds. Third, we introduce perception-aware losses into the network and force the network to capture the perceptual information from two different-modality sensors. The extensive experiments on two benchmark data sets demonstrate the superior performance of our method. For example, on nuScenes <ref type="bibr" target="#b6">[7]</ref>, PMF outperforms Cylinder3D <ref type="bibr" target="#b63">[64]</ref>, a state-ofthe-art LiDAR-only method, by 0.8% in mIoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we revisit the existing literature on 2D and 3D semantic segmentation, i.e., camera-only methods, LiDAR-only methods and multi-sensor fusion methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Camera-Only Methods</head><p>Camera-only semantic segmentation aims to predict the pixel-wise labels of 2D images. FCN <ref type="bibr" target="#b34">[35]</ref> is a fundamental work in semantic segmentation, which proposes an end-toend fully convolutional architecture based on image classification networks. In addition to FCN, recent works have achieved significant improvements via exploring multi-scale information <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b62">63]</ref>, dilated convolution <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b52">53]</ref>, and attention mechanisms <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b57">58]</ref>. However, camera-only methods are easily disturbed by lighting (e.g., underexposure or overexposure) and may not be robust to outdoor scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">LiDAR-Only Methods</head><p>To address the drawbacks of cameras, LiDAR is an important sensor on an autonomous car, as it is robust to more complex scenes. According to the preprocessing pipeline, existing methods for point clouds mainly contains two categories, including direct methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b63">64]</ref> and projectionbased methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref>. Direct methods perform semantic segmentation by processing the raw 3D point clouds directly. PointNet <ref type="bibr" target="#b43">[44]</ref> is a pioneering work in this category that extracts point cloud features by multi-layer perception. A subsequent extension, i.e., PointNet++ <ref type="bibr" target="#b44">[45]</ref>, further aggregates a multi-scale sampling mechanism to aggregate global and local features. However, these methods do not consider the varying sparsity of point clouds in outdoor scenes. Cylin-der3D <ref type="bibr" target="#b63">[64]</ref> addresses this issue by using 3D cylindrical partitions and asymmetrical 3D convolutional networks. However, direct methods have a high computational complexity, which limits their applicability in auto-driving. Projectionbased methods are more efficient because they convert 3D point clouds to a 2D grid. In projection-based methods, researchers focus on exploiting effective projection methods, such as spherical projection <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b53">54]</ref> and bird's-eye projection <ref type="bibr" target="#b61">[62]</ref>. Such 2D representations allow researchers to investigate efficient network architectures based on existing 2D convolutional networks <ref type="bibr">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr">21]</ref>. In addition to projectionbased methods, one can easily improve the efficiency of networks by existing neural architecture search <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b41">42]</ref> and model compression techniques <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b56">57</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Multi-Sensor Fusion Methods</head><p>To leverage the benefits of both camera and LiDAR, recent work has attempted to fuse information from two complementary sensors to improve the accuracy and robustness of the 3D semantic segmentation algorithm <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b51">52]</ref>. RGBAL <ref type="bibr" target="#b36">[37]</ref> converts RGB images to a polar-grid mapping representation and designs early and mid-level fusion strategies. PointPainting <ref type="bibr" target="#b51">[52]</ref> obtains the segmentation results of images and projects them to the LiDAR space by using bird's-eye projection <ref type="bibr" target="#b61">[62]</ref> or spherical projection <ref type="bibr" target="#b39">[40]</ref>. The projected segmentation scores are concatenated with the original point cloud features to improve the performance of LiDAR networks. Unlike existing methods that perform Project the point clouds P by using perspective projection to obtain X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>Use { X, X} as the inputs of TSNet and compute the output probabilities { O, O} with Eq. (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Compute the perceptual confidence C and C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Construct perception-aware losses to measure the perceptual difference with Eqs. <ref type="bibr" target="#b6">(7)</ref> and (10). <ref type="bibr">6:</ref> Update M and M by minimizing the objective in Eqs. <ref type="bibr" target="#b7">(8)</ref> and <ref type="bibr" target="#b10">(11)</ref>. 7: end while feature fusion in the LiDAR domain, PMF exploits a collaborative fusion of multimodal data in camera coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this work, we propose a perception-aware multi-sensor fusion (PMF) scheme to perform effective fusion of the perceptual information from both RGB images and point clouds. Specifically, as shown in <ref type="figure">Figure 3</ref>, PMF contains three components: (1) perspective projection; (2) a twostream network (TSNet) with residual-based fusion modules;</p><p>(3) perception-aware losses. The general scheme of PMF is shown in Algorithm 1. We first project the point clouds to the camera coordinate system by using perspective projection. Then, we use a two-stream network that contains a camera stream and a LiDAR stream to extract perceptual features from the two modalities, separately. The features from the camera stream are fused into the LiDAR stream by residualbased fusion modules. Finally, we introduce perceptionaware losses into the optimization of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formulation of Perspective Projection</head><p>Existing methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b51">52]</ref> mainly project images to the LiDAR coordinate system using spherical projection. However, due to the sparse nature of point clouds, most of the appearance information from the images is lost with spherical projection (see <ref type="figure" target="#fig_2">Figure 1</ref>). To address this issue, we propose perspective projection to project the sparse point clouds to the camera coordinate system.</p><p>Let {P, X, y} be one of the training samples from a given data set, where P ? R 4?N indicates a point cloud from LiDAR and N denotes the number of points. Each point P i in point cloud P consists of 3D coordinates (x, y, z) and a reflectance value (r). Let X ? R 3?H?W be an image from an RGB camera, where H and W represent the height and width of the image, respectively. y ? R N is the set of semantic labels for point cloud P.</p><p>In perspective projection, we aim to project the point cloud P from LiDAR coordinate to the camera coordinate to obtain the 2D LiDAR features X ? R C?H?W . Here, C indicates the number of channels w.r.t. the projected point cloud. Following <ref type="bibr" target="#b16">[17]</ref>, we obtain P i = (x, y, z, 1) by appending a fourth column to P i and compute the projected point P i = ( x, y, z) in the camera coordinates by</p><formula xml:id="formula_0">P i = TRP i ,<label>(1)</label></formula><p>where T ? R 3?4 is the projection matrix from LiDAR coordinates to camera coordinates. R ? R 4?4 is expanded from the rectifying rotation matrix R (0) ? R 3?3 by appending a fourth zero row and column and setting R(4, 4) = 1. The calibration parameters T and R (0) can be obtained by the approach in <ref type="bibr" target="#b18">[19]</ref>. Subsequently, the corresponding pixel (h, w) in the projected image X w.r.t. the point P i is computed by h = x/ z and w = y/ z. Because the point cloud is very sparse, each pixel in the projected X may not have a corresponding point p. Therefore, we first initialize all pixels in X to 0. Following <ref type="bibr" target="#b12">[13]</ref>, we then compute 5-channel LiDAR features, i.e., (d, x, y, z, r), for each pixel (h, w) in the projected 2D image X, where d = x 2 + y 2 + z 2 represents the range value of each point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Architecture Design of PMF</head><p>As images and point clouds are different-modality data, it is difficult to handle both types of information from the two modalities by using a single network <ref type="bibr" target="#b29">[30]</ref>. Motivated by <ref type="bibr" target="#b14">[15,</ref><ref type="bibr">49]</ref>, we propose a two-stream network (TSNet) that contains a camera stream and a LiDAR stream to process the features from camera and LiDAR, separately, as illustrated  </p><formula xml:id="formula_1">O = M (X), O = M ( X).<label>(2)</label></formula><p>Since the features of images contain many details of objects, we then introduce a residual-based fusion module, as illustrated in <ref type="figure" target="#fig_4">Figure 4</ref>, to fuse the image features to the Li-DAR stream. Let {F l ? R C l ?H l ?W l } L l=1 be a set of image features from the camera stream, where l indicates the layer in which we obtain the features. C l indicates the number of channels of the l-th layer in the camera stream. H l and W l indicate the height and width of the feature maps from the l-th layer, respectively. Let { F l ? R C l ?H l ?W l } L l=1 be the features from the LiDAR stream, where C l indicates the number of channels of the l-th layer in the LiDAR stream. To obtain the fused features, we first concatenate the features from each network and use a convolutional layer to reduce the number of channels of the fused features. The fused</p><formula xml:id="formula_2">features F f use l ? R C l ?H l ?W l are computed by F f use l = f l ([ F l ; F l ]),<label>(3)</label></formula><p>where [?; ?] indicates the concatenation operation. f l (?) is the convolution operation w.r.t. the l-th fusion module.</p><p>Considering that the camera is easily affected by different lighting and weather conditions, the information from RGB images is not reliable in an outdoor environment. We use the fused features as the complement of the original LiDAR features and design the fusion module based on the residual structure <ref type="bibr" target="#b23">[24]</ref>. Incorporating with the attention module <ref type="bibr">[5]</ref>, the output features F out l ? R C l ?H l ?W l of the fusion module are computed by</p><formula xml:id="formula_3">F out l = F l + ?(g l (F f use l )) F f use l ,<label>(4)</label></formula><p>where ?(x) = 1/(1 + e ?x ) indicates the sigmoid function. g l (?) indicates the convolution operation in the attention module w.r.t. the l-th fusion module. indicates the elementwise multiplication operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Construction of Perception-Aware Loss</head><p>The construction of perception-aware loss is very important in our method. As demonstrated in <ref type="figure" target="#fig_1">Figure 2</ref>, because the point clouds are very sparse, the LiDAR stream network learns only the local features of points while ignoring the shape of objects. In contrast, the camera stream can easily capture the shape and texture of objects from dense images. In other words, the perceptual features captured by the camera stream and LiDAR stream are different. With this intuition, we introduce a perception-aware loss to make the fusion network focus on the perceptual features from the camera and LiDAR.</p><p>To measure the perceptual confidence of the predictions w.r.t. the LiDAR stream, we first compute the entropy map E ? R H?W by</p><formula xml:id="formula_4">E h,w = ? 1 log S S s=1 O s,h,w log( O s,h,w ).<label>(5)</label></formula><p>Following <ref type="bibr" target="#b45">[46]</ref>, we use log S to normalize the entropy to (0, 1]. Then, the perceptual confidence map C w.r.t. the LiDAR stream is computed by C = 1 ? E. For the camera stream, the confidence map is computed by C = 1 ? E. Note that not all information from the camera stream is useful. For example, the camera stream is confident inside objects but may make mistakes at the edge. In addition, the predictions with lower confidence scores are more likely to be wrong. Incorporating with a confidence threshold, we measure the importance of perceptual information from the camera stream by</p><formula xml:id="formula_5">? h,w = max( C h,w ? C h,w , 0), if C h,w &gt; ?, 0, otherwise.<label>(6)</label></formula><p>Here ? indicates the confidence threshold. Inspired by <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b60">61]</ref>, to learn the perceptual information from the camera stream, we construct the perceptionaware loss w.r.t. the LiDAR stream by</p><formula xml:id="formula_6">L per = 1 Q H h=1 W w=1 ? h,w D KL ( O :,h,w ||O :,h,w ),<label>(7)</label></formula><p>where Q = H ? W and D KL (?||?) indicates the Kullback-Leibler divergence <ref type="bibr" target="#b24">[25]</ref>.</p><p>In addition to the perception-aware loss, we also use multi-class focal loss <ref type="bibr" target="#b31">[32]</ref> and Lov?sz-softmax loss <ref type="bibr">[4]</ref>, which are commonly used in existing segmentation work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b63">64]</ref>, to train the LiDAR stream. <ref type="bibr">2</ref> The objective w.r.t. the LiDAR stream is defined by</p><formula xml:id="formula_7">L = L f oc + ? L lov + ? L per ,<label>(8)</label></formula><p>where L f oc and L lov indicate the multi-class focal loss and Lov?sz-softmax loss, respectively. ? and ? are the hyperparameters that balance different losses. Similar to the LiDAR stream, we construct the objective for the optimization of the camera stream. Following Eq. <ref type="formula" target="#formula_5">(6)</ref>, the importance of the information from the LiDAR stream is computed by</p><formula xml:id="formula_8">? h,w = max(C h,w ? C h,w , 0), if C h,w &gt; ?, 0, otherwise.<label>(9)</label></formula><p>The perception-aware loss w.r.t. the camera stream is</p><formula xml:id="formula_9">L per = 1 Q H h=1 W w=1 ? h,w D KL (O :,h,w || O :,h,w ). (10)</formula><p>Then the objective w.r.t. the camera stream is defined by</p><formula xml:id="formula_10">L = L f oc + ?L lov + ?L per .<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we empirically evaluate the performance of PMF on the benchmark data sets, including Se-manticKITTI <ref type="bibr">[3]</ref> and nuScenes <ref type="bibr" target="#b6">[7]</ref>. SemanticKITTI is a large-scale data set based on the KITTI Odometry Benchmark <ref type="bibr" target="#b17">[18]</ref>, providing 43,000 scans with pointwise semantic annotation, where 21,000 scans (sequence 00-10) are available for training and validation. The data set has 19 semantic classes for the evaluation of semantic benchmarks. nuScenes contains 1,000 driving scenes with different weather and light conditions. The scenes are split into 28,130 training frames and 6,019 validation frames. Unlike SemanticKITTI, which provides only the images of the front-view camera, nuScenes has 6 cameras for different views of LiDAR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We implement the proposed method in PyTorch <ref type="bibr" target="#b42">[43]</ref>, and use ResNet-34 <ref type="bibr" target="#b23">[24]</ref> and SalsaNext <ref type="bibr" target="#b12">[13]</ref> as the backbones of the camera stream and LiDAR stream, respectively. Because we process the point clouds in the camera coordinates, we incorporate ASPP <ref type="bibr" target="#b8">[9]</ref> into the LiDAR stream network to adjust the receptive field adaptively. To leverage the benefits  of existing image classification models, we initialize the parameters of ResNet-34 with the pretrained ImageNet models from <ref type="bibr" target="#b42">[43]</ref>. We also adopt hybrid optimization methods <ref type="bibr" target="#b59">[60]</ref> to train the networks w.r.t. different modalities, i.e., SGD with Nesterov <ref type="bibr" target="#b40">[41]</ref> for the camera stream and Adam <ref type="bibr" target="#b28">[29]</ref> for the LiDAR stream. We train the networks for 50 epochs on both the benchmark data sets. The learning rate starts at 0.001 and decays to 0 with a cosine policy <ref type="bibr" target="#b35">[36]</ref>. We set the batch size to 8 on SemanticKITTI and 24 on nuScenes. We set ?, ?, ? to 0.7,0.5, and 1.0, respectively. <ref type="bibr">3</ref> To prevent overfitting, a series of data augmentation strategies are used, including random horizontal flipping, color jitter, 2D random rotation, and random cropping. Our source code is available at https://github.com/ICEORY/PMF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on SemanticKITTI</head><p>To evaluate our method on SemanticKITTI, we compare PMF with several state-of-the-art LiDAR-only methods including SalsaNext <ref type="bibr" target="#b12">[13]</ref>, Cylinder3D <ref type="bibr" target="#b63">[64]</ref>, etc. Since Se-manticKITTI provides only the images of the front-view camera, we project the point clouds to a perspective view and keep only the available points on the images to build a subset of SemanticKITTI. Following <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b63">64]</ref>, we use sequence 08 for validation. The remaining sequences (00-07 and 09-10) are used as the training set. We evaluate the <ref type="bibr">3</ref> We investigate the effect of ?, ?, ? in the supplementary material. release models of the state-of-the-art LiDAR-only methods on our data set. Because SPVNAS <ref type="bibr" target="#b50">[51]</ref> did not release its best model, we report the result of the best-released model (with 65G MACs). In addition, we reimplement two state-ofthe-art fusion-based methods, i.e., RGBAL <ref type="bibr" target="#b36">[37]</ref> and Point-Painting <ref type="bibr" target="#b51">[52]</ref>, on our data set.</p><p>From <ref type="table" target="#tab_1">Table 1</ref>, PMF achieves the best performance among projection-based methods. For example, PMF outperforms SalsaNext by 4.5% in mIoU. However, PMF performs worse than the state-of-the-art 3D convolutional method, i.e., Cylin-der3D, by 1.0% in mIoU. As long-distance perception is also critical to the safety of autonomous cars, we also conduct a distance-based evaluation on SemanticKITTI. From <ref type="figure" target="#fig_5">Figure 5</ref>, because the point clouds becomes sparse when the distance increases, LiDAR-only methods suffer from great perfor-   <ref type="figure">Figure 7</ref>. Qualitative results on nuScenes. We use the corresponding images (night) as the background of both the predictions and labels. We highlight the difference between the results of PMF and the baseline with the red dashed circle. mance degradation at long distances. In contrast, since the images provide more information for distant objects, fusionbased methods outperform LiDAR-only methods at large distances. Specifically, PMF achieves the best performance when the distance is larger than 30 meters. This suggests that our method is more suitable to address segmentation with sparse point clouds. This ability originates from our fusion strategy, which effectively incorporates RGB images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on nuScenes</head><p>Following <ref type="bibr" target="#b63">[64]</ref>, to evaluate our method on more complex scenes, we compare PMF with the state-of-the-art methods on the nuScenes LiDAR-seg validation set. The experimental results are shown in <ref type="table" target="#tab_2">Table 2</ref>. Note that the point clouds of nuScenes are sparser than those of SemanticKITTI (35k points/frame vs. 125k points/frame). Thus, it is more challenging for 3D segmentation tasks. In this case, PMF achieves the best performance compared with the LiDARonly methods. Specifically, PMF outperforms Cylinder3D by 0.8% in mIoU. Moreover, compared with the state-of-theart 2D convolutional method, i.e., SalsaNext, PMF achieves a 4.7% improvement in mIoU. These results are consistent with our expectation. Since PMF incorporates RGB images, our fusion strategy is capable of addressing such challenging segmentation under sparse point clouds. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Evaluation</head><p>To better understand the benefits of PMF, we visualize the predictions of PMF on the benchmark data sets. <ref type="bibr">4</ref> From <ref type="figure" target="#fig_6">Figure 6</ref>, compared with Cylinder3D, PMF achieves better performance at the edges of objects. For example, as shown in <ref type="figure" target="#fig_6">Figure 6 (d)</ref>, the truck segmented by PMF has a more complete shape. More critically, PMF is robust to different lighting conditions. Specifically, as illustrated in <ref type="figure">Figure 7</ref>, PMF outperforms the baselines on more challenging scenes (e.g., night). In addition, as demonstrated in <ref type="figure" target="#fig_6">Figure 6</ref> (e) and <ref type="figure">Figure 7</ref> (c), PMF generates dense segmentation results that combine the benefits of both the camera and LiDAR, which is significantly different from existing LiDAR-only and fusion-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Adversarial Analysis</head><p>To investigate the robustness of PMF on adversarial samples, we first insert extra objects (e.g., a traffic sign) to the images and keeping the point clouds unchanged. <ref type="bibr">5</ref> In addition, we implement a camera-only method, i.e., FCN <ref type="bibr" target="#b34">[35]</ref>, on SemanticKITTI as the baseline. Note that we do not use any adversarial training technique during training. As demonstrated in <ref type="figure" target="#fig_8">Figure 8</ref>, the camera-only methods are easily affected by changes in the input images. In contrast, <ref type="bibr">4</ref> More visualization results on SemanticKITTI and nuScenes are shown in the supplementary material. <ref type="bibr">5</ref> More adversarial samples are shown in the supplementary material. because PMF integrates reliable point cloud information, the noise in the images is reduced during feature fusion and imposes only a slight effect on the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Efficiency Analysis</head><p>In this section, we evaluate the efficiency of PMF on GeForce RTX 3090. Note that we consider the efficiency of PMF in two aspects. First, since predictions of the camera stream are fused into the LiDAR stream, we remove the decoder of the camera stream to speed up the inference. Second, our PMF is built on 2D convolutions and can be easily optimized by existing inference toolkits, e.g., TensorRT. In contrast, Cylinder3D is built on 3D sparse convolutions <ref type="bibr" target="#b19">[20]</ref> and is difficult to be accelerated by TensorRT. We report the inference time of different models optimized by TensorRT in <ref type="table" target="#tab_5">Table 3</ref>. From the results, our PMF achieves the best performance on nuScenes and is 2.8? faster than Cylinder3D (22.3 ms vs. 62.5 ms) with fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Effect of Network Components</head><p>We study the effect of the network components of PMF, i.e., perspective projection, ASPP, residual-based fusion modules, and perception-aware loss. The experimental results are shown in <ref type="table" target="#tab_5">Table 4</ref>. Since we use only the frontview point clouds of SemanticKITTI, we train SalsaNext as the baseline on our data set using the officially released code. Comparing the first and second lines in <ref type="table" target="#tab_5">Table 4</ref>, perspective projection achieves only a 0.4% mIoU improvement over spherical projection with LiDAR-only input. In contrast, comparing the fourth and fifth lines, perspective projection  <ref type="figure">Figure 9</ref>. Comparisons of the predictions w.r.t. the networks trained with and without perception-aware loss. PL denotes the perceptionaware loss. Red indicates predictions with higher confidence scores. We only show the predictions of Car for the sake of clarity. brings a 5.9% mIoU improvement over spherical projection with multimodal data inputs. From the third and fifth lines, our fusion modules bring 2.0% mIoU improvement to the fusion network. Moreover, comparing the fifth and sixth lines, the perception-aware losses improve the performance of the network by 2.2% in mIoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Effect of Perception-Aware Loss</head><p>To investigate the effect of perception-aware loss, we visualize the predictions of the LiDAR stream networks with and without perception-aware loss in <ref type="figure">Figure 9</ref>. From the results, perception-aware loss helps the LiDAR stream capture the perceptual information from the images. For example, the model trained with perception-aware loss learns the complete shape of cars, while the baseline model focuses only on the local features of points. As the perception-aware loss introduces the perceptual difference between the RGB images and the point clouds, it enables an effective fusion of the perceptual information from the data of both modalities. As a result, our PMF generates dense predictions that combine the benefits of both the images and point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we have proposed a perception-aware multisensor fusion scheme for 3D LiDAR semantic segmentation. Unlike existing methods that conduct feature fusion in the LiDAR coordinate system, we project the point clouds to the camera coordinate system to enable a collaborative fusion of the perceptual features from the two modalities. Moreover, by fusing complementary information from both cameras and LiDAR, PMF is robust to complex outdoor scene. The experimental results on two benchmarks show the superiority of our method. In the future, we will extend PMF to other challenging tasks in auto-driving, e.g., object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material for "Perception-Aware Multi-Sensor Fusion for 3D LiDAR Semantic Segmentation"</head><p>Zhuangwei Zhuang We organize our supplementary material as follows.</p><p>? In Section 1, we introduce the details of the two additional losses, namely, multi-class focal loss and Lov?sz-softmax loss, in the objective of Perception-aware Multi-sensor Fusion (PMF).</p><p>? In Section 2, we give more implementation details of PMF.</p><p>? In Section 3, we investigate the effect of the proposed residual-based fusion module.</p><p>? In Section 4, we study the effect of hyperparameters ?, ?, ?.</p><p>? In Section 5, we provide more visualization results of PMF on SemanticKITTI validation set.</p><p>? In Section 6, we show more visualization results of PMF on nuScenes validation set.</p><p>? In Section 7, we give more visualization results of PMF on the adversarial samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Details of the Loss Functions</head><p>For convenience, we show the architecture of PMF in <ref type="figure">Figure I</ref>. In Section 3.3 of the main paper, we have introduced the objective of PMF. Specifically, we propose the perception-aware losses to measure the vast perceptual difference between the data from RGB camera and LiDAR. Following <ref type="bibr">[2,</ref><ref type="bibr">6]</ref>, we use the multi-class focal loss <ref type="bibr">[4]</ref> to address the class imbalance issue and Lov?sz-softmax loss <ref type="bibr">[1]</ref> to measure the scale variance in semantic segmentation, respectively. Therefore, the objective L w.r.t. the LiDAR stream in two-stream network (TSNet) is defined as</p><formula xml:id="formula_11">L = L f oc + ? L lov + ? L per ,</formula><p>where L f oc , L lov , L per indicate the multi-class focal loss, Lov?sz-softmax loss and perception-aware loss w.r.t. the LiDAR stream, respectively. Here, ? and ? are hyper-parameters. The objective L w.r.t. the camera stream in TSNet is defined as</p><formula xml:id="formula_12">L = L f oc + ?L lov + ?L per ,</formula><p>where L f oc , L lov , L per indicate the multi-class focal loss, Lov?sz-softmax loss and perception-aware loss w.r.t. the camera stream, respectively.</p><p>In this section, we give the details of the multi-class focal loss <ref type="bibr">[4]</ref> and Lov?sz-softmax loss <ref type="bibr">[1]</ref> in the objective of PMF. Let {P, X, y} be one of the training samples from a given data set, where P ? R 4?N indicates a point cloud, N denotes the number of points. Each point P i = (x, y, z, r) ? consists of 3D coordinates (x, y, z) and an reflectance value (r). y ? R N denotes the semantic labels for point cloud P. Let Y ? R H?W be the projected labels in the camera coordinates. H and W indicate the height and width, respectively. For each point P i , we project the 3D coordinates (x, y, z) to the pixel (h, w) in the camera coordinate system by using perspective projection. Then, we initialize all pixels in Y by 0 and compute the projected labels in Y by Y h,w := y i . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">More Implementation Details of PMF</head><p>In this section, we give more implementation details of PMF. We first discuss the number of fusion modules in the two-stream network (See <ref type="figure">Figure I)</ref> and then introduce the method to obtain the sparse predictions from the dense ones. Number of fusion modules. In <ref type="figure">Figure I</ref>, we insert L fusion modules into the LiDAR stream to fuse the features from the camera. Note that one can add the fusion modules after each layer in the network. However, this can be computationally expensive yet unnecessary. Inspired by <ref type="bibr">[3,</ref><ref type="bibr">5]</ref>, we only insert four fusion modules into the LiDAR stream to fuse the multi-scale features from the camera stream. Specifically, we fuse the camera features from the 7-th, 15-th, 27-th, 33-th convolutional layers of ResNet-34 into the LiDAR features from the 14-th, 19-th, 24-th, 29-th convolutional layers of SalsaNext, respectively. Method to obtain the sparse predictions. With the proposed perception-aware losses, PMF generates dense segmentation results with information from RGB images and point clouds. We then obtain the sparse prediction from the dense results. Let O ? R S?H?W be the output probabilities of the LiDAR stream. S indicates the number of classes. H and W indicate the height and width of the predictions, respectively. Let Y ? R H?W be the dense predictions from the LiDAR stream. Then, the dense predictions is computed by</p><formula xml:id="formula_13">Y h,w = arg max s O s,h,w .</formula><p>Let y ? R N be the sparse predictions of point cloud P. As shown in <ref type="figure">Figure II</ref>, for each point P i , we first project the 3D coordinates (x, y, z) to the camera coordinate system by using perspective projection and compute the corresponding pixel (h, w) in the projected image. Then the semantic prediction y i w.r.t. the point P i is computed by</p><formula xml:id="formula_14">y i := Y h,w .</formula><p>Note that for the point cloud with multi-camera views, e.g., nuScenes, there are overlaps between different camera views. To address this issue, we first run forward propagation for each camera view and merge the results by assigning the predictions with the highest confidence scores to the points in the overlaps of different views.  <ref type="figure">Figure II</ref>. Illustration of the pipeline to obtain the sparse segmentation from the dense prediction results. X indicates the projected point cloud. Y and y indicate the dense predictions and sparse predictions, respectively. For each point Pi, we first compute the corresponding pixel (h, w) in the camera coordinate system by perspective projection. Second, we get the dense segmentation Y from the prediction results of PMF. Last, we obtain the corresponding sparse prediction yi w.r.t. the point Pi from the dense segmentation Y h,w .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Effect of Residual-based Fusion Module</head><p>In Section 3.2 of the main paper, we have proposed the residual-based fusion (RF) modules to fuse the features from RGB images into the LiDAR stream. To investigate the effect of components in RF modules, we replace the fusion modules in PMF with two variants of RF modules (see <ref type="figure">Figure III</ref>) and evaluate the performance on SemanticKITTI. From <ref type="table" target="#tab_5">Table A</ref>, the residual connection improves the performance by 1.2% in mIoU. Besides, the attention module yields an improvement of 0.8% in mIoU. These results demonstrate the effectiveness of each component in the proposed residual-based fusion module.</p><p>To further study the effect of the proposed residual-based fusion modules, we visualize the features of the first fusion module in PMF. From <ref type="figure" target="#fig_14">Figure IV</ref>, features from RGB images provide more appearance information (e.g., texture) than those from point clouds. With the proposed fusion module, PMF is able to fuse both information from RGB camera and LiDAR. Besides, the noises from RGB images (e.g., the shadows of trees) are also reduced during feature fusion.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Effect of hyperparameters ?, ?, ?</head><p>To investigate the effect of ? , we first set ? and ? to 1 and train PMF with ? ? {0.1, 0.3, 0.5, 0.7, 0.9} on SemanticKITTI. From <ref type="table" target="#tab_5">Table B</ref>, the model with ? = 0.7 achieves the best performance on the benchmark data set. We then set ? = 0.7 and ? = 1 to train models with ? ? {0.0, 0.5, 1.0, 5.0, 10.0}. From <ref type="table" target="#tab_5">Table C</ref>, PMF achieves the best performance with ? = 0.5. Last, we set ? to 0.7 and ? to 0.5 to train models with ? ? {0.0, 0.5, 1.0, 1.5, 2.0}. From <ref type="table" target="#tab_5">Table D</ref>, the model with ? = 1.0 achieves the best result on the data set. Therefore, in our experiments, we set ?, ?, ? to 0.7, 0.5, and 1.0.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">More Visualization Results on SemanticKITTI</head><p>In Section 4.4 of the main paper, we have provided the qualitative results on SemanticKITTI. In this section, we give more visualization results on SemanticKITTI in <ref type="figure" target="#fig_14">Figure V</ref>. From the results, our PMF is robust to different lighting conditions in RGB images, such as the shadows of trees and exposure on the surface of buildings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">More Visualization Results on nuScenes</head><p>We provide more visualization results on nuScenes in <ref type="figure" target="#fig_14">Figure VI</ref>. From the results, our PMF shows its superiority on more challenging scenes, i.e., night-time and sparse point clouds. For example, as shown in the 5-th to 8-th row in <ref type="figure" target="#fig_14">Figure VI</ref>, our PMF still performs well when most of the information from RGB images is missing at night. These results suggest that our method can address the segmentation with different lighting conditions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">More Visualization Results on Adversarial Samples</head><p>We provide more results of PMF on adversarial samples in <ref type="figure" target="#fig_14">Figure VII</ref>. To obtain the adversarial samples, we insert extract objects, i.e., car, traffic sign, bicyclist, into RGB images and keeping the point clouds no changed. From the results, our PMF reduces most of the noises from the images and is more robust to the adversarial samples than the camera-based methods. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( d )Figure 1 .</head><label>d1</label><figDesc>Results of Perspective Projection (c) Results of Spherical Projection (a) Input RGB Image (b) Input Point Cloud 0030 Comparisons of spherical projection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Comparisons of the predictions from images and point clouds. Deep neural networks capture different perceptual information from RGB images and point clouds. Red indicates predictions with higher scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>General Scheme of PMF Require: Training data {P, X, y}, TSNet with submodels M, M , hyperparameters ?, ?, ?. 1: while not convergent do 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of the residual-based fusion (RF) module. RF fuses features from both the camera and LiDAR to generate the complementary information of the original LiDAR features. in Figure 3. In this way, we can use the network architectures designed for images and point clouds as the backbones of each stream in TSNet. Let M and M be the LiDAR stream and the camera stream in TSNet, respectively. Let O ? R S?H?W and O ? R S?H?W be the output probabilities w.r.t. each network, where S indicates the number of semantic classes. The outputs of TSNet are computed by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Distance-based evaluation on SemanticKITTI. As the distance increases, the point cloud become sparse.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative results on SemanticKITTI. The red dashed circle indicates the difference between the results of PMF and the baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Ground-truth (c) PMF-dense (Ours)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>(b) Input Point Clouds (c) FCN (Camera-only) (d) PMF (Ours) (a) Input Images with Noise Comparisons of PMF and camera-only methods on adversarial samples. The camera-only methods use only RGB images as inputs, while PMF uses both images and point clouds as inputs. We highlight the inserted traffic sign with red box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(a) Predictions of Car without PL (b) Predictions of Car with PL</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure I .?? 1 ?</head><label>I1</label><figDesc>Illustration of perception-aware multi-sensor fusion (PMF). PMF consists of three components: (1) perspective projection; (2) a two-stream network (TSNet) with feature fusion modules; and (3) perception-aware losses Lper, Lper w.r.t. the camera stream and the LiDAR stream, respectively. We first project the point clouds to camera coordinate with perspective projection and learn the features from both RGB images and point clouds using TSNet. Then, the image features are fused into the LiDAR stream network by fusion modules. Last, we use perception-aware losses to measure the perceptual difference between the two modalities.Multi-class focal loss. Let F L(p) = ?(1 ? p) 2 log(p) be the focal-loss function. O ? R S?H?W indicates the output probabilities of the LiDAR stream, where S denotes the number of classes. The multi-class focal loss w.r.t. the LiDAR stream is defined as L w = s}F L( O s,h,w ), h,w = s} indicates the number of available labels. 1{?} indicates the indicator function. Let O ? R S?H?W denotes the output probabilities of the camera stream. Then, the multi-class focal loss w.r.t. the camera stream is w = s}F L(O s,h,w ), Lov?sz-softmax loss. The Lov?sz-softmax loss w.r.t. the LiDAR stream is defined as L lov = 1 S S s=1 Js ( m(s)), where m i (s) = 1 ? O s,h,w if s = Y h,w , O s,h,w otherwise. Js indicates the Lov?sz extension of the Jaccard index for class s. Here, (h, w) is obtained from the 3D coordinates (x, y, z) of P i by using perspective projection. m(s) ? [0, 1] N indicates the vector of errors. The Lov?sz-softmax loss w.r.t. the camera stream is defined as O s,h,w if s = Y h,w , O s,h,w otherwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure</head><label></label><figDesc>III. Illustration of different fusion modules. (a) indicates the naive concatenation fusion, (b) indicates the naive concatenation with residual connection, (c) indicates our residual-based fusion module.(a) LiDAR Features (b) Camera Features (c) Output Features Figure IV. Visualization of features of the first residual-based fusion module in PMF. For clarity, we only visualize the first 16 feature maps of the LiDAR features, camera features, and output features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>mIoU (%) 61.7 63.9 63.6 63.7 63.6 Table D. Effect of ?. We highlight the best result in bold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure V .</head><label>V</label><figDesc>More visualization results on SemanticKITTI. Better views by zoom in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure</head><label></label><figDesc>VI. More visualization results on nuScenes. Better views by zoom in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>(b) Input Point Clouds (c) FCN (Image-only) (d) PMF (Ours) (e) Ground-truth (a) Input Images with Noise Figure VII. More visualization results of PMF on adversarial samples. FCN only uses RGB images as inputs, while PMF uses both RGB images and point clouds as inputs. We highlight the position of the inserted objects by red boxes. Better views by zoom in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Lper, Lper w.r.t. the camera stream and the LiDAR stream. We first project the point clouds to camera coordinate with perspective projection and learn the features from both the RGB images and point clouds using TSNet. The image features are fused into the LiDAR stream network by fusion modules. Last, we use perception-aware losses to help the network focus on the perceptual features of both images and point clouds.</figDesc><table><row><cell>Original Point Cloud</cell><cell>RGB Image</cell><cell></cell><cell>?</cell><cell>Perceptual</cell></row><row><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>Differenc?</cell></row><row><cell>Perspective Projection</cell><cell>Projected Point Cloud</cell><cell>Two-stream Network</cell><cell>Segmentation Results</cell><cell>Perceptual Confidence</cell></row><row><cell></cell><cell>Camera-stream</cell><cell>LiDAR-stream</cell><cell>Fusion Modules</cell><cell></cell></row><row><cell cols="5">Figure 3. Illustration of perception-aware multi-sensor fusion (PMF). PMF consists of three components: (1) perspective projection; (2)</cell></row><row><cell cols="4">a two-stream network (TSNet) with feature fusion modules; and (3) perception-aware losses</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparisons on SemanticKITTI validation set. L indicates LiDAR-only methods. L+C indicates fusion-based methods. * indicates the results based on our implementation. The bold numbers indicate the best results, and the blue numbers indicate the second best results.</figDesc><table><row><cell>Method</cell><cell>Input</cell><cell>car</cell><cell>bicycle</cell><cell>motorcycle</cell><cell>truck</cell><cell>other-vehicle</cell><cell>person</cell><cell>bicyclist</cell><cell>motorcyclist</cell><cell>road</cell><cell>parking</cell><cell>sidewalk</cell><cell>other-ground</cell><cell>building</cell><cell>fence</cell><cell>vegetation</cell><cell>trunk</cell><cell>terrain</cell><cell>pole</cell><cell>traffic-sign</cell><cell>mIoU (%)</cell></row><row><cell>#Points (k) RandLANet [26] RangeNet++ [40] SequeezeSegV2 [55] SequeezeSegV3 [56] SalsaNext [13] MinkowskiNet [11] SPVNAS [51] Cylinder3D [64] PointPainting* [52] RGBAL* [37] PMF (Ours)</cell><cell cols="9">-L L L L L L L L L+C 94.7 17.7 35.0 28.8 55.0 59.4 63.6 0.0 6384 44 52 101 471 127 129 5 92.0 8.0 12.8 74.8 46.7 52.3 46.0 0.0 89.4 26.5 48.4 33.9 26.7 54.8 69.4 0.0 82.7 15.1 22.7 25.6 26.9 22.9 44.5 0.0 87.1 34.3 48.6 47.5 47.1 58.1 53.8 0.0 90.5 44.6 49.6 86.3 54.6 74.0 81.4 0.0 95.0 23.9 50.4 55.3 45.9 65.6 82.2 0.0 96.5 44.8 63.1 59.9 64.3 72.0 86.0 0.0 96.4 61.5 78.2 66.3 69.8 80.8 93.3 0.0 L+C 87.3 36.1 26.4 64.6 54.6 58.1 72.7 0.0 L+C 95.4 47.8 62.9 68.4 75.2 78.9 71.6 0.0</cell><cell cols="12">21434 974 8149 67 6304 1691 20391 882 8125 317 93.4 32.7 73.4 0.1 84.0 43.5 83.7 57.3 73.1 48.0 27.3 50.0 64 -92.9 37.0 69.9 0.0 83.4 51.0 83.3 54.0 68.1 49.8 34.0 51.2 92.7 39.7 70.7 0.1 71.6 37.0 74.6 35.8 68.1 21.8 22.2 40.8 95.3 43.1 78.2 0.3 78.9 53.2 82.3 55.5 70.4 46.3 33.2 53.3 93.4 40.6 69.1 0.0 84.6 53.0 83.6 64.3 64.2 54.4 39.8 59.4 94.3 43.7 76.4 0.0 87.9 57.6 87.4 67.7 71.5 63.5 43.6 58.5 93.9 42.4 75.9 0.0 88.8 59.1 88.0 67.5 73.0 63.5 44.3 62.3 94.9 41.5 78.0 1.4 87.5 50.0 86.7 72.2 68.8 63.0 42.1 64.9 95.3 39.9 77.6 0.4 87.5 55.1 87.7 67.0 72.9 61.8 36.5 54.5 95.1 45.6 77.5 0.8 78.9 53.4 84.3 61.7 72.9 56.1 41.5 56.2 96.4 43.5 80.5 0.1 88.7 60.1 88.6 72.7 75.3 65.5 43.0 63.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparisons on the nuScenes validation set. The bold numbers indicate the best results.</figDesc><table><row><cell>Method</cell><cell>barrier</cell><cell>bicycle</cell><cell>bus</cell><cell>car</cell><cell>construction</cell><cell>motorcycle</cell><cell>pedestrian</cell><cell>traffic-cone</cell><cell>trailer</cell><cell>truck</cell><cell>driveable</cell><cell>other-flat</cell><cell>sidewalk</cell><cell>terrain</cell><cell>manmade</cell><cell>vegetation</cell><cell>mIoU (%)</cell></row><row><cell>#Points (k)</cell><cell>1629</cell><cell>21</cell><cell cols="3">851 6130 194</cell><cell>81</cell><cell cols="10">417 112 370 2560 56048 1972 12631 13620 31667 21948</cell><cell>-</cell></row><row><cell cols="11">RangeNet++ [40] 66.0 21.3 77.2 80.9 30.2 66.8 69.6 52.1 54.2 72.3 PolarNet [62] 74.7 28.2 85.3 90.9 35.1 77.5 71.3 58.8 57.4 76.1 Salsanext [13] 74.8 34.1 85.9 88.4 42.2 72.4 72.2 63.1 61.3 76.5 Cylinder3D [64] 4 PMF (Ours) 74.1 46.6 89.8 92.1 57.0 77.7 80.9 70.9 64.6 82.9</cell><cell>94.1 96.5 96.0 96.8 95.5</cell><cell>66.6 71.1 70.8 71.6 73.3</cell><cell>63.5 74.7 71.2 76.4 73.6</cell><cell>70.1 74.0 71.5 75.4 74.8</cell><cell>83.1 87.3 86.7 90.5 89.4</cell><cell>79.8 85.7 84.4 87.4 87.7</cell><cell>65.5 71.0 72.2 76.1 76.9</cell></row></table><note>76.4 40.3 91.3 93.8 51.3 78.0 78.9 64.9 62.1 84.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Inference time of different methods on GeForce RTX 3090 using TensorRT. "-" indicates that the results are not available. For fair comparison, Cylinder3D is accelerated by sparse convolution. Ablation study for the network components on the Se-manticKITTI validation set. PP denotes perspective projection. RF denotes the residual-based fusion module. PL denotes perceptionaware loss. The bold number is the best result.</figDesc><table><row><cell>Method</cell><cell cols="2">#FLOPs #Params.</cell><cell>Inference time</cell><cell cols="2">mIoU nuScenes SemanticKITTI</cell></row><row><cell>PointPainting [52] RGBAL [37] SalsaNext [13] Cylinder3D [64] PMF (Ours)</cell><cell>51.0 G 55.0 G 31.4 G -854.7 G</cell><cell>28.1 M 13.2 M 6.7 M 55.9 M 36.3 M</cell><cell>2.3 ms 2.7 ms 1.6 ms 62.5 ms 22.3 ms</cell><cell>--72.2% 76.1% 76.9%</cell><cell>54.5% 56.2% 59.4% 64.9% 63.9%</cell></row><row><cell cols="6">Baseline PP ASPP RF PL mIoU (%)</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell>57.2</cell><cell></cell></row><row><cell>2 3 4 5 6</cell><cell></cell><cell></cell><cell></cell><cell>57.6 59.7 55.8 61.7 63.9</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>,2 Rong Li 1,2 Kui Jia 1 Qicheng Wang 3 Yuanqing Li 2,1 ? Mingkui Tan 1,2 ? 1 South China University of Technology 2 Pazhou Lab 3 Shenzhen Youjia Innov Tech Co., Ltd {z.zhuangwei, selirong}@mail.scut.edu.cn, wangqicheng@minieye.cc {auyqli, kuijia, mingkuitan}@scut.edu.cn</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table A .</head><label>A</label><figDesc>Comparisons of different fusion modules. The bold number indicates the best result.</figDesc><table><row><cell cols="7">Fusion Module Module A Module B Residual-based Fusion (Ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mIoU (%)</cell><cell></cell><cell>61.9</cell><cell>63.1</cell><cell>63.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Camera Features</cell><cell>C</cell><cell>Convolution</cell><cell>Activation</cell><cell>Features Camera</cell><cell>C</cell><cell>Convolution</cell><cell>Activation</cell><cell>Convolution</cell><cell>Activation</cell><cell>Convolution</cell></row><row><cell>LiDAR</cell><cell></cell><cell></cell><cell>Output</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Features</cell><cell></cell><cell></cell><cell>Features</cell><cell>LiDAR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Output</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Features</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Features</cell></row><row><cell>C Concatenation</cell><cell></cell><cell cols="2">Sigmoid Function</cell><cell cols="3">Convolutional Layer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Element-wise Add</cell><cell></cell><cell cols="2">Element-wise Multiply</cell><cell cols="2">Activation Layer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table B .</head><label>B</label><figDesc>Effect of ? . We highlight the best result in bold. ) 63.2 63.2 63.2 63.6 63.5 Table C. Effect of ?. We highlight the best result in bold.</figDesc><table><row><cell>?</cell><cell>0.1</cell><cell>0.3</cell><cell>0.5</cell><cell>0.7</cell><cell>0.9</cell></row><row><cell>mIoU (%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">See Section 4.5 for more details.arXiv:2106.15277v2 [cs.CV] 18 Aug 2021 (c) Projected Point Cloud (d) Predictions of Car from Point Cloud (a) RGB Image (b) Predictions of Car from Image</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The details of the multi-class focal loss and Lov?sz-softmax loss can be found in the supplementary material.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Salsanet: Fast road and vehicle segmentation in lidar point clouds for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saimir</forename><surname>Eren Erdal Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Selcuk</forename><surname>Baci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cavdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantickitti: A dataset for semantic scene understanding of lidar sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrill</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The lov?sz-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amal</forename><forename type="middle">Rannen</forename><surname>Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4413" to="4421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Yolov4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934,2020.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="44" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">nuscenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Once-for-all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09791</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Salsanext: Fast semantic segmentation of lidar point clouds for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Cortinhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tzelepis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eren</forename><forename type="middle">Erdal</forename><surname>Aksoy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03653</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bernt Schiele, and Pietro Perona. Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wojek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-supervised moving vehicle tracking with stereo sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7053" to="7062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset. The International</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A toolbox for automatic calibration of range and camera sensors using a single shot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Moosmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oemer</forename><surname>Car</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schuster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Nat: Neural architecture transformer for accurate and compact architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards accurate and compact architectures via neural architecture transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="11108" to="11117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">xmuda: Cross-modal unsupervised domain adaptation for 3d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raoul</forename><surname>De Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilie</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fuseseg: Lidar point cloud segmentation fusing multi-modal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Krispel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
	<note>Anton Van Den Hengel, and Ian Reid</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Discriminationaware network pruning for deep model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuangwei</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Lpd-net: 3d point cloud learning for large-scale place recognition and environment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanzhe</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hesheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2831" to="2840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rgb and lidar fusion based 3d semantic segmentation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khaled</forename><surname>El Madawy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rashed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><forename type="middle">El</forename><surname>Sallab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nasr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yogamani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Transportation Systems Conference</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="552" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ankit Laddha, and Carlos Vallespi-Gonzalez. Sensor fusion for joint 3d object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darshan</forename><surname>Charland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hegde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rangenet++: Fast and accurate lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Vizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrill</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4213" to="4220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A method for solving the convex programming problem with convergence rate o (1/k?2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yurii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USSR Academy of Sciences</title>
		<meeting>the USSR Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="volume">269</biblScope>
			<biblScope unit="page" from="543" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Disturbanceimmune weight sharing for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13089</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On measures of entropy and information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfr?d</forename><surname>R?nyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>the Fourth Berkeley Symposium on Mathematical Statistics and Probability</meeting>
		<imprint>
			<date type="published" when="1961" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Contributions to the Theory of Statistics. The Regents of the University of California</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Towards 3d point cloud based object maps for household environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoltan</forename><surname>Radu Bogdan Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Csaba Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Dolha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="927" to="941" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Lego-loam: Lightweight and ground-optimized lidar odometry and mapping on variable terrain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tixiao</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Englot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4758" to="4765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Darts: Deceiving autonomous cars with toxic signs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chawin</forename><surname>Sitawarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><forename type="middle">Nitin</forename><surname>Bhagoji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mosenia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mung</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Mittal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06430</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Searching efficient 3d architectures with sparse point-voxel convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="685" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pointpainting: Sequential fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bassam</forename><surname>Helou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4604" to="4612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panqu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrison</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1451" to="1460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Squeezeseg: Convolutional neural nets with recurrent crf for real-time road-object segmentation from 3d lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Squeezesegv2: Improved model structure and unsupervised domain adaptation for road-object segmentation from a lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Squeezesegv3: Spatially-adaptive convolution for efficient point-cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Generative lowbitwidth data free quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoukai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haokun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuangrun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Ocnet: Object context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00916</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep fusionnet for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="6" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Multimodality cut and paste for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12741</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4320" to="4328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Polarnet: An improved grid representation for online lidar point clouds semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zerong</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="9601" to="9610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Cylindrical and asymmetrical 3d convolution networks for lidar segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhou</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="9939" to="9948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The lov?sz-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amal</forename><forename type="middle">Rannen</forename><surname>Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4413" to="4421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Salsanext: Fast semantic segmentation of lidar point clouds for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Cortinhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tzelepis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eren</forename><forename type="middle">Erdal</forename><surname>Aksoy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03653</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Fuseseg: Lidar point cloud segmentation fusing multi-modal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Krispel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Rgb and lidar fusion based 3d semantic segmentation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khaled</forename><surname>El Madawy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rashed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><forename type="middle">El</forename><surname>Sallab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nasr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yogamani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Transportation Systems Conference</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Cylindrical and asymmetrical 3d convolution networks for lidar segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhou</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9939" to="9948" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
