<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Auxiliary Tasks and Exploration Enable ObjectGoal Navigation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Wijmans</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Auxiliary Tasks and Exploration Enable ObjectGoal Navigation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ObjectGoal Navigation (OBJECTNAV) is an embodied task wherein agents are to navigate to an object instance in an unseen environment. Prior works have shown that end-to-end OBJECTNAV agents that use vanilla visual and recurrent modules, e.g. a CNN+RNN, perform poorly due to overfitting and sample inefficiency. This has motivated current stateof-the-art methods to mix analytic and learned components and operate on explicit spatial maps of the environment. We instead re-enable a generic learned agent by adding auxiliary learning tasks and an exploration reward. Our agents achieve 24.5% success and 8.1% SPL, a 37% and 8% relative improvement over prior state-of-the-art, respectively, on the Habitat ObjectNav Challenge <ref type="bibr" target="#b34">[35]</ref>. From our analysis, we propose that agents will act to simplify their visual inputs so as to smooth their RNN dynamics, and that auxiliary tasks reduce overfitting by minimizing effective RNN dimensionality; i.e. a performant OBJECTNAV agent that must maintain coherent plans over long horizons does so by learning smooth, low-dimensional recurrent dynamics.</p><p>Site: joel99.github.io/objectnav/</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Consider how a robot placed in a novel home environment should find a target object, e.g. 'Find a chair'. This task, known as ObjectGoal Navigation (OBJECTNAV), requires the agent to search through the unseen environment and, upon seeing a goal, navigate around obstacles to reach it.</p><p>Current state-of-the-art OBJECTNAV agents <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">35]</ref> build explicit spatial maps of the environment and leverage a mix of analytic and learned planning on top of these maps. This is in contrast to the state-of-the-art methods in POINTNAV, a related task where agents navigate to specified goal coordinates (instead of object categories). In POINTNAV, the best method <ref type="bibr" target="#b38">[39]</ref> scales a generic architecture (i.e. a CNN and RNN) to 2.5 billion frames of experience. This approach outpaces methods with explicit spatial grounding, to the point * Correspondence to joel.ye@gatech.edu  <ref type="figure">Figure 1</ref>. Overview of the OBJECTNAV agent architecture. The agent receives RGBD input, a GPS+Compass sensor, and must navigate to a goal instance. Building on <ref type="bibr" target="#b39">[40]</ref>, we introduce new auxiliary tasks, a semantic segmentation visual input, a Semantic Goal Exists (SGE) feature that describes the fraction of the frame occupied by the goal object, and a method for tethering secondary policies that learns from its own reward signal with off-policy updates. We encourage the acting policy to explore, and the tethered policy to perform efficient OBJECTNAV.</p><p>of essentially solving POINTNAV in Habitat <ref type="bibr" target="#b27">[28]</ref>.</p><p>In this work, we explore how to enable such a generic agent for OBJECTNAV. OBJECTNAV is considerably more challenging than POINTNAV; the agent that nearly solves POINT-NAV only achieves 6% success on OBJECTNAV <ref type="bibr" target="#b34">[35]</ref>. While POINTNAV agents can use the onboard GPS+Compass sensor as a compact representation to measure progress towards the goal, OBJECTNAV agents cannot; they instead need to be competent at exploration since the target object can be anywhere in the environment. As such, a vanilla CNN+RNN agent tasked with learning these complex representations for OBJECTNAV is likely to be under-equipped with only an RL reward, even if the reward is shaped to encourage navigation towards a goal <ref type="bibr" target="#b34">[35]</ref>. Even with an additional exploration reward and no other feedback to learn better environment representations, efficient exploration is hard <ref type="bibr" target="#b5">[6]</ref>.</p><p>Our approach builds on a recent advance in POINTNAV <ref type="bibr" target="#b39">[40]</ref> that combines simple architectures with auxiliary learning tasks to greatly improve sample complexity in visually complex environments. Specifically, 1) we update the agent's inputs to incorporate egocentric semantic segmentation and a feature that explicitly describes how much of the target object class is in view. 2) We additionally introduce three new auxiliary learning tasks: two general task for learning an inverse dynamics model and one exploration-centric task that predicts map coverage, and 3) to teach the agent to explore, we add an exploration term to the reward.</p><p>However, exploration is different than OBJECTNAV and we find that agents trained with an exploration reward will continue to explore after locating goal instances. To mitigate this task mis-alignment, we propose a 'tethered-policy' multitask learning technique. In this method, we learn a second policy adapted only for the sparse OBJECTNAV reward while still acting via a primary policy given the exploration reward. Tethering allows faster adaptation to the sparse OBJECTNAV reward, which in turns improves agent efficiency over simple fine-tuning of a single policy on the sparse reward.</p><p>Our results show that end-to-end learning can achieve stateof-the-art results in OBJECTNAV once equipped with generic representation learning and exploration objectives. Nonetheless we are far from human-like competence at the task. To guide future work, we analyze various aspects of our agent, but center in particular around its recurrent dynamics. We propose that zero-shot transfer to RedNet segmentation is greatly degraded by the chaotic dynamics it causes in agent RNNs, that several failure modes are caused by lapses in agent memory, and that auxiliary tasks reduce overfitting by constraining the effective dimensionality of agent RNNs.</p><p>Concretely, our contributions are: 1) Objectives to train a simple CNN+RNN architecture that lead to 24.5% success on OBJECTNAV (+37% relative improvement over prior state-of-the-art). This suggests that, despite their current prevalence, explicit maps need not be necessary to learn complex Embodied AI tasks. 2) A method for tethering secondary policies to an agent.</p><p>Tethered policies learn from separate reward signals with off-policy updates. We use a tethered policy with no exploration reward to achieve 8.1% SPL (+8% relative improvement over prior work). 3) An analysis of the agent. Through examination of the agent's behavior, representations, and recurrent dynamics, we find a) the agent seeks out simple visual inputs corresponding to smoother RNN dynamics, b) agent failures are often related to agent memory, and c) auxiliary tasks regularize agent RNNs by constraining their effective dimensionality. We hypothesize a key component of performant OBJECTNAV agents will be the ability to plan with smooth, low-dimensional recurrent dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Approach</head><p>OBJECTNAV Definition and Dataset. <ref type="bibr" target="#b1">[2]</ref> In OBJECTNAV, an agent must navigate to an instance of a specified object category in an unseen environment. The agent does not receive a map of the environment and must navigate using its (noiseless) onboard sensors: an RGBD camera, a GPS+Compass sensor which provides location and orienta-tion relative to start of episode. The agent also receives a goal object category ID. The full action space is discrete and consists of MOVE_FORWARD, TURN_LEFT, TURN_RIGHT, LOOK_UP, LOOK_DOWN, and STOP. We experiment with both the full 6-action setting and a restricted 4-action setting that excludes LOOK_UP and LOOK_DOWN. To our knowledge, no other works have considered the 6-action setting (perhaps due to the increased complexity of specifying tilt actions in 2D map-based planners). The agent must stop at a location within 1m of an object of the specified category and be able to view the object from that location. There is a 500 step limit to succeed on the episode.</p><p>We experiment on the Matterport dataset (MP3D <ref type="bibr" target="#b3">[4]</ref>), which has 90 scenes and 40 labeled semantic object categories. There are 21 goal categories, as specified in the Habitat 2020 Challenge <ref type="bibr" target="#b34">[35]</ref>. We train our agent with a 3D GPS (i.e. with vertical localization) to encourage the agent to reason about 3D exploration, as a considerable number of MP3D scenes incorporate elevation changes. However, we evaluate the agent with a 2D GPS for compatibility (by zero-ing vertical axis) with the Habitat Challenge parameters. We provide a comparison of these settings in (Section A.4). Additional Features. We accelerate agent learning by providing two semantic features: semantic segmentation (segm.) for the visual input, and a "Semantic Goal Exists" (SGE) scalar which equals the fraction of the visual input that is occupied by the goal category (computed from the semantic segm.). During training, we use the ground truth semantic segm. directly from the MP3D annotations. During evaluation, the segm. is predicted from RGBD using a RedNet <ref type="bibr" target="#b17">[18]</ref> model finetuned to predict the 21 goal categories. The Red-Net model is taken off-the-shelf (trained on SUNRGBD <ref type="bibr" target="#b31">[32]</ref>) and finetuned on 100k randomly sampled forward-facing views from MP3D. We provide additional details in Section A.15. The SGE feature distills the domain knowledge that the agent should navigate to the goal once it is seen; this knowledge is built into the planners in prior works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Agent Architecture</head><p>Our agent uses the split-belief architecture introduced in <ref type="bibr" target="#b39">[40]</ref>, shown in <ref type="figure">Fig. 1</ref>. This approach first embeds all sensory inputs with feed-forward modules. The visual inputs are first downsampled by 0.5 i.e. from 480 ? 640 to 240 ? 320, and the semantic segm. channel is also projected to 4D by associating each semantic ID with a learnable vector. After this preprocessing, visual inputs are fed through a ResNet18 <ref type="bibr" target="#b14">[15]</ref>. The goal object ID is directly embedded into a 32D vector. The embedded visual and goal vectors are concatenated with the GPS-Compass and SGE inputs to form an observation embedding. Next, a set of recurrent "belief modules" integrate these observation embeddings over time. These belief modules are independent GRUs <ref type="bibr" target="#b6">[7]</ref>, each associated with a separate auxiliary task; Ye et al. <ref type="bibr" target="#b39">[40]</ref> proposes that the split modules, as opposed to one larger recurrent network, enable orthogonal auxiliary tasks to be learned while minimizing task conflict. We term output cell states from all GRUs as "beliefs." Beliefs are fused using an attention layer conditioned on the observation embedding.</p><p>The fused belief is then directly used in a linear actor-critic policy head. We refer to this agent as the base agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Learning Signals</head><p>Rewards. The base agent receives a sparse success reward r success , a slack reward r slack to encourage faster goal-seeking, and an exploration reward r explore . Measuring exploration is a dense indicator of progress, and encourages an intuitively prerequisite skill for OBJECTNAV. We use a visitation-based coverage reward, in which we first divide the map into a voxel grid with 2.5m ? 2.5m ? 2.5m voxels and reward the agent for visiting each voxel. This form of visitationbased exploration bonus was found to work well for mapbased agents <ref type="bibr" target="#b25">[26]</ref>. We smooth r explore by decaying it by the number of steps the agent has spent in the voxel (visit count v). Further, to ensure the agent eventually prioritizes OBJECTNAV, we decay r explore based on episode timestep t with a decay constant d = 0.995. In summary:</p><formula xml:id="formula_0">r total = r success + r slack + r explore (1a) r success = 2.5</formula><p>on success (1b) r slack = ?10 ?4 per step (1c)</p><formula xml:id="formula_1">r explore = 0.25 ? d t v (1d)</formula><p>Tethering to an Exploration Policy. Using exploration to guide behavior can distract the agent from properly terminating at the goal point, e.g. when it is standing next to the goal but can instead choose to continue to explore a wide open space (we observe this in Section 4.1). To alleviate this pathology, we introduce a tethered-policy method which attaches an additional policy head to the fused belief output. Each of the two policy heads can be fed their own desired rewards, and the agent can act according to any mix of the two policies. Note this means that each policy share the same action space. We implement a simple schedule in which the agent first acts with a policy optimized with the full shaped reward (r total ) for an initial period of training, and then with a second policy which only uses the sparse r success . The first phase is intended to teach the agent how to explore, and the second to encourage efficient OBJECTNAV. We use importance-weighted VTrace <ref type="bibr" target="#b9">[10]</ref> returns to update the nonbehavioral policy to account for experience being off-policy from its perspective (details in Section A.13). Tethering is similarly motivated as SAC-X <ref type="bibr" target="#b26">[27]</ref>, but implemented as a lightweight extension to a standard RL agent. Auxiliary Tasks. We use 6 auxiliary tasks. As in Ye et al. <ref type="bibr" target="#b39">[40]</ref>, we use 2 instances of CPC|A <ref type="bibr" target="#b13">[14]</ref>, a self-supervised contrastive task, at horizons of k = {4, 16} steps. We also use PBL <ref type="bibr" target="#b12">[13]</ref> with a horizon of k = 8 steps. These tasks use agent states to make predictions about the environment.</p><p>We also introduce two general-purpose inverse tasks, which predict agent actions given environment transitions: Action Distribution Prediction (ADP) and Generalized Inverse Dynamics (GID). ADP and GID both predict actions taken between two observations k frames apart (i.e. {a [t:t+k] }), and are both conditioned on the belief at the first frame h t and the visual embedding (i.e. CNN output) ? t+k . ADP uses a 2-layer MLP to directly predict an action distribution and evaluates the KL-divergence between this prediction and the empirical distribution of the next k actions:</p><formula xml:id="formula_2">L ADP = KL(MLP(h t , ? t+k ), a [t:t+k] )<label>(2)</label></formula><p>GID predicts individual actions using a separate GRU with state g GID and two linear layers f, f ? . The GRU is initialized with the same inputs as in ADP:</p><formula xml:id="formula_3">g GID t = f (h t , ? t+k )<label>(3)</label></formula><p>The GRU is updated with actions taken:</p><formula xml:id="formula_4">g GID t+i = GRU(a t+i?1 , g GID t+i?1 )<label>(4)</label></formula><formula xml:id="formula_5">L GID = k?1 i=1 CrossEnt(f ? (g GID t+i ), a t+i )<label>(5)</label></formula><p>Finally, we introduce an exploration-specific Coverage Prediction (CP) task that leverages the GPS sensor provided to the agent. CP follows a similar structure as GID, i.e. using a GRU for sequence prediction. However, this GRU's initial state is conditioned not on a visual embedding, but on the number of steps the agent has spent in its current voxel, v(s t ). This conditioning helps the GRU anticipate how close the agent is to the edge of the voxel. The GRU predicts the change in coverage at each of the next k steps:  <ref type="bibr" target="#b34">[35]</ref>. ? intervals provide 95% CI; VAL bold values are significantly better than non-bold values (p &lt; 0.05, paired t-test), TEST bolding is for emphasis. Our auxiliary task and exploration enabled agent (row 1) outperforms prior state of the art success (over row 7) and triples the vanilla learning agent (row 6). Fine-tuning the agent with RedNet segmentation reverses the ranks of 4 and 6-action agents (rows 1 and 3 vs 2 and 4, analysis in Section 3.1), helping the 6-action agent to achieve a new state-of-the-art 24.5% success. Finally, tuning on the sparse OBJECTNAV success reward helps the agent set state-of-the-art 8.1% SPL (row 6 vs 9).</p><formula xml:id="formula_6">g CP t = f ?? (h t , v(s t ))<label>(6)</label></formula><formula xml:id="formula_7">g CP t+i = GRU(a t+i?1 , g CP t+i?1 )<label>(7)</label></formula><formula xml:id="formula_8">L CP = k i=1 CrossEnt(f ??? (g CP t+i ), ? cov (t, t + i)) (8) where f ?? ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>In our experiments, we train each of our agents for 8 GPUweeks (192 GPU-hours), amounting to ?125M frames per agent. For the tethered variant, we use the sparse policy for the final 25M frames. The two primary metrics we report are Success and Success weighted by Path Length (SPL), a measure of trajectory efficiency, defined over N episodes as</p><formula xml:id="formula_9">SPL = 1 N N i=1 S i l i max(p i , l i )</formula><p>where S i is a binary indicator of success on episode i, l i is length of optimal path, and p i is length of agent path. We evaluate checkpoints every ?4M frames and report metrics from the checkpoint with the highest SPL on VAL. Auxiliary Tasks and Exploration produce an effective OBJECTNAV agent. We compare against results from the Habitat Challenge 2020 leaderboard in <ref type="table" target="#tab_0">Table 1</ref>. We first note the large drop from VAL to TEST-STD (e.g.-14% success in row 1). Since we cannot access the test split, we conjecture that the splits were randomly sampled such that the 9 test scenes were disproportionately challenging. <ref type="bibr" target="#b1">2</ref> While other works do not highlight this shift, we note that even agents with strong priors (rows 6, 7) have high performance variance, e.g. 5% success gaps between the two test splits (TEST-STD, TEST-CHALLENGE <ref type="bibr" target="#b34">[35]</ref>). We provide additional commentary and check for agent biases in Section A.1. <ref type="bibr" target="#b1">2</ref> Distribution shift is expected given small VAL, TEST splits.</p><p>Using RedNet segm. directly (i.e. zero-shot transfer from GT , row 1), the 4-action agent reaches 19.9% success (+11% relative over prior best, row 1 vs 8). This is 3x the performance of the E2E baseline that is rewarded for approaching a goal, a method which "solved" POINTNAV (row 7).</p><p>We tune the 4-action agent with RedNet segm. for an additional 10M frames to account for the ground-truth-to-RedNet distribution shift, but surprisingly performance drops (row 1 vs 2). Conversely, the 6-action agent which initially performs worse than the 4-action agent (row 1 vs 3) overtakes the 4-action agent after finetuning with RedNet (row 2 vs 4). We analyze this reversal in Section 3.1. Though these tuned agents match in VAL (row 1 vs 4), the 6-action agent improves on TEST-STD, to 24.5%, +37% relative over prior state-of-the-art (row 4 vs 8).</p><p>However, both 4 and 6-Action agents are less efficient than prior methods (SPL column). Qualitative examination of these agents indicate they prefer to wander around goals for some time before stopping at them correctly, likely due to the exploration reward. After finetuning on the sparse OBJECTNAV reward through tethered training, SPL rises and Success falls (row 3 vs 5). We show the Success drop is likely due to reduced exploration in Section A.2. Nonetheless, after tuning this agent on RedNet, we achieve +0.6% (+8% relative) over state of the art SPL (row 6 vs 9).</p><p>We provide ablations for our design choices in <ref type="table" target="#tab_1">Table 2</ref>. Our auxiliary task ablations are compared against the 4-action agent (rows 1-4). These tasks provide modest gains in both metrics (&lt;1% for ADP, 1% GID, 2% CP). Next, ablating the Success % (?) SPL % (?)  <ref type="table">Table 3</ref>. Performance on a 300-episode subset of TRAIN and VAL splits, reported as "with GT segmentation (with RedNet segmentation)". Best metrics are bolded for emphasis. In both splits, all agents degrade significantly with RedNet segmentations, but 6-action agents more so. Under GT segmentation (numbers outside parentheses), agents are minimally overfit, though SPL does degrade slightly from TRAIN to VAL. In the GT setting, 6-action agents outperform 4-action agents. SGE sensor drops performance by a large amount (-14%, row 1 vs 5). In Section A.5, we note that SGE is much more effectively incorporated as a feature than as an auxiliary task. Finally, we also compare tethered policy training against direct finetuning (row 6 vs 7) for the 6-action agent. While direct finetuning also improves SPL (vs SPL reported in row 3 in <ref type="table" target="#tab_0">Table 1</ref>), tethered training is overall more effective at preserving agent performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Stable segmentation is critical to untuned 6action agents.</head><p>We found that 4-action agents suffer a performance drop after finetuning on RedNet, while 6-action agents improve.</p><p>To understand why, we first observed that the RedNet is greatly overfit (Section A.15), we decouple our agent from RedNet by evaluating agent performance on both TRAIN and VAL with GT and RedNet segm. , shown in <ref type="table">Table 3</ref>. We first note that most agents are minimally overfit to TRAIN, with a moderate impact to SPL and &lt; 3% drop in success. RedNet Seg. degrades performance significantly, but in particular, RedNet Seg. hurts 6-action agents more, even on TRAIN, where the effects of an overfit RedNet are reduced. This suggests 6-action agents are more sensitive to RedNet statistics in particular, and hence able to overtake the 4-action agent once finetuned with RedNet.</p><p>We examine agent trajectories to understand this sensitivity (videos provided in supplement). We first find that during training with GT semantics, 6-action agents learn to navigate while spending most steps facing downwards towards the floor (while the 4-action agent can only look straight ahead). This happens despite there being minimal obstacles on the floor while looking straightforward offers greater chance to see more distant goals. Intuitively, the visual input provided by the floor is simpler than the view of a room. We thus posit a stability hypothesis: the 6-action agent learns to exploit a simpler signal to enable stabler recurrent dynamics. That is, because it is harder to maintain memory over long term trajectories when faced with noisy inputs, agents will simplify their input even at slight cost to information gained. Such a hypothesis may relate to results on improved generalization when restricting the visual field of a gridworld agent <ref type="bibr" target="#b16">[17]</ref> and when withholding inputs from parts of a recurrent module <ref type="bibr" target="#b11">[12]</ref>. This hypothesis suggests that standard RNN gating is imperfect at preventing input-induced information decay in the RNN state, and presents an avenue for future work.</p><p>We next observe that RedNet segm. is low quality and unstable, flickering across consecutive frames and without welldefined object boundaries. These are expected failures: poor segm. is due in part to the noisiness of the underlying MP3D meshes, and consistent segm. is a broader challenge <ref type="bibr" target="#b35">[36]</ref>. Predicted segm. of frames of the floor are particularly poor, likely since RedNet is trained with front-facing views. Consequently, the 6-action agent behavior was very erratic, with repeated alternating turning actions. If the 6-action agent did learn to exploit the floor for simple visual inputs, it would have a reduced incentive to filter noisy frontal views, and thus would be particularly vulnerable to degraded segm. Then, one possible reason to explain earlier trends is that 1. RedNet overfitting implies that previously general agents which are tuned with RedNet will in turn overfit, 2. Untuned 6-action agents avoid this overfit transfer but suffer significantly more from unstable semantics, and 3. Tuned 6-action agents overfit, but gain performance by maintaining stable behavior while receiving unstable semantics. We quantify this instability in Section A.7. The fact that tuning agents with RedNet improves agent performance suggests robustness to noisy inputs is partially learnable, though the agent will not do so if it has a simpler option (facing the ground).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Agent Analysis</head><p>These agents demonstrate promising performance (? 55% val success) in navigating large, complex environments, especially when granted GT segm. Nonetheless, their performance is far from perfect, and they still display qualitative failure modes that a human would not have, such as repeated circling inside a room. In this section we analyze agent be- havioral modes, knowledge, and internal dynamics to inform future directions towards human-level OBJECTNAV, and to develop an intuition for how a competent OBJECTNAV agent operates. We scope our analysis to GT segm. and the base 6-action agent, unless otherwise noted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Behavioral Analysis</head><p>In our behavioral analysis we aim to understand: How can OBJECTNAV agents approach 1.0 Success? To identify prominent modes of the remaining failures, we conduct a qualitative coding of agent behavior for both the base and tethered 6-action agent. Specifically, we sample 300 validation episodes and manually label the failures (there are 125 for base, 151 for tether) and group them according to common trends. A subset of failure modes are described in <ref type="table" target="#tab_2">Table 4</ref> and their relative prevalence for the base agent is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. A full list of failure modes are in Section A.6, and example videos are available in the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name Description</head><p>Explore A generic failure to find the goal despite steady exploration. Includes semantic failures e.g. going outdoors to find a bed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Plateau</head><p>Repeated collisions against the same piece of debris causes a plateau in coverage. Includes debris which traps agent in spawn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loop</head><p>Poor exploration due to looping over the same locations or backtracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection</head><p>Despite positive SGE, the agent does not notice nor successfully navigate to the goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Last mile</head><p>Gets stuck near the goal.</p><p>Commitment Sees and approaches the goal but passes it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open</head><p>Explores an open area without any objects.  successful exploration, which at best comprises 'explore' and 'loop'. They get stuck ('plateau' and 'last mile'), suffer pathologies that might be attributed to the exploration reward ('commitment' and 'open'), and deal with a handful of issues due to scene quality (labeled as 'misc' here). However, some failure modes do preclude others. For e.g., an agent which is trapped by reconstruction debris at spawn ('plateau') is still liable to not find the goal due to general exploration issues. Additionally, the agent does not make panoramic turns to identify promising goal locations, as a human might. We attribute this to the visitation-based exploration reward; a different reward which promotes simply viewing more areas instead of visiting them may mitigate this. Alternately, nonepisodic exploration rewards may promote the appropriate behavior with less misalignment with OBJECTNAV.</p><p>These exploration-related pathologies are not cured by swapping to the sparse OBJECTNAV reward, which degrades success ( </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Probing Learned Knowledge</head><p>How well do agent representations match our intuitions? Does its behavior match represented features? e.g., "commitment" failures, which generally occur when the agent spots the goal early in the episode, should only be feasible if the agent maintains a time representation and knows it has time to continue gaining coverage rewards. Conversely, does the agent represent prominent variables that a human might? e.g., "Commitment" and "Detection" failures both imply that agents do not remember SGE features from prior timesteps. To test these questions, we train linear decoders to probe our agent for time and the most goal seen. We first record representations from 300 episodes on both TRAIN and VAL. We train the decoders using TRAIN representations and test them on VAL representations (? 60 ? 70K steps each).</p><p>We plot decoder performance in <ref type="figure" target="#fig_2">Fig. 3</ref>, aggregating performance across individual beliefs. Though the features are represented across beliefs, there is high variance in probe performance across beliefs. Moreover, probe performance on the fused belief is worse than in individual beliefs. These results reveal semantic differences among individual beliefs and the fused belief, suggesting some abstraction is lost in fusion despite it only being a weighted sum. Separately, the tethered agent represents Max SGE more prominently than the base agent, matching the intuition that the base agent does not retain goals seen in previous steps perfectly.</p><p>With the same approach, we find little evidence of features that a human might use in OBJECTNAV: time spent in location, room ID, and distance to goal (Section A.8). Supplying these features directly could improve the agent.   Top: For each of 6 agents (4-Act, 6-Act, 6-Act + Tether, No CP, No ADP, No GID), we sort agent beliefs by the span of their memory. This span is computed by sampling 50 FPs and counting at each FP the number of eigenvalues with time constants above 10 steps. The more eigenvalues past threshold, the larger the span, and the lower the rank. Beliefs are labeled by their task, and beliefs without tasks (from ablations) are labeled with BASE. We count the number of times a belief appears in a given rank (e.g. since BASE only appears in 3 agents, it only has 3 counts). Consistent rank indicates relative span is stable. Bottom: We measure how memory changes over time for the CPC|A-4 and CP tasks by counting the number of eigenvalues with time constants above 100 steps at several points during training. less correlated with observations than the other beliefs. This suggests non-reactive computation needn't be hidden. Auxiliary tasks specify fixed point memory spans. We would ideally like to understand what computational structure the agent RNNs use beyond coarse-grained statistics like curvature. To approach this question, we employ fixed point (FP) analysis <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>. FP analysis simplifies the study of a full RNN into a study of its "slow points," where the RNN update is small and well-approximated by a linear function. We provide details on methodology in Section A.10. FPs are typically studied both through their overall layout (the manifold) and their local dynamics. While FP manifolds have been used to directly link RNN computation to dynamical structures, e.g. ring attractors, we find our RNNs are much higher-dimensional than those in prior work and thus difficult to clearly classify (notes in Section A.11). This leaves local dynamic structure. At an identified FP h, the RNN update is well described by a first order approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Dynamical Analysis</head><formula xml:id="formula_10">Let ?h t := h t ? h, for h t near h, then ?h t+1 ? J rec h ?h t .<label>(9)</label></formula><p>Then the decomposition of J rec h assesses FP "memory." Specifically, a delta along an eigenvector v with corresponding eigenvalue ? ? 1 would decay to 0, while an eigenvector v ? with ? ? ? 1 would persist indefinitely. A simple method to quantify FP memory is to count the eigenvalues past a threshold value. For interpretability we count eigenvalue time constants instead of the values themselves, as in <ref type="bibr" target="#b20">[21]</ref> (Section A.10); we call this quantity a memory span.</p><p>We measure and rank these spans for all beliefs of several different agents, producing <ref type="figure" target="#fig_4">Fig. 5</ref>. The auxiliary task corresponding to a belief appears to play a consistent role in specifying FP memory span, and thus in organizing recurrent dynamics, across seeds, action suites (4 vs 6 action), and RL objectives (tether or not). Among the tasks, only CP induces larger spans than the base belief. This matches the intuition that CP could require the tracking of an unbounded number of variables. Separately, the fact that the base belief has larger span than other tasks reveals a potential mechanism by which auxiliary tasks encourage generalization. Note that smaller memories imply there are more dimensions along which RNN state decays towards fixed points, which in turn should imply preference towards lower-dimensional trajectories (even if variable inputs prevent this empirically). Then, the fact that most auxiliary task-augmented beliefs achieve smaller memories than the vanilla "Base" belief demonstrates auxiliary tasks may preferentially select for low-dimensional trajectories. Such low-dimensional trajectories could be necessary for generalization; indeed this is theorized to be the reason why monkeys engage low-d dynamics in working memory tasks <ref type="bibr" target="#b7">[8]</ref>. Note that this hypothesis concerns how auxiliary tasks promote generalization in recurrent beliefs (i.e. beyond general visual representations).</p><p>In the bottom figure of <ref type="figure" target="#fig_4">Fig. 5</ref> we plot memory span for several agents at various points in training. Memory span trends upward over training. Consistent with the first plot, CPC|A-4 appears to constrain the span to be low. CP, on the other hand, may be constrained by the capacity of the RNN (the belief hidden size is 196). It is possible that an ideal OBJECTNAV agent would maintain stable low-d dynamics, but this would need to be reconciled with how such an agent would need to track its path to keep exploring new areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>Learning an appropriate state representation is a central goal of reinforcement learning. Representations pretrained in static visual tasks, such as the semantic segm. used in our agent, have been successfully applied to navigation in complex environments <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b28">29]</ref>, though results in <ref type="bibr" target="#b39">[40]</ref> suggest non-semantic representations may be less effective than endto-end learned representations. While auxiliary tasks are commonly used to improve agent performance <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref>, we believe their role in preventing overfitting to complex environments is underappreciated. Analyzing Embodied Recurrent Computation. Understanding an agent's recurrent computation will be key to understanding their complex behavior. While complex computation resists simple analyses, e.g. our probes, the AI community has successfully used sophisticated probing tasks to reveal recurrent representations reflect considerably more environmental knowledge than do static (visual) representations <ref type="bibr" target="#b36">[37]</ref>, and can contain compositional knowledge <ref type="bibr" target="#b8">[9]</ref>. We have explored a different perspective by analyzing the dynamic rules our agent learns; similar analyses have provided considerable insight in simpler tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">33]</ref>. Unfortunately, our agent RNNs are much higher-dimensional than those studied in previous settings and stymies simple fixed point analyses. However, we are encouraged by the different perspective (e.g. memory) such dynamic analyses provide, and believe embodied AI provides a good testbed for future development of these dynamic analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>Our work uses a recurrent agent with an implicit environment representation to set state-of-the-art for OBJECTNAV. Our agents are minimally overfit and have yet to saturate training performance, meaning that our approach re-enables scaling as a viable path to improving OBJECTNAV. This success relies on auxiliary tasks and an exploration policy, both of which are generic RL ingredients. Future works should consider these ingredients in their generic CNN+RNN baselines, since the default of only providing rewards is demonstrably ineffective in complex environments. Our analysis indicates that understanding misbehaving agent dynamics is a promising direction, and our results suggest generic learned agents can still drive progress in embodied navigation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average Success</head><p>Tethered Base <ref type="figure" target="#fig_1">Figure A2</ref>. While scene difficulty varies, difficulty distribution is not extremely heavy-tailed. TEST-STD shift could result from 1-2 more difficult scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>We use the appendix to elaborate on agent behavior and methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Agent Performance against goal distance, category, and scene</head><p>Our experiments show large performance variance across OBJECTNAV splits. We check for anomalous agent biases by comparing agent success rates conditioned on several episode attributes. Since the large gap between GT and RedNet performance is already established, we provide these plots with GT segmentation.</p><p>The base agent has reasonable performance variance across all categories, succeeding less often on rarer categories ( <ref type="figure" target="#fig_5">Fig. A1</ref>) and more distant goals <ref type="figure" target="#fig_2">(Fig. A3</ref>). Scene variance <ref type="figure" target="#fig_1">(Fig. A2)</ref> is wide (0.3 to 0.85), but not particularly heavytailed; the TEST-STD drop is likely simply due to increased difficulty. This variance blurs the line that defines state of the art and greatly motivates increasing current OBJECTNAV </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Success Count</head><p>Total Episodes Tethered Base <ref type="figure" target="#fig_2">Figure A3</ref>. Agent success decreases when goals become more distant, and tethered agent is relatively worse at more distant goals.</p><p>dataset sizes.</p><p>We also provide the tethered agent's performance; they preferentially succeed on shorter paths relative to the base agent, which can succeed on even very distant goals <ref type="figure" target="#fig_2">Fig. A3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Sparse Reward Reduces Exploration and Causes Quitting</head><p>The tether agent performs worse than the base agent. We note two primary reasons in <ref type="figure" target="#fig_3">Fig. A4</ref>: reduced exploration (top) and early quitting due to estimation error (bottom). In particular the early quitting is likely a general symptom of using sparse reward. On episodes that are difficult, either due to environmental characteristics (e.g. it is outside) or due to goals (e.g. clothes, which is difficult at train time), value estimates will occasionally dip below 0 and cause the agent to randomly stop. This may be mitigated by an increased discount factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Train-Val Gap</head><p>Though <ref type="table">Table 3</ref> suggests virtually no overfitting, our agent does appear to begin moderately overfitting past 0.5 success in <ref type="figure" target="#fig_4">Fig. A5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. 2D vs 3D GPS Comparison</head><p>Though we train with the 3D GPS sensor provided by the simulator, all presented results have used a 2D GPS without the vertical dimension at evaluation, to be consistent with Habitat Challenge settings. We compare 2D vs 3D validation scores in <ref type="table" target="#tab_0">Table A1</ref>. We do not see a large change in performance; our agents dos not leverage the vertical dimension much. Value Prediction Base Tether <ref type="figure" target="#fig_3">Figure A4</ref>. Top: We plot exploration rate (coverage over time) for tether and base agents. In both success and failure conditions, the exploration rate is worse in the tethered agent. Bottom: We plot value function traces for 100 failure episodes. Tether value predictions approaches 0 quickly, but due to estimation error also dips below 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Additional Auxiliary Task Ablations</head><p>We provide 2 additional ablations: 1. removing SGE by turning it into a feature and 2. removing all auxiliary tasks (and using one large RNN). Results are in <ref type="table" target="#tab_1">Table A2</ref>. SGE works better as a feature than as an auxiliary task (row 2 vs 3). Since SGE is a feature that is easily derived from the semantic input, its large contribution to learning efficiency may feel unintuitive. However, when we encourage SGE by decoding it with an auxiliary task, performance Success % (?) SPL % (?)  <ref type="table" target="#tab_1">Table A2</ref>. Additional ablations on 4-action base for 1. incorporating SGE as an auxiliary task (Aux SGE) and 2. not providing any auxiliary tasks (No Aux). counterintuitively decreases. The auxiliary task is easily learned, i.e. its loss flatlines near 0 quickly, and module weights suggest this content is stored in sparsely (i.e. &lt; 10 peaks in the probe weights). This result may warrant additional investigation to understand what types of priors are best introduced as auxiliary tasks vs. as features.</p><p>Auxiliary tasks contribute to performance (row 1 vs 4). If we remove all auxiliary tasks, performance drops moderately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6. Behavioral Analysis Details</head><p>We provide a failure mode breakdown for the tethered agent <ref type="figure" target="#fig_9">(Fig. A6)</ref>. The tethered agent fails minimally from exploration-reward specific failures (e.g. "Commitment"),</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name Description Rule</head><p>Plateau Repeated collisions against the same piece of debris causes a plateau in coverage. Includes debris which traps agent in spawn.</p><p>Agent collides &gt; 50 times before visiting 2 new voxels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loop</head><p>Poor exploration due to looping over the same locations or backtracking.</p><p>Episode &gt; 250 steps, expected fraction of episode spent in current voxel &gt; 0.15 and collisions &lt; 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection</head><p>Despite positive SGE, the agent does not notice nor successfully navigate to the goal.</p><p>Max SGE seen in episode ? [0.02, 0.1].</p><p>Commitment Sees and approaches the goal but passes it. In first 200 steps, SGE &gt; 0.1 and goal distance &lt; 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Last mile</head><p>Gets stuck near the goal. On stop, SGE &gt; 0 and distance is &lt; 1m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open</head><p>Explores an open area without any objects. Coverage &gt; 10 voxels and &lt; 10 collisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quit</head><p>Quitting despite lack of obstacles. Apply if above rules do not apply and episode length &lt; 250.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Void</head><p>The goal is seen through a crack in the mesh, which appears to disturb agent behavior.</p><p>Qualitative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Goal Bug</head><p>A goal instance has no associated viewpoints where the agent can stop.</p><p>Qualitative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Explore</head><p>A generic failure to find the goal despite steady exploration. Includes semantic failures e.g. going outdoors to find a bed.</p><p>The default if no other modes apply. <ref type="table">Table A3</ref>. Description of observed failure modes for 6-action agents and associated heuristics. but qualitatively fails from a "Quitting" mode often, where the agent stops despite reasonable exploration and no goal in sight. We also provide a full list of observed failure modes <ref type="table">(Table A3)</ref>, along with quantitative heuristics which is consistent with manual annotations &gt; 80% of the time, to help give a sense of the annotation criteria.</p><p>A.7. Action Entropy to quantify agent instability.</p><p>Throughout our analysis we have referred to unstable agent behavior. We quantify this instability by measuring action entropy, i.e. the entropy of the distribution of actions taken over the course of each episode. Specifically, we measure for each episode the action entropy of a rolling window of 10 steps, averaged across window locations, and show how its distribution differs between agents and episode success or failure in <ref type="figure">Fig. A7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8. Additional Probing Results</head><p>We additionally run probes for time spent in location (as in the "loop" failure mode), room ID, and distance to goal, as shown in <ref type="figure">Fig. A8</ref>. These features appear negligibly repre-  <ref type="figure">Figure A7</ref>. We plot the average action entropy of a rolling window as measured on 300 VAL episodes, for 2 agents and with and without GT segm. Failures tend to correlate with higher action entropy, more strongly for the tethered agent. Predicted segm. pulls action entropy toward an intermediate value for the tethered agent i.e. destabilizes successful episodes, and stabilizes failure episodes. GT and predicted segmentation effects are muted for 6-action agent, perhaps due to its wandering behavior.</p><p>sented across beliefs. Base Tether <ref type="figure">Figure A8</ref>. We run the same probing procedure for visit count (number of timesteps spent in current voxel), distance to goal, room category (room category reports classification accuracy instead of R 2 ). Visit count and room category is better matched by timestep than any belief, and distance to goal R 2 is at best around 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.9. Curvature Computation</head><p>We use curvature to summarize the stability of a given sequence of representations x 1 . . . x t . We start by calculating normalized displacement vectors v i = x i+1 ? x i ,v i = vi ||vi|| . We compute the local discrete curvature as the angle between successive vectors: c i = arccos(v i ,v i+1 ). Then we report the global curvature of the sequence as the mean of these local curvatures. This procedure mirrors <ref type="bibr" target="#b15">[16]</ref>. Global curvatures reported in text are averaged over validation episodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.10. Fixed Point Analysis Methodology</head><p>Fixed point (FP) analysis of RNNs center around the idea that an RNN's nonlinear dynamics and computation can be understood through its behavior around a set of "slow (fixed) points." These points act as a dynamical skeleton which e.g. determines the flow field at other points in hidden space. For example, it has been shown that sentiment analysis RNNs act as line attractors, where the hidden state's position along a line represents the read out sentiment <ref type="bibr" target="#b20">[21]</ref>. FP analysis begins with an optimization to identify these slow points, i.e. hidden states that satisfy ||RNN(h, ?) ? h|| 2 &lt; ?, for some input ? and threshold ?. For this initial analysis we follow <ref type="bibr" target="#b21">[22]</ref> and let ? be the average input, i.e. the average of observation embeddings collected across the 300episode VAL subset. Note that this average includes an SGE &gt; 0 signal; in case this has large impact on belief dynamics, we verified that fixed point results were qualitatively similar after setting this signal to 0. We optimize 10K fixed points, by sampling 10K random hidden states experienced in the 300-episode subset and performing gradient descent.</p><p>We optimize fixed points for each belief, on each agent; by optimization termination most points have an RNN update norm &lt; 1 ? 10 ?6 , though we take a subset with norms &lt; 4 ? 10 ?7 . From this population of slow points we sample 50 points for <ref type="figure" target="#fig_4">Fig. 5</ref>. Then, we compute the Jacobian of the RNN update with respect to the hidden state, and compute the Jacobian's eigendecomposition. The Jacobian will likely have complex eigenvalues; to convert these into a term representing memory, we compute a time constant (as in <ref type="bibr" target="#b21">[22]</ref>:</p><formula xml:id="formula_11">? (?) = | 1 log ||?|| |<label>(10)</label></formula><p>We show an example eigenvalue spectrum and associated time constants in <ref type="figure">Fig. A9</ref>. There does not appear to be large variation in spectra across fixed points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.11. High Dimensional Computation in RNNs</head><p>Prior works find FPs to be arranged in low-dimensional manifolds and are able to directly link RNN memory structure to, e.g., ring attractors and simplicial structures. We find that the RNN FP subspace for OBJECTNAV is high-dimensional, as measured e.g. by the participation ratio of the fixed point subspace. We believe this is due to 1) OBJECTNAV being considerably more complex than previously studied tasks, and 2) unconverged training. This high-dimensionality makes it is difficult to clearly link agent beliefs to any known attractors, e.g. inputs project to &gt; 3 dimensions. Though we observe certain behavioral phenomena that we expect to be reflected in RNN dynamics, e.g. the agent will do a near 360?when it is blocked, we find this difficult to visualize. We show examples fixed point subspaces for 3 beliefs in <ref type="figure" target="#fig_5">Fig. A10</ref>. These layouts are not qualitatively consistent across agents, nor beliefs, even though memory structure is.</p><p>While 20D fixed point subspaces do frustrate current techniques, we could alternately be surprised that the RNN which has 196 dimensions has such a relatively low-dimensional subspace. It would be valuable to both the vision community and beyond to understand how to scale up this technique to a moderately higher number of dimensions.  <ref type="figure">Figure A9</ref>. As an example, for the Jacobians of 10 sample fixed points from the PBL belief of the tether agent, we plot Top: their eigenvalues, Bottom: the associated time constants. Different fixed points have different colors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.12. Agent Embedding Visualization</head><p>A common way to gain intuition for agent knowledge is by examining agent embedding layers. Our agent incorporates two semantic embedding layers, one which embeds the semantic frame (for the ResNet), and one which embeds the semantic goal; we visualize the goal embedding with PCA in <ref type="figure" target="#fig_5">Fig. A11</ref>. It is difficult to draw falsifiable conclusions about these embeddings. For example, though there are promising clusters of (table, chair, cushion) and (toilet, sink, bathtub, shower) goals, this is also confounded as those same categories tend to have high average success or failure, respectively; i.e. instead of room semantics, the embeddings may simply imply goal difficulty. We do not show the semantic channel embedding as there does not appear to be any meaningful arrangement in PC-space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.13. Tethered Policy Updates</head><p>We tether a second policy to the acting policy by sharing a base network (we only split policies at the linear actor-critic heads) and training the second policy with a different reward. The first policy is naturally off-policy for the second policy; we thus incorporate V-Trace-like <ref type="bibr">[</ref> CovPred | Global PR: 24 Local PR: 23 <ref type="figure" target="#fig_5">Figure A10</ref>. For all beliefs of the tether agent, we plot fixed points (colored by goal category of the state that the fixed point was initialized at) along with sample hidden state trajectories (connected with lines). Plots are in the top-2 PCs of the hidden states. Layouts are different across beliefs, but the top-2 PCs only account for a moderate amount of variance among the fixed points. The RNN correseponding to CPC|A-4 only has 1 unique fixed point, but it is not solely an attractor. r = ? tether (a|s) ? tether old (a|s) with ? tether (a|s) With ? t V = r t + ?V xt+1 ? V xt being a TD for V . We set clip terms a = 0.01, b = 1.0. The overall RL loss (both actor and critic losses) are averaged across policies, i.e. equal weighting of acting and tethered policy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.14. Training and Auxiliary Task Details</head><p>We train our agent via Proximal Policy Optimization PPO) <ref type="bibr" target="#b30">[31]</ref> with Generalized Advantage Estimation (GAE) <ref type="bibr" target="#b29">[30]</ref> and using the Adam optimizer <ref type="bibr" target="#b18">[19]</ref>. Agent parameter counts were all 5 ? 6 million parameters, excluding parameters in auxiliary modules. This amounts to GRU hidden sizes of 196 (except for the No Aux ablation, which has 512). General hyperparameters follow <ref type="bibr" target="#b39">[40]</ref>:</p><p>Rollout Workers: n = 4</p><p>Rollout Length: t = 128</p><p>PPO Epochs = 4</p><p>PPO Mini-batches = 2</p><p>Discount ? = 0.99</p><p>GAE ? = 0.95 (16) lr = 2.5 ? 10 ?4</p><p>Adam ? = 1 ? 10 ?5</p><p>Gradient Norm Cap = 0.5</p><p>PPO Clip = 0.2 <ref type="bibr" target="#b19">(20)</ref> except PPO Clip factor, which was set to 0.2 instead of 0.1, as per recommendations in <ref type="bibr" target="#b37">[38]</ref>.</p><p>Our complete loss is:</p><p>L total (? m ; ? a ) = L RL (? m ) ? ?H action (?) + L Aux (? m ; ? a ) H attn is the entropy of the attention distribution over the different auxiliary tasks. We set ? = 0.01 for 4-action agents (as in <ref type="bibr" target="#b39">[40]</ref>) and ? = 0.0075 for 6-action agents. We set ? = 0.075, which amounts to belief weighting across to be ? equal; we find little difference when weighting is more flexible. Auxiliary task loss coefficients ? were determined such that the loss terms were in the same order of magnitude at initialization.</p><p>For extended details, the configurations of our experiments are available in the code release.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.15. Rednet Tuning</head><p>We use RedNet to predict segmentation from RGBD at test time. We finetune the model (which was pretrained on SUN-RGBD) with 100K randomly sampled front-facing views rendered in the Habitat simulator (16K validation views). This procedure is the one used in <ref type="bibr" target="#b2">[3]</ref>. We initially trained the model to segment all 40 MP3D categories; we present its accuracies in <ref type="table" target="#tab_2">Table A4</ref>. Unlike our agent, this RedNet is greatly overfit. Since our agent did not appear to greatly leverage semantic cues during exploration (through Section 4.1, Section A.12), we reduced the complexity of the Red-Net task by asking it to only segment the 21 goal categories (other categories were cast to VOID). This improves performance of segmentation on the goal categories, and resulted in slight improvement in validation scores after 10M frames of agent tuning, so we used it in the main text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.16. Negative Results</head><p>In the course of our experiments, we found that:</p><p>? Adding a curiosity module (ICM <ref type="bibr" target="#b24">[25]</ref>) as non-episodic intrinsic reward failed to change performance significantly.</p><p>? Forcing agent recall (to fix the "loop" failure mode) with an Action Recall auxiliary task failed to change agent performance. This task was implemented as ADP with a negative horizon.</p><p>? Controlling the agent's sense of time by projecting beliefs out of the probed time dimension was unstable. We could not e.g. solve "Commitment" failure modes by setting the time variable to near end-episode without degrading performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.17. RedNet vs GT with CI</head><p>We present a full version of <ref type="table">Table 3</ref> with 95% CI estimates in <ref type="table">Table A5</ref>.  <ref type="table">Table A5</ref>. Performance on a 300-episode subset of TRAIN and VAL splits, reported as "with GT segmentation (with RedNet segmentation)". Reproducing <ref type="table">Table 3</ref> with 95% CIs included. Bold values are significantly better (p &lt; 0.05) than non-bold values in a paired t-test on 300 episodes. Note that the VAL performance with RedNet is reported on the full split, taken from the primary experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Breakdown of failure modes for base 6-action agent in 300 episodes of VAL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>We probe time and max SGE seen on each belief, the fused belief, and the observation embedding ("Obs") as a baseline. Error bars show 95% CI across single beliefs. Both concepts are encoded, though imprecisely and with high variance across beliefs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>We measure curvature of observation embeddings and beliefs of the base 6-action agent, plotting the distribution from transitions in 300 episodes. All but CP closely correlate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Fixed point (FP) memories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure A1 .</head><label>A1</label><figDesc>While the base agent succeeds on all tasks but "TV Monitor", the tethered agent struggles to solve 5 categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure A5 .</head><label>A5</label><figDesc>Base 6 Action Agent, training and validation curves. Note: Training curve is taken directly from training logs, i.e. it evaluates statistics on the rollout and not on the whole training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure A6 .</head><label>A6</label><figDesc>lo r e P la t e a u L o o p D e t e c t io n V o id L a s t m il e M is c O p e n G o a l b u We provide breakdown of tether failure modes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure A11 .</head><label>A11</label><figDesc>? act old (a|s) and 2. add a clipped importance weight term c to the value function target fromv s = V (x s ) + s+n?1 t=s ? t?s ? ? t V to v s = V (x s ) + s+n?1 t=s ? t?s ? clip(? tether (a|s) ? act (a|s) , a, b)? t V We project goal and semantic channel embeddings into their top-2 PCs. (Inset indicates variance accounted for by PCs, title indicates that participation ratio (approximate dimensionality) of goal is around 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>f ??? are linear layers, g CP denote the GRU's state, and ? cov (t, t+i) denotes the change in coverage, i.e. number of new voxels visited between time t and t + i Primary variants on the MP3D VAL and TEST-STD splits, along with prior SoTA from the Habitat Challenge leaderboard</figDesc><table><row><cell>1 . Perfect</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablations on VAL. Bold values are significantly better than unbolded values in the same group. New auxiliary tasks provide moderate performance gains, and tethered training preserves Success better than directly tuning on sparse OBJECTNAV reward.</figDesc><table><row><cell cols="2">1) Base (4-Action)</cell><cell></cell><cell cols="2">34.4?2.0</cell><cell>9.58?0.75</cell></row><row><cell cols="3">2) -Action Distribution Prediction</cell><cell cols="2">34.1?2.0</cell><cell>9.27?0.69</cell></row><row><cell cols="3">3) -Generalized Inverse Dynamics</cell><cell cols="2">33.7?2.0</cell><cell>9.05?0.69</cell></row><row><cell cols="2">4) -Coverage Prediction</cell><cell></cell><cell></cell><cell>32.4?2.0</cell><cell>8.7s7?0.69</cell></row><row><cell cols="2">5) -Semantic Goal Exists</cell><cell></cell><cell></cell><cell>20.3?1.7</cell><cell>4.14?0.47</cell></row><row><cell cols="2">6) 6-Action + Tether</cell><cell></cell><cell cols="2">26.6?1.9</cell><cell>9.79?0.82</cell></row><row><cell cols="2">7) 6-Action + Sparse Tuning</cell><cell></cell><cell></cell><cell>23.1?1.8</cell><cell>8.43?0.76</cell></row><row><cell></cell><cell cols="2">Success % (?)</cell><cell></cell><cell>SPL % (?)</cell></row><row><cell></cell><cell>TRAIN</cell><cell>VAL</cell><cell></cell><cell>TRAIN</cell><cell>VAL</cell></row><row><cell>1) 4-Act</cell><cell cols="3">50.3 (36.0) 43.3 (34.4)</cell><cell>18.1 (12.4) 12.3 (9.6)</cell></row><row><cell>2) 6-Act</cell><cell cols="3">56.0 (21.7) 58.0 (30.8)</cell><cell>21.5 (8.2) 16.9 (7.6)</cell></row></table><note>3) 6-Act + Tether 54.0 (27.3) 52.2 (26.0) 27.9 (11.5) 26.0 (9.8)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Description of prominent failure modes.</figDesc><table><row><cell></cell><cell>0.4 0.5</cell><cell>Time</cell><cell>Max SGE</cell></row><row><cell>Probe R 2</cell><cell>0.0 0.1 0.2 0.3</cell><cell>Beliefs Fused Obs</cell><cell>Beliefs Fused Obs</cell></row><row><cell></cell><cell></cell><cell>6-Act 6-Act + Tether Variant</cell><cell>6-Act 6-Act + Tether Variant</cell></row></table><note>We find that agent failures are diverse and beyond only un-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table /><note>, row 6 and 7). Tethered agents explore worse, and behavioral coding finds a new dominant failure mode wherein the agent quits early (see Section A.2).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table A4 .</head><label>A4</label><figDesc>RedNet<ref type="bibr" target="#b17">[18]</ref> performance on the MP3D dataset after tuning on 40 MP3D categories, or after tuning on 21 goal categories. Note that these numbers include accuracy on VOID class which skew numbers (especially accuracy) upward.</figDesc><table><row><cell></cell><cell cols="3">mIoU mRecall mPrecision Acc</cell></row><row><cell>train</cell><cell>40-class 77.09 86.46 21-class 37.13 92.25</cell><cell>87.22 39.23</cell><cell>91.62 81.96</cell></row><row><cell>val</cell><cell>40-Class 24.95 36.96</cell><cell>39.62</cell><cell>61.88</cell></row><row><cell></cell><cell>21-Class 15.93 39.45</cell><cell>21.65</cell><cell>69.01</cell></row><row><cell>22)</cell><cell></cell><cell></cell><cell></cell></row></table><note>L Aux (? m ; ? a ) =nAux i=1 ?i L i Aux (? m ; ? i a ) ? ?H attn (? m ) (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>( 36?5.5 ) 43.3?5.6 ( 34.4?2.0 ) 18.1?2.7 ( 12.4?2.3 ) 12.3?2.1 ( 9.58?0.75 ) 2) 6-Act 56?5.6 ( 21.7?4.7 ) 58.0?5.6 ( 30.8?1.9 ) 21.5?2.9 ( 8.24?2 ) 16.9?2.3 ( 7.60?0.64 ) 3) 6-Act + Tether 54?5.7 ( 27.3?5.1 ) 48.7?5.7 ( 26.6?1.9 ) 27.9?3.5 (11.5?2.5 ) 19.1?2.7 ( 9.79?0.82 )</figDesc><table><row><cell></cell><cell>Success % (?)</cell><cell></cell><cell>SPL % (?)</cell></row><row><cell></cell><cell>TRAIN</cell><cell>VAL</cell><cell>TRAIN</cell><cell>VAL</cell></row><row><cell>1) 4-Act</cell><cell>50.3?5.7</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that ?cov(t, t + i) ? {0, . . . , i}</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. The authors thank Vincent Cartillier for help preparing the RedNet models. EW is supported in part by an ARCS fellowship. The Georgia Tech effort was supported in part by NSF, AFRL, DARPA, ONR YIPs, ARO PECASE, Amazon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government, or any sponsor.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The geometry of integration in text classification rnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Ramasesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Sussillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maheswaranathan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devendra</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06757</idno>
		<title level="m">Roozbeh Mottaghi, Manolis Savva, and Amir Roshan Zamir. On evaluation of embodied navigation agents</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Semantic mapnet: Building allocentric semanticmaps and representations from egocentric views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Cartillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhile</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Matterport3d: Learning from rgb-d data in indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://kaldir.vc.in.tum.de/matterport/MP_TOS.pdf.2" />
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV), 2017. MatterPort3D dataset license available at</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object goal navigation using goal-oriented semantic exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devendra</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhiraj</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to explore using active neural mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devendra</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhiraj</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Low-dimensional dynamics for working memory and time encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Cueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Saez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Encarni</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aldo</forename><surname>Genovesio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Jazayeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranulfo</forename><surname>Romo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Daniel</forename><surname>Salzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">N</forename><surname>Shadlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Fusi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">37</biblScope>
			<biblScope unit="page" from="23021" to="23032" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Probing emergent semantics in predictive agents via question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Carnevale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamza</forename><surname>Merzic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosalia</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Abramson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>Alden Hung, Arun Ahuja, Stephen Clark, Gregory Wayne, and Felix Hill</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymir</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Firoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fixedpointfinder: A tensorflow toolbox for identifying and characterizing fixed points in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sussillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">31</biblScope>
			<biblScope unit="page">1003</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shagun</forename><surname>Sodhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<title level="m">Yoshua Bengio, and Bernhard Sch? ?lkopf. Recurrent independent mechanisms</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Bootstrap latent-predictive representations for multitask reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Bernardo Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Bastien Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad Gheshlaghi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Daniel Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bernardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Munos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06407</idno>
		<title level="m">Neural predictive belief representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Perceptual straightening of natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Robbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Goris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Environmental drivers of systematicity and generalization in a situated agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Lampinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosalia</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Rednet: Residual encoder-decoder network for indoor rgb-d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lunan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijun</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Sscnav: Confidence-aware semantic scene completion for visual semantic navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqing</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">How recurrent networks implement contextual processing in sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sussillo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08013</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Reverse engineering recurrent networks for sentiment classification reveals line attractor dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sussillo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning to navigate in complex environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Banino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<idno>abs/1611.03673</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visual representations for semantic target driven navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wahid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Curiosity-driven exploration by self-supervised prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">An exploration of embodied visual exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Santhosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning by playing -solving sparse reward tasks from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lampe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Neunert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Degrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Van De Wiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Habitat: A platform for embodied AI research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Maksymets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavana</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Mid-level visual representations improve generalization and sample efficiency for learning visuomotor policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Emi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11971</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">High-dimensional continuous control using generalized advantage estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Opening the black box: lowdimensional dynamics in high-dimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sussillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Barak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="626" to="649" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A neural network that finds a naturalistic solution for the production of muscle activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sussillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Churchland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">V</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shenoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1025" to="1033" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Habitat challenge</title>
		<ptr target="https://aihabitat.org/challenge/2020,2020.1" />
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>Habitat Team</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised temporal consistency metric for video segmentation in highlyautomated driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serin</forename><surname>Varghese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasin</forename><surname>Bayzidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sounak</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">David</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><forename type="middle">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Schlicht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Huger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Fingscheidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="336" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning generalizable visual representations via interactive gameplay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Weihs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiana</forename><surname>Ehsani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winson</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Herrasti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">How to train pointgoal navigation agents on a (sample and compute) budget</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">DD-PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Auxiliary tasks speed up learning pointgoal navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoRL</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
