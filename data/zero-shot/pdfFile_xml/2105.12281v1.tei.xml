<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FINNger -Applying artificial intelligence to ease math learning for children</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><forename type="middle">B</forename><surname>Audibert</surname></persName>
							<email>rbaudibert@inf.ufrgs.br</email>
							<affiliation key="aff0">
								<orgName type="institution">INF -UFRGS Porto Alegre</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vin?cius</forename><forename type="middle">M</forename><surname>Maschio</surname></persName>
							<email>vmmaschio@inf.ufrgs.br</email>
							<affiliation key="aff1">
								<orgName type="institution">INF -UFRGS</orgName>
								<address>
									<settlement>Porto Alegre</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FINNger -Applying artificial intelligence to ease math learning for children</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Kids have an amazing capacity to use modern electronic devices such as tablets, smartphones, etc. This has been incredibly boosted by the ease of access of these devices given the expansion of such devices through the world, reaching even third world countries. Also, it is well known that children tend to have difficulty learning some subjects at pre-school. We as a society focus extensively on alphabetization, but in the end, children end up having differences in another essential area: Mathematics. With this work, we create the basis for an intuitive application that could join the fact that children have a lot of ease when using such technological applications, trying to shrink the gap between a fun and enjoyable activity with something that will improve the children knowledge and ability to understand concepts when in a low age, by using a novel convolutional neural network to achieve so, named FINNger.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Kids have an amazing capacity to use modern electronic devices such as tablets, smartphones, etc. This has been incredibly boosted by the ease of access of these devices given the expansion of such devices through the world, reaching even third world countries <ref type="bibr" target="#b0">[1]</ref>. So, an easy-to-follow step would be to use this technology when teaching/developing our children. It is well known that children tend to have difficulty learning some subjects at pre-school. We as a society focus extensively on alphabetization, but in the end, children end up having differences in another essential area: Mathematics. We then focus on creating an intuitive application that could join the fact that children have a lot of ease when using such technological applications, trying to shrink the gap between a fun and enjoyable activity with something that will improve the children knowledge and ability to understand concepts when in a low age.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. OUR APPLICATION</head><p>On this assignment, we propose to build a desktop application where kids could, using their camera and/or any image capturing device, show to the computer their hands (see <ref type="figure">Fig.  1</ref>), and the computer would be able to infer how many fingers that child raised and be able to compute some basic arithmetic with it. Ideally, and in future work, this application should be able to run in a smartphone or a tablet, preferentially a lowend one, easing the access to non-developed countries (the ones which would benefit the most from it) to the application.</p><p>We propose two different approaches for this: <ref type="figure">Fig. 1</ref>. Example of a kid hand which we should label as 2 raised fingers ? using an edge detection algorithm with a custom heuristic to compute how many raised fingers there is. This is mostly a baseline method. ? using a convolutional neural network trained in a big number of images so that the model can generalize enough so that it can differentiate between different hand sizes and colors, different backgrounds, and different illumination setups.</p><p>III. DATASET To develop our application, a dataset was needed: both to test the edge detection algorithm and heuristic, and to train the neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. koryakinp/fingers</head><p>[7] is a dataset developed by Pavel Koryakin made available on https://kaggle.com with a public domain license. It consists of 21600 black and white 128x128 images of right and left hands, holding from 0 to 5 raised fingers, in a noisy background. This 21600 images are furter split on 18000 images which will be used to train the dataset, and 3600 which will be used to validate/test it. Some example of the available images can be seen on <ref type="figure" target="#fig_0">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Custom dataset</head><p>However, we found that the above dataset wouldn't closely relate to reality since the images have a very specific illumination and pose, hardly related to a real-life image. Given that, we proposed to create a custom dataset where we could have images with different lighting and real-life background setups.</p><p>Therefore, we created a dataset with 3200 images (200 for each number quantity on each hand), from which we used 2840 (85%) for the training dataset and the remaining 360 for the testing/validation phase.</p><p>To generate it, we developed a custom application to make the task easier, being able to generate the full dataset in only 15 minutes, which provide us the ability to easily expand it to a bigger size in the future. The application (developed in Python) asks for the number of images that we want to generate, as well as which hand and quantity of numbers we are taking photos (so that we can automatically label it), and proceeds to take photos using a webcam.</p><p>Some examples of pictures taken for this dataset can be seen on <ref type="figure" target="#fig_1">Fig. 3</ref>. One can notice that the image even has the presence of the body of the person, and not only its hand, more closely relating to a real-life situation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. IMPLEMENTATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Edge Detection</head><p>Edge detection is a process that attempts to distinguish significant intensity changes from an image. This differentiation This process is done to being able to create a delimitation between the hand and the image's background.</p><p>1) Distinguishing from background: In the process to differentiate between the object we are trying to evaluate and the background, we start by converting the image colors to HSV using <ref type="bibr" target="#b8">[8]</ref>. With this result, we can separate the object within the background by the range of the colors, using a lower -RGB(0, 0, 255) -and a higher -RGB(0, 0, 90) -color skin for a range of the hand, as can be found on <ref type="figure" target="#fig_2">Fig. 4</ref>.  <ref type="figure" target="#fig_2">Fig. 4</ref>, we can create a mask of the object we need to distinguish. Using <ref type="bibr" target="#b8">[8]</ref> for the process, we generate a mask and apply on it a gaussian blur to remove unwanted noises, such as blank points, which can be found on <ref type="figure" target="#fig_3">Fig. 5</ref>.</p><p>3) Distinguishing fingers: Using a tool from <ref type="bibr" target="#b8">[8]</ref>, we can find the contours of the mask <ref type="figure" target="#fig_3">(Fig. 5</ref>), apply the Convex Hull function to remove the concavity between the finger, and obtain a geometric shape from it, as can be found on <ref type="figure">Fig. 6</ref>. With the coordinates of this geometric shape, we can obtain the hand's centroid and size, to be used as parameters to draw a circle that covers all the fingers and the wrist of the hand, as we can see on <ref type="figure">Fig. 7</ref>. Applying this circle as a mask and finding its contours, as can be found on <ref type="figure" target="#fig_5">Fig. 8</ref>, we can obtain only the parts which the fingers and wrist touch the circle.  with a higher arbitrary maximum size. As final, we count the remaining contours and the result will be the number of fingers on this image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Convolutional Neural Network</head><p>To introduce the Convolutional Neural Network model it is important to understand better how some of its parts work. This is a rough overview, and a survey with a thorough explanation can be found on <ref type="bibr" target="#b1">[2]</ref>.</p><p>? Activation In a Neural Network, we define how we will output the values from one layer through the application of an activation function. A neural network can have different activation functions in different parts of its network, as explained by the fact that an activation function can have a high impact on network learning. An in-depth explanation of activation functions and their use case can be found on <ref type="bibr" target="#b3">[4]</ref>. An example of possible activation functions can be found on <ref type="figure" target="#fig_6">Fig. 9</ref>. ? Batch Normalization Layer Batch Normalization layers were introduced on [5] as a means to avoid Internal Covariate Shift, which, according to <ref type="bibr" target="#b4">[5]</ref>, can be defined "[...] as the change in the distribution of network activations due to the change in network parameters during training.", i.e., the fact that when our data passes from a layer to the following one the data on it can vary its range creating numerical instability. Intuitively, what this layer does is to re-scale and re-center our layer values in an acceptable (and learnable) range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Max-Pooling Layer</head><p>A max-pooling layer is a non-learnable layer that decreases our data shape/size. A simple and intuitive explanation can be found on <ref type="figure">Fig. 10</ref>. This layer has a kernel with size k ? R n which is iterated through the input data, reducing the input dimensionality. For example, for a matrix M ? R nXm , if we applied a (i, j) kernel-sized max-pooling layer to it we would end up with a new matrix N ? R n/i X m/j . <ref type="figure">Fig. 10</ref>. Example of a 2D MaxPooling application using a 2x2 kernel <ref type="bibr" target="#b2">[3]</ref> ? Dropout Layer Proposed on <ref type="bibr" target="#b5">[6]</ref>, dropout layers are used to avoid overfitting. Randomly dropping some of the previous layer values, we avoid the network completely memorizing the input since it needs to keep track of the classification however values are randomly dropped. Note that we do not always the same values, and in fact, we drop every feature with probability p using samples from a Bernoulli distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The model -FINNger</head><p>The model, from now on referred to by its name FINNger, was developed in Python using PyTorch [10] <ref type="bibr" target="#b11">[11]</ref>. We also relied heavily on cv2 <ref type="bibr" target="#b8">[8]</ref> and numpy <ref type="bibr" target="#b9">[9]</ref> to parse and manipulate the images.</p><p>We trained several different models before settling with the one described below. In the beginning, we tried detecting both the quantity of the numbers and the hand (left or right) on the image, given the fact that the dataset had this information. After, we ended up dropping this because we didn't use this information for anything in our work.</p><p>Remember that batch normalization and dropout layers do not change the input shape, outputting the same shape. Also, every convolutional layer uses a (4, 4) kernel size. It is built in the form of the following building blocks, remembering that the input has shape (3, 96, 96): kernel, outputting a (128, 12, 12) matrix f) C 3 d 0.4 : Dropout layer with p = 0.4, this is, setting, randomly, 40% of the input values to 0 4) F C 1 : Fully connected layer, which flattens the previous R 3 matrix to a one-dimensional vector when taking it as input. It then outputs a (128) sized vector. 5) F C 1 : Fully connected layer, responsible for outputting the classification, and, therefore, outputting a (6)-shaped vector. In the end we apply a "log softmax" function to the output, as shown in Eq. 5:</p><p>LogSoftmax(x i ) = log exp x i j exp x j A schema representing the full model architecture can be found on <ref type="figure" target="#fig_3">Fig. 15</ref> where the displayed numbers represent the output shape of that layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Edge Detection</head><p>We evaluated all the images from the koryakinp/fingers dataset to analyze how well the Edge Detection performed. We were able to achieve the success accuracy of 72.4% using the testing images and 72.9% using the training images, with a standard deviation of 1.15 fingers on both. We used these results to create a correlation graph, as you can see on <ref type="figure" target="#fig_7">Fig.  11</ref>, and we could notice that the Edge Detection had most of its errors when evaluating images with 5 fingers, where the results took were evaluated missing 1 finger on the image. This happened mostly because the process to differentiate the fingers couldn't distinguish between touching fingers. We can notice that there were some evaluations to 6 fingers, where the conditional to remove the wrist didn't work as expected, where the wrist's size wasn't big enough to be caught in the condition. This behavior occurred in the results with a value of 0.08% using the testing images and 0.11% using the training images. The Edge Detection applied to the custom dataset couldn't work satisfactorily, there was almost 30% of accuracy. However, in general, all the evaluations were false positives, when the evaluation results were unintentionally successful. We analyzed the evaluation process and noticed that the HSV conversion and masking to separate the hand from the background as we used on <ref type="figure" target="#fig_2">Fig. 4 and Fig. 6</ref>, didn't work as expected, separating parts of the background because the environment wasn't controlled as the koryakinp/fingers dataset. <ref type="figure" target="#fig_7">Fig. 11</ref>. Correlation between the expected values (lines) and the heuristic output (columns)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. FINNger</head><p>On early attempts we used <ref type="bibr" target="#b6">[7]</ref>'s dataset, but it ended up not having good results when facing real-life examples, even though it achieved 92% accuracy on the validation dataset after only 10 epochs. We also tried using our custom dataset mixed with <ref type="bibr" target="#b6">[7]</ref> but it ended up ignoring our dataset, and training/learning only <ref type="bibr" target="#b6">[7]</ref> characteristics. The possible problem with the dataset is the fact that the images are more illuminated than the background (in real life this is probably the opposite), which highlights the fact that the network worked at night when we had a dark background and an indirectly lightened hand.</p><p>We then changed our approach to using only our custom dataset, implementing data augmentation to fix the fact that the dataset is small when considering the size of the datasets that deep learning usually needs. Our data augmentation process was fairly simple, creating random translations and horizontal flip of the image. We did not implement rotation data augmentation and this can be seen in the empirical results when trying to use the resulting network, noticing that if we tilt the hand a little bit, the accuracy decreases drastically.</p><p>We trained our FINNger neural network for 100 epochs with Adam <ref type="bibr" target="#b12">[12]</ref> optimizer. As hyperparameters, we chose 0.0003 for the learning rate and 0.0001 for the weight decay. We didn't do any exploration with the hyperparameters since these values were the first tested ones (based on previous works neural networks) and they worked pretty well.</p><p>In our validation dataset, we achieve a maximum 98% accuracy as depicted on <ref type="figure" target="#fig_0">Fig. 12</ref> after circa 75 epochs, and capping around it oscillating back and forth to 95%. Plotting a correlation matrix, as seen on <ref type="figure" target="#fig_1">Fig. 13</ref> we can see that we have a high correlation, and when missing the correct value we rarely overestimate, tending to get the results wrong saying we have fewer numbers than we have. Important to notice that we will never say we have more than 5 fingers (like we did on the "Edge Detection" approach) because the neural network can only classify the input image in 6 categories, namely [0, 1, 2, 3, 4, 5]. When tested against real children's hands, we don't achieve good results. The network tends to detect 0 fingers, except when the children have the hand fully open. This is probably related to the fact that the proportion of children's hands are different than from an adult one, with the fingers being a lot smaller in comparison. We hypothesize however that if we trained our neural network using children's hands' images we could achieve an acceptable result.</p><p>Also, we developed a basic UI, see <ref type="figure" target="#fig_2">Fig. 14,</ref> where we can check the UI result and do the calculations as we proposed so that it could be used by children to learn basic arithmetic. On the top, we have the value that the neural network currently thinks it is being shown to it. Below it, we display the calculation: to compute it you press enter on the terminal when you want to "capture" the result. On the bottom, we are displaying, for debug purposes, the values outputted by the network.</p><p>You can verify on the example <ref type="figure" target="#fig_2">(Fig. 14)</ref> that it is correctly detecting a 2. Checking the network output we can see that it has great confidence that it is a 2, although it also thinks it might be a 3 from the value in the fourth vector position, which makes sense since 3 has the more similar "hand format" to 2. The code for the model (training and evaluation), as well as the dataset creation application can be found on <ref type="bibr" target="#b13">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DRAWBACKS</head><p>In the process to evaluate hands with different colors or different background, there were many cases where the Edge Detection couldn't distinguish a hand successfully, these are related to the wide range of colors the background and the hand can variate, making it difficult to find the right specific range of colors for a hand. And because the background can have the same colors as a hand, this approach has some limitation to not mix the background within the hand's Edge Detection.</p><p>The Convolutional Neural Network couldn't be trained with a diverse of scenarios enough to learn about hands with different colors or sizes such as child's hands. This limitation restricted the accuracy when applying the CNN to a wide range of scenarios. When evaluating with children, because of the small size of the hands, the results were mostly evaluated to zero fingers (or closed hand). We also noticed that what the CNN was to distinguish lines on the images, which made it find fingers on objects with shapes as straight lines, such as furnishings, bookshelves, or any other objects on the scenario. We believe these errors could reduce using better quality images with a higher range of colors (instead of training it with B&amp;W images).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. FUTURE WORK</head><p>To build onto this work, we could work on the following:</p><p>? Improve the finger counting heuristic on the edge detection algorithm, to better use the convex hull, possibly counting the number of outstanding edges instead of using the "circle" technique; ? Create a bigger and more comprehensive dataset using different illumination and location settings. Also, actually using children hands so that the neural network can correctly infer there fingers (learning that fingers can be short); ? Create a syntetic dataset using <ref type="bibr" target="#b6">[7]</ref>'s hands and adding custom random backgrounds, changing the illumination, rotation and translation generating, possibly, an infinitelysized dataset; ? Create a mobile application which uses the trained model, so that it can be more easily acessed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>We showed that we can have basic and satisfactory results using a good edge detection algorithm if we have strict and controlled position and illumination settings.</p><p>For the neural network, we arrived at the conclusion that we had as the hypothesis in the beginning: it would work for similar images to the dataset, but probably not achieve optimal results with children hands. We hypothesize that the only gap to be filled to achieve a perfect application is a better dataset. Using any of the combinations of the techniques proposed in the "Future Work" section would also improve our work and make it better to be used in real life.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Example images available on [7] dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Example images from the custom dataset becomes relevant by the use of a regularizing filter operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Image converted to HSV colors 2) Separation: With the color range and the HSV image found on</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Mask with gaussian blur generated fromFig. 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Contoured Image using Convex Hull from<ref type="bibr" target="#b8">[8]</ref> Circle applied on image 4) Counting Heuristic: To develop a heuristic to count the number of fingers an image may have, first we use the information of each contour's size found onFig. 8to identify if there is a wrist, using a conditional to remove a contour</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Circle applied as a mask on image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Possible activation functions<ref type="bibr" target="#b3">[4]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>1 ) C 1 :</head><label>11</label><figDesc>Convolutional block 1, composed of the following layers: a) C 1 c 1 : 2D Convolutional layer outputting a (64, 96, 96) matrix, with ReLU activation b) C 1 b 1 : Batch normalization layer c) C 1 c 2 : 2D Convolutional layer outputting a (64, 96, 96) matrix, with ReLU activation d) C 1 b 2 : Batch normalization layer e) C 1 mp 2X2 : 2D MaxPooling layer with a (2, 2) kernel, outputting a (64, 48, 48) matrix f) C 1 d 0.2 : Dropout layer with p = 0.2, this is, setting, randomly, 20% of the input values to 0 2) C 2 : Convolutional block 2, composed of the following layers: a) C 2 c 1 : 2D Convolutional layer outputting a (128, 48, 48) matrix, with ReLU activation b) C 2 b 1 : Batch normalization layer c) C 2 c 2 : 2D Convolutional layer outputting a (128, 48, 48) matrix, with ReLU activation d) C 2 b 2 : Batch normalization layer e) C 2 mp 2X2 : 2D MaxPooling layer with a (2, 2) kernel, outputting a (128, 24, 24) matrix f) C 2 d 0.3 : Dropout layer with p = 0.3, this is, setting, randomly, 30% of the input values to 0 3) C 3 : Convolutional block 3, composed of the following layers: a) C 3 c 1 : 2D Convolutional layer outputting a (128, 24, 24) matrix, with ReLU activation b) C 3 b 1 : Batch normalization layer c) C 3 c 2 : 2D Convolutional layer outputting a (128, 24, 24) matrix, with ReLU activation d) C 3 b 2 : Batch normalization layer e) C 3 mp 2X2 : 2D MaxPooling layer with a (2, 2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .</head><label>12</label><figDesc>Model accuracy over the 100 epochs where the brown line depicts the exact accuracy and the orange line a smoothed polynomial</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 .</head><label>13</label><figDesc>Correlation between the expected values (lines) and the network output (columns)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14 .</head><label>14</label><figDesc>UI from our application</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Smartphone Ownership Is Growing Rapidly Around the World, but Not Always Equally</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Silver</surname></persName>
		</author>
		<ptr target="https://www.pewresearch.org/global/wp-content/uploads/sites/2/2019/02" />
	</analytic>
	<monogr>
		<title level="j">Pew-Research-Center Global-Technology-Use</title>
		<imprint>
			<date type="published" when="2019-02" />
		</imprint>
	</monogr>
	<note>Pew Research Center</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2004.02806" />
		<title level="m">A Survey of Convolutional Neural Networks: Analysis, Applications, and Prospects</title>
		<imprint/>
	</monogr>
	<note>CoRR, 2020. Available at</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Max-pooling / Pooling -Computer Science Wiki</title>
		<ptr target="https://computersciencewiki.org/index.php?title=Max-pooling/Pooling&amp;oldid=7839" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>Computer Science Wiki</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Introduction to Different Activation Functions for Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jadon</surname></persName>
		</author>
		<ptr target="https://medium.com/@shrutijadon10104776/survey-on-activation-functions-for-deep-learning-9689331ba092" />
	</analytic>
	<monogr>
		<title level="m">Medium</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1502.03167" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing coadaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1207.0580" />
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">koryakinp/fingers Kaggle&apos;s Dataset&apos;, Kaggle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koryakin</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/koryakinp/fingers" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Fig. 15. Our model</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The OpenCV Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<ptr target="https://opencv.org" />
	</analytic>
	<monogr>
		<title level="m">Dr. Dobb&apos;s Journal of Software Tools</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Array programming with NumPy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Harris</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-020-2649-2</idno>
		<ptr target="https://doi.org/10.1038/s41586-020-2649-2" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
		<imprint/>
	</monogr>
	<note>NEURIPS2019. Available at</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">PyTorch Website Documentation</title>
		<ptr target="https://pytorch.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1412.6980.pdf" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">FINNger -Applying artificial intelligence to easemath learning for children&quot; paper, INF/UFRGS, 2021</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Audibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Maschio</surname></persName>
		</author>
		<ptr target="https://github.com/rafaeelaudibert/FINNger" />
	</analytic>
	<monogr>
		<title level="m">Code for</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
