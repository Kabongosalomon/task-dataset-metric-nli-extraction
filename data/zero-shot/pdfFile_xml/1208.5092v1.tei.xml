<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Degree Linkage: Agglomerative Clustering on a Directed Graph</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012-08-25">25 Aug 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Degree Linkage: Agglomerative Clustering on a Directed Graph</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2012-08-25">25 Aug 2012</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a simple but effective graph-based agglomerative algorithm, for clustering high-dimensional data. We explore the different roles of two fundamental concepts in graph theory, indegree and outdegree, in the context of clustering. The average indegree reflects the density near a sample, and the average outdegree characterizes the local geometry around a sample. Based on such insights, we define the affinity measure of clusters via the product of average indegree and average outdegree. The product-based affinity makes our algorithm robust to noise. The algorithm has three main advantages: good performance, easy implementation, and high computational efficiency. We test the algorithm on two fundamental computer vision problems: image clustering and object matching. Extensive experiments demonstrate that it outperforms the stateof-the-arts in both applications. <ref type="table">1  1 The  code  and  supplemental  materials  are  publicly  available  at</ref> </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many problems in computer vision involve clustering. Partitional clustering, such as k-means <ref type="bibr" target="#b0">[1]</ref>, determines all clusters at once, while agglomerative clustering <ref type="bibr" target="#b0">[1]</ref> begins with a large number of small clusters, and iteratively selects two clusters with the largest affinity under some measures to merge, until some stopping condition is reached. Agglomerative clustering has been studied for more than half a century, and used in many applications <ref type="bibr" target="#b0">[1]</ref>, because it is conceptually simple and produces an informative hierarchical structure of clusters.</p><p>Classical agglomerative clustering algorithms have several limitations <ref type="bibr" target="#b0">[1]</ref>, which have restricted their wider applications in computer vision. The data in computer vision applications are usually high dimensional. The distributions of data clusters are often in different densities, sizes, and shapes, and form manifold structures. In addition, there are often noise and outliers in data. The conventional agglomerative clustering algorithms, such as the well-known linkage methods <ref type="bibr" target="#b0">[1]</ref>, usually fail to tackle these challenges. As their affinities are directly computed using pairwise distances between samples and cannot capture the global manifold structures in high-dimensional spaces, (a) Indegree can be use to detect the change of densities. The density in Cluster a is high, and the density in Cluster b is low. The vertices inside Cluster a are strongly connected, but there is no outedge to vertices outside Cluster a. So the indegree of k from Cluster a is nonzero, while the indegree of i (a vertex in Cluster b) and j (an outlier) from Cluster a are zero. If an undirected graph is considered without separating indegrees and outdegrees, both i and j have the same degree from Cluster a as k. (b) The product of the indegree and outdegree is an affinity measure robust to noisy edges between the two clusters. Under this measure, Cluster a and Cluster b have a zero affinity, i.e., the sum of product of indegree and outdegree for all vertices is 0, and thus they are separated well.</p><p>these algorithms have problems of clustering high-dimensional data, and are quite sensitive to noise and outliers <ref type="bibr" target="#b0">[1]</ref>.</p><p>To tackle these problems, we propose a simple and fast graph-based agglomerative clustering algorithm. The graph representation of data has been extensively exploited in various machine learning topics <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, but has rarely been utilized in agglomerative clustering. Our algorithm builds K-nearest-neighbor (K-NN) graphs using the pairwise distances between samples, since studies <ref type="bibr" target="#b3">[4]</ref> show the effectiveness of using local neighborhood graphs to model data lying on a low-dimensional manifold embedded in a high-dimensional space.</p><p>We use the indegree and outdegree, fundamental concepts in graph theory, to characterize the affinity between two clusters. The outdegree of a vertex to a cluster measures the similarity between the vertex and the cluster. If many of the K-NNs of the vertex belong the cluster, the outdegree is large. The outdegree can capture the manifold structures in the high dimensional space. The indegree of a vertex from a cluster reflects the density near the vertex. It is effective for detecting the change of densities, which often occurs at the boundary of clusters. Therefore, we use it to separate clusters close in space but different in densities, and also reduce the effect of noise. An example is shown in <ref type="figure">Fig. 1(a)</ref>. To our best knowledge, properties of the indegree and outdegree have not been explored by any existing clustering algorithm, although they were successfully applied in analysis of complex networks such as World Wide Web <ref type="bibr" target="#b8">[9]</ref> and social networks <ref type="bibr" target="#b9">[10]</ref> and showed interesting results. Our affinity measure between two clusters is defined as follows. First, the structural affinity from a vertex to a cluster is defined via the product of the average indegree from the cluster and average outdegree to the cluster. Intuitively, if a vertex belongs to a cluster, it should be strongly connected to the cluster, i.e., both its indegree and outdegree are large. Otherwise, either the indegree or outdegree is small. Therefore, the product of indegree and outdegree can be a good affinity measure ( <ref type="figure">Fig. 1(b)</ref>). We show that the correlation between the inter-cluster indegree and outdegree is weak across different vertices, if the two clusters belong to different ground-truth clusters, using synthetic data in <ref type="figure" target="#fig_2">Fig. 3</ref>. Then, the affinity between two clusters is naturally the aggregated affinity measure for all the vertices in the two clusters.</p><p>Our algorithm has three main advantages as follows. First of all, it has outstanding performance, especially on noisy data and multiscale data (i.e., clusters in different densities). The visual comparisons with linkage methods <ref type="bibr" target="#b0">[1]</ref>, graph-based average linkage, affinity propagation (AP) <ref type="bibr" target="#b6">[7]</ref>, spectral clustering (SC) <ref type="bibr" target="#b2">[3]</ref>, and directed graph spectral clustering (DGSC) <ref type="bibr" target="#b7">[8]</ref> on synthetic multiscale data are shown in <ref type="figure">Fig. 2</ref>. Noise and multiple scales can degrade the performance of spectral clustering greatly <ref type="bibr" target="#b10">[11]</ref>, while the indegree and outdegree in our algorithm detect the boundary of scales automatically 2 and reduce the effect of noise. In Sec. 4, extensive experiments on real data, including imagery data and feature correspondence data, demonstrate its superiority over state-of-the-art methods. These experiments aim at two fundamental problems in computer vision, i.e., image clustering and object matching, and the results suggest many potential applications of our work.</p><p>Second, it is easy to implement. This affinity measure can be expressed in a matrix form and implemented with vector additions and inner-products. Therefore, our algorithm can be implemented without any dependency on external numerical libraries, such as eigen-decomposition which was extensively employed by many clustering algorithms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Finally, it is very fast. We propose an acceleration method for our algorithm. In practice, our algorithm is much faster than spectral clustering <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, especially on largescale data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The literature dedicated to agglomerative clustering is abundant <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. Linkages <ref type="bibr" target="#b0">[1]</ref>, e.g., average linkage, define the affinity based on pairwise distances between samples. Since pairwise distances do not well capture the global structures of data, these methods fail on clustering data with complex structures and are sensitive to noise <ref type="bibr" target="#b0">[1]</ref> (see the example in <ref type="figure">Fig. 2</ref>). Many variants of linkage methods, such as DBSCAN <ref type="bibr" target="#b14">[15]</ref>, have been proposed in the data mining community and show satisfactory performance. However, they usually fail to tackle the great challenge from high-dimensional spaces, because their sophisticated affinity measures are based on observations from low-dimensional data <ref type="bibr" target="#b15">[16]</ref>.</p><p>Several algorithms <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> has attempted to perform agglomerative clustering on the graph representation of data. Chameleon <ref type="bibr" target="#b16">[17]</ref> defines the cluster affinity from relative interconnectivity and relative closeness, both of which are based on a min-cut bisection of clusters. Although good performance was shown on 2D toy datasets, it suffers from high computational cost because its affinity measure is based on a min-cut algorithm. Zell <ref type="bibr" target="#b17">[18]</ref> describes the structure of a cluster via the zeta function and defined the affinity based on the structural changes after merging. It needs to compute matrix inverse in each affinity computation, so it is much slower than our simple algorithm (see Sec. 4.1). Felzenszwalb and Huttenlocher proposed an effective algorithm for image segmentation <ref type="bibr" target="#b18">[19]</ref>.</p><p>Besides agglomerative clustering, K-means <ref type="bibr" target="#b0">[1]</ref> and spectral clustering <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">20]</ref> are among the most widely used clustering algorithms. However, K-means is sensitive to the initialization and difficult to handle clusters with varying densities and sizes, or manifold shapes. Although spectral clustering can handle the manifold data well, its performance usually degrades greatly with the existence of noise and outliers, because the eigenvectors of graph Laplacian are sensitive to noisy perturbations <ref type="bibr" target="#b4">[5]</ref>. Affinity Propagation <ref type="bibr" target="#b6">[7]</ref> explores the intrinsic data structures by message passing among data points. Although it performs well on high-dimensional data, it usually requires considerable run-time, especially when the preference value cannot be manually set.</p><p>Directed graphs have been studied for spectral clustering (e.g., <ref type="bibr" target="#b7">[8]</ref>). However, these methods symmetrize the directed graph before the clustering task. In contrast, we only symmetrize the affinity between two clusters, while keep the directed graph during the clustering process. Therefore, our algorithm utilizes more information from the asymmetry and is more robust to noisy edges (see <ref type="figure">Fig. 2</ref> for a comparison between DGSC [8] and our algorithm).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Graph Degree Linkage</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Neighborhood Graph</head><p>Given a set of samples X = {x 1 , x 2 , ..., x n }, we build a directed graph G = (V, E), where V is the set of vertices corresponding to the samples in X , and E is the set of edges connecting vertices. The graph is associated with a weighted adjacency matrix W = [w ij ], where w ij is the weight of the edge from vertex i to vertex j. w ij = 0 if and only if there is no edge from i to j.</p><p>To capture the manifold structures in high-dimensional spaces, we use the K-NN graph, in which the weights are defined as</p><formula xml:id="formula_0">w ij = exp ? dist(i,j) 2 ? 2 , if x j ? N K i , 0, otherwise,<label>(1)</label></formula><p>where dist(i, j) is the distance between x i and x j , N K i is the set of K-nearest neighbors of x i , and ? 2 is set as</p><formula xml:id="formula_1">? 2 = a nK n i=1 xj?N K i dist(i, j) 2 .</formula><p>K and a are free parameters to be set. In a K-NN graph, there is an edge pointing from</p><formula xml:id="formula_2">x i to x j with weight w ij , if x j ? N K i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Algorithm Overview</head><p>The graph degree linkage (GDL) algorithm begins with a number of initial small clusters, and iteratively selects two clusters with the maximum affinity to merge. The affinities are computed on the K-NN graph, based on the indegree and outdegree of vertices in the two clusters. The initial small clusters are simply constructed as weakly connected components of a K 0 -NN graph, where the neighborhood size K 0 is small, typically as 1 or 2. Then, each component is an initial cluster, and each sample is assigned to only one cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1 A connected component of an undirected graph is a maximal connected subgraph in which any two vertices are connected to each other by paths. A weakly connected component of a directed graph is a connected component of the undirected graph produced by replacing all of its directed edges with undirected edges.</head><p>The GDL algorithm is presented as Algorithm 1, with details given in the following subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Affinity Measure via Product of Indegree and Outdegree</head><p>The affinity measure between two clusters is the key of an agglomerative clustering algorithm. Our affinity measure is based on indegree and outdegree in the graph representation. For simplicity, we start from measuring the affinity between a vertex and a cluster.</p><p>Indegree and outdegree. Considering a vertex and a cluster, the connectivity between them by inedges and outedges can be quantified using the concepts of indegree and outdegree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Graph Degree Linkage (GDL)</head><p>Input: a set of n samples X = {x1, x2, ? ? ? , xn}, and the target number of clusters nT . Build the K 0 -NN graph, and detect its weakly connected components as initial clusters. Denote the set of initial clusters as V c = {C1, ? ? ? , Cn c }, where nc is the number of clusters. Build the K-NN graph, and get the weighted adjacency matrix W. while nc &gt; nT do Search two clusters Ca and C b , such that {Ca,</p><formula xml:id="formula_3">C b } = argmax Ca,C b ?V c AC a ,C b , where AC a ,C b</formula><p>is the affinity measure between Ca and C b , computed using Eq. <ref type="bibr" target="#b4">(5)</ref>.</p><formula xml:id="formula_4">V c ? {V c \ {Ca, C b }} ? {Ca ? C b }, and nc = nc ? 1. end while Output: V c .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2</head><p>Given a vertex i, the average indegree from and the average outdegree to a cluster C is defined as</p><formula xml:id="formula_5">deg ? i (C) = 1 |C| j?C w ji and deg + i (C) = 1 |C| j?C w ij , respectively, where |C| is the cardinality of set C.</formula><p>As we stated in Sec. 1, the indegree measures the density near sample i, and the outdegree characterizes the K-NN similarity from vertex i to cluster C. We use the size of the cluster to normalize the degrees, otherwise, the algorithm may favor of merging large clusters instead of merging small clusters with dense connections. We find that in practice the normalized degrees work much better than the unnormalized degrees.</p><p>Affinity between a vertex and a cluster. A vertex should be merged to a cluster if it is strongly connected to the cluster by both inedges and outedges. Mathematically, the correlation of two types of degree is weak, if the vertex and the cluster belong to different ground-truth clusters, and strong, otherwise. To verify this intuition, we show such statistics on synthetic data in <ref type="figure" target="#fig_2">Fig. 3</ref>. Therefore, we define the affinity as the product of the average indegree and average outdegree, i.e.,</p><formula xml:id="formula_6">A i?C = deg ? i (C) deg + i (C).<label>(2)</label></formula><p>This affinity is robust to noisy edges between different ground-truth clusters because the product can be zero if the inedges and outedges do not coincide. Affinity between two clusters. Following the above, we define the asymmetric affinity from cluster C b to cluster C a by summing up with respect to all the vertices in C b , i.e.,</p><formula xml:id="formula_7">A C b ?Ca = i?C b A i?Ca = i?C b deg ? i (C a ) deg + i (C a ).<label>(3)</label></formula><p>Finally, we have the symmetric affinity used in our algorithm as</p><formula xml:id="formula_8">A Ca,C b = A C b ?Ca + A Ca?C b<label>(4)</label></formula><p>Efficient computation of affinity. Our affinity measure can be computed efficiently using the following theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 1</head><p>The affinity between C a and C b defined in Eq. (4) can be expressed in the matrix form</p><formula xml:id="formula_9">A Ca,C b = 1 |C a | 2 1 T |Ca| W Ca,C b W C b ,Ca 1 |Ca| + 1 |C b | 2 1 T |C b | W C b ,Ca W Ca,C b 1 |C b | ,<label>(5)</label></formula><p>Cluster C1</p><p>Cluster C2</p><p>Mean PNZ <ref type="figure" target="#fig_2">Fig. 3</ref>. To verify the robustness of the product of indegree and outdegree as an affinity measure from a vertex i to a cluster C, we compare statistics in two cases: i and C belong to different ground-truth clusters, e.g., i ? C1 and C = C2 as in (a), and i and C are in the same ground-truth cluster, e.g., i ? C1 and C = C1 as in (b). We see that, in the first case, the product is a quantity more robust than the indegree or outdegree. where W Ca,C b is the submatrix of W whose row indices correspond to the vertices in C a and column indices correspond to the vertices in C b , i.e., the weights of edges from C a to C b , and 1 L is an all-one vector of length L.</p><formula xml:id="formula_10">deg ? i (C) 0.73 0.85 deg + i (C) 0.94 0.54 deg ? i (C) deg + i (C) 0.47 0.38 Mean PNZ deg ? i (C) 0.87 1.00 deg + i (C) 0.87 1.00 deg ? i (C) deg + i (C) 0.86 1.00 Correlation deg ? i (C), deg + i (C) = 0.02 Correlation deg ? i (C), deg + i (C) = 0.80 Synthetic data (a) i ? C 1 , C = C 2 (b) i ? C 1 , C = C 1</formula><formula xml:id="formula_11">For all i ? C1, such that deg ? i &gt; 0 or deg + i &gt; 0,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 1</head><p>The computation is reduced to vector additions and inner-products. So, our algorithm is easy to implement.</p><p>Proof. It is easy to see that</p><formula xml:id="formula_12">deg ? i (C a ) = 1 |C a | 1 T |Ca| W Ca,C b i ,<label>(6)</label></formula><formula xml:id="formula_13">deg + i (C a ) = 1 |C a | W C b ,Ca 1 |Ca| i ,<label>(7)</label></formula><p>where [v] i is the i-th element of vector v. Then, by Eq. (3), we can obtain the following lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 2</head><formula xml:id="formula_14">A C b ?Ca = 1 |C a | 2 1 T |Ca| W Ca,C b W C b ,Ca 1 |Ca| .<label>(8)</label></formula><p>Finally, Theorem 1 can be directly implied by Lemma 2 using Eq. (4).</p><p>Comparison to average linkage. The GDL algorithm is different from average linkage in the following three aspects. First of all, the conventional average linkage is based on pairwise distances <ref type="bibr" target="#b0">[1]</ref>. Although we find that average linkage has much better performance on the K-NN graph than pairwise distances, we are unaware of any literature which studied the graph-based average linkage algorithm. Second, graph-based average linkage simply symmetrizes the directed graph by setting w ij = w ji = (w ij + w ji )/2, while our algorithm uses the directed graph. Third, graph-based average linkage can be interpreted as defining the affinity measure</p><formula xml:id="formula_15">A C b ?Ca = 1 |C b | i?C b [deg ? i (C a ) + deg + i (C a )</formula><p>]/2 using our indegree-outdegree framework. The sum of the indegree and outdegree is not as robust as the product of them to noise. Experimental results in <ref type="figure">Fig.  2</ref> and Sec. 4.1 demonstrate the superiority of GDL to graph-based average linkage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementations of GDL</head><p>We present two implementations of the GDL algorithm: an exact algorithm via an efficient update formula and an approximate algorithm called Accelerated GDL (AGDL). Both implementations have the time complexity of O(n 2 ) (see Theorem 3).</p><p>Update formula. In each iteration, we select two clusters C a and C b with the largest affinity and merge them as C ab = C a ? C b . Then, we need to update the asymmetric affinity A C ab ?Cc and A Cc?C ab , for any other cluster C c .</p><p>Using Lemma 2, we find that A C ab ?Cc can be computed as follows.</p><formula xml:id="formula_16">A C ab ?Cc = A Ca?Cc + A C b ?Cc .<label>(9)</label></formula><p>By storing all the asymmetric affinities, the update is simple.</p><p>As the same update formula cannot be applied to A Cc?C ab , we have to compute it directly using Eq. (8). However, the total complexity is O(n) in each iteration, due to the row sparsity of W (see Sec. 7 in the supplemental materials for details).</p><p>The GDL algorithm with the update formula (GDL-U) is presented as Algorithm 2 in the supplemental materials.</p><p>Accelerated GDL. Although the GDL-U algorithm is simple and fast, we further propose AGDL. The major computational cost is on computing the affinities. To reduce the number of affinities computed in each iteration, AGDL maintains a neighbor set of size K c for each cluster in V c , to approximate its K c -nearest cluster set. Then, finding the maximum affinity among all pairs of clusters can then be approximated by searching it in all the neighbor sets. Updating the neighbor sets involves computation of the affinity between the new cluster and a small set of clusters, instead of all the other clusters.</p><p>Denote the neighbor set of a cluster C as N C . Initially N C consists of C's K c -nearest clusters. Once two clusters C a and C b are merged, we need to update the neighbor sets which include C a or C b , and create the neighbor set of C a ? C b . We utilize two assumptions that (1) if C a or C b is among the K c -nearest clusters of C c , C a ? C b is probably among the K c -nearest clusters of C c ; (2) if C c is among the K c -nearest clusters of C a or C b , C c is probably among the K c -nearest clusters of C a ? C b . So, the new cluster C a ? C b is added to the neighbor sets which include C a or C b previously. To create the neighbor set for C a ? C b , we select the K c -nearest clusters from N Ca ? N C b .</p><p>The AGDL algorithm is summarized in Algorithm 3 in the supplemental materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Time Complexity Analysis</head><p>We have the following theorem about the time complexity of the GDL, GDL-U and AGDL algorithms (please refer to Sec. 7 in the supplemental materials for the proof). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we demonstrate the effectiveness of GDL and AGDL on image clustering and object matching. All the experiments are run in MATLAB on a PC with 3.20GHz CPU and 8G memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Image Clustering</head><p>We carry out experiments on six publicly available image benchmarks, including object image databases (COIL-20 and COIL-100), hand-written digit databases (MNIST and USPS), and facial image databases (Extended Yale-B, FRGC ver2.0). <ref type="bibr" target="#b2">3</ref> For MNIST, we use all the images in the testing set. For FRGC ver2.0, we use all the facial images in the training set of experiment 4. The statistics of all the datasets are presented in <ref type="table" target="#tab_2">Table  2</ref>. We adopt widely used features for different kinds of images: the intensities of pixels as features and Euclidean distance for object and digit images, and local binary patterns (LBP) as features and ? 2 distance for facial images. We compare the GDL-U and AGDL with eight representative algorithms, i.e., kmedoids (k-med) <ref type="bibr" target="#b0">[1]</ref>, average linkage (Link) <ref type="bibr" target="#b0">[1]</ref>, graph-based average linkage (G-Link), normalized cuts (NCuts) <ref type="bibr" target="#b1">[2]</ref>  <ref type="bibr" target="#b3">4</ref> , NJW spectral clustering (NJW-SC) <ref type="bibr" target="#b2">[3]</ref>, directed graph spectral clustering (DGSC) <ref type="bibr" target="#b7">[8]</ref>, self-tuning spectral clustering (STSC) <ref type="bibr" target="#b10">[11]</ref> and Zell <ref type="bibr" target="#b17">[18]</ref>. Here we use k-medoids instead of k-means because it can handle the case where distances between points are not measured by Euclidean distances. To fairly compare the graph-based algorithms, we fix K = 20 and select a with the best performance from the set {10 i , i ? [?2 : 0.5 : 2]} on all the datasets. For our algorithms, the parameters are fixed as K 0 = 1, K c = 10. The numbers of ground-truth clusters are used as the input of all algorithms (e.g., n T in our algorithm).</p><p>We adopt the widely used Normalized Mutual Information (NMI) <ref type="bibr" target="#b11">[12]</ref> to quantitatively evaluate the performance of clustering algorithms. The NMI quantifies the normalized statistical information shared between two distributions. A larger NMI value indicates a better clustering result.  The results measured in NMI are given in <ref type="table">Table 1</ref>. k-medoids and average linkage perform similar, as they heavily rely on the computation of pairwise distances and thus are sensitive to noise, and cannot well capture the complex cluster structures in the real data sets. NCuts, NJW-SC, and Zell have good performance on most data sets, as they capture the underlying manifold structures of the data. STSC works fine on some synthetic multiscale datasets in <ref type="bibr" target="#b10">[11]</ref> but its results are worse than ours on several real datasets in comparison. Note that STSC adaptively estimated the parameter ? 2 at every point to reflect the variation of local density while ours explores indgree/outdegree and fixes ? 2 as constant. The effective and robust affinity measure for agglomerative clustering makes our GDL-U and AGDL algorithm performs the best among all the algorithms. The AGDL's results are nearly the same as GDL-U.</p><p>Compared to other graph-based algorithms, GDL-U and AGDL are more robust to the parameter ? for building the graph, as well as the noise in the data (see <ref type="figure">Fig. 4</ref>). The noise added to images can degrade the performance of other algorithms greatly, but our performance is barely affected.</p><p>For the graph-based algorithms, we show their time cost in <ref type="table" target="#tab_2">Table 2</ref>. AGDL costs the least amount of time among all the algorithms. GDL is faster than NCuts, NJW-SC, and DGSC, and is much faster than Zell. G-Link, which has worse performance than AGDL, is comparable to AGDL on time cost. 1200 outliers in red color, detected, F-score 0.981). according to the ground truth); </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Feature Correspondence Clustering for Object Matching</head><p>We show the effectiveness of our clustering algorithm in the presence of outliers via feature correspondence clustering. Feature correspondence clustering is commonly used for robust object matching <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22]</ref>, which can deal with geometric distortions of objects across images and is a fundamental problem in computer vision. We demonstrate that our algorithm can be effectively integrated with the framework of feature correspondence clustering. Therefore, it has a range of potential applications, such as object recognition, image retrieval, and 3D reconstruction.</p><p>We compare with two recent state-of-the-art methods, i.e., agglomerative correspondence clustering (ACC) <ref type="bibr" target="#b13">[14]</ref> and graph shift (GS) <ref type="bibr" target="#b21">[22]</ref>. <ref type="bibr" target="#b4">5</ref> Overview of experiments. We follow the experiments in the ACC paper <ref type="bibr" target="#b13">[14]</ref>. We use composite images and their warped versions ( <ref type="figure" target="#fig_4">Fig. 5(a)</ref>) to simulate cluttered scenes where deformable objects appear. Then we can use the ground-truth for performance evaluation. Namely, we compute the precision and recall rates of detected correspondences ( <ref type="figure" target="#fig_4">Fig. 5(c)</ref>), given a set of correspondences with ground-truth ( <ref type="figure" target="#fig_4">Fig. 5(b)</ref>). A good clustering algorithm can group inliers and separate outliers. It is a more direct way of evaluating the performance of clustering algorithms than other experiments, such as object recognition. Experimental settings. We generate a pair of 3 ? 3 tiled images that contain M common sub-image(s). The common sub-images are randomly selected from the model images of the ETHZ toys dataset 6 , and the non-common sub-images are from test images of the same dataset. The positions of all sub-images are randomly determined. When M &gt; 1, the common sub-images are chosen as different objects. To simulate deformation, one of the paired images is warped using the thin-plate spline (TPS) model. An example of paired test images are shown in <ref type="figure" target="#fig_4">Fig. 5(a)</ref>. 9 ? 9 crossing points from a 10 ? 10 meshgrid on the image are chosen as the control points of the TPS model. Then, all the control points are perturbed by Gaussian noise of N (0, ? 2 n ) independently, and the TPS warping is applied based on the perturbations of control points. To obtain the candidate correspondences between two tiled images, features are extracted by the MSER detector, and the best 3, 000 correspondences are collected according to similarity of the SIFT descriptors. Using the warping model, each correspondence has a ground-truth label: true if its error is smaller than three pixels, and false otherwise. <ref type="figure" target="#fig_4">Fig.  5(b)</ref> shows the correspondences as lines, among which the yellow ones represent true correspondences. Then, the performance of different algorithms are quantitatively eval-uated. We use the F-score, a traditional statistical measure of accuracy, which is defined as [precision ? recall/(precision + recall)].</p><p>Parameters of ACC and GS. As we strictly follow the test protocol in the ACC paper <ref type="bibr" target="#b13">[14]</ref>, we use the default parameters in their codes. For GS, we compute the affinity matrix W ij = max(? ? d ij /? 2 s , 0) as the paper <ref type="bibr" target="#b21">[22]</ref>, where d ij is the distance between correspondence i and correspondence j as defined in the ACC paper <ref type="bibr" target="#b13">[14]</ref>. ?, ? s and other parameters in GS are tuned to be the best.</p><p>Parameters of AGDL. For our AGDL algorithm (i.e., Algorithm 3), the parameters are fixed as n T = 50, a = 10, K = 35, K 0 = 2, and K c = 10. We found that the GDL works well in a large range of n T , as the number of ground-truth clusters (i.e., M ) is very small and we can eliminate the outlier clusters by postprocessing. <ref type="bibr" target="#b6">7</ref> Results. As shown in <ref type="figure">Fig. 6</ref>, we vary the number of outliers, the level of deformation, and the number of common sub-images, and compare the F-scores of detected correspondences by different algorithms. Both ACC and GS perform excellently on this task. It is challenging to beat them, which are very recent methods designed specifically for object matching. However, our simple clustering algorithm outperforms them. We find our AGDL algorithm performs consistently better than both ACC and GS under different settings. AGDL has a higher F-score than both in 95.6% of the random trials under all the setting combinations. We attribute the success of AGDL to the effective cluster affinity measure which is robust to noise and outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present a fast and effective method for agglomerative clustering on a directed graph. Our algorithm is based on indegree and outdegree, fundamental concepts in graph theory. The indegree and outdegree have been widely studied in complex networks, but have not received much attention in clustering. We analyze their roles in modeling the structures of data, and show their power via the proposed graph degree linkage algorithm. We demonstrated the superiority of this simple algorithm on image clustering and object matching. We believe our work provides not only a simple and powerful clustering algorithm to many applications in computer vision, but also an insightful analysis of the graph representation of data via indegree and outdegree.</p><p>6 Implementations of GDL Algorithm 2 Graph Degree Linkage with the update formula (GDL-U) Input: a set of n samples X = {x1, x2, ? ? ? , xn}, and the target number of clusters nT . Build the K 0 -NN graph, and detect its weakly connected components as initial clusters. Denote the set of initial clusters as V c = {C1, ? ? ? , Cn c }, where nc is the number of clusters. Build the K-NN graph, and get the weighted adjacency matrix W. Initialize the asymmetric affinity table AC a ?C b for Ca, C b ? V c . while nc &gt; nT do Search two clusters Ca and C b , such that {Ca, </p><formula xml:id="formula_17">C b } = argmax Ca,C b ?V c AC a,Cb ; V c ? {V c \ {Ca, C b }} ? {C ab }, where C ab = Ca ? C b ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Accelerated Graph Degree Linkage (AGDL)</head><p>Input: a set of n sample vectors X = {x1, x2, ? ? ? , xn}, and the target number of clusters nT . Build the K 0 -NN graph, and detect its weakly connected components as initial clusters. Denote the set of initial clusters as V c = {C1, ? ? ? , Cn c }, where nc is the number of clusters. Build the K-NN graph, and get the weighted adjacency matrix W. Create a neighbor set for each cluster in V c , and initialize it as the K c -nearest cluster set. while nc &gt; nT do Search two clusters Ca and C b from the affinity of pairs of clusters associated with the neighbor sets, such that {Ca, </p><formula xml:id="formula_18">C b } = argmax Ca?N C b or C b ?N Ca AC a,Cb ; V c ? {V c \ {Ca, C b }} ? {C ab }, where C ab = Ca ? C b ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Proof of Theorem 3</head><p>Proof. For (a), we analyze the time complexity for each part of the GDL algorithm.</p><p>(1) The directed graph construction has a complexity of at most O(Kn 2 ) (naive implementation). Note that n ? K, and thus we omit K in the complexities hereinafter.</p><p>(2) The complexity of constructing initial clusters is O(n), as the number of edges in the graph G is O(K 0 n), where K 0 is 1 or 2. (3) In our clustering algorithm, we use an n c ? n c table to store the affinities between clusters. As the initial clusters are of small sizes, we can assume O(1) complexity for computing the affinity between each pair of two clusters. So, it requires a complexity of O(n 2 0 ) to initialize the table, where n 0 is the number of initial clusters (n 0 &lt; n). (4) In each iteration, it costs O(n 2 c ) to find the maximum value in the cluster affinity table. To update the cluster affinity table after merging the two clusters with maximum affinity value, we need to compute (n c ? 1) affinities, and each affinity is computed with complexity of O(|C a | + |C b |) using Eq. (5) (because W is a sparse matrix with K nonzero elements in each row). Therefore, the complexity for each iteration is at most O(n c n). For (b), we can reduce the complexity in each iteration from O(n c n) in (a) to O(n). We can maintain a table to store the nearest cluster of each cluster. <ref type="bibr" target="#b7">8</ref> In each iteration, finding the maximum value and updating the table cost approximately O(n c ). For the affinity table, the updating scheme of A C ab ?Cc as in Eq. 9 costs O(n c ) for all the new affinities. To compute A Cc?C ab , the total complexity for all the new affinities is less than the complexity of computing W C ab , * W * ,C ab , which is O(nK), as W C ab , * is Ksparse in each row. W C ab , * is the submatrix of W whose row indices correspond to the vertices in C ab and column indices are from 1 to n.</p><p>Finally, the total complexity for GDL-U is O(n 2 ).</p><p>For (c), there are several differences in the AGDL:</p><p>-In (3), we use the neighbor sets of clusters instead of the cluster affinity table. The construction of all the neighbor sets costs O(K c n 2 0 ). -In (4), we need to find the maximum affinity value in the neighbor sets (with complexity of O(K c n c ) and compute O(K c (1 + ? )) affinities to update the neighbor sets with complexity of O(K c n)). Because the size of the union of neighbor sets of C a and C b is less than 2K c ), and for real data, we can assume that the number of clusters whose neighbor set includes C a or C b is less than 2? K c , where ? is usually a small constant close to 1. Therefore, the complexity for each iteration is at most O(n).</p><p>So, the time complexity of the AGDL algorithm is O(n 2 ).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Quantitative Results in Clustering Error for Image Clustering</head><p>The quantitative results, measured in CE <ref type="bibr" target="#b11">[12]</ref>, are given in <ref type="table">Table 3</ref>. The CE is defined as the minimum overall error rate among all possible permutation mappings between true class labels and clusters. A smaller CE value indicates a better clustering result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Outlier Elimination for Object Matching</head><p>For AGDL, we observe that there are many inedges and outedges inside a cluster of inliers, while less edges inside a cluster of outliers because outliers are in low density regions. Inspired by this, we define the connectivity score of a cluster C as i?C deg ? i (C) + deg + i (C) . We find that there are always large differences between the scores of inlier clusters and outlier clusters (see <ref type="figure">Fig. 7</ref>). Therefore, we rank the final clusters by their connectivity scores. Namely, we sort their scores in descending order, and then search the largest gap between two consecutive scores. The set of clusters are divided into two subsets without intersection. The subset of clusters with small scores is treated as the collection of outliers and removed. For ACC and GS, we use their default methods for outlier elimination.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 1. (a) Indegree can be use to detect the change of densities. The density in Cluster a is high, and the density in Cluster b is low. The vertices inside Cluster a are strongly connected, but there is no outedge to vertices outside Cluster a. So the indegree of k from Cluster a is nonzero, while the indegree of i (a vertex in Cluster b) and j (an outlier) from Cluster a are zero. If an undirected graph is considered without separating indegrees and outdegrees, both i and j have the same degree from Cluster a as k. (b) The product of the indegree and outdegree is an affinity measure robust to noisy edges between the two clusters. Under this measure, Cluster a and Cluster b have a zero affinity, i.e., the sum of product of indegree and outdegree for all vertices is 0, and thus they are separated well.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>the mean and proportion of nonzero values (PNZ) of deg ? i deg + i are much smaller than those of deg ? i and deg + i , which implies a small affinity between i and C. Here the square root is for fair comparison of the quantities. In contrast, in the second case, the mean and PNZ of deg ? i deg + i are close to those of deg ? i and deg + i , which means that the product keeps the large affinity well. The correlation of deg ? i and deg + i , which is weak in (a) and strong in (b), further verifies the effectiveness of our affinity measure for reducing noisy edges across ground-truth clusters and keeping edges inside ground-truth clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Theorem 3 (</head><label>3</label><figDesc>a) The time complexity of the GDL algorithm (i.e., Algorithm 1) is O(n 3 ). (b) The time complexity of the GDL-U algorithm (i.e., Algorithm 2) is O(n 2 ). (c) The time complexity of the AGDL algorithm (i.e., Algorithm 3) is O(n 2 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 .Fig. 4 .</head><label>14</label><figDesc>Quantitative clustering results in NMI on real imagery data. A larger NMI value indicates a better clustering result. The results shown in a boldface are significantly better than the others, with a significance level of 0.01. Dataset k-med Link G-Link NCuts NJW-SC DGSC STSC Zell GDL-U AGDL COIL-20 0.710 0.647 0.896 0.884 0.889 0.904 0.895 0.911 0.937 0.937 COIL-100 0.706 0.606 0.855 0.823 0.854 0.858 0.858 0.913 0Variations of performance of different clustering algorithms on the COIL-20 dataset, (a) when the parameter a for controlling ? in Eq. (1) changes; (b) when we add Gaussian noise N (0, ? 2 n ) to the images. The NMI differences between ?n = 0 and ?n = 160 are 0.048, 0.065, 0.067, 0.012, for G-Link, NJW-SC, DGSC, and AGDL, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Example of object matching through feature correspondence clustering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4 Fig. 6 .</head><label>46</label><figDesc>Performance comparison of different algorithms. In each sub-figure, one of the three factors, i.e., the number of outliers, the level of deformation ?n, and the number of common subimages M , is varied, while the other two are fixed as the values appearing at the top. All the results are averaged over 30 random trials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( 5 )</head><label>5</label><figDesc>The number of iterations is (n 0 ? n T ).By replacing n 0 and n c with their upper bound n, a loose upper bound of the time complexity of the GDL algorithm is O(n 3 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>3 .Fig. 7 .</head><label>37</label><figDesc>Quantitative clustering results in CE on real imagery data. A smaller CE value indicates a better clustering result. The results shown in a boldface are significantly better than the others, with a significance level of 0.01. Dataset k-med Link G-Link NCuts NJW-SC DGSC STSC Zell GDL-U AGDL COIL-20 0.401 0.677 0.213 0.246 0.228 0.201 0.158 0.187 0.142 0.142 COIL-100 0.570 0.819 0.394 0.462 0.411 0.396 0.391 0.351 0The connectivity scores of clusters sorted in descending order. The threshold for separating inliers and outliers is shown in a red dash line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The time cost (in seconds) of the algorithms. The minimum time cost on each dataset is in bold. The statistics of each dataset are shown for reference.</figDesc><table><row><cell>Dataset</cell><cell cols="4">Sample Num Cluster Num NCuts NJW-SC DGSC Zell GDL-U AGDL</cell></row><row><cell>COIL-20</cell><cell>1440</cell><cell>20</cell><cell cols="2">3.880 6.399 8.280 15.22 0.265 0.277</cell></row><row><cell>COIL-100</cell><cell>7200</cell><cell>100</cell><cell cols="2">133.8 239.7 326.4 432.9 12.81 5.530</cell></row><row><cell>USPS</cell><cell>11000</cell><cell>10</cell><cell cols="2">263.0 461.6 538.9 9703 53.64 29.01</cell></row><row><cell>MNIST</cell><cell>10000</cell><cell>10</cell><cell cols="2">247.2 384.4 460.4 64003 35.60 17.18</cell></row><row><cell>Yale-B</cell><cell>2414</cell><cell>38</cell><cell cols="2">9.412 13.99 16.00 178.2 0.731 0.564</cell></row><row><cell>FRGC</cell><cell>12776</cell><cell>222</cell><cell cols="2">577.4 914.3 1012.2 65021 49.15 18.62</cell></row><row><cell cols="4">(a) A pair of composite images, (b) Initial correspondences</cell><cell>(c) Detected correspondences</cell></row><row><cell cols="2">one of which is warped with</cell><cell cols="3">(533 inliers in yellow color, by AGDL (532 true, 552</cell></row><row><cell>?n = 50;</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>and nc = nc ? 1; For all Cc, compute AC ab ?Cc using the update formula, i.e., Eq. (9), and AC c ?C ab using Eq.<ref type="bibr" target="#b7">(8)</ref>.</figDesc><table><row><cell>end while</cell></row><row><cell>Output: V c .</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>and nc = nc ? 1; For all Cc, such that Ca ? NC c or C b ? NC c , add C ab to NC c , and compute the affinity AC ab ,Cc ; Find the K c -nearest clusters for C ab in the set NC a ? NC b , to form NC ab ; Remove Ca and C b from the neighbor sets, and remove NC a and NC b .</figDesc><table><row><cell>end while</cell></row><row><cell>Output: V c .</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">E.g., if cluster a has higher density than cluster b, the boundary of cluster a will have high indegree and low outdegree, while the boundary of cluster b will have low indegree and high outdegree.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">COIL-20 and COIL-100 are from http://www.cs.columbia.edu/CAVE/software/. MNIST and USPS are from http://www.cs.nyu.edu/?roweis/data.html. Extended Yale-B is from http://vision.ucsd.edu/?leekc/ExtYaleDatabase/ExtYaleB FRGC ver2.0 is from http://face.nist.gov/frgc/.<ref type="bibr" target="#b3">4</ref> The code is downloaded from http://www.cis.upenn.edu/?jshi/software/, which implements the multiclass normalized cuts algorithm<ref type="bibr" target="#b19">[20]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The code of ACC and GS are downloaded from http://cv.snu.ac.kr/research/?acc/, and http://sites.google.com/site/lhrbss/, respectively. We do not present the results of spectral matching (SM)<ref type="bibr" target="#b20">[21]</ref>, because both ACC and GS outperformed SM greatly<ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22]</ref>, especially when there existed at least two clusters of correspondences according to the ground-truth.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">http://www.vision.ee.ethz.ch/?calvin/datasets.html.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Please see Sec. 9 in the supplemental materials for details of outlier elimination in our algorithm. Different from ACC, which utilizes additional information, i.e., geometrical locations of feature points, we only use the K-NN graph in outlier elimination.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We can use a heap to achieve better efficiency for this part. But it is not the bottleneck for both the complexity analysis and run-time of GDL-U.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work is partially supported by the General Research Fund sponsored by the Research Grants Council of Hong Kong (Project No. CUHK416510, CUHK417110 and CUHK417011) and National Natural Science Foundation of China (Project No.61005057). It is also supported through Introduced Innovative R&amp;D Team of Guangdong Province 201001D0104648280 and Shenzhen Key Lab of Computer Vision and Pattern Recognition. The authors would like to thank Tianfan Xue for proof reading and Wei Li for help on the ACC code.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The elements of statistical learning: Data mining, inference, and prediction. second edn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Isoperimetric graph partitioning for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="469" to="475" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning semi-Riemannian metrics for semisupervised feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="611" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Clustering by passing messages between data points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dueck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">315</biblScope>
			<biblScope unit="issue">5814</biblScope>
			<biblScope unit="page" from="972" to="976" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning from labeled and unlabeled data on a directed graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<editor>ICML.</editor>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="604" to="632" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Measurement and analysis of online social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mislove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marcon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gummadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Druschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM Conf. on Internet Measurement</title>
		<meeting>ACM SIGCOMM Conf. on Internet Measurement</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-tuning spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A local learning approach for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast agglomerative clustering using a k-nearest neighbor graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Franti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Virmajoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hautamaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1875" to="1881" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Feature correspondence and deformable object matching via agglomerative correspondence clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Density-based clustering in spatial databases: The algorithm GDBSCAN and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="169" to="194" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Finding clusters of different sizes, shapes, and densities in noisy, high dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ert?z</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM International Conf. on data mining</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Chameleon: Hierarchical clustering using dynamic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="68" to="75" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cyclizing clusters via zeta function of a graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient graph-based image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multiclass spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A spectral technique for correspondence problems using pairwise constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Common visual pattern discovery via spatially coherent correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
