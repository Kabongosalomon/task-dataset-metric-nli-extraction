<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Universal Adversarial Perturbations with Generative Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Hayes</surname></persName>
							<email>j.hayes@cs.ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Danezis</surname></persName>
							<email>g.danezis@ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Universal Adversarial Perturbations with Generative Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural networks are known to be vulnerable to adversarial examples, inputs that have been intentionally perturbed to remain visually similar to the source input, but cause a misclassification. It was recently shown that given a dataset and classifier, there exists so called universal adversarial perturbations, a single perturbation that causes a misclassification when applied to any input. In this work, we introduce universal adversarial networks, a generative network that is capable of fooling a target classifier when it's generated output is added to a clean sample from a dataset. We show that this technique improves on known universal adversarial attacks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Neural networks are known to be vulnerable to adversarial examples, inputs that have been intentionally perturbed to remain visually similar to the source input, but cause a misclassification. It was recently shown that given a dataset and classifier, there exists so called universal adversarial perturbations, a single perturbation that causes a misclassification when applied to any input. In this work, we introduce universal adversarial networks, a generative network that is capable of fooling a target classifier when it's generated output is added to a clean sample from a dataset. We show that this technique improves on known universal adversarial attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I . I N T R O D U C T I O N</head><p>Machine Learning models are increasingly relied upon for safety and business critical tasks such as in medicine <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b41">[41]</ref>, robotics and automotive <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b40">[40]</ref>, security <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b38">[38]</ref> and financial <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b36">[36]</ref> applications. Recent research shows that machine learning models trained on entirely uncorrupted data, are still vulnerable to adversarial examples <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b37">[37]</ref>: samples that have been maliciously altered so as to be misclassified by a target model while appearing unaltered to the human eye.</p><p>Most work has focused on generating perturbations that cause a specific input to be misclassified, however, it has been shown that adversarial perturbations generalize across many inputs <ref type="bibr" target="#b35">[35]</ref>. Moosavi-Dezfooli et al. <ref type="bibr" target="#b18">[19]</ref> showed, in the most extreme case, that given a target model and a dataset, it is possible to construct a single perturbation that when applied to any input, will cause a misclassification with high likelihood.</p><p>These are referred to as universal adversarial perturbations (UAPs).</p><p>In this work, we study the capacity for generative models to learn to craft UAPs on image datasets, we refer to these networks as universal adversarial networks (UANs). We show that a UAN is able to sample from noise and generate a perturbation such that when applied to any input from the dataset, it will result in a misclassification in the target model. Furthermore, we show perturbations produced by UANs: improve on state-ofthe-art methods for crafting UAPs (Section IV-A), have robust transferable properties (Section IV-D), and reduce the success of recently proposed defenses <ref type="bibr" target="#b0">[1]</ref> (Section V).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I I . B A C K G R O U N D</head><p>We define adversarial examples and UAPs along with some terminology and notation. We then introduce the threat model considered, and the datasets we use to evaluate the attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Adversarial Examples</head><p>Szegedy et al. <ref type="bibr" target="#b35">[35]</ref> casts the construction of adversarial examples as an optimization problem. Given a target model, f , and a source input x, which is classified correctly by f as c, the attacker aims to find a perturbation, ?, such that x + ? is perceptually identical to x but f (x + ?) = c. The attacker tries to minimize the distance between the source image and adversarial image under an appropriate measure. The problem space can be framed to find a specific misclassification in a targeted attack, or any misclassification, referred to as a non-targeted attack.</p><p>In the absence of a distance measure that accurately captures the perceptual differences between a source and adversarial image, the p metric is usually minimized <ref type="bibr" target="#b35">[35]</ref>. Related work commonly uses the 2 and ? metrics <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b42">[42]</ref>. The 2 metric measures the Euclidean distance between two images, while the ? metric measures the largest pixel-wise difference between two images (Chebyshev distance). We follow this practice here and construct attacks optimizing under both metrics.</p><p>A UAP is an adversarial perturbation that is independent of the source image. Given a target model, f , and a dataset, X, a UAP is a perturbation, ?, such that ?x ? X, x + ? is a valid input and Pr(f (x + ?) = f (x)) = 1 ? ? , where 0 &lt; ? &lt;&lt; 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Threat Model</head><p>We consider an attacker whose goal is to craft UAPs against a target model, f . The adversarial image constructed by the attacker should be visually indistinguishable to a source image, evaluated through either the 2 or ? metric.</p><p>Our attacks assume white-box access to f , as we backpropagate the error of the target model back to the UAN. In line with related work on UAPs <ref type="bibr" target="#b18">[19]</ref>, we consider a worst-case scenario with respect to data access, assuming that the attacker has knowledge of, and shares access to, any training data samples. We will not discuss the real-world limitations of that assumption here, but will follow that practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Datasets</head><p>We evaluate attacks using two popular datasets in adversarial examples research, CIFAR-10 <ref type="bibr" target="#b14">[15]</ref> and ImageNet <ref type="bibr" target="#b28">[29]</ref>.</p><p>The CIFAR-10 dataset consists of 60,000, 32?32 RGB images of different objects in ten classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. This is split into 50,000 training images and 10,000 validation images. <ref type="figure">Fig. 1</ref>: Overview of the attack. A random sample from a normal distribution is fed into a UAN. This outputs a perturbation, which is then scaled and added to an image. The new image is then clipped and fed into the target model.</p><p>Our pre-trained models: VGG-19 <ref type="bibr" target="#b31">[31]</ref>, ResNet-101 <ref type="bibr" target="#b8">[9]</ref>, and DenseNet <ref type="bibr" target="#b10">[11]</ref>, used as the target models, score 91.19%, 93.75%, and 95.00% test accuracy, respectively. State-of-the-art models on CIFAR-10 are approximately 95% accurate.</p><p>We use the validation dataset of ImageNet, which consists of 50,000 RGB images, scaled to 224?224. The images contain 1,000 classes. The 50,000 images are split into 40,000 training set images and 10,000 validation set images. We ensure classes are balanced, such that any class contains 40 images in the training set and 10 images in the validation set. Our pre-trained models: VGG-19 <ref type="bibr" target="#b31">[31]</ref>, ResNet-152 <ref type="bibr" target="#b8">[9]</ref>, and Inception-V3 <ref type="bibr" target="#b34">[34]</ref>, used as the target models, score 71.03%, 78.40%, and 77.22% top-1 test accuracy, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I I I . U N I V E R S A L A D V E R S A R I A L N E T W O R K S</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Attack Description</head><p>An overview of the attack is given in <ref type="figure">Figure 1</ref>. Let a UAN model be denoted by U, and a target model by f . U takes as input a vector, z, sampled from a normal distribution N (0, 1) 100 , and outputs a perturbation, ?. This is then scaled by a factor ? ? (0, ? p ], where is the maximum permitted perturbation and p = 2 or ?. In practice, we start with a small ? (e.g. ? = 10? ? p ) and increment this value whenever the training loss plateaus. The scaled perturbation ? = ? ? ?, is added to an image x from a dataset X, to produce an adversarial image. This is then clipped into the target model's input range before being fed into the target model, f , which outputs a probability vector, ? 1 . If arg max i f (x) = arg max i f (? + x), a successful adversarial example has been found. Since U(z) is not conditioned on any image in the dataset, U learns how to construct image independent adversarial perturbations, namely universal adversarial perturbations.</p><p>Given an input x ? X, let the class label predicted by f be c 0 . For non-targeted attacks, any misclassification in the target model suffices, thus, the non-targeted attack aims to maximize the most probable predicted class other than c 0 . Our non-targeted loss function is adapted from works by Carlini and Wagner <ref type="bibr" target="#b3">[4]</ref> and Chen et al. <ref type="bibr" target="#b4">[5]</ref>, and is given by:</p><formula xml:id="formula_0">Lnt = log[f (? + x)]c 0 ? max i =c 0 log[f (? + x)]i L f s + ? ? ? p L dist<label>(1)</label></formula><p>1 If f outputs logits instead of a probability vector, we take the softmax of the logits.</p><p>The first term in (1), L f s , is minimized when the adversarial predicted class is not c 0 . This is adapted from the Carlini and Wagner loss function <ref type="bibr" target="#b3">[4]</ref> that introduces a confidence threshold, ?. If we want universal adversarial perturbations that cause misclassifications with high confidence, we stop minimizing only when:</p><formula xml:id="formula_1">? &gt; max i =c 0 log[f (? + x)]i ? log[f (? + x)]c 0</formula><p>In specifying a confidence threshold for adversarial examples, (1) becomes:</p><formula xml:id="formula_2">Lnt = max{log[f (? + x)]c 0 ? max i =c 0 log[f (? + x)]i, ??} + ? ? ? p<label>(2)</label></formula><p>In all experiments we set ? = 0, and so stop optimizing once an adversarial example is found. To minimize the perturbation applied to an image, L f s is summed with a distance loss, L dist = ? ? ? p , where ? ? R + ; this minimizes the norm of the universal adversarial perturbation. The logarithmic term in L f s is necessary since most target models have a skewed probability distribution, with one class prediction dominating all others, thus the logarithmic term reduces the effect of this dominance.</p><p>For a targeted attack, we compute a universal adversarial perturbation that transforms any image to a chosen class, c. Under this setting, we optimize using the follow loss function:</p><formula xml:id="formula_3">Lt = max{max i =c log[f (? + x)]i ? log[f (? + x)]c, ??} + ? ? ? p ,<label>(3)</label></formula><p>The full description of the UAN model is given in <ref type="table" target="#tab_0">Table I</ref> and hyperparameters used in experiments are given in <ref type="table" target="#tab_0">Table II.</ref> We define the relative perturbation, ? p = ? p x p ; the value of the norm of ? over the norm of the original image, x. We set ? p = 0.04 in all experiments <ref type="bibr">2 3</ref> . For all experiments in Section IV, we report the error rate of the target model on adversarial images; a perfect attack would achieve an error rate of 1.00, while a perfect classifier achieves an error rate of 0.00.  </p><formula xml:id="formula_4">L = ? log( K i=1l i(?)), such that ||?||? &lt; ?,</formula><p>where,l i (?) is the average of the output at layer i for perturbation ?, and ? is the maximum permitted perturbation.</p><p>Table III compares our UAN method of generating UAPs against the two attacks described above for both CIFAR-10 and ImageNet, in a non-targeted attack setting. We consistently outperform both attack methods. UAPs for the ImageNet and CIFAR-10 datasets are given in <ref type="figure">Figure 2</ref> and <ref type="figure">Figure 3</ref>, respectively. A selection of adversarial images for the ImageNet dataset is given in <ref type="figure">Figure 9</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Transferability</head><p>An adversarial image is transferable if it successfully fools a model that was not its original target. Transferability is a yardstick for the robustness of adversarial examples, and is the main property used by Papernot et al. <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> to construct black-box adversarial examples. They construct a white-box attack on a local target model that has been trained to replicate the intended target models decision boundaries, and show that the adversarial examples can successfully transfer to fool the black-box target model.</p><p>To measure the transferability properties of perturbations crafted by a UAN, we create 10,000 adversarial images (constructed via the ? metric) -one for each image in the CIFAR-10 validation set -and apply them to a target model that was not used to train the UAN. <ref type="table" target="#tab_0">Table IV</ref> presents results for transferability of a non-targeted attack on three target models -VGG-19, ResNet-101, and DenseNet. We find that UAPs crafted using a UAN do transfer to other models. For example, a UAN trained on VGG-19, and evaluated on ResNet-101, the error rate is 61.2%, a drop of just 5.4% from evaluating on the original target model .</p><p>We also measure the capacity for a UAN to learn to fool an ensemble of target models. We trained a UAN against VGG-19, ResNet-101, and DenseNet, simultaneously, on CIFAR-10, where the UAN loss function is a linear combination of the losses of each target model. From <ref type="table" target="#tab_0">Table IV</ref>, we see that a UAN trained against an ensemble of target models is able to fool at comparable rates to single target models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Generalizability</head><p>Moosavi-Dezfooli et al. <ref type="bibr" target="#b18">[19]</ref> have shown that UAPs are not unique; there exists many candidates that perform equally well against a target model. If a UAN is truly modeling the distribution of UAPs the output should not be unique. In <ref type="figure" target="#fig_2">Figure 5</ref>, we measure the MSE (mean square error) and SSIM (structural similarity index) <ref type="bibr" target="#b39">[39]</ref> of U(z 1 ), U(z 2 ) for  <ref type="bibr" target="#b18">[19]</ref> and Mopuri et al. <ref type="bibr" target="#b20">[21]</ref>. Note that the Mopuri et al. <ref type="bibr" target="#b20">[21]</ref> method for crafting UAPs is only optimized under the ? metric. We set ? p = 0.04, this is equivalent to = 2000 for an 2 attack and = 10 for an ? attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric</head><p>Attack CIFAR-10   z 1 , z 2 ? N (0, 1) 100 , z 1 = z 2 , at successive training steps, for the ImageNet dataset. Since we expect a high degree of structure in a UAP, SSIM is measured in addition to MSE, as it has been argued that MSE does not map well to a human's perception of image structure <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b39">[39]</ref>. At the beginning of training, there is litle structural similarity between U(z 1 ) and U(z 2 ). Throughout training the SSIM score never increases beyond 0.8, while the MSE continually increases. While the structural similary of UAPs learned by a UAN is high, it does learn to generalize to multiple UAPs that are unique from one another. Similar effects, albeit scaled down due to the smaller image size, are found for the CIFAR-10 dataset in <ref type="figure">Figure 10</ref>.</p><formula xml:id="formula_5">ImageNet V G G -1 9 R E S N E T -1 0 1 D E N S E N E T V G G -1 9 R E S N E T -1 5 2 I N C E P T I O N -V 3 UAN</formula><p>Does a UAN that learns to generalize to multiple UAPs do so to the detriment of attack accuracy? We verify this is not the case by training a UAN on a fixed noise vector and comparing to a UAN trained with non-fixed noise vectors. We found similar error rates for the two settings (see <ref type="table" target="#tab_5">Table V)</ref>; there is no loss in accuracy by extending a UAN to output multiple adversarial perturbations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Targeted Attacks</head><p>We follow the same experimental set-up as in Section IV-A, however now the attacker chooses a class, c, they would like the target model to classify an adversarial example as, and success is calculated as the probability that an adversarial example is classified as c. <ref type="figure" target="#fig_1">Figure 4</ref> shows, for each class in CIFAR-10, the error rate of the target model as we allow larger perturbations. For nearly every class, attacks on ResNet-101 are most successful, while attacks on VGG-19 are least successful. This is in agreement with our findings in a non-targeted attack setting (cf <ref type="table" target="#tab_0">. Table III</ref>). Despite VGG-19 being the most difficult target model to attack, it is the most well calibrated; the error rate on the training set is nearly identical to the error rate on the validation set for all classes, while there are small deviations between these two scores for ResNet-101 and DenseNet.</p><p>By looking only at results on VGG-19, one may infer that the choice of target class heavily influences the error rate (e.g. crafting UAP's for the dog and ship classes is more difficult than others). However, this is not replicated with ResNet-101 or DenseNet. We do not observe any dependencies between attack success and the target class; the attack success at different perturbation rates is similar for all classes. <ref type="figure" target="#fig_3">Figure 6</ref> shows this attack applied to a DenseNet target model for the CIFAR-10 dataset for all source/target class pairs. Nearly all attacks are indistinguishable from the source image. Similar results are found in <ref type="figure">Figure 11</ref> and <ref type="figure">Figure 12</ref> for VGG-19 and ResNet-101 target models, respectively.</p><p>Interestingly, all targeted attacks follow a sigmoidal curve shape. Empirically, we found that for all three target models, there existed images that were weakly classified correctly (there was almost no difference between the largest probability score and probability score at the target class) and strongly classified correctly (there was three to four orders of magnitude difference between the probability score at the largest class and the probability score at the target class). At the beginning of training, the UAN discovers a perturbation that causes misclassifications when applied to the weakly classified images, but takes longer to find adversarial perturbations for the majority of images, resulting in a long tail at the beginning of training. With a similar effect taking place at the end of training to find adversarial perturbations for strongly classified images. For the ImageNet dataset, we selected three classes at random and performed a targeted attack. Error rates and selected samples are given in <ref type="figure" target="#fig_2">Figures 13 to 15</ref>. We observed that the generated UAPs resembled the structure of the target class. For example, a golf ball pattern can be clearly seen in perturbations in <ref type="figure">Figure 13</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Importance of training set size</head><p>So far, we have assumed the attacker shares full access to any images that were used to train the target model. However in practice, this may not be the case -an attacker may only have access to the type or a subsample of the training data. We therefore evaluate our non-targeted ? attack under stronger assumptions of attacker access to training data. <ref type="figure" target="#fig_4">Figure 7</ref> shows the error rate caused by a UAN trained on subsets of the CIFAR-10 training set. As expected, training on more data samples improves the success of the attack; perturbations from a UAN trained on only 50 images (5 from each class) fools 17.1% of validation set images in ResNet-101. The attack is successful when applied to nearly a fifth of images while only learning from 0.1% of the training set. The attack succeeds in 80.2% of cases when trained on 20% of the training set -in other words, there is virtually no difference in test accuracy when training on between 80-100% of the training set.  We find no significant difference in error rates between a UAN that has been trained on many data samples and few data samples. The amount of data samples provided to the UAN does not significantly impact its ability to learn to craft adversarial perturbations, all that must be known is the structure of the dataset on which the target model was trained. We note that this is in agreement with Papernot et al.'s <ref type="bibr" target="#b23">[24]</ref> findings on the number of source images required to launch attacks on black-box models.</p><p>In addition to measuring attacker success for different training set sizes, we experimented with different batch sizes, ranging from 16 to 128, for the CIFAR-10 dataset. However, we did not observe any significant deviations in the error rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. AT T A C K I N G A D V E R S A R I A L T R A I N I N G</head><p>Adversarial training <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b15">[16]</ref> modifies the training of a model in order to make it more robust to adversarial examples. During training, the loss function L(?, x, y) is replaced by ? ? L(?, x, y) + (1 ? ?) ? L(?, x + ? , y). By augmenting the original data to include adversarial counterparts, the model learns to classify adversarial examples correctly. Non-generative attacks have shown to be successful against adversarially trained models, however, recent work <ref type="bibr" target="#b0">[1]</ref> suggested that this may not be the case for UAPs. In <ref type="bibr" target="#b0">[1]</ref>, adversarial training is successfully applied to a CIFAR-10 classifier, effectively eliminating the adversarial effect of UAPs.</p><p>In our work, we verified that this is case; adversarial training eliminates UAP success. However, we find that adversarially trained models are still vulnerable to UAN trained against the defended model.</p><p>Similarly to Hamm <ref type="bibr" target="#b7">[8]</ref>, we play a cat-and-mouse game where (1) a UAN is trained against a target model, and (2) the target model is retrained with adversial examples crafted from (1) (denoted ADV TM). This generates a sequence: UAN1 ? ADV TM1 ? UAN2 ? ADV TM2 ? UAN3 ? .... We let this game play out for many rounds, and claim that if adversarial training is a defense against UAPs, over many rounds the classification error on adversarial examples should tend to zero. <ref type="figure" target="#fig_5">Figure 8</ref> shows such a cat-and-mouse game over 20 rounds of (1) and 20 round of <ref type="bibr" target="#b1">(2)</ref>. An adversarially trained target model is able to classify nearly all adversarial examples correctly, at any given round. However, attacks against adversarially retrained models are only somewhat mitigated; there is a 25% reduction is attack success between the first and final round. After this, the cycle reaches an equilibrium, with no improvement in successive attacks or defended models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V I . C O N C L U S I O N</head><p>We presented a first-of-its-kind universal adversarial example attack that uses machine learning at the heart of its construction. We comprehensively evaluated the attack under many different settings, showing that it produces quality adversarial examples capable of fooling a target model in both targeted and non-targeted attacks. The attack transfers to many different target models, and improves on other state-of-the-art universal adversarial perturbation construction methods.  </p><formula xml:id="formula_6">A P P E N D I X A A N O T E O N R E C E N T C O N C U R R E N T W O R K</formula><p>We are unaware of any previous work that studies the relationship between generative models and universal adversarial perturbations. However, we note that two recent studies <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> also craft perturbations using generative models <ref type="bibr" target="#b3">4</ref> . Poursaeed et al. <ref type="bibr" target="#b25">[26]</ref> have a similar set-up to our attack, optimizing under both 2 and ? metrics, however, they did not include a distance minimization term within the objective function, instead relying a scaling factor before applying the perturbation to an image. The Mopuri et al. <ref type="bibr" target="#b26">[27]</ref> attack is only optimized using the ? metric. They also eschew a distance minimization term, and instead include a diversity term within the objective function, so that the objective does not get stuck in a local minima resulting in a limited number of effective perturbations. We were unable to obtain source code for Mopuri et al.'s <ref type="bibr" target="#b26">[27]</ref> attack and were unsuccessful in replicating results, and so we report comparison in results only on the target models that were shared in both pre-prints, on ImageNet (since other works only report results on this dataset for the task of generating UAPs) using the ? metric (see <ref type="table" target="#tab_0">Table VI</ref>). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :Fig. 3 :</head><label>23</label><figDesc>UAPs generated by a UAN for ImageNet. (a) VGG-19 (b) ResNet-101 (c) DenseNet UAPs generated by a UAN for CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>CIFAR-10 ? targeted attack. Each figure shows the error rate as the size of the adversarial perturbation is increased. This can be interpreted as the success rate of fooling the target model into classifying any image in CIFAR-10 as the chosen class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>MSE and SSIM scores of UAPs throughout training a UAN against VGG-19 for the ImageNet dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Our ? attack against a DenseNet target model on the CIFAR-10 dataset, for every source/target pair. Displayed images were selected at random.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>Non-targeted ? attack against ResNet-101 on the CIFAR-10 dataset. We vary the number of samples the UAN is trained on, and report results on the validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 :</head><label>8</label><figDesc>A cat-and-mouse game of non-targeted ? attacks and adversarial training for a VGG-19 target model on CIFAR-10. The upper green points are the target model accuracies on adversarial images after adversarial training, the lower red crosses are the target model accuracies on adversarial images after the attack. The dotted line is target model accuracy on source images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>V I I . A C K N O W L E D G E M E N T SJamie Hayes is funded by a Google PhD Fellowship in Machine Learning.R E F E R E N C E S</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :Fig. 10 :Fig. 11 :Fig. 12 :Fig. 13 :Fig. 14 :Fig. 15 :</head><label>9101112131415</label><figDesc>(a) Inception-V3: Fire engine (54.6%), ? , Wrecker (79.4%) (b) ResNet-152:Table lamp (87.2%), ? , Tabby cat (41.9%) (c) VGG-19: Radio telescope (97.5%), ? , Great Pyrenees (36.7%) Selection of successful adversarial examples (with target model confidence) from non-targeted ? attacks on ImageNet. From left to right: Source image, UAP, adversarial image. MSE and SSIM scores of UAPs throughout training a UAN against VGG-19 for the CIFAR-10 dataset. Our ? attack against a VGG-19 target model on the CIFAR-10 dataset, for every source/target pair. Displayed images were selected at random. Our ? attack against a ResNet-101 target model on the CIFAR-10 dataset, for every source/target pair. Displayed images were selected at random. 11 (a) Inception-V3: American egret (95.0%), ? , Golf ball (98.8%). Overall target model error rate: 0.654 (b) ResNet-152: Binoculars (99.9%), ? , Golf ball (62.9%). Overall target model error rate: 0.734 (c) VGG-19: Indian cobra (99.9%), ? , Golf ball (99.7%). Overall target model error rate: 0.514 Selection of successful adversarial examples (with target model confidence) for targeted ? attacks on ImageNet. The target class was randomly chosen to be Golf ball. From left to right: Source image, UAP, adversarial image. (a) Inception-V3: Pedestal (98.4%), ? , Broccoli (88.7%). Overall target model error rate: 0.598 (b) ResNet-152: Tibetan mastiff (88.4%), ? , Broccoli (98.1%). Overall target model error rate: 0.691 (c) VGG-19: Marmot (95.4%), ? , Broccoli (48.4%). Overall target model error rate: 0.480 Selection of successful adversarial examples (with target model confidence) for targeted ? attacks on ImageNet. The target class was randomly chosen to be Broccoli. From left to right: Source image, UAP, adversarial image. (a) Inception-V3: Lionfish (89.7%), ? , Stone wall (54.0%). Overall target model error rate: 0.533 (b) ResNet-152: Pinwheel (99.9%), ? , Stone wall (47.0%). Overall target model error rate: 0.587 (c) VGG-19: Golf ball (99.9%), ? , Stone wall (23.7%). Overall target model error rate: 0.447 Selection of successful adversarial examples (with target model confidence) for targeted ? attacks on ImageNet. The target class was randomly chosen to be Stone wall. From left to right: Source image, UAP, adversarial image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>UAN model architecture. IS refers to the image size: 32 for CIFAR-10 experiments and 224 for ImageNet experiments.</figDesc><table><row><cell>Layer</cell><cell>Shape</cell></row><row><cell>Input Deconv + Batch Norm + ReLU Deconv + Batch Norm + ReLU Deconv + Batch Norm + ReLU Deconv + Batch Norm + ReLU Deconv + Batch Norm + ReLU FC + Batch Norm + ReLU FC + Batch Norm + ReLU FC</cell><cell>100 256 ? 3 ? 3 128 ? 5 ? 5 64 ? 9 ? 9 32 ? 17 ? 17 3 ? 33 ? 33 512 1024 3 ? IS ? IS</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>UAN hyperparameters. Moosavi-Dezfooli et al.<ref type="bibr" target="#b18">[19]</ref> constructs a UAP iteratively; at each step an input is combined with the current constructed UAP, if the combination does not fool the target model, a new perturbation with minimal norm is found that does fool the target model. The attack terminates when a threshold error rate is met. Mopuri et al.<ref type="bibr" target="#b20">[21]</ref> develop a method for finding a UAP for a target model that is independent of the dataset. They construct a UAP by first starting with random noise and iteratively update it to over-saturate features learned at successive layers in the target model, causing neurons at each layer to output useless information to cause the desired misclassification. They optimize the UAP by adjusting it with respect to the loss term:</figDesc><table><row><cell>Parameter</cell><cell cols="2">Dataset</cell></row><row><cell></cell><cell>CIFAR-10</cell><cell>ImageNet</cell></row><row><cell>Learning Rate Beta 1 Beta 2 Batch Size Epochs p loss weight (?)</cell><cell>2 ? 10 ?4 0.5 0.999 128 500 4.0</cell><cell>2 ? 10 ?4 0.5 0.999 64 150 4.0</cell></row></table><note>I V. E VA L U AT I O N A. Comparison with previous work We now compare our method for crafting UAPs with two state-of-the-art methods:??</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Comparison of error rates for UAN against Moosavi-Dezfooli et al.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Error rates for non-targeted CIFAR-10 attack, under the ? metric. UAPs are constructed using row models and tested against pre-trained column models.</figDesc><table><row><cell></cell><cell>V G G -1 9</cell><cell>D E N S E N E T</cell><cell>R E S N E T -1 0 1</cell></row><row><cell>V G G -1 9 D E N S E N E T R E S N E T -1 0 1</cell><cell>0.666 0.543 0.514</cell><cell>0.550 0.750 0.681</cell><cell>0.612 0.648 0.851</cell></row><row><cell>E N S E M B L E</cell><cell>0.499</cell><cell>0.742</cell><cell>0.849</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Error rates for ? attacks on CIFAR-10. We compare between a UAN trained on fixed noise vectors and a UAN trained on non-fixed noise vectors.</figDesc><table><row><cell></cell><cell>Fixed z</cell><cell>Non-fixed z</cell></row><row><cell>V G G -1 9 R E S N E T -1 0 1 D E N S E N E T</cell><cell>0.661 0.859 0.760</cell><cell>0.666 0.851 0.750</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI :</head><label>VI</label><figDesc>Error rates for non-targeted ? attacks on Ima-geNet.</figDesc><table><row><cell></cell><cell>V G G -1 9</cell><cell>I N C E P T I O N -V 1 [ 3 3 ]</cell></row><row><cell>UAN Poursaeed et al. [26] Mopuri et al. [27]</cell><cell>0.846 0.801 0.838</cell><cell>0.809 0.792 0.904</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Code available at https://github.com/jhayes14/UAN 3 Note, this is equivalent to the experimental settings in Moosavi-Dezfooli et al. [19] of = 10 for p = ?, and = 2000 for p = 2.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Poursaeed et al's.<ref type="bibr" target="#b25">[26]</ref> pre-print was made available online 12 hours before our first version was made available, while Mopuri et al's.<ref type="bibr" target="#b26">[27]</ref> pre-print was made available three days afterwards.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Universality, robustness, and detectability of adversarial perturbations under adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anonymous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Submitted to International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey of data mining and machine learning methods for cyber security intrusion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Buczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Guven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys &amp; Tutorials</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1153" to="1176" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Adversarial examples are not easily detected: Bypassing ten detection methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07263</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (SP), 2017 IEEE Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks without Training Substitute Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">On Security and Sparsity of Linear Classifiers for Adversarial Settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Demontis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Russu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Machine vs Machine: Defending Classifiers Against Learningbased Adversarial Attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hamm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adversarial Example Defenses: Ensembles of Weak Defenses are not Strong</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02284</idno>
		<title level="m">Adversarial attacks on neural network policies</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A minimax theorem with applications to machine learning, signal processing, and finance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1344" to="1367" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adversarial examples for generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-02" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02533</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Machine learning techniques for the computer security domain of anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Machine learning in financial crisis prediction: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="421" to="436" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Universal adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.08401</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepfool: a simple and accurate method to fool deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2574" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fast feature fool: A data independent approach to universal adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Mopuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05572</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Predicting the futurebig data, machine learning, and clinical medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Obermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Emanuel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The New England journal of medicine</title>
		<imprint>
			<biblScope unit="volume">375</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page">1216</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Transferability in machine learning: from phenomena to black-box attacks using adversarial samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07277</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Practical black-box attacks against deep learning systems using adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02697</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Perceptual criteria for image quality evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Safranek</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Generative Adversarial Perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Poursaeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">NAG: Network for Adversary Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Reddy Mopuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ojha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Machine learning for high-speed corner detection. Computer Vision-ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rosten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="430" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Diffuse large b-cell lymphoma outcome prediction by gene-expression profiling and supervised machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Shipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tamayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Kutok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gaasenbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Angelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Pinkus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature medicine</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="68" to="74" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Active learning for on-road vehicle detection: A comparative study. Machine vision and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sivaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Support vector machine for regression and applications to financial forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Trafalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE-INNS-ENNS International Joint Conference on</title>
		<meeting>the IEEE-INNS-ENNS International Joint Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="348" to="353" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tram?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03453</idno>
		<title level="m">The space of transferable adversarial examples</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stealing machine learning models via prediction apis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tram?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Juels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Security Symposium</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="601" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Mean squared error: Love it or leave it? a new look at signal fidelity measures. IEEE signal processing magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="98" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A rapid learning algorithm for vehicle classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">295</biblScope>
			<biblScope unit="page" from="395" to="406" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Predicting hepatitis b virus-positive metastatic hepatocellular carcinomas using gene expression profiling and supervised machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Forgues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Robles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature medicine</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">416</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adversarial feature selection against evasion attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="766" to="777" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
