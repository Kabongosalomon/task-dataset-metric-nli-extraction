<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Human pose estimation via Convolutional Part Heatmap Regression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
							<email>adrian.bulat@nottingham.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">University of Nottingham</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
							<email>yorgos.tzimiropoulos@nottingham.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">University of Nottingham</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Human pose estimation via Convolutional Part Heatmap Regression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Human pose estimation</term>
					<term>part heatmap regression</term>
					<term>Convo- lutional Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper is on human pose estimation using Convolutional Neural Networks. Our main contribution is a CNN cascaded architecture specifically designed for learning part relationships and spatial context, and robustly inferring pose even for the case of severe part occlusions. To this end, we propose a detection-followed-by-regression CNN cascade. The first part of our cascade outputs part detection heatmaps and the second part performs regression on these heatmaps. The benefits of the proposed architecture are multi-fold: It guides the network where to focus in the image and effectively encodes part constraints and context. More importantly, it can effectively cope with occlusions because part detection heatmaps for occluded parts provide low confidence scores which subsequently guide the regression part of our network to rely on contextual information in order to predict the location of these parts. Additionally, we show that the proposed cascade is flexible enough to readily allow the integration of various CNN architectures for both detection and regression, including recent ones based on residual learning. Finally, we illustrate that our cascade achieves top performance on the MPII and LSP data sets. Code can be downloaded from</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Articulated human pose estimation from images is a Computer Vision problem of extraordinary difficulty. Algorithms have to deal with the very large number of feasible human poses, large changes in human appearance (e.g. foreshortening, clothing), part occlusions (including self-occlusions) and the presence of multiple people within close proximity to each other. A key question for addressing these problems is how to extract strong low and mid-level appearance features capturing discriminative as well as relevant contextual information and how to model complex part relationships allowing for effective yet efficient pose inference. Being capable of performing these tasks in an end-to-end fashion, Convolutional Neural Networks (CNNs) have been recently shown to feature remarkably robust  <ref type="figure">figure)</ref> is a part detection network trained to detect the individual body parts using a per-pixel sigmoid loss. Its output is a set of N part heatmaps. The second one is a regression subnetwork that jointly regresses the part heatmaps stacked along with the input image to confidence maps representing the location of the body parts.  The first row shows the produced part detection heatmaps for both visible (neck, head, left knee) and occluded (ankle, wrist, right knee) parts (drawn with a dashed line). Observe that the confidence for the occluded parts is much lower than that of the non-occluded parts but still higher than that of the background providing useful context about their rough location. The second row shows the output of our regression subnetwork. Observe that the confidence for the visible parts is higher and more localized and clearly the network is able to provide high confidence for the correct location of the occluded parts. Note: image taken from LSP test set.</p><p>performance and high part localization accuracy. Yet, the accurate estimation of the locations of occluded body parts is still considered a difficult open problem. The main contribution of this paper is a CNN cascaded architecture specifically designed to alleviate this problem. There is a very large amount of work on the problem of human pose estimation. Prior to the advent of neural networks most prior work was primarily based on pictorial structures <ref type="bibr" target="#b0">[1]</ref> which model the human body as a collection of rigid templates and a set of pairwise potentials taking the form of a tree structure, thus allowing for efficient and exact inference at test time. Recent work includes sophisticated extensions like mixture, hierarchical, multimodal and strong appearance models <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>, non-tree models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> as well as cascaded/sequential prediction models like pose machines <ref type="bibr" target="#b8">[9]</ref>.</p><p>More recently methods based on Convolutional Neural Networks have been shown to produce remarkable performance for a variety of difficult Computer Vision tasks including recognition <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, detection <ref type="bibr" target="#b11">[12]</ref> and semantic segmentation <ref type="bibr" target="#b12">[13]</ref> outperforming prior work by a large margin. A key feature of these approaches is that they integrate non-linear hierarchical feature extraction with the classification or regression task in hand being also able to capitalize on very large data sets that are now readily available. In the context of human pose estimation, it is natural to formulate the problem as a regression one in which CNN features are regressed in order to provide joint prediction of the body parts <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>. For the case of non-visible parts though, learning the complex mapping from occluded part appearances to part locations is hard and the network has to rely on contextual information (provided from the other visible parts) to infer the occluded parts' location. In this paper, we show how to circumvent this problem by proposing a detection-followed-by-regression CNN cascade for articulated human pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Main Contribution</head><p>The proposed architecture is a CNN cascade consisting of two components (see <ref type="figure" target="#fig_0">Fig. 1</ref>): the first component (part detection network) is a deep network for part detection that produces detection heatmaps, one for each part of the human body. We train part detectors jointly using pixelwise sigmoid cross entropy loss function <ref type="bibr" target="#b17">[18]</ref>. The second component is a deep regression subnetwork that jointly regresses the location of all parts (both visible and occluded), trained via confidence map regression <ref type="bibr" target="#b15">[16]</ref>. Besides the two subnetworks, the key feature of the proposed architecture is the input to the regression subnetwork: we propose to use a stacked representation comprising the part heatmaps produced by the detection network. The proposed representation guides the network where to focus and encodes structural part relationships. Additionally, our cascade does not suffer from the problem of regressing occluded part appearances: because the part heatmaps for the occluded parts provide low confidence scores, they subsequently guide the regression part of our network to rely on contextual information (provided by the remaining parts) in order to predict the location of these parts. See <ref type="figure" target="#fig_2">Fig. 2</ref> for a graphical representation of our paper's main idea.</p><p>The proposed cascade is very simple, can be trained end-to-end, and is flexible enough to readily allow the integration of various CNN architectures for both our detection and regression subnetworks. To this end, we illustrate two instances of our cascade, one based on the more traditional VGG converted to fully convolutional (FCN) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13]</ref> and one based on residual learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19]</ref>. Both architectures achieve top performance on both MPII <ref type="bibr" target="#b19">[20]</ref> and LSP <ref type="bibr" target="#b20">[21]</ref> data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Closely Related Work</head><p>Overview of prior work. Recently proposed methods for articulated human pose estimation using CNNs can be classified as detection-based <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref> or regression-based <ref type="bibr">[14-17, 27, 28]</ref>. Detection-based methods are relying on powerful CNN-based part detectors which are then combined using a graphical model <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> or refined using regression <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. Regression-based methods try to learn a mapping from image and CNN features to part locations. A notable development has been the replacement of the standard L2 loss between the predicted and ground truth part locations with the so-called confidence map regression which defines an L2 loss between predicted and ground truth confidence maps encoded as 2D Gaussians centered at the part locations <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23]</ref> (these regression confidence maps are not to be confused with the part detection heatmaps proposed in our work). As a mapping from CNN features to part locations might be difficult to learn in one shot, regression-based methods can be also applied sequentially (i.e. in a cascaded manner) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>. Our CNN cascade is based on a two-step detection-followed-by-regression approach (see <ref type="figure" target="#fig_0">Fig. 1</ref>) and as such is related to both detection-based <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> and regression-based methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>Relation to regression-based methods. Our detection-followed-by-regression cascade is related to <ref type="bibr" target="#b15">[16]</ref> which can be seen as a two-step regression-followedby-regression approach. As a first step <ref type="bibr" target="#b15">[16]</ref> performs confidence map regression (based on an L2 loss) as opposed to our part detection step which is learnt via pixelwise sigmoid cross entropy loss. Then, in <ref type="bibr" target="#b15">[16]</ref> pre-confidence maps are used as input to a subsequent regression network. We empirically found that such maps are too localised providing small spatial support. In contrast, our part heatmaps can provide large spatial context for regression. For comparison purposes, we implemented the idea of <ref type="bibr" target="#b15">[16]</ref> using two different architectures, one based on VGG-FCN and one on residual learning, and show that the proposed detection-followed-by-regression cascade outperforms it for both cases (see section 4.2). In order to improve performance, regression methods applied in a sequential, cascaded fashion have been recently proposed in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. In particular, <ref type="bibr" target="#b27">[28]</ref> has recently reported outstanding results on both LSP <ref type="bibr" target="#b20">[21]</ref> and MPII <ref type="bibr" target="#b19">[20]</ref> data sets using a six-stage CNN cascade.</p><p>Relation to detection-based methods. Regarding detection-based methods, <ref type="bibr" target="#b24">[25]</ref> has produced state-of-the-art results on both MPII and LSP data sets using a VGG-FCN network <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13]</ref> to detect the body parts along with an L2 loss for regression that refines the part prediction. Hence, <ref type="bibr" target="#b24">[25]</ref> does not include a sub-sequent part heatmap regression network as our method does. The work of <ref type="bibr" target="#b23">[24]</ref> uses a part detection network as a first step in order to provide crude estimates for the part locations. Subsequently, CNN features are cropped around these estimates and used for refinement using regression. Hence, <ref type="bibr" target="#b23">[24]</ref> does not include a subsequent part heatmap regression network as our method does, and hence does not account for contextual information but allows only for local refinement.</p><p>Residual learning. Notably, all the aforementioned methods were developed prior to the advent of residual learning <ref type="bibr" target="#b9">[10]</ref>. Very recently, residual learning was applied for the problem of human pose estimation in <ref type="bibr" target="#b25">[26]</ref> and <ref type="bibr" target="#b18">[19]</ref>. Residual learning was used for part detection in the system of <ref type="bibr" target="#b25">[26]</ref>. The "stacked hourglass network" of <ref type="bibr" target="#b18">[19]</ref> elegantly extends FCN <ref type="bibr" target="#b12">[13]</ref> and deconvolution nets <ref type="bibr" target="#b28">[29]</ref> within residual learning, also allowing for a more sophisticated and heavy processing during top-down processing. We explore residual learning within the proposed CNN cascade; notably for our residual regression subnetwork, we used a single "hourglass network" <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>The proposed part heatmap regression is a CNN cascade illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. Our cascade consists of two connected subnetworks. The first subnetwork is a part detection network trained to detect the individual body parts using a per-pixel softmax loss. The output of this network is a set of N part detection heatmaps. The second subnetwork is a regression subnetwork that jointly regresses the part detection heatmaps stacked with the image/CNN features to confidence maps representing the location of the body parts.</p><p>We implemented two instances of part heatmap regression: in the first one, both subnetworks are based on VGG-FCN <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13]</ref> and in the second one, on residual learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19]</ref>. For both cases, the subnetworks and their training are described in detail in the following subsections. The following paragraphs outline important details about the training of the cascade, and are actually independent of the architecture used (VGG-FCN or residual).</p><p>Part detection subnetwork. While <ref type="bibr" target="#b12">[13]</ref> uses a per-pixel softmax loss encoding different classes with different numeric levels, in practice, for the human body this is suboptimal because the parts are usually within close proximity to each other, having high chance of overlapping. Therefore, we follow an approach similar to <ref type="bibr" target="#b17">[18]</ref> and encode part label information as a set of N binary maps, one for each part, in which the values within a certain radius around the provided ground truth location are set to 1 and the values for the remaining background are set to 0. This way, we thus tackle the problem of having multiple parts in the very same region. Note that the detection network is trained using visible parts only, which is fundamentally different from the previous regression-based approaches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>The radius defining "correct location" was selected so that the targeted body part is fully included inside. Empirically, we determined that a value of 10px to be optimal for a body size of 200px of an upright standing person.</p><p>We train our body part detectors jointly using pixelwise sigmoid cross entropy loss function:</p><formula xml:id="formula_0">l 1 = 1 N N n=1 W i=1 H j=1 [p n ij logp n ij + (1 ? p n ij ) log(1 ?p n ij )],<label>(1)</label></formula><p>where p n ij denotes the ground truth map of the nth part at pixel location (i, j) (constructed as described above) andp n ij is the corresponding sigmoid output at the same location.</p><p>Regression subnetwork. While the detectors alone provide good performance, they lack a strong relationship model that is required to improve (a) accuracy and (b) robustness particularly required in situations where specific parts are occluded. To this end, we propose an additional subnetwork that jointly regresses the location of all parts (both visible and occluded). The input of this subnetwork is a multi-channel representation produced by stacking the N heatmaps produced by the part detection subnetwork, along with the input image. (see <ref type="figure" target="#fig_0">Fig. 1</ref>). This multichannel representation guides the network where to focus and encodes structural part relationships. Additionally, it ensures that our network does not suffer from the problem of regressing occluded part appearances: because the part detection heatmaps for the occluded parts provide low confidence scores, they subsequently guide the regression part of our network to rely on contextual information (provided by the remaining parts) in order to predict the location of these parts.</p><p>The goal of our regression subnetwork is to predict the points' location via regression. However, direct regression of the points is a difficult and highly nonlinear problem caused mainly by the fact that only one single correct value needs to be predicted. We address this by following a simpler alternative route <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23]</ref>, regressing a set of confidence maps located in the immediate vicinity of the correct location (instead of regressing a single value). The ground truth consists of a set of N layers, one for each part, in which the correct location of each part, be it visible or not is represented by Gaussian with a standard deviation of 5px.</p><p>We train our subnetwork to regress the location of all parts jointly using the following L2 loss:</p><formula xml:id="formula_1">l 2 = 1 N N n=1 ij M n (i, j) ? M n (i, j) 2 ,<label>(2)</label></formula><p>where M n (i, j) and M n (i, j) represent the predicted and the ground truth confidence maps at pixel location (i, j) for the nth part, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">VGG-FCN part heatmap regression</head><p>Part detection subnetwork. We based our part detection network architecture on the VGG-16 network <ref type="bibr" target="#b10">[11]</ref> converted to fully convolutional by replacing the fully connected layers with convolutional layers of kernel size of 1 <ref type="bibr" target="#b12">[13]</ref>. Because the localization accuracy offered by the 32px stride is insufficient, we make use of the entire algorithm as in <ref type="bibr" target="#b12">[13]</ref> by combining the earlier level CNN features, thus reducing the stride to 8px. For convenience, the network is shown in <ref type="figure">Fig. 3</ref> and <ref type="table" target="#tab_0">Table 1</ref>.</p><formula xml:id="formula_2">A1 A2 A3 A4 A5 A6 A7 A8 A8 + A8</formula><p>A9 + A9 A9 <ref type="figure">Fig. 3</ref>: The VGG-FCN subnetwork used for body part detection. The blocks A1-A9 are defined in <ref type="table" target="#tab_0">Table 1</ref>. Regression subnetwork. We have chosen a very simple architecture for our regression sub-network, consisting of 7 convolutional layers. The network is shown in <ref type="figure">Fig. 4</ref> and <ref type="table" target="#tab_1">Table 2</ref>. The first 4 of these layers use a large kernel size that varies from 7 to 15, in order to capture a sufficient local context and to increase the receptive field size which is crucial for learning long-term relationships. The last 3 layers have a kernel size equal to 1.</p><p>Training. For training on MPII, all images were cropped after centering on the person and then scaled such that a standing-up human has height 300px. All images were resized to a resolution of 380x380px. To avoid overfitting, we performed image flipping, scaling (between 0.7 and 1.3) and rotation (between -40 and 40 degrees). Both rotation and scaling were applied using a set of predefined step sizes. Training the network is a straightforward process. We started by first training the body part detection network, fine-tuning from VGG-16 <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13]</ref> pre-trained on ImageNet <ref type="bibr" target="#b29">[30]</ref>. The detectors were then trained for about 20 epochs using a learning rate progressively decreasing from 1e ? 8 to 1e ? 9. For the regression subnetwork, all layers were initialized with a Gaussian distribution (std=0.01). To accelerate the training and avoid early divergence we froze the training of the detector layers, training only the subnetwork. We let this train for 20 epochs with a learning rate of 0.00001 and then 0.000001. We then trained jointly both networks for 10 epochs. We found that one can train both the part detection network and the regression subnetwork jointly, right from the beginning, however, the aforementioned approach results in faster training.</p><p>For LSP, we fine-tuned the network for 10 epochs on the 1000 images of the training set. Because LSP provides the ground truth for only 14 key points, during fine-tuning we experimented with two different strategies: (i) generating the points artificially and (ii) stopping the backpropagation for the missing points. The later approach produced better results overall. The training was done using the caffe <ref type="bibr" target="#b30">[31]</ref> bindings for Torch7 <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C1</head><p>C2 C3 C4 C5 C6 C7 C8 <ref type="figure">Fig. 4</ref>: The VGG-based subnetwork used for regression. The blocks C1-C8 are defined in <ref type="table" target="#tab_1">Table 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Residual part heatmap regression</head><p>Part detection subnetwork. Motivated by recent developments in image recognition <ref type="bibr" target="#b9">[10]</ref>, we used ResNet-152 as a base network for part detection. Doing so requires making the network able to make predictions at pixel level which is a relative straightforward process (similar ways to do this are described in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>). The network is shown in <ref type="figure" target="#fig_3">Fig. 5</ref> and <ref type="table" target="#tab_2">Table 3</ref>. Blocks B1-B4 are the same as the ones in the original ResNet, and B5 was slightly modified. We firstly removed both the fully connected layer after B5 and then the preceding average pooling layer. Then, we added a scoring convolutional layer B6 with N outputs, one for each part. Next, to address the extremely low output resolution, we firstly modified B5 by changing the stride of its convolutional layers from 2px to 1px and then added (after B6) a deconvolution <ref type="bibr" target="#b28">[29]</ref> layer B7 with a kernel size and stride of 4, that upsamples the output layers to match the resolution of the input. We argue that for our detection subnetwork, knowing the exact part location is not needed. All added layers were initialized with 0 and trained using rmsprop <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B1</head><p>B2 B3 B4 B5 B6 B7  <ref type="table" target="#tab_2">Table 3</ref>. See also text. 1x conv layer <ref type="bibr">(16,1x1,1x1)</ref> 1x deconv layer <ref type="bibr">(16,4x4,4x4)</ref> Regression subnetwork. For the residual regression subnetwork, we used a (slightly) modified "hourglass network" <ref type="bibr" target="#b18">[19]</ref>, which is a recently proposed stateof-the-art architecture for bottom-up, top-down inference. The network is shown in <ref type="figure" target="#fig_4">Fig. 6</ref> and <ref type="table">Table 4</ref>. Briefly, the network builds on top of the concepts described in <ref type="bibr" target="#b12">[13]</ref>, improving a few fundamental aspects. The first one is that extends <ref type="bibr" target="#b12">[13]</ref> within residual learning. The second one is that instead of passing the lower level futures through a convolution layer with the same number of channels as the final scoring layer, the network passes the features through a set of 3 convolutional blocks that allow the network to reanalyse and learn how to combine features  <ref type="table">Table 4</ref>. See also text.</p><p>extracted at different resolutions. See <ref type="bibr" target="#b18">[19]</ref> for more details. Our modification was in the introduction of deconvolution layers D5 for recovering the lost spatial resolution (as opposed to nearest neighbour upsampling used in <ref type="bibr" target="#b18">[19]</ref>). Also, as in the detection network, the output is brought back to the input's resolution using another trained deconvolutional layer D5. <ref type="table">Table 4</ref>: Block specification for the "hourglass network". Torch notations (channels, kernel, stride) and (kernel, stride) are used to define the conv and pooling layers. The bottleneck modules are defined as in <ref type="bibr" target="#b35">[36]</ref>. 1x conv scoring layer <ref type="bibr">(16, 1x1, 1x1)</ref> Training. For training on MPII, we applied similar augmentations as before, with the difference being that augmentations were applied randomly. Also, due to memory issues, the input image was rescaled to 256x256px. Again, we started by first training the body part detection network, fine-tuning from ResNet-152 <ref type="bibr" target="#b9">[10]</ref> pre-trained on ImageNet <ref type="bibr" target="#b29">[30]</ref>. The detectors were then trained for about 50 epochs using a learning rate progressively decreasing from 1e ? 3 to 2.5e ? 5. For the regression "hourglass" subnetwork, we froze the learning for the detector layers, training only the regression subnetwork. We let this train for 40 epochs using a learning rate of 1e ? 4 and then 2.5e ? 5. In the end, the networks were trained jointly for 50 more epochs. While we experimented with different initialization strategies, all of them seemed to produce similar results. For the final model, all layers from the regression subnetwork were zero-initialized, except for the deconvolution layers, which were initialized using bilinear upsampling filters, as in <ref type="bibr" target="#b12">[13]</ref>. The network made use of batch normalization, and was trained with a batch size of 8. For LSP, we follow the same procedure as the one for VGG-FCN, changing only the number of epochs to 30. The network was implemented and trained using Torch7 <ref type="bibr" target="#b31">[32]</ref>. The code, along with the pretrained models will be published on our webpage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>We report results for two sets of experiments on the two most challenging data sets for human pose estimation, namely LSP <ref type="bibr" target="#b20">[21]</ref> and MPII <ref type="bibr" target="#b19">[20]</ref>. A summary of our results is as follows:</p><p>-We show the benefit of the proposed detection-followed-by-regression cascade over a two-step regression approach, similar to the idea described in <ref type="bibr" target="#b15">[16]</ref>, when implemented with both VGG-FCN and residual architectures. -We provide an analysis of the different components of our network illustrating their importance on overall performance. We show that stacking the part heatmaps as proposed in our work is necessary for achieving high performance, and that this performance is significantly better than that of the part detection network alone.</p><p>-We show the benefit of using a residual architecture over VGG-FCN.</p><p>-We compare the performance of our method with that of recently published methods illustrating that both versions of our cascade achieve top performance on both the MPII and LSP data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis</head><p>We carried out a series of experiments in order to investigate the impact of the various components of our architecture on performance. In all cases, training and testing was done on MPII training and validation set, respectively. The results are summarized in <ref type="table" target="#tab_4">Table 5</ref>. In particular, we report the performance of i the overall part heatmap regression (which is equivalent to "Detection+regression") for both residual and VGG-FCN architectures. ii the residual part detection network alone (Detection only). iii the residual detection network but trained to perform direct regression (Regression only). iv a two-step regression approach as in <ref type="bibr" target="#b15">[16]</ref> (Regression+regression), but implemented with both residual and VGG-FCN architectures.</p><p>We first observe that there is a large performance gap between residual part heatmap regression and the same cascade but implemented with a VGG-FCN. Residual detection alone works well, but the regression subnetwork provides a large boost in performance, showing that using the stacked part heatmaps as input to residual regression is necessary for achieving high performance.</p><p>Furthermore, we observe that direct regression alone (case iii above) performs better than detection alone, but overall our detection-followed-by-regression cascade significantly outperforms the two-step regression approach. Notably, we found that the proposed part heatmap regression is also considerably easier to train. Not surprisingly, the gap between detection-followed-by-regression and two-step regression when both are implemented with VGG-FCN is much bigger. Overall, these results clearly verify the importance of using (a) part detection heatmaps to guide the regression subnetwork and (b) a residual architecture.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with state-of-the-art</head><p>In this section, we compare the performance of our method with that of published methods currently representing the state-of-the-art. <ref type="table" target="#tab_5">Tables 6 and 7</ref> summarize our results on MPII and LSP, respectively. Our results show that both VGG-based and residual part heatmap regression are very competitive with the latter, along with the other two residual-based architectures <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26]</ref>, being top performers on both datasets. Notably, very close in performance is the method of <ref type="bibr" target="#b27">[28]</ref> which is not based on residual learning but performs a sequence of 6 CNN regressions, being also much more challenging to train <ref type="bibr" target="#b27">[28]</ref>. Examples of fitting results from MPII and LSP for the case of residual part heatmap regression can be seen in <ref type="figure" target="#fig_5">Fig. 7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgement</head><p>We would like to thank Leonid Pishchulin for graciously producing our results on MPII with unprecedented quickness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We proposed a CNN cascaded architecture for human pose estimation particularly suitable for learning part relationships and spatial context, and robustly inferring pose even for the case of severe part occlusions. Key feature of our network is the joint regression of part detection heatmaps. The proposed architecture is very simple and can be trained end-to-end, achieving top performance on the MPII and LSP data sets. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Proposed architecture: Our CNN cascade consists of two connected deep subnetworks. The first one (upper part in the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Paper's main idea:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>The architecture of the residual part detection subnetwork. The network is based on ResNet-152 and its composing blocks. The blocks B1-B7 are defined in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>The "hourglass network"<ref type="bibr" target="#b18">[19]</ref> used as the residual regression network. The Blocks D1-D7 are defined in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Examples of poses obtained using our method on MPII (first 3 rows), and LSP (4th and 5th row). Observe that our method copes well with both occlusions and difficult poses. The last row shows some fail cases caused by combinations of extreme occlusion and rare poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>A1</cell><cell>A2</cell><cell>A3</cell><cell>A4</cell><cell>A5</cell><cell>A6</cell><cell>A7</cell><cell>A8</cell><cell>A9</cell></row><row><cell>2x conv</cell><cell>2x conv</cell><cell>3x conv</cell><cell>3x conv</cell><cell>3X conv</cell><cell>conv layer</cell><cell>conv layer</cell><cell>conv</cell><cell>bilinear</cell></row><row><cell>layer (64,</cell><cell>layer (128,</cell><cell>layer (256,</cell><cell>layer (512,</cell><cell>layer(512,</cell><cell>(4096,</cell><cell>(4096,</cell><cell>layer(16,</cell><cell>upsample</cell></row><row><cell>3x3, 1x1),</cell><cell>3x3, 1x1),</cell><cell>3x3, 1x1),</cell><cell>3x3, 1x1),</cell><cell>1x1, 1x1),</cell><cell>7x7, 1x1)</cell><cell>1x1, 1x1)</cell><cell>1x1, 1x1)</cell><cell></cell></row><row><cell>pooling</cell><cell>pooling</cell><cell>pooling</cell><cell>pooling</cell><cell>pooling</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Block specification for the VGG-FCN part detection subnetwork. Torch notations (channels, kernel, stride) and (kernel, stride) are used to define the conv and pooling layers.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Block specification for the VGG-based regression subnetwork. Torch notations (channels, kernel, stride) and (kernel, stride) are used to define the conv and pooling layers.</figDesc><table><row><cell>C1</cell><cell>C2</cell><cell>C3</cell><cell>C4</cell><cell>C5</cell><cell>C6</cell><cell>C7</cell><cell>C8</cell></row><row><cell>conv</cell><cell>conv</cell><cell>conv</cell><cell>conv</cell><cell>conv</cell><cell>conv</cell><cell>conv</cell><cell>deconv</cell></row><row><cell>layer(64,</cell><cell>layer(64,</cell><cell>layer(128,</cell><cell>layer(256,</cell><cell>layer(512,</cell><cell>layer(512,</cell><cell>layer(16,</cell><cell>layer (16,</cell></row><row><cell>9x9, 1x1)</cell><cell>13x13, 1x1)</cell><cell>13x13, 1x1)</cell><cell>15x15, 1x1)</cell><cell>1x1, 1x1)</cell><cell>1x1, 1x1)</cell><cell>1x1, 1x1)</cell><cell>8x8, 4x4)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Block specification for the residual part detection network. Torch notations (channels, kernel, stride) and (kernel, stride) are used to define the conv and pooling layers. The bottleneck modules are defined as in<ref type="bibr" target="#b9">[10]</ref>.</figDesc><table><row><cell>B1</cell><cell>B2</cell><cell>B3</cell><cell>B4</cell><cell>B5</cell><cell>B6</cell><cell>B7</cell></row><row><cell>1x conv layer</cell><cell>3x bottleneck</cell><cell>8x bottleneck</cell><cell>38x</cell><cell>3x bottleneck</cell><cell></cell><cell></cell></row><row><cell>(64,7x7,2x2)</cell><cell>modules</cell><cell>modules</cell><cell>bottleneck</cell><cell>modules</cell><cell></cell><cell></cell></row><row><cell>1x pooling</cell><cell>[(64,1x1),</cell><cell>[(128,1x1),</cell><cell>modules</cell><cell>[(512,1x1),</cell><cell></cell><cell></cell></row><row><cell>(3x3, 2x2)</cell><cell>(64,3x3),</cell><cell>(128,3x3),</cell><cell>[(256,1x1),</cell><cell>(512,3x3),</cell><cell></cell><cell></cell></row><row><cell></cell><cell>(256,1x1)]</cell><cell>(512,1x1)]</cell><cell>(256,3x3),</cell><cell>(2048,1x1)]</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>(1024,1x1)]</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison between different variants of the proposed residual architecture on MPII validation set, using PCKh metric. The overall residual part heatmap regression architecture is equivalent to "Detection+regression".</figDesc><table><row><cell></cell><cell cols="2">Head Shoulder Elbow Wrist Hip Knee Ankle Total</cell></row><row><cell cols="2">Part heatmap regression(Res) 97.3 95.2</cell><cell>89.9 85.3 89.4 85.7 81.9 88.2</cell></row><row><cell cols="2">Part heatmap regression(VGG) 95.6 92.2</cell><cell>83.5 78.3 84.5 77.3 70.0 83.2</cell></row><row><cell>Detection only(Res)</cell><cell>96.2 91.3</cell><cell>83.4 74.5 83.1 76.6 71.3 82.6</cell></row><row><cell>Regression only(Res)</cell><cell>96.4 92.8</cell><cell>84.5 77.3 84.5 79.9 74.0 84.2</cell></row><row><cell>Regression+regression(Res)</cell><cell>96.7 93.6</cell><cell>86.1 80.1 88.1 80.5 76.7 85.7</cell></row><row><cell cols="2">Regression+regression(VGG) 92.8 85.6</cell><cell>77.5 70.4 73.5 69.3 66.5 76.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>PCKh-based comparison with state-of-the-art on MPII</figDesc><table><row><cell></cell><cell cols="2">Head Shoulder Elbow Wrist Hip Knee Ankle Total</cell></row><row><cell>Part heatmap regression(Res)</cell><cell>97.9 95.1</cell><cell>89.9 85.3 89.4 85.7 81.9 89.7</cell></row><row><cell cols="2">Part heatmap regression(VGG) 96.8 91.3</cell><cell>82.9 77.5 83.2 74.4 67.5 82.7</cell></row><row><cell>Newell et al., arXiv'16 [19]</cell><cell>97.6 95.4</cell><cell>90.0 85.2 88.7 85.0 80.6 89.4</cell></row><row><cell>Wei et al., CVPR'16 [28]</cell><cell>97.8 95.0</cell><cell>88.7 84.0 88.4 82.8 79.4 88.5</cell></row><row><cell cols="2">Insafutdinov et al., arXiv'16 [26] 96.6 94.6</cell><cell>88.5 84.4 87.6 83.9 79.4 88.3</cell></row><row><cell>Gkioxary et al., arXiv'16 [37]</cell><cell>96.2 93.1</cell><cell>86.7 82.1 85.2 81.4 74.1 86.1</cell></row><row><cell>Lifshitz et al., arXiv'16 [38]</cell><cell>97.8 93.3</cell><cell>85.7 80.4 85.3 76.6 70.2 85.0</cell></row><row><cell cols="2">Pishchulin et. al., CVPR'16 [25] 94.1 90.2</cell><cell>83.4 77.3 82.6 75.7 68.6 82.4</cell></row><row><cell cols="2">Hu&amp;Ramanan., CVPR'16 [39] 95.0 91.6</cell><cell>83.0 76.6 81.9 74.25 69.5 82.4</cell></row><row><cell>Carreira et al., CVPR'16 [27]</cell><cell>95.7 91.7</cell><cell>81.7 72.4 82.8 73.2 66.4 81.3</cell></row><row><cell>Tompson et al., NIPS'14 [23]</cell><cell>95.8 90.3</cell><cell>80.5 74.3 77.6 69.7 62.8 79.6</cell></row><row><cell>Tompson et al., CVPR'15 [24]</cell><cell>96.1 91.9</cell><cell>83.9 77.8 80.9 72.3 64.8 82.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>PCK-based comparison with the state-of-the-art on LSP Part heatmap regression(VGG) 94.8 86.6 79.5 73.5 88.1 83.2 78.5 83.5 Wei et al., CVPR'16 [28] 97.8 92.5 87.0 83.9 91.5 90.8 89.9 90.5 Insafutdinov et al., arXiv'16 [26] 97.4 92.7 87.5 84.4 91.5 89.9 87.2 90.1 Pishchulin et al.CVPR'16 [25] 97.0 91.0 83.8 78.1 91.0 86.7 82.0 87.1 Lifshitz et al., arXiv'16 [38] 96.8 89.0 82.7 79.1 90.9 86.0 82.5 86.7 Yang et al., CVPR'16 [40] 90.6 78.1 73.8 68.8 74.8 69.9 58.9 73.6 Carreira et al., CVPR'16 [27] 90.5 81.8 65.8 59.8 81.6 70.6 62.0 73.1 Tompson et al., NIPS'14 [23] 90.6 79.2 67.9 63.4 69.5 71.0 64.2 72.3 Fan et al., CVPR'15 [41] 92.4 75.2 65.3 64.0 75.7 68.3 70.4 73.0 Chen&amp;Yuille, NIPS'14 [22] 91.8 78.2 71.8 65.5 73.3 70.2 63.4 73.4</figDesc><table><row><cell></cell><cell cols="2">Head Shoulder Elbow Wrist Hip Knee Ankle Total</cell></row><row><cell>Part heatmap regression(Res)</cell><cell>96.3 92.2</cell><cell>88.2 85.2 92.2 91.5 88.6 90.7</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="79" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-ofparts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Exploring the spatial hierarchy of mixture models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modec: Multimodal decomposable models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Strong appearance and expressive spatial models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using linking features in learning non-parametric part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Human pose estimation using body parts dependent joint regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Pose machines: Articulated pose estimation via inference machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep convolutional neural networks for efficient pose estimation in gesture videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Robust optimization for deep regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07063</idno>
		<title level="m">Fine-grained pose prediction, normalization, and recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06937</idno>
		<title level="m">Stacked hourglass networks for human pose estimation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.03170</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.06550</idno>
		<title level="m">Human pose estimation with iterative error feedback</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<title level="m">Convolutional pose machines. In: CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2018" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop. Number EPFL-CONF-192376</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">High-performance semantic segmentation using very deep fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04339</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06409</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<title level="m">Identity mappings in deep residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02346</idno>
		<title level="m">Chained predictions using convolutional neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lifshitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08212</idno>
		<title level="m">Human pose estimation using deep consensus voting</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.05699</idno>
		<title level="m">Bottom-up and top-down reasoning with convolutional latent-variable models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Combining local appearance and holistic view: Dual-source deep neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
