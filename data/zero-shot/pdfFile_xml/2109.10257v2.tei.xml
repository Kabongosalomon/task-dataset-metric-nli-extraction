<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Skeleton-Graph: Long-Term 3D Motion Prediction From 2D Observations Using Deep Spatio-Temporal Graph CNNs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abduallah</forename><surname>Mohamed</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas At Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huancheng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas At Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas At Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Claudel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas At Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Skeleton-Graph: Long-Term 3D Motion Prediction From 2D Observations Using Deep Spatio-Temporal Graph CNNs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Several applications such as autonomous driving, augmented reality and virtual reality require a precise prediction of the 3D human pose. Recently, a new problem was introduced in the field to predict the 3D human poses from observed 2D poses. We propose Skeleton-Graph, a deep spatio-temporal graph CNN model that predicts the future 3D skeleton poses in a single pass from the 2D ones. Unlike prior works, Skeleton-Graph focuses on modeling the interaction between the skeleton joints by exploiting their spatial configuration. This is being achieved by formulating the problem as a graph structure while learning a suitable graph adjacency kernel. By the design, Skeleton-Graph predicts the future 3D poses without divergence in the long-term, unlike prior works. We also introduce a new metric that measures the divergence of predictions in the long term. Our results show an FDE improvement of at least 27% and an ADE of 4% on both the GTA-IM and PROX datasets respectively in comparison with prior works. Also, we are 88% and 93% less divergence on the long-term motion prediction in comparison with prior works on both GTA-IM and PROX datasets. Code is available at https://github.com/ abduallahmohamed/Skeleton-Graph.git.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>An accurate 3D pose prediction model is vital for several applications. In intersection management and autonomous vehicles, one can prevent an accident based on the 3D poses of pedestrians <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b31">30]</ref>. In Virtual and Augmented Reality (VR/ AR) the predictions of the 3D pose help in deepening the immersive experience <ref type="bibr" target="#b28">[28]</ref>. Where in drones and autonomous driving, 3D pose prediction helps in accurate maneuver and motion planning through the environment <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b10">11]</ref>. The process of obtaining the 2D poses is quite inexpensive in comparison to the process of obtaining the 3D poses. The 2D process does not require special sensors such as depth sensors installed on the device. <ref type="figure">Figure 1</ref>: Skeleton-Graph given an input of 2D spatiotemporal graph of observed human skeleton poses. Then, in a single pass it predicts the next 3D poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Past 2D Poses Observations Future3D Poses Predictions Skeleton-Graph</head><p>Thus it is cheaper to obtain the 2D poses from an ordinary vision sensor <ref type="bibr" target="#b2">[3]</ref>. However, 2D poses are not suitable for the increasing trends of requirements in a new range of applications such as the ones mentioned before <ref type="bibr" target="#b33">[32,</ref><ref type="bibr" target="#b32">31]</ref>. Thus it is tempting to develop models that directly obtain 3D poses from 2D ones, saving the need for expensive equipment and upscaling to the new trends.</p><p>The recent work of <ref type="bibr" target="#b5">[6]</ref> introduced a new long-term trajectory prediction problem that focuses on the concept of obtaining 3D poses from 2D ones. The introduced problem goal is to predict the future expensive 3D motion trajectories from the cheaply obtained 2D observed motion. In their work, a deep model called GPP-Net was introduced to address this problem. It is a three-stage deep model that uses several concepts such as Variational Auto Encoders (VAEs) and tailored stages for both paths and poses predictions. Beside GPP-Net, other 3D human motion estimation models such as TR <ref type="bibr" target="#b37">[36]</ref>, VP <ref type="bibr" target="#b27">[27]</ref>, LTD <ref type="bibr" target="#b39">[38]</ref> were evaluated on this problem.</p><p>By examining the results and the architectures of these prior works, we found that three main design components were used separately in each prior work to enhance the results but not collectively in one work. The first component is the exploitation of the spatial configuration of the skeleton explicitly through the deep architecture itself <ref type="bibr" target="#b39">[38]</ref>. The skeleton spatial configuration includes useful information the leads to better predictions. The joints correlate with each other in terms of the angles and distances between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2109.10257v2 [cs.CV] 27 Sep 2021</head><p>This kind of information when introduced to a deep model it will constraints the model output not to produce random points that are far away from the ground truth. The second component is the usage of the vision signal <ref type="bibr" target="#b5">[6]</ref>. The vision signal of the observed sequence contains information such as the objects in the environment and the geometry of the scene. This information helps in resolving the ambiguity in some prediction scenarios. For example, it is not valid to predict a skeleton standing on a table or a couch. The third and last component is to avoid the use of deep recurrent architectures. Recurrent architectures in long-term prediction problems tend to accumulate prediction errors from a step to the next step. We noticed this behavior in prior works <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b39">38]</ref> that used these recurrent models.</p><p>Thus we introduce Skeleton-Graph in which we combine these three design components in one work overcoming the shortcoming of the prior works. First, to use the spatial configuration information of the skeleton we model the problem as a spatio-temporal graph end to end. We rely on the adjacency matrix to encode the relationship between the skeleton joints. Instead of a fixed adjacency matrix, we let our model learn it and analyze it in our ablation study. Secondly, we utilize the vision signal by fusing it into our model. Thus, we utilize the context information to enhance our results. We found out that the visual signal in some cases enhances short-term predictions. Lastly, to avoid divergence over the long-term we use a full CNN approach end to end without any recurrent behavior. This led to better long-term predictions in comparison with prior works. We introduce a new metric that measures the divergence over the prediction horizon to quantitatively judge this criterion of prediction stability.</p><p>This work is organized as follows, we start with the literature review of related 2D and 3D pose estimation methods, as well as deep graph models and trajectory prediction methods. Then, we follow up with the problem formulation and description of Skeleton-Graph method. Next, we discuss the problem of inconsistency in the prediction of the 3D skeleton poses and introduce a new objective that ensures the predicted 3D skeleton looks natural. We analyze the performance of our approach both quantitatively and qualitatively and discuss the prediction stability over the long term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>2D pose estimation: With the resurgence of neural nets, data-driven prediction paradigms have become more dominant. DeepPose <ref type="bibr" target="#b36">[35]</ref> was the first major paper that applied deep learning to human pose estimation. In this approach, pose estimation is formulated as a CNN-based regression problem towards body joints. The work of <ref type="bibr" target="#b35">[34]</ref> generates heatmaps, describing the likelihood of the skeleton joints by running an image through multiple resolution banks in paral-lel to simultaneously capturing features at a variety of images scales. <ref type="bibr" target="#b26">[26]</ref> Introduced a novel and intuitive architecture that consists of steps of pooling and upsampling layers to capture information at every scale. The previous approaches work well but were complicated in comparison with the next work. The work of <ref type="bibr" target="#b40">[39]</ref> came up with a quite simple but efficient structure that consists of ResNet and few deconvolutional layers instead of the upsampling mechanism.</p><p>3D pose estimation: Recovering 3D pose from 2D RGB images is considered more difficult than 2D pose estimation, due to the larger 3D pose space and more ambiguities. The work of <ref type="bibr" target="#b21">[22]</ref> built a framework that consists of a joint points regression task and point detection task. They train a network that directly regresses 3D points from an image. Later, <ref type="bibr" target="#b6">[7]</ref> explored a simple architecture that reasons through intermediate 2D pose estimations instead of directly estimating 3D pose from an image. Further <ref type="bibr" target="#b24">[24]</ref> explored the sources of error in 3D pose estimation. They doubted the source of error is either from 2D to 3D mapping or from the improper visual analysis of the scene. They concluded that lifting the ground truth 2D joints locations to 3D space is not the source of error and it is a relatively straightforward task but the visual analysis is the main issue. However, since these methods completely ignore the image context, the predicted human motion may not be consistent with the scene. To perceive the scene context in the pose estimation task, <ref type="bibr" target="#b12">[13]</ref> exploits static 3D scene structure to better estimate human pose from monocular images considering environment constraints. A limitation of the current formulation is that it does not model scene occlusion. More recently, <ref type="bibr" target="#b5">[6]</ref> formulates a new task of long-term 3D human motion prediction with scene context in terms of 3D poses and develops a novel three-stage computational framework that utilizes scene context for goal-oriented motion prediction.</p><p>Advances in deep graph models: Recent advances in deep graph CNNs <ref type="bibr" target="#b18">[19]</ref> lead to a jump in the domain of pose and trajectory predictions <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b38">37]</ref> as graphs CNNs can exploit both the spatial configurations of the agents and their corresponding inner features. Several lines of work successfully classified human skeletons' actions by considering a spatio-temporal graph formulation of the skeleton <ref type="bibr" target="#b41">[40,</ref><ref type="bibr" target="#b16">17]</ref>. The recent work of <ref type="bibr" target="#b20">[21]</ref> proposed a multi-scale deep graph network to predict 3D human motion from 3D history.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Formulation and Technical Approach</head><p>Given a pair of observations of T time steps 2D human pose P 2D and T steps 2D images I 2D of the scene the goal is to predict the nextT steps 3D human poses P 3D . The human skeleton have J keypoints such that P 2D ? R J?2 and P 3D ? R J?3 . Also, the pose prediction problem includes a path prediction problem within it. The path prediction problem is important because it is required to keep track of the 3D poses and to warp or re-normalize them when needed. The position of the path is the center of the torso.</p><p>In what follows, we describe our approach to solve the problem at hand. We first start by modeling the problem as a spatio-temporal graph that represents the 2D motion of the human skeleton over time. Then we describe the Skeleton-Graph model wrapping with the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The formulation of the spatio-temporal graph</head><p>We start modeling the problem as a spatio-temporal skeleton graph. On each observed time</p><formula xml:id="formula_0">-step t, {t ? Z | 0 ? t ? T } we define a graph G t = (V t , E t ).</formula><p>Where V t is the graph vertices at time step t. Each vertex will hold the pose P t 2D information of the corresponding j, {j ? Z | 0 ? j &lt; J} skeleton key-point. The E t is the collection of graph edges. The adjacency matrix A t defines the weight values of the graph edges. In our formulation we set the weights to be one, later we describe in our model how we learn proper values for entries of A t . Now, we can represent our input as a spatio-temporal graph The spatio-temporal graph CNN (SPGCNN) Our design of this layer processes the graph vertices data which has the shape of T ? J ? [x, y] in two steps. The first step is a spatial step where it applies CNNs over the graph nodes weighting the CNN kernel by the values of the adjacency matrix. This spatial step takes into account the connection between the skeleton nodes by exploiting the adjacency matrix. This aligns with our design goal of using this information to enhance the results. Then, a temporal step is applied to the tensor from the spatial step. This step is a simple CNNs but acts on the time T as a features channel. We refer to the work of <ref type="bibr" target="#b41">[40]</ref> regarding more details about the spatiotemporal graph CNNs. The output of this layer is a graph embedding that represents the observed 2D skeleton motion over time and has the shape of T ? J ? F , where F is the learned features of each joint.</p><formula xml:id="formula_1">SG input = {G 2D t | t ? Z, 0 ? t ? T } and the output is {P t 3D | t ? Z, T &lt; t ?T } which is the next 3D poses.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Skeleton-Graph model</head><p>Learning a proper adjacency matrix The work of <ref type="bibr" target="#b25">[25]</ref> proposed a kernel function to weigh the adjacency matrix that is tailored to their problem. Instead, in our approach we let the model discover the best weights of the adjacency matrix <ref type="bibr" target="#b39">[38]</ref>. The adjacency matrix A has the dimensions of T ? J ? J, one can imagine it as a 2D image with T features channels. Thus, we use CNNs that take the temporal skeleton adjacency matrix as an input and learns a new one. Then, this learned adjacency? is used within the previous component of our model. In our ablation study, we show the benefit of using this learned matrix. The learned adjacency matrix? has the dimensions of T ? J ? J but differs from the original one A in terms of the entries. This can be seen in <ref type="figure">Figure 6</ref>.</p><p>Vision graph fusion As we observe 2D images I 2D , using them could be beneficial to our model. We have two options, the first as in <ref type="bibr" target="#b5">[6]</ref> to use the last observed image only I 2D T . The intuition behind using the last image is that it is the nearest observation of what is to be predicted next. We also tested the idea of using the full sequence of the observed images I 2D 0,..,T to use more visual context. Yet, empirically as we will show in the experiments section using the last image improves the path prediction while the whole sequence of images improves the pose prediction on one of the datasets. The vision feature extractor is a CNNs that is designed to down-size the images in terms of the spatial aspects (Width and height) to match the graph embedding. Then, both the image embedding and the graph embedding are concatenated. This can be seen in <ref type="figure">Figure 3</ref>. This simple concatenation gives the model the ability to learn a fused representation leveraging both visual and graph contexts. The final representation has the dimensions of T ? J ? F . This representation is to be used by the next layer to predict the nextT 3D poses.</p><p>The time-extrapolator CNN (TXCNN) This is the last step in Skeleton-Graph model. As we arrive at the embedding that represents the history of the 2D observations, we attempt to predict the nextT 3D steps. As in <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b42">41]</ref>, using CNNs was proven to be better than recurrent models as it does not result in diverged predictions. The TX-CNN receives an embedding of the history with the shape of T ? J ? F . The TXCNN treats the time as a features channel and through ordinary CNNs, it predicts the nextT 3D posesP steps. Simply, it extrapolates the time into the future moving it from T observed steps toT predicted steps. The final output of our model isT ? J ? [x, y, z] which is the predicted 3D poses.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The Skeleton Consistency Objective</head><p>During our experiments, we noticed that we have accurate results exceeding the state-of-the-art but the predicted 3D skeletons do not look natural. For example, a skeleton might be setting but the distance between the neck and body is awkward. Examples of these abnormal skeletons can be seen in <ref type="figure" target="#fig_2">Figure 4</ref>. Due to this, we introduce a loss function that forces the predicted skeletons to be more consistent. We call this loss Skeleton Consistency Loss (SCL) which enforces the correct bone length and angles between the joints in the predicted 3D poses. The skeleton consistency concept was introduced in prior works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b34">33]</ref> in which they focus on the skeleton reconstruction problem not the 3D pose prediction problem. For the bone length, the prior approaches and ours use L 2 norm to force the correct bone length. For the angles between the joints, some used the rotation angles but we preferred to use the cosine similarity because of its well-defined range. As mentioned before our consistency loss is composed of two parts. The first part is a cosine similarity between each skeleton joint J in comparison with the predicted joints. It is defined as follows:</p><formula xml:id="formula_2">SCL cos = 1 T (J?1) t?T J?1 i=1 C P t i , P t i+1 ? C P t i ,P t i+1 1</formula><p>Where C is the cosine similarity. The second part of the SCL is what enforces the reasonable bone length. It is defined as follows:</p><formula xml:id="formula_3">SCL L2 = 1 T J(J?1) 2 t?T j?J J i=j+1 P t i ? P t j 2 ? P t i ?P t j 2 1</formula><p>Thus, the SCL loss function will be defined as:</p><formula xml:id="formula_4">L SCL = ? 1 SCL cos + ? 2 SCL L2<label>(1)</label></formula><p>Where ? 1 and ? 2 are weighting parameters. We found that setting both ? 1 = 0.0005 and ? 2 = 0.1 works the best. Now, we can define the objective function that we train against: </p><formula xml:id="formula_5">L Skeleton-Graph = 1 T JT t=1 J j=1 P t j ? P t j 2 2 + L SCL (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we describe the datasets used in the training, the training settings, and evaluation metrics. Then we compare with several prior models followed by an ablation study of our model both in quantitatively and qualitatively manners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets description</head><p>GTA-IM: GTA Indoor Motion Dataset <ref type="bibr" target="#b5">[6]</ref> emphasizes on human-scene interactions. The motivation of this dataset is to fix the problem that real datasets of human-scene interaction has which is the noisy 3D human pose annotations and limited long-range human motion. The synthetic data of motions and interactions were collected from 3D video game Grand Theft Auto V by controlling characters, cameras, and the physical system. The data set contains 50 human characters acting inside 10 different large indoor scenes. The dataset includes RGBD frames with 1920 ? 1080 resolution, the corresponding ground-truth 3D human pose joints, human skeleton segmentation, and the camera parameters. We split 8 scenes for training and 2 scenes for evaluation following the settings of <ref type="bibr" target="#b5">[6]</ref>. We also transfer the 3D path into the camera coordinate frame for both training and evaluation. PROX: Proximal Relationships with Object eXclusion (PROX) is a new dataset captured using the Kinect-One sensor by <ref type="bibr" target="#b12">[13]</ref>. It contains 12 different 3D scenes with a total of 60 recorded scenarios. Each video is 30 FPS with camera parameters, calibration parameters, and human body segments. 3D skeleton points(25 joints) of the human pose are captured by Kinect-One sensor and 2D keypoints(25 joints) are captured by OpenPose. In our experiment, we split PROX dataset with 52 training sequences and 8 sequences for testing following the settings of <ref type="bibr" target="#b5">[6]</ref>. The usage of OpenPose to generate the ground truth makes it less accurate than the GTA-IM dataset. This is because the ground truth will inherit the errors of OpenPose. Also, the PROX dataset was mostly captured in a lab environment making it less diverse in terms of visual features. These flaws of the PROX dataset will impact our results as we will see in the ablation study section. A similar discussion was raised by the authors of <ref type="bibr" target="#b5">[6]</ref>. Also, having results on the PROX dataset shows the robustness of our method in the case of noisy estimated 2D poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation metrics</head><p>The main metric for evaluating the 3D path and 3D pose prediction is the Mean Per Joint Position Error (MPJPE) <ref type="bibr" target="#b17">[18]</ref>. For a frame t ?T and a skeleton with J joints , MPJPE is computed as:</p><formula xml:id="formula_6">E M P JP E (t) = 1 J J j=1 P t j ? P t j 2<label>(3)</label></formula><p>The prior formulation is used to compute the 3D pose error. In case of the 3D path error, a specif joint is chosen such as the center of the skeleton torso. We also use two common metrics that can be found in the literature of pedestrian trajectory prediction <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">25]</ref> to evaluate our performance. Though this metric only discusses the divergence over the prediction horizon and it does not indicate the accuracy of the predictions, unlike the ADE and FDE metrics.</p><formula xml:id="formula_7">ADE = 1 2 1 T T t=1 E M P JP E (t) pose + 1 T T t=1 E M P JP E (t) path (4) FDE = 1 2 E M P JP E (t =T ) pose + E M P JP E (t =T ) path (5) STB ? = 1 2 Var E M P JP E (t)|t ?T pose + Var E M P JP E (t)|t ?T path<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Training settings</head><p>For all of our experiments, we use SGD optimizer. The initial learning rate for GTA-IM is 0.01 and 0.03 for the PROX dataset. The number of training epochs is 450 and we decrease the learning rate by a factor of 0.2 every 200 epochs. We use a batch size of 128 and 1 second of observation and 2 seconds for predictions following the settings of <ref type="bibr" target="#b5">[6]</ref>. We used GTX-1080Ti for training on a 128 GB RAM machine. The need for a large RAM comes when the models are trained using the visual signal. Training each model took between 8 hours and 24 hours depending on the used dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison with Prior Methods on GTA-IM &amp; PROX Datasets</head><p>In this section, we perform quantitative evaluations of our method. The evaluation results of the 3D path and pose predictions on the GTA-IM dataset are shown in <ref type="table" target="#tab_2">Table 1</ref> while the PROX dataset results are in <ref type="table" target="#tab_3">Table 2</ref>. Overall, Skeleton-Graph outperforms the prior methods on several metrics. For the FDE we are 105 mm more accurate than GPP-Net <ref type="bibr" target="#b5">[6]</ref> on the GTA-IM dataset and 110 mm more accurate than GPP-Net on the PROX dataset. This shows that we did not accumulate error over the long-term prediction, unlike prior methods. The ADE is slightly better than the previous stateof-the-art GPP-Net by 10mm. This means, on average we have a more accurate 3D path and pose predictions. For the divergence in the long-term, the STB ? has a drastic drop in comparison with prior works. We are 88% better on the GTA-IM dataset and 93% better on the PROX dataset. This can be seen in the tables where our 0.5 seconds are close to the 2 seconds prediction in terms of MPJPE which means no divergence happening in the long-term, unlike prior works. We also notice that our model on the 0.5 seconds horizon in GTA-IM does not perform better than the prior methods. The same notice can be seen in the PROX dataset results. We highlight this as a trade-off between accuracy over short-term prediction versus the stability of prediction over the longterm due to not using recurrent approaches. As discussed in the introduction recurrent approaches tend to be accurate in the short term. For example, we notice that prior methods tend to be accurate over the short-term but diverge drastically over the long term, unlike our model. Overall, though our model behaves like this in the short term, the overall average performance is still better than the prior methods by at least 27% and 4% on both the FDE and ADE metrics, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Ablation Study</head><p>In this section, we extensively analyze the different behaviors of Skeleton-Graph model in both quantitative and qualitative ways. We start with a quantitative analysis of the different configurations of our model such as the adjacency learning and the skeleton consistency loss. We follow up with a visual analysis of these components and their effect on the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Quantitative analysis of model components</head><p>In this section we study the different components of Skeleton-Graph model. The components are: Learning adjacency CNN +?, image embedding +I 2D T , video embedding +I 2D</p><p>1..T and the two different components of our skeleton consistency loss SCL cos , SCL L2 . <ref type="table">Table 3</ref> shows an evaluation of these configuration on both GTA-IM and PROX datasets.</p><p>Plain Skeleton-Graph model: We directly train the raw skeleton-graph model without self-learning adjacent matrix, consistency constraints, and visual signal. This raw model alone outperforms the previous work on both the GTA-IM and PROX datasets on both FDE and ADE metrics. This validates that the graph approach we utilize in our model and the full CNN approach can remarkably decrease predictive MPJPE in the pose prediction task. We also notice that the results are divergence-free along the GTA-IM and PROX datasets with STB ? that is better than prior methods.</p><p>Skeleton-Graph with learning adjacency +?: The work of <ref type="bibr" target="#b25">[25]</ref> suggested that the kernel function that governs the value of the adjacency matrix influences the results of the trajectory prediction significantly. Overall, the adjacency matrix is important in GCNNs because it governs the interaction between the graph nodes. Instead of searching for a handmade kernel function, we decided to let the model discovers the proper weights for the adjacency matrix. From <ref type="table">Table 3</ref> we can see that the usage of the learned adjacency matrix STB ? improves the performance in comparison with the plain model. This indicates that the model discovered a better interaction between the graph nodes. <ref type="figure">Figure 6</ref> illustrates a heat map of the learned adjacency STB ? . We notice an increase in the connections between the two legs joints, the same happens between the two hands joints. This em-   phasizes that the motion of humans has a strong pattern that is related to the coordination between both the hands and the legs. We also notice that the spine connections are almost the same as the original skeleton joints. This indicates that the spine does not contribute too much to the motion pattern. Skeleton-Graph +? + Skeleton Consistency Loss (SCL): Though the model with the learned adjacency matrix performs well, the model will generate many weird human poses without the consistency constraints. We show 3 failure cases in <ref type="figure" target="#fig_2">Figure 4</ref> where the angles and distances between joints are out of the normal range of a human body. We add cosine similarity+SCL cos and joints distance+SCL L2 in the loss function and fine-tune hyper-parameters(optimal weights) to combine the three terms of loss: predictiontarget loss, cosine similarity loss, and norm loss. We get improved performance by using these constraints as shown in <ref type="table">Table 3</ref> on the GTA-IM dataset. On the other hand, in the PROX dataset, it seems only the+SCL L2 enhances the performance. This is because of the nature of the PROX dataset. The PROX dataset ground truth is machine-generated so it is not accurate when compared with the GTA-IM dataset which the ground truth is obtained from the game directly. This is behavior was seen before in the work of <ref type="bibr" target="#b5">[6]</ref>. However, adding consistency loss does increase the training time in comparison with the plain model. Yet, it results in more accurate FDE and ADE results and more natural-looking skeletons as we will see in the qualitative study section.</p><p>Skeleton-Graph +?+SCL+ visual signal: We start with the GTA-IM dataset. The prior work of <ref type="bibr" target="#b5">[6]</ref> shown that using the visual signal of the last observed frame enhances the performance. From <ref type="table">Table 3</ref> we notice that using the last frame +I 2D T did enhance the short-term 3D path error in comparison with the previous components on the GTA-IM dataset. Yet, it did not enhance the short-term 3D pose error. When we used the full sequence of observed images +I 2D 0..T on the GTA-IM dataset, the 3D pose error was the lowest among all the modes of configurations. This indicates that it helped predicting the 3D poses. Yet, for the 3D path, it had the highest error among all the configurations. This shows a trade-off between the path and poses objectives influenced by the presence of the visual signal. Overall, using the vision signal resulted in a performance that is similar to the usage of the consistency objective on the ADE and FDE metrics with a noticeable inner performance enhancement in the short term of both path and pose predictions. For the PROX dataset using the vision signal either the last image, +I 2D T or the full sequence +I 2D 1..T resulted in a divergence of the results. This aligns with the findings of <ref type="bibr" target="#b5">[6]</ref> over the PROX dataset. In addition, the PROX were captured in an empty lab environment, and thus it no as rich as the GTA-IM dataset. This made the dataset to be less diverse in terms of the background and the camera poses. So inherently the vision signal becomes less useful leading to ambiguity in the predictions as seen in <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Qualitative study of the models' components</head><p>To understand the effect of each configuration in our model, we visualize the predicted 3D human poses per each configuration as shown in <ref type="figure" target="#fig_4">Figure 5</ref>. Starting from the standard model with self-learning adjacent matrix alone +?. The entire predicted pose looks natural in some areas and unnatural in other areas. The predicted right leg stretches out naturally and the left leg bends just like the target. However, the left hand in the pose is too close to the head and looks weird since there is no norm loss SCL L2 in the objective function. After we add norm loss(seen on the top right), the prediction has a reasonable left hand but the right leg joints penetrate the ground which is not possible in reality. Once the cosine loss being added SCL cos to the objective, the penetration problem is gone but the distances between joints  <ref type="table" target="#tab_2">GTA-IM   150  165  175  191  198  211  220  234  213  193  16  159  169  178  192  196  208  216  229  211  193  14  161  169  176  187  199  209  217  230  208  193  10  159  169  177  191  191  204  212  225  208  191  14  159  168  177  190  193  205  213  226  208  191  11  154  163  172  186  198  209  217  230  208  191  11  165  174  181  193  190  202  211  224  208  192  11   PROX   340  348  353  360  369  375  379  386  373  364  7  309  314  317  323  335  339  342  347  335  328  5  280  287  290  296  297  303  307  314  305  296  5  264  269  272  277  281  287  291  298  288  280  6  293  299  302  308  292  298  302  308  308  300  5  353  353  355  360  358  362  365  370  365  359  3  283  290  294  301  308  314  319  325  313  304  6   Table 3</ref>: Skeleton-Graph ablation. +? is the model with learned adjacency. SCL cos and +SCL L2 indicates the usage of the SCL components. The usage of last image is +I 2D T , +I 2D 1..T indicates the usage of whole observed sequence. All results are in mm, the lower the better. The (0.5, 1, 1.5, 2) are time steps in seconds. Bolded numbers are the best in each column.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We showed that the usage of graph CNNs with selflearning adjacency matrix and formulating the problem as a spatio-temporal graph is suitable for the problem of 3D skeleton motion predictions. We achieved state-of-the-art results on well-known benchmarks. The design of our model results in divergence-free predictions in the long term, unlike prior works. This was shown using the introduced STB ? metric. The deformation in prediction was solved using a skeleton consistency loss. The integration of the vision signal improved the results on the GTA-IM dataset. In the future, we would like to target the short-term prediction accuracy issue and investigate different methods for integrating the visual signals. <ref type="figure" target="#fig_2">Figure 4</ref> illustrates several failure cases by our model. In case (1) we notice a skeleton sitting but our model was not able to capture the torso crouch to represent this mode. Case (2) the predicted skeleton trajectory overshoots, in other terms the model predicted too much of a momentum from the history of the observations. Case <ref type="formula" target="#formula_6">(3)</ref> and <ref type="formula" target="#formula_8">(4)</ref> are complex situations where the target skeleton changes from sitting to standing, we notice our model was not able to capture this trend. This is probably because of the lack of history that represents this trend in the 2D observations. Case <ref type="bibr" target="#b4">(5)</ref> and <ref type="formula" target="#formula_7">(6)</ref> is a skeleton sitting or relaxing on an object, though our prediction are close, they look abnormal. This happens because in these two cases the joints angles vary a lot in which the predictions become harder. We believe in the future adding more dynamics oriented constraints can enhance these prediction errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Failure Cases</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Skeleton-Graph Model Configuration</head><p>We found out that the number of TXCNN and SPGCNN layers affects the model performance significantly. <ref type="table" target="#tab_6">Table 5</ref> we show the effect with the number of TXCNN and SPGCNN on the model performance. We used residual connection while going deeper using this layers. First, we notice that going deeper with the number of SPGCNN results in a significant performance drop on both path and pose estimation. This is the same behavior noted in <ref type="bibr" target="#b19">[20]</ref>. Going deeper with the number of TXCNNs beyond 11 layers resulted in a drop of the performance. We notice that the performance for both 9 and 11 TXCNN layer are close to each other. Also, the result of the ablation is the same on both PROX and GTA-IM datasets. This means that our model is expected to behave in the same way on different datasets, a kind of agnostically to the dataset. <ref type="table">Table 6</ref> shows the inner details of our model. The structure of each component in terms of the parameters of the CNN layers, the usage of batch norm and the location of the activation functions are all shown in this table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model Architecture</head><p>(1)  </p><formula xml:id="formula_8">(2) (3)<label>(4)</label></formula><formula xml:id="formula_9">(5)<label>(6)</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>From a top</head><label></label><figDesc>view, our model consists of four main components. The spatio-temporal graph CNN (SPGCNN) receives the spatio-temporal graph of the observed motion and generates a representation for it. The second component lies within the SPGCNN component. It is a CNNs that learns an adjacency matrix based on the temporal skeleton structure. The third component is the vision graph fusion. This component mixes both the graph representation from SPGCNN and the visual signal from the vision extractor. Lastly, the time-extrapolator CNN (TXCNN) receives this fused representation of both visual and graph and predicts the next 3D poses. In the upcoming section, we explain each component separately. These components can be seen in Figure 2. Our code is open-sourced and includes all the implementation details of the model. The supplementary materials include the fine implementation details of each component.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Skeleton-Graph model components. The model receives as an input a 2D skeleton temporal T graph poses V, A and predicts the the next 3D skeleton temporal poses. The model can learn a suitable adjacency? matrix through the adjacency CNN. Also, it can use the observed visual signal in the form of a still image or a video. The time-extrapolator CNN is responsible for predicting the nextT temporal 3D poses, while the spatio-temporal graph CNN processes the 2D temporal graphs. J, F, C, [x, y, z] are the number of the skeleton joints, the learned features dimensions, the vision features channels and the joint coordinates, respectively. Vision graph fusion. C, W, H are the image channels, width and height.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Different cases of deformation of the predicted 3D skeletons. Arrow indicates the time direction. The first two poses (a) and (b) show awkward joint angles between each joint. The case (c) one the right shows failure in both predicted angle and bone length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>First, the Mean Average Displacement Error(ADE) defined in equation 4 which judges the overall performance of both pose and path errors overall the predicted trajectory. Second, the Mean Final Displacement Error(FDE) which judges the performance at the final time step of the trajectory. The FDE is an indicator of the accumulation of the errors in the prediction, in other terms a low FDE means fewer errors were accumulated. The FDE defined in equation 5. Lastly, we define a measure for the stability over the prediction horizon STB ? Equation 6. It is the average error for both path and pose predictions. If the predictions deviate or accumulate errors over the long-term this metric increases and vice versa.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Effect of different configurations of Skeleton-Graph model on the 3D pose predictions. Arrow indicates the time direction. Top left:Skeleton-Graph model with learned adjacency +?; Bottom left: Skeleton-Graph model +? + weighted L SCL ; Top right: Skeleton-Graph model +? +SCL L2 only; Bottom right: Skeleton-Graph model +? +SCL cos .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>0 1 Figure 6 :</head><label>16</label><figDesc>Heat map of the learned adjacency matrix? versus the original adjacency that represents the connection between the skeleton joints.become unnatural especially in the first five frames(seen on the bottom right). When both SCL L2 and SCL cos are used in training our model the resulted skeletons look very natural as shown in the bottom left in Figure 5. More qualitative cases are in the supplementary materials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results of the GTA-IM dataset. Results of 3D path and pose MPJPE error are reported in mm. The lower, the better.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">3D path error (mm)</cell><cell></cell><cell></cell><cell cols="2">3D pose error (mm)</cell><cell></cell><cell></cell><cell>?</cell><cell></cell></row><row><cell>Time step (second)</cell><cell>0.5</cell><cell>1</cell><cell>1.5</cell><cell>2</cell><cell>0.5</cell><cell>1</cell><cell>1.5</cell><cell>2</cell><cell>FDE</cell><cell>ADE</cell><cell>STB?</cell></row><row><cell>TR [36]</cell><cell>487</cell><cell>583</cell><cell>682</cell><cell>783</cell><cell>512</cell><cell>603</cell><cell>698</cell><cell>801</cell><cell>792</cell><cell>644</cell><cell>126</cell></row><row><cell>TR [36] + VP [27]</cell><cell>262</cell><cell>358</cell><cell>461</cell><cell>548</cell><cell>297</cell><cell>398</cell><cell>502</cell><cell>590</cell><cell>569</cell><cell>427</cell><cell>126</cell></row><row><cell>VP [27] + LTD [38]</cell><cell>194</cell><cell>263</cell><cell>332</cell><cell>394</cell><cell>216</cell><cell>274</cell><cell>335</cell><cell>394</cell><cell>394</cell><cell>300</cell><cell>82</cell></row><row><cell>GPP-Net [6]</cell><cell>189</cell><cell>245</cell><cell>317</cell><cell>389</cell><cell>190</cell><cell>264</cell><cell>335</cell><cell>406</cell><cell>398</cell><cell>292</cell><cell>90</cell></row><row><cell>Skeleton-Graph (ours)</cell><cell>264</cell><cell>269</cell><cell>272</cell><cell>277</cell><cell>281</cell><cell>287</cell><cell>291</cell><cell>298</cell><cell>288</cell><cell>280</cell><cell>6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results of the PROX dataset. Results of 3D path and pose MPJPE error are reported in mm. The lower, the better.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Several failure cases predicted by Skeleton-Graph . The green skeletons represents ground-truth and red skeleton represents prediction. The deeper color the skeleton is, the earlier moment it is corresponding to.</figDesc><table><row><cell>Dataset</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>7</cell><cell>9</cell><cell>11</cell></row><row><cell></cell><cell cols="6">1 320/362 267/290 211/247 221/250 213/237 230/245</cell></row><row><cell>GTA-IM</cell><cell cols="6">3 262/296 252/278 230/251 198/246 212/239 210/225 5 272/298 273/277 234/257 206/234 203/241 202/231</cell></row><row><cell></cell><cell cols="6">7 271/295 264/288 233/263 208/250 233/260 233/260</cell></row><row><cell></cell><cell cols="6">1 413/455 361/373 340/319 339/361 367/314 336/347</cell></row><row><cell>PROX</cell><cell cols="6">3 462/454 433/506 362/397 403/372 346/345 376/367 5 499/416 397/441 426/414 439/468 741/443 351/274</cell></row><row><cell></cell><cell cols="6">7 582/679 455/420 400/408 420/426 409/484 371/351</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>The effect of the choice of the number of TXCNN and SPGCNN on prediction results. The row is for the number of TXCNN layer while the column is for the number of SPGCNN layers. All readings are in mm, the lower the better. The numbers are Path/Pose errors. The error is over 2 seconds prediction horizon.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement: We thank the reviewers for their feedback. This research was supported by NSF (National Science Foundation) CPS No.1739964 and the CAMMSE UTC (US DOT).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Social lstm: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="961" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recent developments on 2d pose estimation from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artur</forename><surname>B?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Kulbacki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Segen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawid</forename><surname>Swi?tkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamil</forename><surname>Wereszczy?ski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Intelligent Information and Database Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d human pose estimation via deep learning from 2d annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernesto</forename><surname>Brau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="582" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><forename type="middle">Magnenat</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2272" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long-term human motion prediction with scene context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Zhi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3d human pose esti-mation= 2d pose estimation+ matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7035" to="7043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uday</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safeer</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="668" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vehicle pose and shape estimation through multiple monocular vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huihuan</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Biomimetics (ROBIO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="709" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient multi-person hierarchical 3d pose estimation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renshu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaoang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenq-Neng</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="163" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient multi-person hierarchical 3d pose estimation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renshu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaoang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenq-Neng</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="163" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Social gan: Socially acceptable trajectories with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2255" to="2264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Resolving 3d human pose ambiguities with 3d scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Act: An autonomous drone cinematography system for action scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7039" to="7046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to capture a film-look video with a camera drone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting Tim</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1871" to="1877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stgat: Modeling spatial-temporal interactions for human trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingfan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huikun</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlu</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoqi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6272" to="6281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatio-temporal inception graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2122" to="2130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic multiscale graph neural networks for 3d skeleton based human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="332" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikanth</forename><surname>Malla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behzad</forename><surname>Dariush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.04853,2020.3</idno>
		<title level="m">Socialstage: Spatio-temporal multi-modal future trajectory forecast</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Social-stgcnn: A social spatio-temporal graph convolutional neural network for human trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abduallah</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Kun Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Claudel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural network 3d body pose tracking and prediction for motion-to-photon latency compensation in distributed virtual reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Pohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armin</forename><surname>Becher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Grauschopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Axenie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="429" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reconstructing 3d human pose from 2d image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<biblScope unit="page" from="573" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<idno>2012. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">David Mateo Rojas, and Andr?s Grimaldos. 3d object pose estimation for robotic packing applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Ch Rodriguez-Garavito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Camacho-Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><forename type="middle">Viviana</forename><surname>?lvarez-Mart?nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cardenas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Engineering Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="453" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Mocap-guided data augmentation for 3d pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?gory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02046</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bogdan Ionescu, and Ioannis A Kakadiaris. 3d human pose estimation: A review of the literature and analysis of covariates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Boteanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Motionet: 3d human motion reconstruction from monocular video with skeleton consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyi</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kfir</forename><surname>Aberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Aristidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graphtcn: Spatio-temporal interaction modeling for human trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofeng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning trajectory dependencies for human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Miaomiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salzemann</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Hongdong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Noticing motion patterns: Temporal cnn with a novel convolution operator for human trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Human motion capture using a drone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE international conference on robotics and automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2027" to="2033" />
		</imprint>
	</monogr>
	<note>Georgios Pavlakos, Vijay Kumar, and Kostas Daniilidis</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Component Layer name / description Layer strucutre Spatio-Temporal Graph CNN Input CNN(2,3,k=3,p=1) Graph GraphCNN(3,3) Learn Adjacency CNN(3,3) BN,PReLU,CNN(2,3,k=3,p=1)</title>
	</analytic>
	<monogr>
		<title level="j">BN Temporal CNN CNN</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>PReLU,CNN(2,3,k=3,p=1),BN</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<title level="m">Vision Features Extractor CNN(3,6,k=3,p=1), BN, PReLU, CNN(6,9,k=3,p=1,s=2), BN, PReLU, CNN</title>
		<meeting><address><addrLine>BN, PReLU</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
	<note>k=3,p=1,s=2), BN, PReLU, CNN(12,15,k=3,p=1,s=2), BN, PReLU, CNN(15,18,k=3,p=1,s=2), BN, PReLU, CNN(18,21,k=3,p=1,s=2)</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Concatenate Concatenate spatio-temporal graph CNN output with the vision features extractor output</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">BN, PReLU Middle CNN(T ,T ,k=3,p=1), BN, PReLU + residual Output CNN(T ,T ,k=3</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Skeleton-Graph architecture description. CNN = Convolutional Neural Layer, k= kernel, p= padding, s= stride, BN= Batch Normalization, PReLU = Parametric ReLU activation function, T= observed time steps, C= vision signal features channels andT = predicted time steps</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
