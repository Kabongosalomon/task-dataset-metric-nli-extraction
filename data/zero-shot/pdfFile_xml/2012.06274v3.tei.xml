<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000. A Topic Coverage Approach to Evaluation of Topic Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damir</forename><surname>Koren?i?</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Division of Electronics</orgName>
								<orgName type="institution">Rudjer Bo?kovi? Institute</orgName>
								<address>
									<postCode>10000</postCode>
									<settlement>Zagreb</settlement>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Strahil</forename><surname>Ristov</surname></persName>
							<email>ristov@irb.hr</email>
							<affiliation key="aff0">
								<orgName type="department">Division of Electronics</orgName>
								<orgName type="institution">Rudjer Bo?kovi? Institute</orgName>
								<address>
									<postCode>10000</postCode>
									<settlement>Zagreb</settlement>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jelena</forename><surname>Repar</surname></persName>
							<email>jelena.repar@irb.hr</email>
							<affiliation key="aff1">
								<orgName type="department">Rudjer Bo?kovi? Institute, Division of Molecular Biology</orgName>
								<address>
									<postCode>10000</postCode>
									<settlement>Zagreb</settlement>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>?najder</surname></persName>
							<email>jan.snajder@fer.hrcorrespondingauthor:damirkoren?i?e-mail:dkorenc@irb.hr.</email>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Electrical Engineering and Computing</orgName>
								<orgName type="institution">University of Zagreb</orgName>
								<address>
									<postCode>10000</postCode>
									<settlement>Zagreb</settlement>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000. A Topic Coverage Approach to Evaluation of Topic Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ACCESS.2021.3109425</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>INDEX TERMS Topic coverage</term>
					<term>Topic coherence</term>
					<term>Topic discovery</term>
					<term>Topic models</term>
					<term>Topic model evaluation</term>
					<term>Topic model stability</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Topic models are widely used unsupervised models capable of learning topics -weighted lists of words and documents -from large collections of text documents. When topic models are used for discovery of topics in text collections, a question that arises naturally is how well the model-induced topics correspond to topics of interest to the analyst. In this paper we revisit and extend a so far neglected approach to topic model evaluation based on measuring topic coverage -computationally matching model topics with a set of reference topics that models are expected to uncover. The approach is well suited for analyzing models' performance in topic discovery and for large-scale analysis of both topic models and measures of model quality. We propose new measures of coverage and evaluate, in a series of experiments, different types of topic models on two distinct text domains for which interest for topic discovery exists. The experiments include evaluation of model quality, analysis of coverage of distinct topic categories, and the analysis of the relationship between coverage and other methods of topic model evaluation. The paper contributes a new supervised measure of coverage, and the first unsupervised measure of coverage. The supervised measure achieves topic matching accuracy close to human agreement. The unsupervised measure correlates highly with the supervised one (Spearman's ? ? 0.95). Other contributions include insights into both topic models and different methods of model evaluation, and the datasets and code for facilitating future research on topic coverage.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Topic models <ref type="bibr" target="#b0">[1]</ref> are unsupervised models that take as input a collection of text documents and learn a set of topics, constructs represented as weighted lists of words and documents. A topic of a topic model is expected to be interpretable as a concept, i.e., correspond to human understanding of a topic occurring in texts. Examples of interpretable model topics can be found in <ref type="table" target="#tab_0">Table  1</ref>. Topics can help an analyst gain insight into textual content, or they can be used to create topic-based representations of words and documents for downstream applications. Since they were introduced, topic models became a popular text analysis and processing tool with numerous applications, including exploratory text analysis <ref type="bibr" target="#b1">[2]</ref>, information retrieval <ref type="bibr" target="#b2">[3]</ref>, natural language processing <ref type="bibr" target="#b3">[4]</ref>, and topic discovery <ref type="bibr" target="#b4">[5]</ref>.</p><p>Although widely used, topic models are prone to random variations and errors due to the stochastic nature of the learning algorithms. In order to mitigate this problem, a number of topic model evaluation methods has been devised <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b11">[12]</ref>. These methods aim to provide tools and metrics for the analysis of topic models and for the construction of models with interpretable topics. For example, models can be evaluated using measures of topic coherence <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, or using measures of model stability -a property of consistent inference of similar topics <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>.</p><p>This paper upgrades an approach to topic model evaluation based on the notion of topic coverage <ref type="bibr" target="#b9">[10]</ref>, i.e., on measuring how well the topics of a topic Examples of interpretable topics of topic models built from a dataset of news texts (top) and biological texts (bottom). Each topic is characterized by top-weighted topic words. Top-weighted topic documents are displayed for the last topic of each dataset . Topic labels are the result of human interpretation. We note that all the biological topics correspond to concepts of phenotypes (organism characteristics).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic label</head><p>Top-10 topic words China china, chinese, beijing xi, russia, asia, global, region, asian, jinping Boston Bombing Trial tsarnaev, boston, bomb, marathon, tamerlan, dzhokhar, trial, penalty, brother, juror Climate Change climate, warming, global, scientist, rise, science, scientific, temperature, inhofe "Al Gore says climate change deniers should pay 'a Price' " "Smaller percentage of Americans worry about global warming now than in 1989" "Florida officials say they were banned from saying 'Climate Change' . . . "</p><p>Spore-forming spore, sporulate, endospore, germinating, vegetative, survive, coat, forespore Milk Fermentation dairy, milk, cheese, starter, yogurt, flavor, lactose, ferment, ripen, food Radiat. &amp; Desicc. Tolerant radiation, repair, desiccation, ionizing, desert, damage, irradiated, radiation-resistant "Deinococcus gobiensis: Insights into the Extreme Environmental Adaptations" "Deinococcus maricopensis is an aerobic, radiation-resistant, Gram-positive . . . " "Deinococcus radiodurans is an extremophilic bacterium, one of the most . . . " model cover a set of pre-compiled concepts. In <ref type="bibr" target="#b9">[10]</ref> the authors describe a method for measuring and visualizing several types of relations between model topics and a set of concepts defined by domain experts. The correspondence of topics to concepts is referred to as "domain relevance", and a concept is considered covered if there exists a matching model topic <ref type="bibr" target="#b9">[10]</ref>. The experiments in <ref type="bibr" target="#b9">[10]</ref> demonstrate the potential of the coverage approach for performing automatic analysis of both topic models and measures of model quality, and show that the relations between concepts and topics depend on models' types and hyperparameters. Despite the demonstrated potential, there is no follow-up work focused on coverage-based evaluation methods. Our work approaches topic coverage as a method of quantitative evaluation rooted in the use case of topic discovery. We propose new, reliable, and practical measures of coverage, and perform a series of experiments on two different datasets. The experiments lead to practical recommendations for topic modeling and provide insights into both topic models and other methods of model evaluation. By providing new measures and the first publicly available 1 coverage datasets and tools, we facilitate future research on both topic coverage and novel methods for topic model evaluation. <ref type="bibr" target="#b0">1</ref> https://github.com/dkorenci/topic_coverage In summary, our work contributes the following:</p><p>? New measures of coverage, including the first unsupervised coverage measure, ? Recommendations for the use of topic models, including the experimental support for the use of the NMF model <ref type="bibr" target="#b13">[14]</ref>, ? Insights into topic models, including the relationship between coverage, the number of model topics, and the size of reference topics, ? Insights into other methods of topic model evaluation, including the inability of the standard measures of coherence and stability to detect highcoverage models, ? Coverage datasets and the source code of the measures and the experiments. The analysis of topic coverage is based on a set of reference topics and on measures of coverage that compute how well the model topics match the reference topics. Reference topics represent the topics of interest that topic models are expected to discover. Once a set of reference topics is compiled, coverage of reference topics by a single topic model instance is the proportion of reference topics covered by model topics. A single reference topic is covered if there exist one or more matching model topics. We use the term "reference topic" instead of the term "reference concept" used in <ref type="bibr" target="#b9">[10]</ref> in order to emphasize that a reference topic is a construct represented in the same way as a model topic -as a weighted list of words and documents.</p><p>The workflow of coverage-based model evaluation consists of three steps. In the first step a set of reference topics is constructed. In the second step a set of topic models is built, expectedly by varying model types and hyperparameters. In the third step the measures of coverage are applied to topic model instances and the coverage scores are analyzed. In the case of using coverage to analyze other measures of model quality, these measures are applied to topic models and their scores are correlated with the output of the coverage measures.</p><p>The coverage approach described in this paper evaluates the models' performance in the process of topic discovery, a prominent application of topic models. During the process of topic discovery an analyst examines and interprets the topics of topic models in order to find useful topics that can offer insight and be used for subsequent text analysis. Topic discovery with topic models has been applied, inter alia, in news analysis <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, political science <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, neuroscience <ref type="bibr" target="#b18">[19]</ref>, and biology <ref type="bibr" target="#b19">[20]</ref>. <ref type="table" target="#tab_0">Table 1</ref> contains examples of topics of interest in an analysis of news issues, and topics useful for an analyst interested in biological concepts.</p><p>From the perspective of topic discovery, coverage achieved by a topic model simply quantifies how useful the model would be to an analyst interested in discovering the reference topics. In case of an exploratory analysis carried out to obtain a broad topical overview, an example set of reference topics would contain highlevel topics covering important aspects of texts. In a more focused analysis, the reference topics would correspond to more specific topics of interest. We note that our approach to coverage is focused primarily on discovering reference topics, and that a reference topic might match more than one model topic. This situation can occur in practice but it does not imply a degradation of models' performance. In other words, two topic models that cover the same number of reference topics relay the same amount of useful information to the analyst.</p><p>In this paper, the design and evaluation of coverage measures and the coverage experiments are carried out on two datasets. These datasets represent two different domains for which interest for topic discovery exists: journalistic news text and biological text. Each of the two datasets consists of a text corpus, a set of reference topics, and a set of topic models. Each set of reference topics is based on the output of an existing topic discovery study. The two datasets are described in detail in Section II.</p><p>The measures of coverage are a basis of a coverage experiment -their reliability determines the reliability of the results, and ease of their construction influences the feasibility of the experiments. Therefore the main contribution of our work consists of the new measures of topic coverage. Measure of coverage proposed in <ref type="bibr" target="#b9">[10]</ref> matches model topics with reference topics via a probabilistic model fitted on data derived from human scores of topic matching. However, the model and the process of its construction are complex, the topic matching scores are crowdsourced from non-experts asked to assess similarity of scientific topics, and the measure is not validated. While the described measure may be used to demonstrate the coverage approach and the related visualization apparatus, it is hard to reproduce and not suitable for calculation of reliable coverage scores.</p><p>We propose a conceptually simple measure of coverage, described in Section III-A, that matches model and reference topics using a standard binary classifier based on a small set of distance-based features. The classifier is trained on a dataset of topic pairs labeled by trained annotators acquainted with the topic semantic, and it achieves matching performance close to human agreement.</p><p>Supervised measures of coverage rely on human annotation of topic pairs, a time-consuming process that hinders quick application of these measures on new datasets. Therefore an important contribution of this paper is the measure described in Section III-B, which is the first unsupervised measure of coverage. It uses topic distance as a criterion for topic matching and operates by integrating a range of coverage scores calculated for a range of distances. We show that this measure has a very high rank correlation with our supervised measure. The unsupervised measure can be effortlessly deployed on new datasets and used for model selection and evaluation by way of ranking a set of topic models. Furthermore, the measure is based on a curve that is a useful tool for visual analysis and comparison of topic models.</p><p>The two proposed measures have applications beyond coverage, which we demonstrate in Section VII-B3 by adapting them to measure model stability. The stability measure based on supervised matching provides an experimental support for the interpretation of stability as the property of consistent uncovering of the same concepts. The stability measure based on the adaptation of the unsupervised coverage measure correlates almost perfectly with a standard stability measure while being much faster to compute.</p><p>The evaluations of topic models that we perform lead to recommendations for the choice of topic models used in topic discovery. The experiments in Section IV, where we evaluate coverage of topic models of different types, show that the NMF model <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b20">[21]</ref> is a good default choice for topic discovery due to high coverage, its ability to precisely pinpoint the reference topics, and consistent performance on both datasets. The experiments in Section V, where we measure the coverage of reference topics divided into size categories, support the use of larger models with more topics. These findings have practical implications since the LDA model <ref type="bibr" target="#b0">[1]</ref> VOLUME <ref type="bibr" target="#b3">4,</ref><ref type="bibr">2016</ref> with a modest number of topics is often a default choice for topic discovery.</p><p>Our research of the neglected coverage approach also contributes to the broader field of topic model evaluation. Namely, the amount of work in this field is modest in comparison with the amount of research focused on new model architectures. On the other hand, there are still no satisfactory methods for automatic semantic validation of topic models <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b21">[22]</ref>. This hinders the applicability of topic models in computational social sciences <ref type="bibr" target="#b11">[12]</ref> and for expert analysis of text collections <ref type="bibr" target="#b21">[22]</ref>. Popular measures of topic coherence <ref type="bibr" target="#b8">[9]</ref> are designed to correlate with human coherence scores <ref type="bibr" target="#b10">[11]</ref>, but it is unclear how well they correlate with models' performance in practice <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Only recently has an experimental validation of coherence measures been performed <ref type="bibr" target="#b21">[22]</ref>, and it revealed that these measures are not a reliable guide for model selection <ref type="bibr" target="#b21">[22]</ref>. Another approach to automatic model evaluation is based on measures of stability, a property of consistent inference of highly similar models. Stability is claimed to be a desirable property of models applied in computational social sciences <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> or for topic discovery <ref type="bibr" target="#b12">[13]</ref>. However, to the best of our knowledge no validation of stability measures has been performed.</p><p>In contrast to the topic coherence and model stability measures, which express abstract model qualities, coverage is grounded in the use case of topic discovery and the coverage scores are interpretable in terms of a match with ground truth reference topics -a set of interpretable topics of interest to an analyst. On the other hand, coverage-based evaluation relies on a fixed topic modeling scenario, defined by a text collection (model input) and a set of reference topics (model output). Therefore coverage cannot be applied for model selection in future applications, but rather for large scale automatic analysis of both topic models and measures of model quality. While the measures, experiments, and datasets we contribute are a starting point, the findings based on analysis of coverage will become more robust as new datasets representing new application settings are constructed and made available.</p><p>Experiments in this paper provide new data points that improve the understanding of both topic models and measures of models quality. In Section V we measure the coverage of reference topics divided into size categories. The results show that small models with fewer topics can cover only large (frequently occurring) reference topics, while the larger models are able to uncover topics of all sizes. The experiment described in Section VI is motivated by the use case of news topic discovery in social sciences. We measure coverage of news topics categorized as either corresponding to a news issue or not, and as being either abstract or concrete. The experiment demonstrates that semantic categories of topics influence their coverage by topic models.</p><p>In Section VII we apply the measures of coverage to analyze the measures of topic coherence <ref type="bibr" target="#b8">[9]</ref> and model stability <ref type="bibr" target="#b7">[8]</ref>. Comparison of coverage and topic coherence, performed in Section VII-A, shows that no strong and consistent correlation between the two exists. These results are consistent both with a prior comparison of coverage and coherence <ref type="bibr" target="#b9">[10]</ref>, and with a recent study that validates coherence measures <ref type="bibr" target="#b21">[22]</ref>. The experiments in Section VII-B, examining the relation between coverage and model stability, are to the best of our knowledge the first attempt to semantically analyze measures of stability. The experiments show no correlation between the two, demonstrating that model stability does not necessarily imply model quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DATASETS</head><p>We perform the coverage experiments on two distinct text domains -news text and biological text. For each text domain we construct a dataset, which consists of three components: a text corpus, a set of reference topics, and a set of topic models. Such a dataset is the basis for coverage experiments and the construction of coverage measures. We refer to these two datasets as the news dataset and the biological dataset.</p><p>The two datasets represent two different text genres: journalistic texts describing political news and expert biological texts describing microorganisms. The datasets are based on text corpora and reference topics from two previous experiments in which topic models were used for topic discovery on news <ref type="bibr" target="#b24">[25]</ref> and biological <ref type="bibr" target="#b19">[20]</ref> text. The reference topics therefore represent useful output of topic discovery, while being representative of concepts discoverable by standard topic models. The set of topic models built for each dataset contains models of standard types, configured with varying number of topics. We next describe the three components of the two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. TEXT CORPORA</head><p>A text corpus is a basis of a dataset since both the reference topics and the topic models are derived from the corpus texts. The dictionary and the document index associated with a corpus are a basis for representing the reference and model topics as topic-word and topicdocument vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a: News corpus</head><p>The news dataset is based on the corpus of mainstream US political news collected by <ref type="bibr" target="#b24">[25]</ref> for evaluating topic model approaches to news agenda analysis. The texts were collected from popular news sites during a three-month period, after which filtering of non-news texts and deduplication was performed, resulting in a total of 24.532 texts. Topic modeling was preceded by text preprocessing that consisted of stop-word removal, morphological normalisation, and removal of low-and highfrequency words. The final dictionary contains 23.155 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b: Biological corpus</head><p>The basis of the biological dataset is the corpus of texts about bacteria and archea microorganisms used for the discovery of phenotype topics <ref type="bibr" target="#b19">[20]</ref>. This corpus contains texts about 1.640 distinct species obtained from five sources: the Wikipedia, the MicrobeWiki containing texts about microorganisms, the HAMAP proteomes database containing protein-related microorganism data, the PubMed database of paper abstracts, and the PubMed Central database of full-text papers. The final corpus contains 5994 documents. The documents were preprocessed by removing English stopwords, parts of the texts containing references, and the words with frequency less than four, after which the words were stemmed. The final dictionary used in coverage experiments contains 6259 words that ocurr in at least 4 of the original text sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. REFERENCE TOPICS</head><p>The set of reference topics defines the measured coverage -by definition, topic models with high coverage are the models capable of detecting a large proportion of reference topics. We conduct our experiments with two sets of reference topics -the news and the biological reference topics. Each set of reference topics is based on an output of a topic discovery study, i.e., obtained by human inspection and interpretation of topic models' topics. In other words, these reference topics are an interpretable and error-free output of topic models, and represent useful concepts discoverable by model topics.</p><p>News reference topics are a result of an analysis of the media agenda <ref type="bibr" target="#b24">[25]</ref> performed with topic models built form news articles. In other words, the news topics represent a broad range of topics that occurr in the news. These topics correspond to persons, organizations, events and stories, and abstract concepts such as news issues and topics related to economy and politics. Biological reference topics are a result of topic discovery performed on biological texts describing microorganisms <ref type="bibr" target="#b19">[20]</ref>, and correspond to concepts of phenotypes, organism characteristics, such as termophilia and various types of pathogenicity.</p><p>Using a set of reference topics that the topic models are able to discover is a decision made to ensure that models' coverage will not be skewed due to the nature of reference topics. Namely, we wish to avoid the scenario where the models display low performance and are mutually indistinguishable because the reference topics represent a subset of topics hard to cover. Therefore we alleviate the coverage problem and leave the harder cases of reference topics for future research. We hypothesize that examples of such hard cases are very specific concepts and high-level abstract concepts devised by humans. Despite being within the models' reach, the reference topics used in the experiments are not trivial to uncover. The subsequent coverage analysis shows that the models cover at best 64% of the reference topics in the case of news topics, while for the biological topics the best case coverage is 44%.</p><p>From the machine perspective, a reference topic is represented in the same way as any model topic -with a vector of topic-word weights and vector of topicdocument weights. The components of these vectors correspond, respectively, to the words in the corpus dictionary and to the indices of the corpus documents. This way all the topic-related computations necessary for the calculation of coverage, such as calculation of distance between topics, make no distinction between the reference topics and the model topics.</p><p>Each set of reference topics is constructed in three main steps. The first step consists of building the topic models. In the second step the models' topics are inspected, interpreted, and filtered. Only uninterpretable news topics are filtered out and no concept-type restrictions are imposed, while all the biological topics that do not correspond to phenotypes are filtered out. Finally, the topic-word and topic-document vectors of the reference topics are constructed from the corresponding model topics. Each of the two methods of reference topics construction reflects the specifics of the corresponding topic discovery approach <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b24">[25]</ref>. The details of the methods are described in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. TOPIC MODELS</head><p>There exists a large number of topic model types representing a range of assumptions and approaches to modeling text structure. We apply the proposed coverage methods to evaluate models from two standard categories -probabilistic topic models <ref type="bibr" target="#b25">[26]</ref> and topic models based on non-negative matrix factorization <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b20">[21]</ref>. There exist numerous model variants belonging to these two categories, as well as alternative architectural approaches such as geometric <ref type="bibr" target="#b26">[27]</ref> and neural <ref type="bibr" target="#b27">[28]</ref> topic models. However, the structure of a topic model is not an apriori guarantee for model performance <ref type="bibr" target="#b28">[29]</ref>, and each model should be validated within the context of its application <ref type="bibr" target="#b28">[29]</ref>. Therefore we focus on the evaluation of topic models commonly used in topic discovery and leave the evaluation of many other models for future work. <ref type="bibr">VOLUME 4, 2016</ref> In the coverage experiments that follow we evaluate models from the categories of parametric and nonparametric probabilistic models, and a model based on nonnegative matrix factorization. The evaluated models all make minimal assumptions about the structure of text, similar to the assumptions of the seminal LDA model <ref type="bibr" target="#b0">[1]</ref>. This makes these models applicable to a generic use case of topic discovery performed on a collection of text documents. Specifically, each of the models assumes that the text of a document can be approximated with a weighted mixture of topics, where each topic is a weighted list of words. Other text-related variables such as sentiment and various metadata <ref type="bibr" target="#b25">[26]</ref> are not included in the models' structure.</p><p>Regardless of type, each model is represented simply as a set of topics, and each topic is represented with vectors of topic-word and topic-document weights. As noted earlier, the reference topics are represented in the same way. This black-box view of topic models makes the proposed coverage methods applicable to a wide variety of topic model types.</p><p>First of the model types we experiment with is the seminal Latent Dirichlet Allocation model LDA <ref type="bibr" target="#b0">[1]</ref>. The LDA model is representative for a wide variety of model types, many of which are its direct extension. LDA assumes a fixed number of topics, and the topic-word and topic-document relations are modeled with matrices of word-in-topic and topic-in-document probabilities. Probabilistic inference algorithms, such as variational inference <ref type="bibr" target="#b0">[1]</ref> and Gibbs sampling <ref type="bibr" target="#b29">[30]</ref>, are capable of learning the topic data from a set of unlabeled text documents. The LDA model has been widely applied in many topic modeling tasks, including topic discovery <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b32">[33]</ref>.</p><p>The second model type is a modification of the LDA model to which we will refer to as "asymmetric LDA" (aLDA). The LDA model assumes that the prior for the document-topic distribution is symmetric, which means that all the topics have an equal prior probability of appearing in a document. In contrast, the aLDA model allows for an asymmetric prior learnable from data, implying topics with varying prior probabilities. This allows for more flexibility in modeling of the documenttopic relation and in effect allows for the topics to be recognized, on the level of the text collection, as being larger or smaller. This approach potentially leads to higher topic quality <ref type="bibr" target="#b33">[34]</ref> and better detection of smaller topics <ref type="bibr" target="#b34">[35]</ref>. The aLDA variant we experiment with is implemented in the HCA software package <ref type="bibr" target="#b35">[36]</ref>, by way of using normalized Gamma priors to model the document-topic distribution.</p><p>The third model type is a nonparametric topic model based on Pitman-Yor priors <ref type="bibr" target="#b35">[36]</ref>, denoted PYP. Unlike the other models, the PYP model is able to learn the number of topics from data. The PYP model, denoted NP-LDA in <ref type="bibr" target="#b35">[36]</ref>, is an extension of the nonparametric HDP topic model based on Hierachical Dirichlet Process <ref type="bibr" target="#b36">[37]</ref>. The HDP model generalizes the LDA model by using a probability distribution over a countably infinite collection of topics <ref type="bibr" target="#b36">[37]</ref>. The PYP model generalizes the HDP model by using the more flexible Pitman-Yor process <ref type="bibr" target="#b37">[38]</ref> to model a distribution over the infinite set of topics. The nonparametric models have been applied for topic discovery <ref type="bibr" target="#b38">[39]</ref> and it is our intuition that the added flexibility of learning the number of topics might lead to better coverage, especially coverage of the smaller topics.</p><p>The fourth topic model type, denoted NMF, utilizes non-negative matrix factorization <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b20">[21]</ref>. The NMF model is based on approximation of the text collection, represented as matrix of document-word weights, in terms of a product of non-negative matrices containing document-topic and topic-word weights. In other words, the topics are the latent factors optimized to approximate the original text matrix under the assumption of non-negativity. The NMF model has been successfully used for topic discovery in several scenarios <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, and has the potential to produce topics with quality equal to or better then the quality of LDA topics <ref type="bibr" target="#b41">[42]</ref>.</p><p>Finally, we describe the set of topic model instances used in the coverage experiments. For each of the model types, with the exception of the nonparametric PYP model, the parameter T defining the number of model topics is varied, as this important parameter defines a model's capacity and affects the structure of its topics. Namely, T influences topic granularity <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, in such a way that a small T leads to broad and general topics, while a large T results in fine-grained and specific topics. The news and the biological datasets contain, respectively, 133 and 112 reference topics. We build the topic models by varying T between values of 50, 100, and 200. These choices of T correspond, respectively, to a number of topics that is smaller then, approximately equal, and larger than the number of reference topics. For the nonparametric PYP model the maximum number of learnable topics is set to 300.</p><p>For each combination of the model type and the number of topics, 10 model instances are built with different random seeds in order to account for stochastic variation, i.e., to obtain a more robust assessment of coverage. Therefore, for each dataset a total of 100 model instances are built: 10 instances for each pair of the model type (LDA, aLDA, or NMF) and the number of topics (50, 100, or 200), plus additional 10 instances of the nonparametric PYP model.</p><p>For each of the two datasets the topic model instances are inferred from the texts of the corresponding corpus, preprocessed by performing stopword removal and word normalization. The text corpora and the preprocessing methods are described in Section II-A. Appendix B contains the details of topic model construction that include the choice of hyperparameters, learning algorithms, and software tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MEASURES OF TOPIC COVERAGE</head><p>Measures of topic coverage compute scores that quantify how well the topics of a topic model cover a set of reference topics. In this section we propose two distinct measures of topic coverage -the SupCov measure, based on supervised approximation of human intuition of topic matching, and an unsupervised AuCDC measure, designed to approximate the supervised measure and serve as a quickly deployable model selection tool. The AuCDC measure calculates coverage by using a distance threshold as a topic matching criterion and aggregates the coverages obtained by varying the threshold.</p><p>The measures we propose are the first coverage measures that are validated and straightforward to construct, while the AuCDC measure is the first unsupervised measure of coverage. These measures are the main contribution of this paper since they improve both the reliability and the feasibility of the coverage experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. COVERAGE BASED ON SUPERVISED TOPIC MATCHING</head><p>The supervised topic coverage measure mimics the procedure in which a human annotator assesses weather there exists a model topic that matches a reference topic. Model coverage can be calculated from this matching information as a proportion of reference topics matched by at least one model topic.</p><p>Although this procedure would result in coverage scores based on human knowledge, it is time-consuming and impractical, especially for large sets of topic model instances. We solve this problem by constructing a supervised model that approximates human intuition of topic matching. Once such a model is available, it can be used for automatic calculation of coverage of arbitrarily many topic models.</p><p>We base our solution on a dataset of topic pairs labeled with matching scores of human annotators. Maching of two topics is defined as the equality of concepts obtained by the topics' interpretation. The topic matching problem is cast as a problem of binary classification of topic pairs into matching and notmatching classes. Several standard classification models are constructed and evaluated, and the best performing model is used in the subsequent experiments for coverage score calculation. The process of data annotation and model building is performed for both the news and the biological dataset.</p><p>In order to calculate the supervised coverage of a set of R reference topics by a topic model with T topics, every reference topic has to be matched, in the worst case, with every model topic. The matching operation consists of feature construction and of the computation of the classifier's output. Our features consist of distances between topic-word and topic-document vectors, which can be calculated in time proportional to either the vocabulary size V or to the number of documents D. The application of the classification model to the features requires constant time. Therefore the asymptotic complexity of calculating supervised coverage is O(RT (V + D)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Topic Pairs Dataset</head><p>In order to learn a matching model that generalizes well to different types of topics, the dataset of topic pairs is sampled from both the reference topics and the topics of topic models of different types and sizes.</p><p>However, in a randomly sampled set of topic pairs a large majority of pairs consists of non-matching topics. This means that supervised topic matching is an imbalanced learning problem <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, a scenario in which inference of high-performing models is hindered since only a small fraction of learning examples that define the structure of the positive class is available. In our case, the positive examples are pairs of matching topics.</p><p>Solutions to this problem include active learning and resampling methods <ref type="bibr" target="#b44">[45]</ref>, but we opt for a simpler solution applicable to topic pairs. This solution relies on the intuition that mutual distance of two topics is in an inverse correlation with the probability of topics' semantic match. Concretely, we sample topic pairs according to mutual distance of topics in order to achieve a higher proportion of pairs with mutually close topics that have higher probability of matching. An inspection of a validation sample of topic pairs confirms that the described procedure leads to a balanced dataset. The elaboration of the problem and the details of the solution can be found in Appendix C.</p><p>The final dataset of representative topic pairs used for model construction is created in the following way. First, a large set of topics is created by building one model instance for each combination of model type and number of topics and taking all the topics of the chosen instances. Next, three copies of each of the reference topics are added to the topic set in order to make the number of reference topics approximately equal to the number of topics of each of the model types. After that a set of all the distinct pairs of two different topics is created and these pairs are divided <ref type="bibr">VOLUME 4, 2016</ref> into ten subsets corresponding to equidistant intervals of topics' cosine distance. Finally, 50 pairs are sampled randomly from each of the subsets, leading to a total of 500 topic pairs. Of these 500 pairs, 300 pairs labeled by the annotators are used for model learning, while the remaining pairs are used for training and calibration of the human annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Annotation of Topic Pairs</head><p>Next we describe the procedure used to annotate topics pairs with human scores of topic matching. The key aspect, which defines the nature of the matching approximated by the supervised model, is the definition of a topic match. We define a topic match as conceptual quality of topics -two topics are considered equal if they are interpretable as the same concepts, where the interpretation of a topic as a concept is as specific as possible. This definition is in line with the approach of measuring precise coverage of the reference topics -we wish to assign high scores to models with topics that match the reference topics precisely. The alternative to this approach would be to focus on matching similar topics, such as sub-topics, super-topics, and overlapping topics.</p><p>More precisely, two topics -constructs described by weighted lists of words and documents -can differ both semantically, on the level of interpreted concepts, and due to the noise caused by stochastic topic model learning algorithms. This random variations manifest as a certain proportion of random or unrelated words and documents within the topic. On the semantic level, we define topic equality as matching of concepts obtained by interpreting topics as specifically as possible. Matching of concepts is defined as equality or near equality of concepts, allowing small variations and similar aspects of a same concept. Stochastic differences are accounted for by labeling topics as equal but with presence of noise. This is the case when one or both topics contain a noticeable amount of noise but the topics are still interpretable and the equality of interpreted concepts exists as previously defined.</p><p>Based on the previous definition, a pair of topics is labeled with 1 in case of topic equality, i.e., when concepts match without noise. A pair is labeled with 0.5 in case of a match with the presence of noise or small semantic variation, and with 0 when the concepts do not match. Preliminary experiments showed that such labeling is simpler and more consensual for annotators than labeling on the binary scale that accounts only for the possibilities of match and mismatch.</p><p>In order to ensure the quality and consistency of annotations, the annotation was conducted according to the methodology of content analysis <ref type="bibr" target="#b46">[47]</ref> -precise instructions were provided, the annotators had the knowledge required to interpret the texts and the topics, and were trained until the measure of mutual agreement reached a satisfactory level.</p><p>The annotation process resulted in a set of 300 topic pairs, each annotated by three annotators. The annotated pairs serve as training data in the process of building the supervised model of topic matching. The details of the annotations process are described in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Supervised Topic Matcher</head><p>At the heart of the proposed supervised measure of topic coverage is a binary classification model that approximates, for a pair of topics, human assessment of weather the topics match or not. This model is used to compute weather reference topics are covered by topics learned by a topic model. In this section we describe the method of construction of such a classification model.</p><p>The classification problem is defined in the following way. Each topic pair was annotated by three annotators with one of the three possible labels. The matching labels are 1 -concepts obtained by topic interpretation match, 0.5 -topics match but either certain amount of noise or a small semantic variation exist, and 0no match. Binary labels are obtained by averaging the labels and applying the decision threshold of 0.75 -if the label average is above 0.75 the topic pair is assigned the positive class that designates a match, and negative class otherwise. In other words, topics are labeled as matching if at least two of the annotators labeled the pair as matching while the third annotator labeling the pair as at least partially matching. The averaging of the annotators' scores is performed in order to obtain more robust labels and the definition of the positive and negative class corresponds to matching of topics on a precise level.</p><p>We consider four standard classification models: logistic regression <ref type="bibr" target="#b47">[48]</ref>, support vector machine <ref type="bibr" target="#b48">[49]</ref> with radial basis function kernel, random forest <ref type="bibr" target="#b49">[50]</ref>, and multilayer perceptron <ref type="bibr" target="#b47">[48]</ref>.</p><p>Topic pairs are represented as features based on four measures of distance: cosine distance, Hellinger distance <ref type="bibr" target="#b50">[51]</ref>, L 1 distance, and L 2 distance. These four distance measures are applied to both the pair of normalized topic-word vectors and to the pair of normalized topic-document vectors. Thus the input for classification of a topic pair consists of eight distancebased features, four based on topic-related words and four based on topic-related documents.</p><p>For each classification model, hyperparameter optimization is performed using nested five-fold crossvalidation in combination with the F 1 measure as the performance metric. Nested five-fold crossvalidation is used to obtain robust assessment of the quality of the model variant with optimized hyperparameters <ref type="bibr" target="#b51">[52]</ref>. The optimization is performed on the entire dataset of 300 labeled topic pairs. The details of feature construction and model construction are described in Appendix E. The performances of the optimized classification models, for both datasets, are laid out in <ref type="table" target="#tab_1">Table 2</ref>. For each model and dataset, the table contains both the average and the standard deviation of F 1 calculated on five outer folds of nested five-fold crossvalidation. Model performance data is complemented with the scores of mutual agreement of human annotators, also measured by F 1 . The human agreement is calculated as average F 1 score of a single annotator predicting the class labels obtained by averaging the annotations of the two remaining annotators. Specifically, annotators' binary class labels are calculated by averaging the annotators' scores and applying the 0.75 threshold to decide if the topics in a pair match. <ref type="table" target="#tab_1">Table 2</ref> shows that the logistic regression model has the highest F 1 values on both datasets. The support vector machine model is a close second, while the other two models, multilayer perceptron and random forest, are not far behind. In addition to logistic regression having top F 1 scores, it is structurally the simplest model with the smallest number of hyperparameters, which we view as an additional advantage. We therefore choose the logistic regression model as the basis of the supervised coverage measure, i.e., as the model for matching reference topics with model topics in order to calculate coverage of the reference set. The final models used to measure coverage in the following experiments are obtained, for each dataset, by first optimizing the hyperparameters with five-fold crossvalidation and then learning the final model with optimized hyperparameters. Both hyperparameter optimization and model learning are performed on the entire set of 300 labeled topic pairs.</p><p>The classification results show that the performance of the supervised models is close to the mutual agreement of human annotators. This shows that the described process leads to supervised models that can approximate human assessment of topic matching well. Specifically, the human scores of topic matching, based on the equality of the interpreted concepts, can be approximated well from a small set of features based on distances between topic-word and topic-document vectors.</p><p>This finding is applicable wherever there is a need for automatic matching of topic-like constructs defined by weighted words and documents. In the context of application of supervised topic matcher for coverage calculation, the previous results support the claim that the computed coverage will be reliable and close to human assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. COVERAGE-DISTANCE CURVE</head><p>The construction of the supervised measure of coverage requires a time-consuming process of construction of a labeled dataset of topic pairs. This process includes both recruiting and training of annotators with sufficient knowledge of the text domain, and the process of topic annotation. Conversely, an unsupervised measure of coverage could be quickly applied to a new evaluation scenario. We propose such a measure and show that it correlates very well with the supervised measure SupCov, which makes it applicable for ranking and selection of topic models by coverage.</p><p>The unuspervised measure, similarly to supervised coverage, computes coverage of a set of reference topics by matching them to the model topics. However, the decision weather two topics match is based simply on a measure of topic distance and a distance thresholdtwo topics match if their distance is below a threshold. For a specific threshold, the coverage of reference set is the proportion of reference topics for which a matching model topic exists. In order to render the measure threshold invariant, the final coverage score is calculated by varying the distance threshold and integrating all the coverages corresponding to different threshold values. More precisely, by varying the distance threshold and calculating corresponding coverage values a curve is formed, composed from points with x-coordinates corresponding to thresholds and y-coordinates corresponding to coverages. In other words, this curve is a graph of a function that maps distance thresholds to corresponding coverages. We call this curve the Coverage-distance curve and refer to it as CD-curve. The final coverage measure, which we label as AuCDC, is then calculated as the area under the CD-curve.</p><p>The CD-curve illustrates the dependence of coverage on the topic distance used as a criterion of topic match. It can therefore be used as a tool for graphical analysis of the coverage of a single topic model and for coverage-based comparison of a number of models. <ref type="figure">Figure 1</ref> contains CD-curves depicting coverages of news reference topics by LDA and NMF models with 50 topics. It can be seen that for the cosine distance threshold of 0.4, coverages for LDA and NMF are approximately 20% and 35%, respectively. This means that if two topics are considered equal when their cosine distance is 0.4 or smaller, 20% of reference topics are matched by at least one LDA model topic, with this percentage being 35% in case of the NMF model. Inversely, the curve can be used to determine the distance threshold, i.e., the required precision of topic matching, necessary to achieve certain level of coverage. The corresponding values of the AuCDC coverage score are 0.410 for the LDA model and 0.434 for the NMF model. However, the curves illustrate finer differences in the nature of coverage. Concretely, the NMF model has better coverage for smaller distance thresholds while the LDA model has better coverage for larger thresholds. This means that the NMF topics match the reference topics more precisely and give better coverage under the assumption of stricter criteria of topic match. On the other hand, the LDA model has better coverage when topic match is more approximate and a reference topic is allowed to be matched by a model topic at a lower degree of similarity. This example also illustrates the intuition behind the AuCDC score -a model with a higher score is expected to have a more elevated CD-curve than a model with a lower score, which means that it covers more reference topics at lower distance thresholds.</p><p>CD-curve and AuCDC measure have both similarities and differences with the popular receiver operating characteristics (ROC) curve and the associated Area Under the ROC Curve (AUC) metric <ref type="bibr" target="#b52">[53]</ref> applicable for evaluation of predictive machine learning models. The difference is that the AUC measures the performance of a set of related predictive models, usually instances obtained by varying an important model parameter. The points on the ROC Curve describe model instances -each point has coordinates corresponding to model sensitivity and model specificity. On the other hand, AuCDC is a measure of coverage of reference topics calculated for a single instance of an unsupervised topic model, and each point on the CD-curve has coordinates corresponding to a distance threshold and the derived coverage. However, both methods build a curve that provides information about model behavior in different scenarios -the ROC curve illustrates the sensitivity/specificity trade-off while the CD-curve illustrates the dependency between matching distance and coverage. And each of the two measures is calculated as the area under the corresponding curve, i.e., by integrating model performance over a range of options.</p><p>AuCDC and the CD-curve are based on a measure of distance between two topics that serves as a criterion of topic match. This base distance measure should satisfy three criteria. First, a base measure of distance should be bounded, i.e., restricted to finite range of values. This property is necessary because the CD-curve is constructed by varying the distance threshold from minimum to maximum distance. A bounded distance measure also enables a comparison between two different models, since their corresponding curves will be constructed over the same threshold range. Second, the measured distance between topics should correlate well with human intuition of topic similarity. Concretely, smaller distances between two topics should correspond to higher probability of topic match, and vice versa. While this requirement is a reasonable guideline, the final test of the measure's semantic is the comparison between the AuCDC measure and the supervised coverage based on human annotations. Finally, in order for the AuCDC measure and the CD-curve to enable comparison between models of different types, the semantic of the base measure should be insensitive to the model type. This requirement is best exemplified by considering the NMF model that produces topics with unbounded positive values and the probabilistic models with topics that are probability distributions. Topic-word vectors of NMF topics can thus contain much larger values, which can affect coordinate distance measures such as the L 1 distance. The L 1 distance between an NMF topic and an LDA topic is therefore expected to be larger than a distance between two LDA topics, regardless of the semantic similarity of topics. Intuitively, a sensible base distance measure should be based on relative proportions of topic-word weights, which is more similar to human approach to topic matching.</p><p>We opt to use, based on the previous considerations, cosine distance between topic-word vectors as the base measure of distance between two topics. Namely, topicword vector is a standard representation of model topics commonly used for calculating topic distance or similarity <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>. Likewise, cosine distance is a standard measure widely used in text mining for comparing high-dimensional vectors <ref type="bibr" target="#b55">[56]</ref> and experiments with topic models show that it correlates well with human intuition of topic similarity <ref type="bibr" target="#b9">[10]</ref>. For two vectors v and w, the cosine distance is defined as: cosd(v, w) = 1 ? v?w v w . The cosine distance is an inverse of cosine similarity of v and w, defined as v?w v w , and corresponding to the cosine of the angle formed by vectors v and w. By definition, the cosine distance is bounded and takes on values between 0 and 2. In case of the large majority of topic models that have non-negative topic vectors, such as probabilistic topic models and non-negative matrix factorization models, the cosine distance takes on values between 0 and 1. Cosine distance thus satisfies all of the previous criteria. It is bounded by definition and expected to correlate well with human intuition of semantic distance. Since it is based on an angle between topic vectors it is also invariant to the sizes of these vectors, i.e., the absolute values of topic-word weights that can vary depending on model type.</p><p>The proposed AuCDC measure is envisioned as a good approximation of the supervised coverage measure SupCov that can be quickly deployed for selection of high-coverage models. We thus evaluate the AuCDC measure, based on cosine distance of topicword vectors, by calculating its Spearman rank correlation coefficient with the SupCov measure on the level of topic model. This correlation shows how well the AuCDC-induced ordering of topic models approximates the ordering induced by SupCov coverage. Standard Pearson coefficients of linear correlation are also calculated in order to get a more complete picture of the measure's properties. For each dataset, the correlations are calculated on the set of 100 topic models of different types and sizes described in section II-C. The 95% bootstrap confidence intervals of the correlation coefficients are calculated using the percentile method and 20.000 bootstrap samples. We note that both the AuCDC and the SupCov measure use cosine distancesupervised topic matcher uses features based on cosine distance, specifically cosine distances of topic-word and topic-document vectors. To check if this influences the strength of correlation, we built a supervised matching model that does not use features based on cosine distance and calculated correlations between the AuCDC and the supervised coverage based on this model, which we denote SupCov-nocos. Except for the difference in features, the supervised matching models are the same -based on logistic regression with hyperparameters optimized by five-fold crossvalidation, as described in Section III-A. <ref type="table" target="#tab_2">Table 3</ref> shows correlations between supervised coverage and the AuCDC coverage. All the correlations are very high, with values above 0.9 on both datasets. The correlation scores are comparable regardless whether the supervised coverage uses features based on cosine distance. This shows that the correlations are not artificially high because the cosine distance is used by both the SupCov and AuCDC measures. Interestingly, Pearson correlations are also high, showing that there exists a strong linear dependency between the AuCDC and the supervised coverage. We take the above correlations as evidence that the AuCDC measure based on cosine distance is indeed a very good unsupervised approximation of supervised topic coverage. Specifically, high rank correlations show that the AuCDC measure can be used to rank topic models by coverage and select the best models. We therefore proceed to use the AuCDC measure, alongside the supervised measure, for evaluation of topic model coverage.</p><p>Finally, we give the precise description of the method used to construct the CD-curve and to calculate the values of AuCDC measure. Given a set of reference topics and a topic model, first a CD-curve is constructed based on cosine distances of topic-word vectors. For a specific distance threshold, a reference topic is considered covered if there exists a model topic such that its cosine distance from the reference topic is below the threshold. The corresponding coverage of the reference set is simply calculated as a proportion of covered reference topics. The AuCDC curve is approximated by segmenting the range of possible cosine distances, the [0, 1] interval, into 50 equidistant subintervals. For distances corresponding to subinterval limits the derived coverages are calculated, which gives a set of (distanceThreshold , coverage) points that serve to approximate the curve. The final AuCDC value is calculated from this approximation by using the trapezoidal rule -the area under a subcurve corresponding to a subinterval is approximated by the area of the trapezoid defined by the interval boundaries and the corresponding coverage values. The final AuCDC value for the entire curve is then obtained as the sum of the areas of individual trapezoids.</p><p>The first step in the calculation of the AuCDC measure is the construction of the matrix containing cosine distances between the reference and the model topics.</p><formula xml:id="formula_0">The time complexity of this operation is O(RT V ),</formula><p>where R is the number of reference topics, T is the number of model topics, and V is the vocabulary size. Once the distance matrix is constructed, the measure can be computed in O(RK) time, where K is the number of distance subintervals. Namely, if for each reference topic the distance to the closest model topic is stored, then for each distance threshold the coverage can be computed in O(R) time. Therefore the asymptotic complexity of computing the AuCDC measure is</p><formula xml:id="formula_1">O(RT V + RK), which is equal to O(RT V ) because K is a small constant value.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. COVERAGE-BASED MODEL EVALUATION</head><p>In this section we apply the proposed measures of topic coverage to analyze the performance of a set of topic models of different sizes and types. The reference topics are constructed by human inspection and selection of topics learned by topic models, and represent topics both within the reach of topic models and useful to a human analyst. Therefore the models are evaluated from the perspective of a use case of topic discovery on two sets of topics -topics occurring in news articles and biological topics corresponding to phenotypes.</p><p>For each dataset four types of topic models are evaluated -the widely used LDA, its variant aLDA, the popular NMF based on matrix factorization, and the nonparametric PYP designed to learn the number of topics. These models are described in more detail in Section II-C. For each of the parametric models, three different configurations of the number of topics are evaluated -50, 100, and 200 topics. These numbers correspond to, respectively, a number smaller than, roughly equal, and larger than the number of topics in the reference set. Ten instances with different random seeds are built per combination of a model type and a number of topics, yielding a total of 100 topic model instances per dataset.</p><p>Coverage is measured using the SupCov measure  <ref type="table" target="#tab_3">Table 4</ref> and the CD-curves are shown in <ref type="figure" target="#fig_1">Figure  2</ref> and <ref type="figure" target="#fig_3">Figure 3</ref>.</p><p>Coverage results for different topic models vary depending on the dataset. On the news dataset, the nonparametric PYP model has the best coverage, followed by the NMF model with 200 topics. The NMF model has the best coverage results among the parametric models, as shown by both measures and the CD-curves in <ref type="figure" target="#fig_1">Figure 2</ref> which illustrate how the NMF models outperform the LDA and aLDA models with the corresponding number of topics. Comparison of the best model PYP and the second best model NMF-200 shows that while PYP has higher SupCov score, the AuCDC scores of the two models are the same. This can be explained by the comparison of the two models' CD-curves showing that the NMF-200 achieves better coverages for small cosine thresholds, while the PYP achieves better coverages for thresholds above 0.37. In other words, the NMF contains more topics that match the reference topics closely, i.e., at smaller cosine distances. This means that the NMF discovers more reference topics at higher level of precision, but the superior SupCov score of the PYP indicates that its topics are still precise enough to be considered as semantically matching.</p><p>On the biological dataset the NMF model achieves much better coverage results than the probabilistic models. The NMF model with 200 topics has the best overall coverage while for the other model sizes the NMF models yield better coverages than the corresponding probabilistic models. The nonparametric PYP model achieves best results among the low-performing probabilistic models and it is comparable with the NMF models with 100 topics. Regardless of the model type, the coverage scores are lower than on the news dataset, which shows that the biological dataset represents a more challenging scenario of topic coverage. On both the news and the biological datasets the coverage score correlates positively with the number of topics -larger models are able to uncover more topics.</p><p>The coverage results support the claim that the NMF model is a good default choice for topic discovery. Namely, on the news dataset the NMF models outperform the probabilistic LDA and aLDA models for all the model sizes, while the NMF model with 200 topics achieves scores competitive with the best-performing nonparametric PYP model configured with the total capacity of 300 topics. On the biological dataset the NMF is clearly the best choice, while the probabilistic models, with the exception of the PYP, have weak coverage scores. These results suggest that the NMF model is a more robust topic discovery tool, likely to perform well on different datasets. The nonparametric PYP model has the best coverage score on the media dataset, while on the biological dataset is has the best results among the probabilistic models. These results support the claim that PYP is a better choice than the nonparametric LDA and aLDA models, since it is expected to achieve better coverage and be more robust to dataset change. Based on the results, the NMF model with 200 topics is a good default choice for performing topic discovery on corpora with between several thousands and several tens of thousands of texts. The results also show that the NMF is superior to LDA, regardless of model size. However, in practice the LDA model is very often a first choice, probably due to tradition and wide availability of implementations. The experiments also demonstrate that the unsupervised AuCDC measure performs well in selection of highperforming topic models and demonstrate the use of CD-curve for more in-depth analysis of model coverage.</p><p>This section demonstrates the merits of coveragebased topic model evaluation and demonstrated the application of the proposed coverage methods for model analysis and the selection of high-performing topic models. However, we note that the proposed methods should be further evaluated through their application on additional datasets and topic modeling settings. We believe that qualitative evaluations focused on human examination of topics would reveal useful information about both the nature of the model coverage and the measures' performance. However, such topic evaluations are time consuming and potentially require expert knowledge. Appendix F supplements the experiments in this section with an analysis of the relationship between topic models' precision and recall, and with an analysis of the running time of the coverage measures.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. COVERAGE OF TOPICS DIVIDED INTO SIZE CATEGORIES</head><p>The size of the reference topics varies in the sense that some topics occur in a large percentage of text documents, while other topics can be found only in a small fraction of documents. In this section we apply the supervised SupCov measure to investigate how topic models cover reference topics of different sizes. This experiment is partly motivated by several articles in which the authors claim that in order to cover smaller topics, one needs models configured with a large number of topics <ref type="bibr" target="#b34">[35]</ref>, models that explicitly perform topic diversification <ref type="bibr" target="#b56">[57]</ref>, or nonparametric models <ref type="bibr" target="#b56">[57]</ref>. Additional motivation stems from the observation that in the process of topic discovery both small and large topics can be of interest to the analyst. Therefore, failure to cover small topics can be a potential drawback of a topic model.</p><p>We define the size of a reference topic as the number of documents in which the topic occurs, and that a topic occurs in a document if at least 10% of the document's text is dedicated to the topic. This heuristical definition is based both on common sense notion of occurrence of topics in texts, as well as on the basic assumption of probabilistic topic models clearly encoded in the structure of the LDA model <ref type="bibr" target="#b0">[1]</ref>. This assumption states that each document is a probabilistic mixture of a set of topics, and each word in the document "belongs to", or talks about, one of these topics.</p><p>We proceed to measure the size of reference topics, represented as a weighted lists of words and documents, in the following way. For each dataset, we use an LDA model that supports both fixed topics and learnable topics. We build such a model with fixed topics configured to correspond to the reference topics, and with additional 20 learnable topics added for flexibility, i.e., for better approximation of the overall topical structure. Total number of the model's topics T is therefore 20 plus the number of reference topics in a dataset. Document-topic and word-topic distribution of the fixed topics simply correspond to the document-topic and word-topic weights of the reference topics normalized to a probability distribution. Inference is performed by standard Gibbs sampling <ref type="bibr" target="#b29">[30]</ref>, with the parameter ? VOLUME 4, 2016 set to 0.01 and parameter ? set to 1/T . Probability formulas used in Gibbs inference are modified in a straightforward way by insertion of the known topicword and topic-document weights of the fixed topics. The learning process of 1000 Gibbs sampling iterations results in learned probabilities of occurrence of fixed reference topics in the corpus documents. Finally, for each reference topic, the size is calculated as the number of documents in which the topic occurs with the probability of at least 10%.</p><p>Reference topics are then divided into quartiles according to the calculated sizes. This is a principled, data-independent method of division of topics into four categories of approximately equal size. The first quartile contains the smallest topics (bottom 25%), while the fourth quartile contains the largest topics (top 25%). <ref type="table" target="#tab_4">Table 5</ref> contains sizes and boundaries of the quartiles for each dataset. It can be seen that the news dataset with the larger document corpus has larger reference topics,i.e., news topics tend to occur in more documents than the biological topics.</p><p>The results showing how the topics in different size quartiles are covered by the topic models are displayed in the <ref type="table" target="#tab_5">Table 6</ref>. The same topic models as in the coverage experiments in Section IV are used. The coverages are calculated in the same way as in Section IV -coverages of 10 different topic model instances are averaged and the 95% bootstrap confidence intervals are calculated using the percentile method. The results show that the larger models with more topics can cover both large and small reference topics, while the smaller models can cover only larger topics. The results for the biological dataset show how the low performance of probabilistic models relates to topic size -these models struggle with covering smaller topics. On the other hand, the NMF achieves much better coverage of smaller topics. The nonparametric PYP covers smaller topics better than the nonparametric probabilistic models and has the best coverage over all of the size categories on the news dataset, but it lags behind the NMF on the biological dataset.</p><p>The relation of coverage and topic size can be interpreted by looking at the structure of topic models.</p><p>Namely, topic models approximate the corpus, represented as the document-word matrix, as a product of the document-topic and topic-word matrices. Furthermore, these models are learned with the goal of optimizing the reconstruction of the corpus data from the small set of topics. We note that while the NMF model is explicitly based on matrix factorization, the described factorization is also in effect performed by the probabilistic topic models <ref type="bibr" target="#b54">[55]</ref>. Therefore, the models with a limited number of topics can achieve better approximation of the text data by learning only larger topics that occur in more documents and thus capture more of the data. On the other hand, large models have additional capacity for fine-grained approximation and thus can capture both large and small topics.</p><p>The results demonstrate that the smaller topic models can successfully cover only the large reference topics while the larger models are able to cover both large and small topics. These results support the previous conjectures that large models and nonparametric topic models are needed in order to cover smaller topics <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b56">[57]</ref>. From a practical perspective, these results support the use of larger topic models for topic discovery since these models can be used both for detection of salient topics and for pinpointing small topics which can be of interest to an analyst. Concretely, in case of the news dataset, the examples of potentially interesting small topics from the first size quartile are topics that can be labeled as "War in Yemen" and "Transgender". In case of the biological dataset, the reference topics of all sizes represent phenotypes discovered from biological text <ref type="bibr" target="#b19">[20]</ref>. On the other hand, the potential problem with large models is, in our experience, a relatively large number of low quality topics. Examples of low quality topics are noisy topics containing random words and documents, and fused topics corresponding to two concepts. A possible remedy for this problem is the augmentation and speed-up of topic inspection process using measures of topic quality. One way to achieve this is to order the model topics by coherence and let the analyst inspect the coherent topics first <ref type="bibr" target="#b57">[58]</ref>. Finally, we note that in order to further support the results in this section, new experiments on other datasets and with </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. COVERAGE OF SEMANTIC CATEGORIES</head><p>Topic models are useful tools for text exploration and topic discovery since they are able to learn topics that humans can interpret as concepts. When topic models are applied in computational social sciences it is often desirable that model topics correspond to concepts from a specific category. Such topics of interest to the researcher have been described as "theoretically interesting" <ref type="bibr" target="#b28">[29]</ref> and "analytically useful" <ref type="bibr" target="#b30">[31]</ref>. An example of a research topic which can benefit from quantitative analysis of news text based on topic models is agenda setting <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b58">[59]</ref>. Agenda setting research <ref type="bibr" target="#b59">[60]</ref> is focused on salience of issues -topics of political or social importance. The standard approach is to investigate how media salience of issues relates to public perception of their importance <ref type="bibr" target="#b59">[60]</ref>. Naturally, when topic models are applied for agenda setting research it is desirable that the model topics correspond to issues. Computational agenda setting studies typically rely on topic models to automatically detect issues in a collection of news texts and to measure their salience <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b58">[59]</ref>. In this section we demonstrate the application of coverage methods to the analysis of how topic models cover the issues occurring in news texts. Such analyses could guide the choice of topic modelling tools for agenda setting studies.</p><p>There exist numerous other research directions, each with its own class of "theoretically interesting" concepts. Examples of such studies include the analysis of news framing <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b60">[61]</ref>, analysis of historical news <ref type="bibr" target="#b61">[62]</ref>, and qualitative analysis of news focused on a specific topic <ref type="bibr" target="#b4">[5]</ref>. For these and numerous other use cases an experiment focused on the coverage of topics of interest could be conducted. Motivated by applications in social sciences where the topics of interest are expectedly abstract concepts, we analyze how the topic models cover abstract reference topics.</p><p>We proceed to measure the coverage of reference topics divided according to two criteria -correspondence to a news issue and topic abstractness. To this end, each reference topic from the news dataset was annotated as being either abstract or concrete, and as being either <ref type="bibr">VOLUME 4, 2016</ref>  The "Climate Change" topic is an example of a topic that is both abstract and an issue, while the "China" and "Boston Bombing Trial" topics are examples of concrete topics. Reference topics were annotated by two annotators. First, a sample of 30 topics was annotated by both annotators and Krippendorph's ? coefficients of inter-annotator agreement were calculated. For topics abstractness ? was 0.67, and for issue vs. non-issue labels ? was 0.44. The levels of agreement reflect the fact that the assessment weather topic corresponds to an issue is more difficult and open to interpretation than the relatively straightforward assessment of topic abstractness. In the next step the topics for which annotators' assessments differred were discussed, which led to an improved understanding of the definitions guiding the annotation process. Annotations of the 30 sampled topics were synchronized, after which each annotator proceeded to annotate half of the remaining topics -approximately 50 topics per annotator. After the annotation was completed, each of the 133 reference topics from the news dataset was labeled both as being either abstract or concrete, and as corresponding to a news issue or not. The coverage of the resulting topic categories by topic models was then measured. Topic models used in the experiments are the same models used in cov-erage experiments in Sections IV and V. The coverage was measured using the supervised coverage measure SupCov based on the requirement of a precise match between the model and reference topics. The coverages are calculated in the same way as in Section IVcoverages of 10 different topic model instances are averaged and the 95% bootstrap confidence intervals are calculated using the percentile method. The coverage results, displayed in <ref type="table" target="#tab_6">Table 7</ref>, show that the NMF model has better coverage of the issue topics than the LDA model, although this advantage becomes smaller as the number of model topics increases. As for the non-issue topics, the NMF model is clearly better than the LDA model. The PYP model achieves the best performance and covers almost 80% of all the issues. Interestingly, issue topics are covered better than non-issue topics across all model types and sizes.</p><p>The NMF model is slightly better than the LDA model in coverage of the abstract topics, and clearly better in coverage of the concrete concepts. The nonparametric PYP performs best for both concrete and abstract concepts. We also observe that the concrete topics are covered better than the abstract topics across all model types and sizes. A possible explanation is that the concrete topics focused on people, events, and organizations, are more prevalent in the news text and thus more easier to detect. Similarly, the higher coverage of the issue topics might be explained by the fact that news articles favor issue topics.</p><p>The coverage experiment described in this section is motivated by applications of topic models in social sciences, where models are expected to correspond to concepts of interest to the researcher. In such studies LDA is often a model of choice, due to tradition and availability of LDA implementations. However, experiments in this section suggest that the NMF and PYP models are a better choice, at least in the case of news analysis, since both models achieve better coverage of abstract topics and issue topics than LDA model. These results are in line with previous experiments in Sections IV and V showing that NMF and PYP models achieve higher overall coverage and are better at pinpointing smaller topics than the LDA model. While the nonparametric PYP model achieves the best overall coverage of all the topic categories, in our opinion the NMF model is a better choice. We base this assessment on evidence in Section IV that shows greater robustness of NMF performance across datasets. Additionally, regardless of model type, models with more topics are a better choice, probably due to their ability to detect small topics, as suggested by the results in Section V.</p><p>The experiment in this section is a demonstration how the coverage-oriented model evaluation can be applied to analyze and select topic models best suited for the purpose od topic discovery in social sciences. Such analyses rely on measures of topic coverage and use-case oriented sets of reference topics that represent concepts of interest. However, to obtain reliable and generalizable results, similar experiments should be performed for more use cases representing different research designs. Ideally, such experiments would generate enough evidence for reliable recommendations for use of specific topic model types. The findings in this section also show variations in coverage of different semantic topic categories. We find a more in-depth investigation of this phenomenon an interesting topic for future work with potential to generate knowledge about the structure of conceptual topics and models expected to approximate them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. COVERAGE AND OTHER TOPIC MODEL EVALUATION METHODS</head><p>In this section we examine how the proposed coverage approach relates to two other topic model evaluation methods -topic coherence and topic model stability.</p><p>The experiments demonstrate that the topic coverage is a property distinct from both coherence and stability. In the case of model stability, we show how the coverage measures can be adapted to approximate model stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. COVERAGE AND TOPIC COHERENCE</head><p>Topic coherence <ref type="bibr" target="#b8">[9]</ref> is an approach to evaluation of topic models based on calculating a measure of coherence of individual topics. A good coherence measure is correlated with topic interpretability in the sense of the topic's correspondence to a single concept <ref type="bibr" target="#b8">[9]</ref>. Topic coherence measures typically use top-weighted topic words as input, compute coherence by aggregating mutual similarity of top words, and are designed to maximize correlation with human coherence scores. Calculation of topic coherence became a popular method of topic model evaluation and many coherence measures have been proposed <ref type="bibr" target="#b10">[11]</ref>.</p><p>Coverage is related to coherence because both approaches aim to approximate the matching between model topics and concepts. However, coherence is more generally and loosely defined as a measure of match between a topic and any concept, while coverage is defined in terms of a predefined set of specific concepts represented by reference topics. Coherence measures rely only on topic-related words and a model of word similarity and are thus easier to deploy than coverage measures that require a set of pre-compiled reference topics. Therefore, coherence measures are more approximative but readily available measures of topic conceptuality, while coverage provides more precise evaluation at the added cost of effort needed to construct reference topics. In this section we experimentally examine the relation between the two approaches by calculating correlations between coherence measures and measures of coverage proposed in Section III.</p><p>Coherence measures calculate coherence scores of model topics and in order to enable comparison of coherence and coverage measures, we adapt the coverage measures to compute coverage-related scores of individual topics. The adaptation is performed by using the existing topic-matching criterion of a coverage measure to compare a model topic with reference topics. The adapted measures thus score individual model topics in terms of their correspondence with the reference topics, which is a straightforward application of existing coverage apparatus to topic-level scoring.</p><p>Concretely, in case of the supervised coverage, the score of a model topic is set to 1 if the topic matches a reference topic or to 0 if no matching reference topic exists. The matches are computed using the supervised topic matcher described in Section III-A. The unsupervised AuCDC measure, described in Section III-B, is based on approximation of equality between a model topic and a reference topic using a cosine distance threshold -the topics are considered equal if their cosine distance is below the threshold. We adapt the AuCDC for calculating the match between a single model topic and the reference topic set by using cosine similarity, the inverse of cosine distance. Specifically, we compute the cosine similarity between a model topic and the set of reference topics, i.e., the similarity between the topic and its most similar reference topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Measures of Topic Coherence</head><p>We compare the coverage measures with state of the art coherence measures that achieved top correlations with human coherence scores in an extensive evaluation experiment <ref type="bibr" target="#b10">[11]</ref>. In <ref type="bibr" target="#b10">[11]</ref>, a generic structure of a coherence measure is formulated and the derived space VOLUME 4, 2016 of possible measures is searched for top performing candidates. The topic coherence scores are calculated by dividing the set of top topic words into subsets and aggregating the similarities between word subsets. Within this framework, the averaging of similarities between individual words is a special case. A coherence measure also depends on a model of word similarity derived from word co-occurrence counts calculated using either the local corpus or the Wikipedia. We evaluate three of top-performing measuresnewly discovered measures labeled CP and CV <ref type="bibr" target="#b10">[11]</ref>, and the previously proposed NPMI measure of <ref type="bibr" target="#b62">[63]</ref>. The NPMI measure is calculated by averaging normalized pointwise mutual information similarity of word pairs. The CP measure averages similarities, defined using conditional probability <ref type="bibr" target="#b63">[64]</ref>, between pairs consisting of a word and its complement set. The CV measure averages similarities between all pairs consisting of a top topic word and the set of those top words with higher topic-word weights. In case of CV, similarities are calculated by representing words as vectors of similarity scores with other top topic words.</p><p>For each of the described measures we experiment with both approaches to defining word similarity -domain-specific similarity derived from word cooccurrences in the local corpus, and the generic mixeddomain similarity based on co-occurrences in the English Wikipedia. All the coherence measures in this experiment use as input top 10 topic words <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Correlation Between Coverage and Coherence</head><p>We proceed to experimentally evaluate the relationship between the described coherence and coverage measures by calculating the Spearman rank correlations. Correlations are calculated on the set of topics of topic models described in Section II-C. These models of different sizes and types yield a total of 13.500 topics per dataset. The 95% bootstrap confidence intervals of the Spearman correlation coefficients are calculated using the percentile method and 20.000 bootstrap samples.</p><p>The results, presented in <ref type="table" target="#tab_7">Table 8</ref>, show that neither strong nor consistent correlation between coherence and coverage exists. The correlation strength varies, depending on the measure type and dataset, and in most cases it is weak to non-existent.</p><p>The results in <ref type="table" target="#tab_7">Table 8</ref> show that in majority of cases coverage correlates better with corpus-based coherence measures than with Wikipedia-based coherence measures. A possible explanation is that reference topics are derived from the topics of models built on corpus texts. Namely, probabilistic topic models implicitly assume that words defining a topic are the words that tend to co-occur in texts <ref type="bibr" target="#b30">[31]</ref>. Therefore topics with high local coherence, being by definition topics whose top-words co-occur in corpus texts, should have better likelihood of being among the reference topics.</p><p>We also observe that the AuCDC measure correlates better with coherence than the SupCov measure based on supervised topic matching. A reasonable explanation is that AuCDC is by design less error-prone to correlation errors since it calculates an approximative similarity score between a model topic and reference topics. Namely, if a coherent topic is missing from the reference set, its cosine similarity to the set can still be high if other similar reference topics exist. And if the supervised matcher does not recognize a coherent topic as matching with a corresponding reference topic, cosine similarity of these two topics is still expected to be high.</p><p>Finally, we observe that the correlations between coherence and coverage are higher in case of the News dataset, while for the Biological dataset they are very weak at best. A reasonable explanation is that the News reference topics are more representative of the set of all the learnable topics. Namely, both sets of reference topics are derived from model topics but Biological topics are filtered to include only topics describing phenotypes. Therefore, non-phenotype topics with high coherence scores are likely to receive low coverage scores that can negatively affect the correlation strength.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Conclusions</head><p>The experiments in this section show that the coverage and coherence clearly differ, although both approaches measure a correspondence between model topics and concepts. In particular the proposed coverage measures, designed to evaluate topic models in terms of their match with reference topics based on concrete topic discovery use cases, cannot be approximated well with state-of-the-art coherence measures.</p><p>These findings open interesting questions about the relation between coherence and coverage measures. We believe that an in-depth investigation of relationships between coverage and coherence could improve both evaluation approaches. Concretely, it would be interesting to investigate the correlations between the two and observe which coherence measures best correlate with coverage of which sets of reference topics. This might better explain the nature of the vaguely defined property of topic coherence by comparing it with precisely defined coverage measures. Additionally, the coherence measures able to approximate coverage well could be used for selection of interpretable models.</p><p>Our findings are in line with the previous work on the analysis of coherence measures. Experiments in <ref type="bibr" target="#b9">[10]</ref> show weak and inconsistent correlation between measures of topic quality and a measure of correspondence between topics and reference concepts, with the coherence measures exhibiting mildly negative correlations. Evaluation of coherence measures via topic-level quality scores based on human interpretation of topics was performed in <ref type="bibr" target="#b21">[22]</ref>. The experiment found neither consistent nor significant correlation between coherence measures on one, and the interpretability scores based on inspection of topic words and documents on the other side. Both our experiment and the two previous experiments show variations in performance of different types of coherence measures. All the experiments also show weak correlation of coherence with interpretable measures of quality grounded in either reference topics or human labels. Both the correlations in <ref type="table" target="#tab_7">Table 8</ref> and those in <ref type="bibr" target="#b21">[22]</ref> tend to be weakly or moderately positive. This indicates that coherence measures can roughly approximate topic interpretability, but fail to calculate scores that are reliable proxies for interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. COVERAGE AND MODEL STABILITY</head><p>In this section we examine the relationship between topic coverage and model stability. In the first experiment we measure correlation between the measures of stability and coverage. Next, we show how the proposed coverage measures can be adapted to calculate stability. This adaptation relies on the fact that both approaches are based on approximation of a match between two topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Topic Model Stability</head><p>A popular approach to the evaluation of topic models is based on the notion of model stability <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b64">[65]</ref>. The approach is motivated by the fact that the learned model instances vary randomly due to model inference algorithms that rely on random initialization and sampling. Instability of topic models has been confirmed both numerically <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b42">[43]</ref> and by model inspection that reveals topic variation among model instances <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. This variation is potentially detrimental for an analyst performing topic discovery <ref type="bibr" target="#b12">[13]</ref>, especially in the case of social sciences where omitted topics and topic variations can influence the results of a study <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Measures of stability are derived from mutual similarity of inferred model instances -a stable topic modeling setting should consistently produce similar models. Similarity between models is calculated either by aligning model topics using a similarity measure <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b42">[43]</ref> or by comparing models represented in terms of words or documents <ref type="bibr" target="#b12">[13]</ref>. Alternatively, an interactive approach based on clustering and visualizing topics of many models has been proposed <ref type="bibr" target="#b23">[24]</ref>.</p><p>In the following experiments, we opt for a common approach to stability calculation based on aligning topics of two model instances by using a measure of topic similarity <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b42">[43]</ref>. The first step of the approach is to find an optimal bipartite matching of topics, i.e., an optimal one-to-one pairing between first model's and second model's topics. This optimal pairing that maximizes pairwise topic similarity is computed using the Hungarian algorithm <ref type="bibr" target="#b65">[66]</ref>. The similarity of two models is then computed as the average similarity of the paired topics. Finally, given a topic model and an inference algorithm, the stability is calculated as the average mutual similarity of the inferred model instances. We use the cosine similarity of topic-word vectors as the measure of topic similarity and we label the described measure of stability as InstanceStabil.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Correlation Between Coverage and Stability</head><p>The correlation between stability and coverage measures is computed at the level of a set of model instances. Since the set of topic models described in Section II-C contains 10 instance sets, we extend it in order to obtain more robust correlation results. Additional models are built using the same procedure, described in Section II-C. In the extended model set the model size, defined by the number of topics parameter T , is varied over a wider range of values. For parametric topic models LDA, aLDA, and NMF, the parameter T is varied between the values of 20 and 500 in steps of 20, yielding 25 size variants per model type. For the nonparametric PYP model, the maximum learnable We proceed to empirically determine the nature of relation between coverage and stability, by calculating Spearman rank correlations between the described InstanceStabil stability measure and the two proposed measures of coverage, SupCov and AuCDC. The 95% bootstrap confidence intervals of the correlation coefficients are calculated using the percentile method and 20.000 bootstrap samples. Results, presented in <ref type="table" target="#tab_8">Table  9</ref>, show that there is no strong correlation between the stability and coverage measures. The correlations are weak to non-existent, ranging from slightly negative to slightly positive. These results show that model coverage is a property of topic models unrelated to model stability.</p><p>The lack of correlation can be explained, at least in part, by the nature of correlation between the number of model topics on one, and the stability and coverage on the other side. As can be seen from <ref type="table" target="#tab_8">Table 9</ref>, stability has a negative while coverage has a positive correlation with the number of topics. This indicates that the larger models tend to be less stable, which is unsurprising since they contain more learnable variables, resulting in more variation among the learned model instances. On the other hand, larger models with the capacity to learn more topics tend to have greater coverage, which is in line with the results of the coverage experiments in Section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Calculating Stability using Coverage Measures</head><p>We proceed to show how the coverage measures can be adapted to measure model stability by way of direct comparison of two model instances.</p><p>First, we adapt the supervised coverage measure SupCov based on supervised matching of model and reference topics. The stability measure derived from SupCov calculates the similarity between two topic models in terms of the reference topics covered by both models. Concretely, given a set of reference topics and a topic matcher, let reftop(m) be the set of reference topics that the topics of the model m cover. The similarity of two models m 1 and m 2 with the same number of topics T is defined as |reftop(m 1 ) ? reftop(m 2 )|/T . In other words, the similarity of two models is defined as the number of reference topics discovered by both instances, relative to the maximum number of discoverable topics.</p><p>This definition of similarity is related to the standard model similarity based on bipartite matching, computed as the average similarity between the pairs of highly similar topics of two models. Namely, in our case the pairs of similar topics correspond to reference topics discovered by both models -each such reference topic matches a topic from the first and a topic from the second model. Therefore, the number of mutually discovered reference topics can be interpreted as the sum of binary similarities of aligned model topics.</p><p>The final stability measure based on SupCov is calculated, for a set of model instances, as the average similarity between instances. We dub the described measure of stability reference set stability and label it as RefsetStabil.</p><p>The AuCDC measure of coverage can also be adapted for measuring model stability by using it to compute the similarity between two model instances. The AuCDC measure, defined in Section III-B, is designed to approximate how well a set of topics of a model m covers a set of reference topics ref . We denote the corresponding coverage score as <ref type="figure">AuCDC (ref , m)</ref>. The AuCDCbased similarity of two topic models m 1 and m 2 is computed as (AuCDC (m 1 , m 2 ) + AuCDC (m 2 , m 1 ))/2. In other words, this is an approximation of how well the sets of topics of the two models cover each other. The coverage is computed in both directions and averaged in order to make the similarity measure symmetrical. The final stability measure is calculated, as in the case of other stability measures, by averaging pairwise similarity on a set of model instances. We dub this measure of stability AuCDC stability and label it as AuCDC-stabil.</p><p>In order to measure how the coverage-based measures of stability relate to the InstanceStabil measure, we calculate Spearman correlations on the extended dataset of models of varying types and sizes divided into 80 sets of model instances. The 95% bootstrap confidence intervals of the correlation coefficients are calculated using the percentile method and 20.000 bootstrap samples.</p><p>The results from <ref type="table" target="#tab_0">Table 10</ref> show that the RefsetStabil stability achieves substantial correlation with standard stability, while in the case of AuCDC-stabil stability the correlation is almost perfect. This difference is not surprising since the RefsetStabil measure matches two models via reference topics, while the AuCDC-stabil measure compares the models directly, which leads to a better approximation of similarity. Namely, in case of the RefsetStabil measure, if two models contain an identical topic that is not in the reference set, the similarity will be negatively affected. Such a model topic not in the reference set could be either a missing conceptual topic or a commonly occurring stopwords or noisy topic. On the other hand, the AuCDC-stabil measure will successfully match the same topic occurring in models being compared.</p><p>Stability of topic models is, by definition, the property of the modeling setup to consistently produce the same topics. Since the topic models are expected to produce topics that can be interpreted as concepts, this implies that stability can be viewed as a property of models to consistently uncover the same concepts. The substantial level of correlation between the RefsetStabil measure and the standard InstanceStabil measure can be interpreted as an experimental confirmation of the previous intuitive claim. Namely, the RefsetStabil measure approximates stability with the amount of reference topics, corresponding to concepts, uncovered by each of the two distinct model instances.</p><p>The correlation between stability based on AuCDC and InstanceStabil is almost perfect. The very high level of correlation is a useful experimental finding, which we take as proof-of-concept for the application of the AuCDC measure for calculation of model stability. This finding has practical benefits, since the AuCDC-stabil stability measure is much faster to compute than the InstanceStabil stability measure. Namely, the InstanceStabil measure calculates model similarity by optimally aligning the model topics using the Hungarian algorithm <ref type="bibr" target="#b65">[66]</ref> with the computational complexity of O(T 3 ), where T is the number of model topics. On the other hand, the AuCDC-stabil measure calculates model similarity using the AuCDC measure, described in detail in Section III-B. The AuCDC measure integrates best-case matching results over a range of distance thresholds and can be computed with time complexity of O(T 2 ), which corresponds to the time necessary to calculate distances between all pairs of topics.</p><p>In practice, the calculation of AuCDC-stabil is orders of magnitude faster. For each of the two datasets, calculation of the InstanceStabil measure on 80 sets of model instances took approximately two weeks. On the other hand, calculations of the AuCDC-stabil measure were completed in under two hours. Therefore the proposed AuCDC-stabil stability measure has a potential to greatly speed up stability-based model evaluation and to make such evaluations viable for large model collections and models with a large number of topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Conclusions</head><p>The experiments in this section show that model stability is a property of topic models which is unrelated to the model's coverage of a set of reference topics. In other words, stable models that consistently uncover the same topics do not necessarily uncover all the useful topics or topics of particular interest to an analyst. This implies that optimizing topic models using the stability as the only criterion might not lead to best quality models. On the other hand, it is reasonable to expect that a topic model able to uncover the majority of topics within its reach would be stable. We believe that follow-up experiments, on new corpora and news sets of reference topics, are needed to further investigate the relationship between the stability and coverage.</p><p>We also show how the proposed coverage measures can be adapted to calculate stability. The adapted measures achieve a good correlation with the standard stability measure, support the interpretation of stability in terms of consistent uncovering of concepts, and provide a computationally efficient alternative to stability calculation. We note that, while the AuCDC-stabil and InstanceStabil measures use different algorithms for model similarity, both rely on cosine-based similarity of topic-word vectors for matching of individual topics. Therefore, in order to fully generalize the approach, experiments with the AuCDC-stabil measure variants based on other measures of topic similarity should be investigated. Such investigations would also ideally include new datasets and more topic model types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. RELATED WORK A. TOPIC MODELS</head><p>Topic models <ref type="bibr" target="#b0">[1]</ref> are unsupervised models of text capable of learning topics from large text collections. Each topic is a construct typically characterized by weighted lists of words and documents and expected to correspond to a concept occurring in texts. Topic models have numerous applications, including exploratory text analysis <ref type="bibr" target="#b1">[2]</ref>, information retrieval <ref type="bibr" target="#b2">[3]</ref>, feature extraction <ref type="bibr" target="#b66">[67]</ref>, natural language processing <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b67">[68]</ref>, and applications in computational social sciences <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b60">[61]</ref>.</p><p>Two prominent families of topic models are probabilistic models <ref type="bibr" target="#b25">[26]</ref>, such as Latent Dirichlet Allocation <ref type="bibr" target="#b0">[1]</ref>, and matrix factorization models, such as Nonnegative Matrix Factorization <ref type="bibr" target="#b20">[21]</ref>. Generative probabilistic models are a dominant approach to topic modelling. These models are based on a probabilistic process of text generation and their structure is defined in terms of a set of random variables and relations between them. There exist a variety of probabilistic model types with structure defined by random variables corresponding to various text metadata <ref type="bibr" target="#b25">[26]</ref>. Unlike the models that assume a fixed number of topics, models relying on Bayesian nonparametric inference are able to infer the number of topics from data <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>.</p><p>Models based on matrix factorization, such as latent semantic analysis <ref type="bibr" target="#b68">[69]</ref> and non-negative matrix factorization <ref type="bibr" target="#b13">[14]</ref> are a popular alternative to generative models. These models learn a set of latent factors, corresponding to topics, by approximating documentword matrix as a product of document-factor and factorword matrices. Especially the NMF model has emerged as a popular alternative to probabilistic topic models <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, and evaluation experiments suggest that its quality could be comparable to or better than the quality of the LDA model <ref type="bibr" target="#b41">[42]</ref>. As with the generative LDA model, there exist structural variations and extensions of the basic NMF model <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b70">[71]</ref>.</p><p>In recent years neural topic models based on deep neural networks emerged as a popular approach <ref type="bibr" target="#b27">[28]</ref>. Neural topic models have several appealing characteristics, including the automatization of the inference process and the ease of architectural extension, the possibility of integration with other neural architectures, and scalability <ref type="bibr" target="#b27">[28]</ref>. This makes neural topic models better suited than the conventional topic models for tasks such as text generation, document summarization, and machine translation <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. TOPIC MODEL EVALUATION</head><p>Topic models are practical since they are unsupervised and require no labeled data and minimal amount of text preprocessing. However, usefulness of topic models depends on the quality of the learned topics, which can vary and can be influenced by a multitude of factors. Namely, deployment of a topic modeling solution involves choosing the model type, model hyperparameters, learning algorithm, and the preprocessing method. In addition, once these choices are made, the process of model inference is stochastic, since the learning algorithms are initialized with random data and in many cases the learning process is based on random sampling. Automatic evaluation of topic models can be used both to choose a better topic modeling approach by narrowing down many available options, and to select model instances with high quality.</p><p>A range of methods that evaluate various aspects of model and topic quality have been developed. The earliest evaluation approach relies on measures of probabilistic fit that compute how well the learned model fits the data. The most prominent measure of that type is perplexity of held-out data <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Perplexity was used in seminal topic modeling paper <ref type="bibr" target="#b0">[1]</ref> and for many years remained a principal method for evaluation of newly proposed topic models. Another probabilistic method, proposed by <ref type="bibr" target="#b71">[72]</ref>, measures the divergence between the learned model's latent variables and empirically estimated properties of these variables.</p><p>An influential paper of <ref type="bibr" target="#b6">[7]</ref> demonstrated that lower perplexity of held out data does not neccesarily correlate with the interpretability of model topics. These findings inspired an approach focused on directly quantifying topic interpretability by calculating topic coherence <ref type="bibr" target="#b8">[9]</ref>. Measures of topic coherence compute a score that aims to approximate how interpretable a topic is in terms of its correspondence to a concept <ref type="bibr" target="#b8">[9]</ref>, and are designed to achieve high correlation with human coherence assessments <ref type="bibr" target="#b10">[11]</ref>. Coherence measures are commonly based on a score of mutual similarity between top topicrelated words, which can be defined using a variety of word representations and similarity measures <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b72">[73]</ref>- <ref type="bibr" target="#b76">[77]</ref>. Alternate approaches include clustering of word embeddings <ref type="bibr" target="#b77">[78]</ref> and querying search engines with top topic words <ref type="bibr" target="#b8">[9]</ref>. In addition to topic coherence measures, alternate approaches to calculating topic quality have been proposed, based on calculating distances between topics and uninformative probability distributions <ref type="bibr" target="#b78">[79]</ref>, and on aligning model topics with WordNet concepts <ref type="bibr" target="#b79">[80]</ref>, <ref type="bibr" target="#b80">[81]</ref>. A recent paper showed that the measures of topic coherence do not correlate well with the ability of humans to interpret and label topics <ref type="bibr" target="#b21">[22]</ref>, and that the coherence measures are not a reliable guide for model selection <ref type="bibr" target="#b21">[22]</ref>. This experiment demonstrates the need for validation of automatic measures of model quality, which is rarely performed.</p><p>The quality of topic models can also be assessed using human judgments in a structured way. <ref type="bibr" target="#b6">[7]</ref> proposed a method for scoring semantic quality of topics using crowd-sourced answers to intrusion queries. Annotators were asked to choose an irrelevant word from a set of words describing a topic, or to choose a topic irrelevant to a document <ref type="bibr" target="#b6">[7]</ref>. <ref type="bibr" target="#b11">[12]</ref> extend the method of <ref type="bibr" target="#b6">[7]</ref> by proposing new intrusion tasks for topical quality, as well as new tasks designed to measure correspondence between a topic and its conceptual label.</p><p>An approach to evaluation focused on model stability is motivated by inherent stochastic variability of learned model instances and the intuition that the consistency of learned topic is a desired property of a good model. A common approach is to quantify stability as average mutual similarity of a number of model instances <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b42">[43]</ref>. Model similarity has been calculated using topic alignment based on bipartite matching <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b42">[43]</ref> or directly comparing models using representations based on either words or documents <ref type="bibr" target="#b12">[13]</ref>. An approach to analyzing stability based on visualization of topic clusters of many model instances has been proposed in <ref type="bibr" target="#b23">[24]</ref>.</p><p>Finally, if the information derived from a topic model is used as input for solving a downstream language processing task, a natural evaluation method is to quantify how much this information improves the performance on the task in question. This approach is exemplified by applications of topic models for tasks such as information retrieval <ref type="bibr" target="#b2">[3]</ref>, word sense disambiguation <ref type="bibr" target="#b81">[82]</ref>, sentiment analysis <ref type="bibr" target="#b82">[83]</ref>, and document classification <ref type="bibr" target="#b83">[84]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. TOPIC COVERAGE</head><p>The problem of topic coverage was first outlined in an article describing a framework for visual analysis of correspondence between expert-defined reference concepts and model topics <ref type="bibr" target="#b9">[10]</ref>. The reference concepts were compiled by information visualization experts that relied on domain knowledge and an indexed database of scientific articles. Matching of concepts and model topics is performed using a model that approximates the probability that a human will judge a concept and a topic to be equivalent. Several types of relations between concepts and model topics are defined. A concept is defined as resolved if it corresponds to a single topic, as fused if it is subsumed by a topic together with another concept, and as repeated in case it corresponds to multiple topics. A concept is considered covered if it corresponds to at least one model topic, directly or as a part of a fused topic. In a series of experiments, topic model types and hyperparameters are varied. The resulting variations in relations between concepts and topics are presented by using the proposed visualizations tools. Finally, the alignment between concepts and topics is used to assess several measures of topic quality, including coherence measures. We note that while our work is focused exclusively on topic coverage, the other types of relations between reference and model topics defined in <ref type="bibr" target="#b9">[10]</ref> are useful tools for model analysis that merit further investigation.</p><p>Although there is no follow-up work to <ref type="bibr" target="#b9">[10]</ref> that focuses on topic coverage, there is work on related ideas. In <ref type="bibr" target="#b84">[85]</ref>, the authors analyze applications of topic models in social sciences, point to the problem of topic coverage, and argue that human-in-the-loop topic modelling might lead to models that best satisfy user needs.</p><p>A method of visual analysis of model stability proposed in <ref type="bibr" target="#b23">[24]</ref> is based on clustering similar topics of many models and visualizing the relation between the models and the topic clusters. Since the topic clusters can be viewed as reference topics, the visualizations in effect depict the random variations in coverage of a number of model instances.</p><p>In <ref type="bibr" target="#b85">[86]</ref> authors propose a topic model analysis based on generating synthetic texts from a set of predefined synthetic topics. From the perspective of topic coverage, such topics can be seen as reference topics, and be used for synthetic coverage experiments, possibly in conjunction with the readily deployable AuCDC measure. The use of synthetic topics could allow for large-scale analysis of numerous topic modelling scenarios without the need for manually crafting reference topics. In <ref type="bibr" target="#b85">[86]</ref>, the synthetic topics are not directly matched to model topics. Instead, the alignment between the two topic sets is computed indirectly, as the mutual information calculated on the level of words assigned to individual topics.</p><p>Experiments in <ref type="bibr" target="#b86">[87]</ref> evaluate several topic models on the tasks of topic identification and topic discovery. Topic identification is defined in terms of the ability of the model-induced document-topic vectors to serve as features for classification and regression. Topic discovery is tested by measuring the alignment between the model-induced topics and gold-standard topic labels of documents. This alignment is calculated as the similarity of the two partitions of documents, one induced by the model topics and the other induced by the gold-standard labels. This approach is similar to the one in <ref type="bibr" target="#b85">[86]</ref>, where the alignment between two topic sets is calculated indirectly, but at the word level. It would be interesting to examine how these indirect measures relate to the coverage measures that directly match model topics to the reference topics represented in terms of word and document lists.</p><p>One approach to measuring topic quality is to align model topics to ontology concepts and define the quality score of a topic in terms of topic-concept relations <ref type="bibr" target="#b79">[80]</ref>, <ref type="bibr" target="#b80">[81]</ref>. From the perspective of coverage, these techniques might prove useful for the reverse task of measuring how the model topics cover concepts in large ontologies. <ref type="bibr" target="#b87">[88]</ref> explore the similarities between model topics and the categories of the Web of Science taxonomy, and point out the problem of comparison between human-and model-generated taxonomies. We believe that one way to approach this problem is from the perspective of coverage of taxonomy concepts. The problem of coverage of abstract and broad concepts in both ontologies and taxonomies might prove interesting and challenging because of the need to conceptualize the relation between these concepts and model topics which tend to be more specific.</p><p>A big advantage of the coverage approach is its applicability for automatic analysis and validation of other measures of model quality. Namely, the amount of work on methods for topic model evaluation is modest in comparison to the amount of research on topic model architectures and applications, and the problem of semantic validation of topic models is far from solved. The automatic evaluation methods, spearheaded by popular coherence measures, are often used to compare a new topic model against a baseline model. While they may be useful for providing a proof-of-concept for new model architectures, coherence measures are not reliable tools for guiding model selection in applications that rely on topic models for text analysis <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b21">[22]</ref>. The methods based on human inspection of topics <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b11">[12]</ref> may provide more reliable assessments but they are time-consuming and rely on the availability of human annotators.</p><p>The coverage approach, unlike the measures of the abstract qualities of coherence and stability, is grounded in a set of interpretable reference topics representing a concrete application scenario of topic discovery. Furhermore, the approach has a potential to lead to creation of many evaluation datasets, each consisting of a text corpus and a set of reference topics. This would enable automatic testing of new topic models in varying topic discovery scenarios, while the measures of model quality could be tested for their ability to select highperforming models. Our research provides measures of coverage, datasets, and software tools that are a starting point for such analyses.</p><p>Experiments based on our methods confirm the previ-ously detected unreliability of the coherence measures, and demonstrate the unrelatedness of topic model stability and coverage. These findings underline the need for future work on improving and understanding the measures of model quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. CONCLUSIONS AND FUTURE WORK</head><p>Topic models are a widely used tool for text exploration, often used for topic discovery on large text collections. This paper explores an approach to topic model evaluation focused on measuring to what extent topic models cover a set of reference topics -representative set of topics of interest in a specific topic discovery scenario.</p><p>Our work revisits and extends the approach first outlined in <ref type="bibr" target="#b9">[10]</ref>, by introducing new, reliable, and practical measures of coverage and performing a series of experiments on two different text domains, news and biological. The measures we propose are the most important contribution of the paper since they make future coverage experiments more reliable and easier to perform. Our experiments lead to findings about both topic models and other methods of topic model evaluation. The findings about topic models include recommendations for the choice of models for topic discovery, the experiments showing how the number of model topics influences coverage, and the demonstration that models' coverage depends on the semantic category of reference topics. Experiments comparing topic coverage with topic coherence and model stability show that standard measures of coherence and stability fail to detect high-coverage models consistently and reliably. These experiments underline the need for re-assessment and improvement of currently popular approaches to topic model evaluation. We also show how the coverage measures can be successfully adapted to calculate model stability. Therefore, we demonstrate that these measures are useful tools for matching models and topics, with applications beyond the coverage-based evaluation.</p><p>The most applicable contributions of our work are the AuCDC measure of coverage and the recommendations for use of topic models in topic discovery. The unsupervised AuCDC measure is a new concept and a quickly deployable tool for model selection that correlates very well with the coverage measure based on supervised topic matching. The AuCDC measure is based on the coverage-distance curve, which is in itself a useful tool for graphical analysis and comparison of topic models. For example, the CD-curve can be used to assess and compare the levels of precision with which different models uncover the reference topics. In addition, the AuCDC measure has applications beyond coverage, since it can be used to assess similarity of topic model instances. Namely, the stability experiments show that the stability based on the AuCDC measure correlates almost perfectly with a standard stability measure.</p><p>As for the recommendations for the applications of topic models for topic discovery, the results of the experiments indicate that the NMF model is a very good choice, having good performance on both text domains and outperforming probabilistic models in many cases. The results of the experiments also support the use of models with a large number of topics. Such models have high coverage scores and are able to cover reference topics of all sizes. On the other hand, the smaller models have poor coverage of small reference topics that can represent useful concepts.</p><p>The development of the coverage approach is still in the early stages and there exist many directions for future research. One set of directions for future research is related to the improvement of the measures proposed in this paper. The proposed supervised coverage measure relies on a time-consuming process of topic pair labeling. We believe that active learning approaches <ref type="bibr" target="#b88">[89]</ref> have the potential to greatly speed up this process. The unsupervised AuCDC measure performs well for model ranking and selection, but it could be further improved by making the computed coverage scores interpretable.</p><p>An important future research direction is the development of methods that facilitate the construction of reference topics. Namely, reference topics are a key element of a coverage experiment, but their construction is technically challenging and time consuming. Therefore such methods would greatly facilitate the application of coverage-based evaluation in new topic modeling scenarios. In our view, a promising approach would be to focus on graphical tools that would help the analyst to either select and modify automatically generated topics, or to create new topics based on expert knowledge. Such graphical tools could include metrics and visualizations for profiling of reference topics. Tools of this kind could also facilitate the construction of incremental versions of a reference topic set. Such evolving collections of reference topics could be used in scenarios where texts and topics change over time.</p><p>Each of the coverage experiments in this article outlines a potential direction for follow-up future work. In general, similar experiments in new topic modeling settings, based on other corpora and types of models, would lead to more robust findings and recommendations. Specifically, we believe the computational social science could benefit from coverage experiments targeted at discovering topic models able to cover reference topics that correspond to concepts of interest in concrete scientific topic-discovery use cases. Construction of reference topic sets should not represent a significant overhead effort in these scenarios, since the interpretation and analysis of a number of model topics is routinely performed as part of model validation.</p><p>Experiments with coverage-based evaluation of various types of topic models may identify model architectures with consistently high performance for different corpora and reference topic sets. We hypothesize that high coverage could be achieved by approaches that rely on pooling and combining of many model instances <ref type="bibr" target="#b89">[90]</ref>, <ref type="bibr" target="#b90">[91]</ref>, models that explicitly model topic diversity <ref type="bibr" target="#b56">[57]</ref>, and models that iteratively learn new topics not uncovered by the previous runs <ref type="bibr" target="#b89">[90]</ref>. Alternative approaches to topic modeling, such as the one based on a combination of dimensionality reduction and soft clustering <ref type="bibr" target="#b91">[92]</ref> , might also achieve good coverage results.</p><p>A promising future direction is the application of the coverage methods to large scale automatic analyses of the underresearched and rarely validated measures of model quality. Such analyses could lead to better understanding and improvement of these measures. For example, it would be quite useful to find or develop coherence measures that can well approximate the coverage of specific types of reference topics. This could lead to coherence measures that are interpretable, and which could be used to approximate coverage without the need for pre-constructed reference topics.</p><p>Topic models can be applied for text classification, either as extractors of topical features <ref type="bibr" target="#b91">[92]</ref>, or as standalone classifiers <ref type="bibr" target="#b92">[93]</ref>. One interesting future work direction is the investigation of the relationship between coverage and classification accuracy. In addition to revealing the nature of this relationship, such experiments might lead to coverage-based recommendations for the use of classifiers based on topic models. A dataset for such an experiment should combine a classification dataset with reference topics, and we view the definition of classification-relevant reference topics as the main challenge. Similar experiments could be performed for applications of topic models to other language processing tasks, such as information retrieval <ref type="bibr" target="#b2">[3]</ref>, word sense disambiguation <ref type="bibr" target="#b81">[82]</ref>, and sentiment analysis <ref type="bibr" target="#b82">[83]</ref>.</p><p>Topic models have also been applied to non-text data, most notably for the analysis of natural images and genetic data <ref type="bibr" target="#b25">[26]</ref>. In these applications topics are uninterpretable and correspond to distributions over genes or low-level visual patterns. The uninterpretability of non-text topics entails two important challenges -the definition of sensible reference topics and the definition of topic matching. A wide-coverage set of topics recognized by a number of different models might be a good starting point, as might be the unsupervised AuCDC measure that avoids the topic matching problem. We believe that the adaptation of the coverage approach to non-text domains represents an interesting direction for future work with the potential to generalize the approach and make it more robust.</p><p>In this paper we propose a definition of the coverage problem motivated by the use case of topic discovery -a reference topic is considered covered if a closely matching model topic exist. We proposed measures in line with this definition and experimented with two sets of reference topics within the reach of the standard topic models. However, the measures and the reference topics, two key aspects of the coverage problem, can be viewed in a more general light. For example, in order to obtain more approximate coverage measures, the definition of topic matching could be loosened to include approximate semantic similarity. Semantic variation among the reference topics could also be factored in the measures' design. It could be quite useful to design measures that favor the models that cover a diverse set of reference topics' subcategories and offer a better overview of the semantic space.</p><p>There are many ways to define potentially useful reference topics. For example, concepts of interest in social sciences, such as the news issues and frames, could be used to define useful sets of reference topics. Reference topics could also correspond to concepts derived from a multitude of existing ontologies or taxonomies. Another possibility is to use user-defined reference topics representing domain-relevant concepts, as exemplified by <ref type="bibr" target="#b9">[10]</ref>. We note that defining of semantic reference topics is challenging, since it requires both a sensible definition of a set of concepts and a method of deciding whether an individual topic is in line with the definition. More broadly, reference topics need not even correspond to human concepts but could be synthetic, such as the topics in the experiment of <ref type="bibr" target="#b85">[86]</ref>.</p><p>Alternative approaches to the coverage problem represent a promising direction for future work. This work will have to deal with technical and conceptual challenges, such as the definition and construction of sensible reference topics, the semantics of matching between the model-generated topics and the reference topics, and the efficient construction of practical measures of coverage.</p><p>We believe that future work on topic coverage can lead to a better understanding of the semantics of machine generated topics and to improved evaluation methods with the potential to reasses the quality of existing models and guide the design of new ones. .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A CONSTRUCTION OF REFERENCE TOPICS</head><p>In this appendix to Section II-B we describe the details of the process of construction of the reference topics. The reference topics are based on models' topics in-spected, interpreted, and filtered by human annotators. More precisely, the reference topics corresponds to concepts discovered in two previous topic discovery experiments. Each of the concepts is based on human inspection of either individual model topics or topic clusters. Therefore, the reference topics are constructed from the model topics used in the previous experiments, and the methods of their construction reflect how the corresponding concepts are related to the model topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a: News reference topics</head><p>Reference topics of the news dataset were derived from the topics of LDA models built and inspected in a study focused on topical analysis of political news texts <ref type="bibr" target="#b24">[25]</ref>. Three LDA models with 50 topics and two LDA models with 100 topics were used. Model topics were inspected by humans and interpreted as concepts, referred to as themes in <ref type="bibr" target="#b24">[25]</ref>. Themes were introduced as a conceptual tool for distributed annotation of model topics by several annotators, and a single theme was allowed to correspond to more than one model topic. A shared list of themes was constructed in the process, with each theme described by a label, a short description, and a list of model topics corresponding to the theme <ref type="bibr" target="#b24">[25]</ref>. Model topics that do not correspond to any theme are thus uninterpretable topics. Each of the reference topics corresponds to one of the 133 themes from <ref type="bibr" target="#b24">[25]</ref>. The topic's word and document vectors were derived, in two steps, from the model topics corresponding to the theme. In the first step at most two corresponding model topics were selected at random, and in the second step the topics' data was improved by human effort. The goal of this improvement was to ensure that the data of a reference topic describes the corresponding theme well. Namely, in the original annotation process a model topic containing a tolerable degree of noise was allowed to be labeled as corresponding to a theme <ref type="bibr" target="#b24">[25]</ref>.</p><p>The improvement was performed by two annotators who inspected model topics associated with each reference topic. Upon inspection, they selected a subset of top topic words and documents that describe the reference topic well. Additionally, each reference topic was labeled with a preference label denoting weather the topic is better described by the words, documents, or equally well by both. This was motivated by the observation that some reference topics were clearly best described by associated words, and some by associated documents.</p><p>Finally, topic-word and topic-document vectors of a reference topic were constructed from the annotators' data using the following procedure. The reference topic's document vector is simply a binary indicator vector describing the documents associated with the topic. The reference topic's word vector is constructed from the associated words' data merged with the document-related data to reflect the preference of either word or document descriptors. First the binary bag-of-words vector vec words describing the topic's words is constructed. The document-related data is represented by the vector vec tf idf , the average of the documents' tf-idf vectors. The final word vector of the reference topic is constructed as a weighted sum w w ?vec words +d w ?vec tf idf . If the topic is best described by words, weights were set as w w = 0.8 and d w = 0.2. Otherwise, if topic is best described by documents, the weights were w w = 0.2 and d w = 0.8, and if there is no preference the weights were w w = 0.5 and d w = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b: Biological reference topics</head><p>Biological reference topics are based on the results of topic discovery performed with the goal of finding topics corresponding to phenotypes -characteristics of organisms <ref type="bibr" target="#b19">[20]</ref>. The original topic discovery process was a part of a set of machine learning methods developed with the purpose of large scale annotation of organisms with corresponding phenotypes <ref type="bibr" target="#b19">[20]</ref>.</p><p>The original topic discovery was performed by human inspection of clusters of topics of NMF models built from biological texts describing microorganisms. One NMF model with 50 topics and one NMF model with 100 topics were built for each of the five subcorpora corresponding to texts of five text sources described in Section II-A. Then the topics of the NMF models with same number of topics were clustered, using as the measure of similarity the Pearson correlation between sets of top topic words. The clusters were then filtered by retaining only the clusters containing topics from at least three out of five subcorpora, guided by the requirement that a phenotype should be consistently uncoverable across text sources. In order to increase the coverage of phenotypes, additional clusters were generated using the described procedure and new topic models were built with different random seeds. In total, five models with 50 topics and four models with 100 topics were built for each of the text sources.</p><p>The obtained clusters were represented by averaging the topic-word vectors of the cluster's topics and selecting the 20 top-weighted words from the resulting vector <ref type="bibr" target="#b19">[20]</ref>. Inspection and interpretation of these clusters was performed by a biologist who selected the high quality clusters, characterized by consistent and relevant words and corresponding to phenoptype concepts. This process resulted in a total of 112 topic clusters corresponding to phenotypes.</p><p>The reference topics of the biological dataset are derived from the described topic clusters, and their topicword and topic-document vectors are constructed in the following way. The topic-word vector of a reference topic is a binary bag-of-words vector of the corresponding cluster's words. The topic-document vector of a reference topic is constructed by averaging the topicdocument vectors of the corresponding cluster's topics. These topic-document vectors are extracted from the NMF models obtained by the original topic discovery study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B CONSTRUCTION OF TOPIC MODELS</head><p>Here we append Section II-C with details of topic models' construction.</p><p>For the construction of the probabilistic models (LDA, aLDA and PYP), we rely on the implementation of inference algorithms provided as part of the HCA package <ref type="bibr" target="#b35">[36]</ref>. This software implements the optimized variant of Gibbs sampling named table indicator sampling <ref type="bibr" target="#b93">[94]</ref>, combined with adaptive rejection sampling <ref type="bibr" target="#b94">[95]</ref> for hyperparameter learning. Following the standard procedure of <ref type="bibr" target="#b29">[30]</ref>, the hyperparameters of the LDA model defining the priors of the topic-document and topic-word distributions are set to ? = 50/T and ? = 0.01. For the aLDA model, the ? hyperparameter is also set to 0.01. Initial values of the Gamma distribution parameters defining the aLDA's prior document-topic distribution are set to a = 0.5 and b = 10, for each of the topics. For the PYP model, the initial values of the concentration and discount parameters of the Pitman-Yor process are set to c = 10 and d = 0.5. A large number of Gibbs sampling cycles is performed since in our case the goal of the learning process is the quality of learned models, not the speed of learning. After 50 warmup cycles of Gibbs sampling, another 800 cycles are run in case of the LDA and aLDA models, while in case of the PYP model with more parameters, another 1500 cycles are run.</p><p>To construct the NMF model instances, we use the method described in <ref type="bibr" target="#b40">[41]</ref>. Text documents are represented as a matrix of tf-idf document-word weights, and the matrix factorization is performed using the projected gradient method <ref type="bibr" target="#b95">[96]</ref> initialized with the results of the non-negative SVD decomposition <ref type="bibr" target="#b96">[97]</ref>. We use the implementation of the described method available as part of the scikit-learn framework <ref type="bibr" target="#b97">[98]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C BALANCING THE DATASET OF TOPIC PAIRS</head><p>Here we append Section III-A1 with details of the problem of imbalance of topic pairs and its solution.</p><p>When a subset of topic pairs is randomly sampled from a set of all possible pairs containing random model topics, a large majority of pairs will contain nonmatching topics. Namely, in an ideal scenario with two models each having T topics and where all the topics <ref type="bibr">VOLUME 4, 2016</ref> match one of T distinct concepts, the probability of match of two randomly selected topics equals 1/T . In a realistic scenario with a large number of concepts and potentially noisy topics, it is reasonable to expect that the probability of match of two topics will be below 1/T , as was confirmed by an inspection of a sample of pairs.</p><p>This means that the topic matching problem falls in the domain of imbalanced learning <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref> a setting in which only a small fraction of positive learning examples is expected in the learning data. This hinders learning of good models since examples that define the structure of the positive class are scarce. Many approaches to alleviate and solve this problems were developed <ref type="bibr" target="#b44">[45]</ref>, including active learning and resampling methods.</p><p>However, in case of the problem of topic matching there exists a simple solution -using a measure of topic distance to sample a more balanced dataset. The intuition behind the approach is that the distance between two topics is inversely correlated with the probability of their match. Therefore, if pairs of mutually close topics are sampled with the same probability as the pairs of distant topics, the final sample is expected to contain more matching topics and thus provide a better dataset for model learning.</p><p>We use the cosine distance of topic-word vectors to measure topic distance, since this measure is able to approximate the human intuition of topic similarity reasonably well <ref type="bibr" target="#b9">[10]</ref>. To create the balanced sample, the dataset of all topic pairs is partitioned into subsets corresponding to distance subintervals. Specifically, cosine distance between positive topic-word vectors ranges between 0 and 1, and we partition the [0, 1] interval into 10 subintervals of equal width. Each subset contains the pairs of topics whose mutual distance falls within the corresponding interval's boundaries. The final balanced sample of topic pairs is created by sampling the same number of pairs from each of the subsets. Inspection of a validation sample showed that it contains 36% of pairs with matching topics, as opposed to less than 1% of matches expected from a fully random sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX D DETAILS OF TOPIC PAIRS ANNOTATION</head><p>Here we append Section III-A2 by describing in detail the process of annotation of topic pairs. For the ease of reference we first repeat, in a compact form, the definition of a topic match, and the definition of the labels used to annotate topic pairs.</p><p>A topic match is defined as conceptual equality of topics -two topics are considered equal if they are interpretable as the same concepts, where the interpretation of a topic as a concept is as specific as possible. On the semantic level, we define topic equality as matching of concepts obtained by interpreting topics as specifically as possible. Matching of concepts is defined as equality or near equality of concepts, allowing small variations and similar aspects of a same concept. Stochastic differences are accounted for by labeling topics as equal but with presence of noise. This is the case when one or both topics contain a noticeable amount of noise but the topics are still interpretable and the equality of interpreted concepts exists as previously defined.</p><p>A pair of topics is labeled with 1 in case of topic equality, i.e., when concepts match without noise. A pair is labeled with 0.5 in case of a match in the presence of noise or small semantic variation, and with 0 when the concepts do not match.</p><p>News topic pairs were annotated by the authors that performed topic discovery and analysis <ref type="bibr" target="#b24">[25]</ref> on the corpus from which the news reference topics were derived, and by students of English studies acquainted with the topics of US politics. Pairs of biological topics were labeled by a biological scientist and students of senior years of biology.</p><p>Precise labeling instructions were formed, containing the previous definition of a topic match, examples of topic pairs, and clarifications of the labeling process. Annotators proceeded to annotate the previously described dataset of topic pairs containing both model and reference topics. Each topic was represented as a list of 15 top-ranked topic words and 15 top-ranked topic documents. Documents were represented as informative summaries -titles of news articles and initial fragments of original text in case of biological texts. The annotators also had access to full text of the documents.</p><p>The process of annotation was performed according to the instructions from <ref type="bibr" target="#b46">[47]</ref>. For each dataset, in each round of annotation all the topic pairs were annotated by three annotators. Annotation quality was assessed using Krippendorff's ? coefficient that measures mutual agreement of the annotators corrected for the possibility of random agreement <ref type="bibr" target="#b46">[47]</ref>. Two versions of ? coefficient were used -nominal ?, which measures strict equality of annotations, and ordinal ? based on the distance of annotations on the ordinal scale. The two versions are labeled as ? n ande ? o , respectively.</p><p>At the beginning of the annotation process, a small pilot set of 15 topic pairs was annotated by all the annotators in order to clarify the instructions. In the next step a calibration set of 50 topic pairs was annotated and the application of annotation instructions was discussed for topic pairs with large disagreement. The ? coefficients of the calibration round were calculated, yielding ? n of 0.568 and ? o of 0.831 for news topics, and ? n of 0.576 and ? o of 0.712 for biological topics. Next a test set of 50 topic pairs was annotated, yielding ? n of 0.663 and ? o of 0.862 for news topics, and ? n of 0.599 and ? o of 0.782 for biological topics. Improvements of the agreement coefficients were interpreted as a consequence of clarification of both the annotation instructions and the method of their application. Lower annotator agreement for pairs of biological topics is likely a consequence of the fact that biological topics, as opposed to news topics, correspond to more complex and abstract concepts that are harder to interpret.</p><p>After the first two annotation rounds the agreement coefficients were deemed sufficiently high for both datasets. This decision was additionally supported by the feedback from the annotators who assessed both the definitions of topic matching and the process of annotation as reasonable and comprehensible. The annotation process was continued and for each dataset another 250 topic pairs were annotated. These pairs were merged with the 50 pairs form the test set to produce the final set of 300 topic pairs, each pair annotated by three annotators.</p><p>For the final sets containing all the annotated pairs, the calculation of ? agreement coefficients yielded ? n of 0.689 and ? o of 0.865 for news topics, and ? n of 0.648 and ? o of 0.797 for biological topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX E CONSTRUCTION OF THE SUPERVISED TOPIC MATCHER</head><p>Here we append Section III-A3 with the details of the methods of feature construction and model construction. The goal of these methods is the construction of a binary classifier that predicts weather a pair of topics matches or not.</p><p>Four standard classifiers are considered: logistic regression <ref type="bibr" target="#b47">[48]</ref>, support vector machine <ref type="bibr" target="#b48">[49]</ref> with radial basis function kernel, random forest <ref type="bibr" target="#b49">[50]</ref>, and multilayer perceptron <ref type="bibr" target="#b47">[48]</ref>. We use the implementations of the models available as part of the scikit-learn framework <ref type="bibr" target="#b97">[98]</ref>. Classification models and the corresponding hyperparameters that we optimize are summarized in <ref type="table" target="#tab_0">Table 11</ref>. Other hyperparameters are set to sensible default values defined by the scikit-learn framework <ref type="bibr" target="#b97">[98]</ref>.</p><p>In order to perform supervised classification of topic pairs, each pair is represented as a vector of features. These features should contain information enabling a good approximation of semantic matching. Preliminary experiments with features constructed by concatenating topic-word and topic-document vectors of the topics in a pair resulted in a relatively low classification performance, yielding F 1 scores between 0.4 and 0.6. A plausible explanation for this result is the so called curse of dimensionality <ref type="bibr" target="#b55">[56]</ref> -degradation of classification accuracy caused by high dimensionality of feature vectors (in our case, tens of thousands) and a small number of learning examples (in our case, a few hundreds).</p><p>A possible solution for this problem is feature extraction <ref type="bibr" target="#b55">[56]</ref> -transformation of high-dimensional representations into small feature vectors containing useful information. Previous experiments with topic models show that distance measures applied to topic-word vectors can be used to approximate semantic similarity of topics <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>. Preliminary experiments with features based on various distance measures applied to topic-word and topic-document vectors showed promising results, yielding F 1 scores between 0.7 and 0.8. Therefore, we opt for this approach to feature extraction.</p><p>We base the features representing a pair of topics on the following four distance measures: cosine distance, Hellinger distance <ref type="bibr" target="#b50">[51]</ref>, L 1 distance, and L 2 distance. These four measures represent four distinct measure types: angular distance, distance between probability distributions, and two standard measures of coordinate distance L 1 and L 2 .</p><p>Before the application of a distance measure, topicword and topic-document vectors are normalized to probability distributions. This is necessary in order for the Hellinger distance to be applicable and, in the case of L 1 and L 2 , to insure the insensitivity of features to the type of topic models. Namely, probabilistic topic models produce topic-word and document-topic vectors that contain small values corresponding to probabilities, while the NMF topic model produce vectors of unbounded and potentially large positive values. Therefore, unnormalized features would result in distance variation that reflects the difference in topics' types.</p><p>The final feature representation of a topic pair is constructed by applying the previous four distance measures to both the pair of normalized topic-word vectors and to the pair of normalized topic-document vectors. This way each topic pair is represented with eight features, four based on topic-related words and four based on topic-related documents.</p><p>For each of the four classification models, we use the entire dataset of 300 labeled topic pairs to assess the performance of the model variant with optimized hyperparameters. The assessment is done using the procedure of nested five fold crossvalidation and the F 1 measure is used to measure the classification performance of the models.</p><p>When performing the standard non nested crossvalidation with K folds, model performance obtained for each combination of hyperparameters is calculated by learning the model on K ? 1 folds (distinct subsets of the learning data), and calculating the performance on the remaining fold. The final quality score is obtained as the average over all K folds. When performing nested crossvalidation, for each "outer" subset of K ? 1 TABLE 11. Supervised models used for classification of topic pairs and models' hyperparameters that are optimized in the process of model selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Hyperparameter Hyperparameter values</p><p>Logistic regression regularization constant 0.001, 0.01, 0.1, 1.0, 10, 100, 1000 regularization norm L1, L2</p><p>Multilayer perceptron hidden layer width 3, 5, 10 regularization constant 0.00001, 0.0001, 0.001, 0.01, 0.1  folds, full hyperparameter optimization is performed using non nested crossvalidation which partitions the subset into K "inner" folds. In other words, nested crossvalidation uses regular crossvalidation to assess the entire process of hyperparameter optimization, not just to assess one combination of hyperparameters. Athough computationally more expensive, nested crossvalidation gives better assessment of the quality of a model obtained by hyperparameter optimization <ref type="bibr" target="#b51">[52]</ref>. We generate crossvalidation folds using stratified sampling in order to preserve, for each fold, the ratio of class labels that is representative for the entire dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random</head><p>The described methods leads to optimized models that achieve an variations in performance that depend on the model and the dataset. The logistic regression model achieves best F 1 scores, and the classifiers' performance is close to the mutual agreement of human annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX F SUPPLEMENTARY COVERAGE-RELATED EXPERIMENTS</head><p>Here we supplement the Section IV with an analysis of models' precision and recall, and with an empirical analysis of the running time of the coverage measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. RELATIONSHIP BETWEEN MODEL PRECISION AND RECALL</head><p>We define the precision and recall of a topic model in terms of the relevant topics (topics matching the reference topics) retrieved by the model. A topic model's recall -fraction of the reference topics retrieved by the model -is equal to the model's coverage. Model precision is the fraction of the relevant model topicsmodel topics that match the reference topics. If more than one model topic matches the same reference topic, only one model topic is counted as relevant. However, such redundancy does not occur in our experiments -for each topic model instance a retrieved reference topic is always matched by a single model topic. This might seem counterintuitive since it is, at least in our experience, not unusual that a model contains mutually similar topics. The explanation is that our supervised matcher, described in Section III-A, is built to match only highly similar topics.</p><p>In this experiment we analyze the precision and recall of topic models analyzed in the coverage experiments of Section IV. The details of these models of various types and sizes are described in Section II-C. Similarly as in the coverage experiments, for each combination of a model type and a number of topics precision and recall scores of the 10 distinct model instances are averaged.</p><p>The results are shown in <ref type="figure" target="#fig_5">Figure 4</ref> and <ref type="figure" target="#fig_6">Figure 5</ref>. Relation between precision and recall depends on both the dataset and the model type. One might expect that as the recall (coverage) rises with the increase of the model size, the precision (the proportion of the relevant model topics) will decline. However, this tradeoff occurs only in some cases, and it does not entail a large loss of <ref type="bibr">VOLUME 4, 2016</ref> precision. For most topic model types, the increase in the number of topics is related to an insignificant decrease or even to a small increase in precision. On the biological dataset, in most cases the precision remains stable as the recall increases. The tradeoff is most noticeable in case of the NMF model on the news dataset. However, even in this case there is no drastic loss of precision -the NMF model with 200 topics more than doubles the recall of the NMF model with 50 topics, while the corresponding loss in precision is only 35%.</p><p>The results of this experiment support the use of models with a larger number of topics, which is in line with the experiments in Sections IV and V. Namely, larger models offer a significant increase in coverage (recall), which rarely comes at a price of a noticeable loss of precision. The NMF model is better then the probabilistic models in terms of precision as well as in terms of recall. In other words, the NMF instances will expectedly contain more relevant topics, which should lead to quicker topic discovery. This is in line with the previous recommendations for the use of the NMF model from Section IV.</p><p>While the larger models do not suffer a large loss of precision, the absolute number of their topics outside the reference set is higher then in the case of smaller models. Therefore an analyst might perceive larger models as less useful. This observation is in line with the recommendation form Section V that tools that speed up the process of topic inspection should be used in conjunction with large models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. RUNNING TIME OF THE COVERAGE MEASURES</head><p>Time complexity of the coverage measures influences the scalability of the coverage experiments. Asymptotic complexity of the proposed coverage measures is analyzed in Section III. In this section we perform an empirical analysis of the measures' running time. For each of the datasets, we timed the calculation of the coverage measures on the set of 100 topic models described in Section II-C.</p><p>In terms of the number of reference topics R, the number of model topics T , the vocabulary size V , and the corpus size D, the complexity of the supervised SupCov measure is O(RT (V + D)), while the complexity of the unsupervised AuCDC measure is O(RT V ). The asymptotic complexities might wrongly suggest similar running times, especially since for both datasets the vocabulary size is very close to the corpus size. However, the results, displayed in <ref type="table" target="#tab_0">Table 12</ref>, show that in practice the unsupervised AuCDC measure is two orders of magnitude faster then the supervised SupCov measure. This is caused by the fact that the time required to process a pair of topics differs greatly between the measures. For the SupCov measure, the processing of a topic pair requires the calculation of eight distance-based features and the computation of the supervised model's output. In contrast, in case of the AuCDC measure the only operation required is the calculation of cosine distance. Additionally, for the AuCDC measure, the distances between all topic pairs are pre-computed using matrix-level computation. This is more efficient than calculating vector distances for each pair of topics.</p><p>The results show that the AuCDC measure is timeefficient and well suited for large experiments. However, the SupCov measure could be optimized by using less features or a more efficient supervised model, assuming that this would not cause a degradation in accuracy. Another possible optimization is to pre-compute the distance-based features using matrix-level operations. Additionally, the computation of both measures could be parallelized by distributing the topic models across the available processor cores. He also worked as a Software Engineer, and as a Teacher at both the high school and the university level. His research interests include text mining, topic modeling, text analysis for computational social science, and data compression.</p><p>STRAHIL RISTOV was born in Zagreb, Croatia, in 1959. He received his B.S. degree in electrical engineering and M.S. and Ph.D. degrees in computer science from the University of Zagreb in 1997.</p><p>Since 1990, he has been a Researcher at the Department of Electronics, Ruder Bo?kovi? Institute in Zagreb and currently holds the position of Senior Associate Scientist. He is the head of the Laboratory for Information and Signal Processing. He is the author or coauthor of 30 journal or conference papers. His research interests include string algorithms, data compression, and algorithms in bioinformatics and population genetics. Since 2001 he has been a Researcher at the Department of Electronics, Microelectronics, Computer and Intelligent Systems at the Faculty of Electrical Engineering and Computing, University of Zagreb, where he currently holds the position of an Associate Professor. He is the author or coauthor of over 100 journal or conference papers. His research interests include natural language processing, with a focus on information extraction and text analysis for computational social science.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>50 FIGURE 1 .</head><label>501</label><figDesc>Coverage-distance curves depicting coverage of reference topics by the LDA and NMF models with 50 topics, for the collection of news texts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 2 .</head><label>2</label><figDesc>Coverage-distance curves depicting coverage of news reference topics by the topic models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIGURE 3 .</head><label>3</label><figDesc>Coverage-distance curves depicting coverage of biological reference topics by the topic models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>forest number of trees 10 ,</head><label>10</label><figDesc>20, 50, 100 number of features 2, 50%, all features maximum tree depth 2, 3, unlimited Support vector machine regularization constant 0.001, 0.01, 0.1, 1.0, 10, 100, 1000 radial basis function ? 0.001, 0.01, 0.1, 1.0, 10, 100, 1000</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FIGURE 4 .</head><label>4</label><figDesc>Models' precision (the fraction of model topics in the reference set) and recall (the fraction of reference topics covered), on the news dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FIGURE 5 .</head><label>5</label><figDesc>F 1 score of approximately 0.8, with 0Models' precision (the fraction of model topics in the reference set) and recall (the fraction of reference topics covered), on the biological dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>JELENA</head><label></label><figDesc>REPAR was born in Croatia, in 1982. She received the B.S./M.S. degree in molecular biology (2006.) and the Ph.D. degree (2012) in biology from the University of Zagreb. From 2006 to 2012 she worked as a Research Assistant, and from 2012 to 2014 as a Senior Research Assistant at the Division of Molecular Biology, Ruder Bo?kovi? Institute, Zagreb, Croatia. From 2014 to 2017 she worked as a Postdoctoral Researcher at MRC London Institute of Medical Sciences, Imperial College London, UK. Since 2018 she has been an Associate Scientist at the Division of Molecular Biology, Ruder Bo?kovi? Institute, Zagreb, Croatia. Her research interests include microbes, DNA repair, genomics and computational biology. JAN ?NAJDER was born in Zagreb, Croatia, in 1977. He received his B.S. degree in computing in 2001 and M.S. and Ph.D. degrees in computer science from the University of Zagreb, in 2006 and 2010, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 .</head><label>1</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 .</head><label>2</label><figDesc>Performance of human annotators and supervised models on the task of matching topic pairs. Both the average scores and the standard deviations are displayed.</figDesc><table><row><cell></cell><cell cols="2">News dataset</cell><cell cols="2">Biological dataset</cell></row><row><cell></cell><cell>F1</cell><cell>st. dev.</cell><cell>F1</cell><cell>st. dev.</cell></row><row><cell>human annotators</cell><cell>0.854</cell><cell>0.034</cell><cell>0.817</cell><cell>0.032</cell></row><row><cell>Logistic regression</cell><cell>0.835</cell><cell>0.079</cell><cell>0.787</cell><cell>0.080</cell></row><row><cell cols="2">Support vector machine 0.830</cell><cell>0.077</cell><cell>0.784</cell><cell>0.080</cell></row><row><cell>Random forest</cell><cell>0.804</cell><cell>0.110</cell><cell>0.776</cell><cell>0.057</cell></row><row><cell>Multilayer perceptron</cell><cell>0.814</cell><cell>0.106</cell><cell>0.723</cell><cell>0.062</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3 .</head><label>3</label><figDesc>Spearman and Pearson correlations between the AuCDC measure and both the SupCov measure and its variant without cosine features. For each correlation coefficient, a 95% bootstrap confidence interval is shown. .95, 0.98] 0.96 [0.95, 0.97] 0.95 [0.90, 0.97] 0.95 [0.94, 0.96] SupCov-nocos 0.96 [0.94, 0.97] 0.93 [0.91, 0.95] 0.95 [0.90, 0.96] 0.96 [0.95, 0.97]</figDesc><table><row><cell></cell><cell></cell><cell>News dataset</cell><cell cols="2">Biological dataset</cell></row><row><cell></cell><cell>Spearman</cell><cell>Pearson</cell><cell>Spearman</cell><cell>Pearson</cell></row><row><cell>SupCov</cell><cell>0.97 [0</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4 .</head><label>4</label><figDesc>Coverage of reference topics by topic models of various types and sizes, measured by the SupCov and AuCDC measures. For each measure and dataset, the best score and the best scores for each number of topics are indicated. For each coverage score, a 95% bootstrap confidence interval is shown. .40 [0.39, 0.41] 0.56 [0.56, 0.56] 0.22 [0.21, 0.23] 0.54 [0.53, 0.54] NMF-200 0.54 [0.53, 0.55] 0.65 [0.64, 0.65] 0.44 [0.43, 0.45] 0.67 [0.67, 0.68] PYP 0.64 [0.62, 0.65] 0.65 [0.64, 0.65] 0.23 [0.22, 0.24] 0.56 [0.56, 0.56] based on supervised matching of topics. The result of the supervised coverage is simply the proportion of reference topics covered, i.e., matched by at least one model topic. The unsupervised AuCDC measure is designed to approximate SupCov for the purpose of ranking and selection of top models and is based on the CD-curve which can serve as a standalone graphical tool for analysis and comparison of model coverage. For each of the measures and for each combination of a model type and a number of topics, coverages of the 10 topic model instances are calculated and averaged to achieve more robust approximations. The 95% bootstrap confidence intervals of the coverage means are calculated using the percentile method and 20.000 bootstrap samples. Coverage results are shown in the</figDesc><table><row><cell></cell><cell></cell><cell>News dataset</cell><cell></cell><cell>Biological dataset</cell></row><row><cell></cell><cell>SupCov</cell><cell>AuCDC</cell><cell>SupCov</cell><cell>AuCDC</cell></row><row><cell>LDA-50</cell><cell cols="4">0.14 [0.13, 0.15] 0.41 [0.41, 0.41] 0.01 [0.00, 0.01] 0.31 [0.30, 0.31]</cell></row><row><cell>LDA-100</cell><cell cols="4">0.31 [0.30, 0.33] 0.51 [0.50, 0.51] 0.07 [0.06, 0.08] 0.40 [0.40, 0.41]</cell></row><row><cell>LDA-200</cell><cell cols="4">0.47 [0.46, 0.48] 0.60 [0.59, 0.60] 0.16 [0.15, 0.17] 0.50 [0.50, 0.51]</cell></row><row><cell>aLDA-50</cell><cell cols="4">0.12 [0.10, 0.13] 0.40 [0.40, 0.41] 0.01 [0.01, 0.01] 0.31 [0.31, 0.32]</cell></row><row><cell cols="5">aLDA-100 0.27 [0.25, 0.28] 0.51 [0.51, 0.51] 0.07 [0.06, 0.08] 0.41 [0.41, 0.42]</cell></row><row><cell cols="5">aLDA-200 0.42 [0.40, 0.44] 0.60 [0.60, 0.61] 0.15 [0.14, 0.16] 0.52 [0.51, 0.52]</cell></row><row><cell>NMF-50</cell><cell cols="4">0.22 [0.22, 0.23] 0.43 [0.43, 0.44] 0.11 [0.11, 0.12] 0.39 [0.39, 0.40]</cell></row><row><cell cols="2">NMF-100 0</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5 .</head><label>5</label><figDesc>Division of reference topics into quartiles by size. Topic size is defined as the number of documents containing a topic. Each quartile is described by the defining range of documents sizes and by the number of topics it contains.</figDesc><table><row><cell></cell><cell></cell><cell>Q1</cell><cell>Q2</cell><cell>Q3</cell><cell>Q4</cell><cell>all topics</cell></row><row><cell>News dataset</cell><cell>size</cell><cell cols="5">0-67 67-112 112-167 167-437 0-437</cell></row><row><cell></cell><cell cols="2">num. topics 34</cell><cell>33</cell><cell>33</cell><cell>33</cell><cell>133</cell></row><row><cell cols="2">Biological dataset size</cell><cell>0-9</cell><cell>9-20</cell><cell>20-42</cell><cell>42-216</cell><cell>0-216</cell></row><row><cell></cell><cell cols="2">num. topics 32</cell><cell>26</cell><cell>27</cell><cell>27</cell><cell>112</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 6 .</head><label>6</label><figDesc>Coverage of size categories of reference topics, for both datasets. Size categories are defined as topic size quartiles, and topic size is defined as the number of documents containing the topic. For each coverage score, a 95% bootstrap confidence interval is shown. 0.01 [0.00, 0.02] 0.05 [0.03, 0.07] 0.06 [0.05, 0.07] 0.14 [0.12, 0.17] 0.07 aLDA-200 0.06 [0.05, 0.07] 0.14 [0.12, 0.16] 0.18 [0.15, 0.21] 0.22 [0.20, 0.25] 0.15 NMF-50 0.03 [0.03, 0.03] 0.03 [0.03, 0.04] 0.10 [0.08, 0.11] 0.33 [0.31, 0.35] 0.11 NMF-100 0.04 [0.02, 0.06] 0.17 [0.16, 0.19] 0.26 [0.25, 0.28] 0.40 [0.39, 0.41] 0.22 NMF-200 0.33 [0.30, 0.36] 0.52 [0.50, 0.53] 0.44 [0.41, 0.45] 0.44 [0.43, 0.46] 0.44 PYP</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>News dataset</cell><cell></cell></row><row><cell></cell><cell>Q1</cell><cell>Q2</cell><cell>Q3</cell><cell>Q4</cell><cell>All topics</cell></row><row><cell>LDA-50</cell><cell cols="5">0.00 [0.00, 0.00] 0.01 [0.00, 0.02] 0.14 [0.11, 0.16] 0.40 [0.37, 0.42] 0.14</cell></row><row><cell>LDA-100</cell><cell cols="5">0.06 [0.05, 0.08] 0.14 [0.11, 0.15] 0.45 [0.41, 0.50] 0.57 [0.55, 0.60] 0.31</cell></row><row><cell>LDA-200</cell><cell cols="5">0.34 [0.31, 0.37] 0.40 [0.39, 0.42] 0.64 [0.61, 0.66] 0.49 [0.47, 0.51] 0.47</cell></row><row><cell>aLDA-50</cell><cell cols="5">0.00 [0.00, 0.00] 0.01 [0.00, 0.02] 0.11 [0.09, 0.14] 0.32 [0.29, 0.35] 0.12</cell></row><row><cell cols="6">aLDA-100 0.08 [0.06, 0.10] 0.13 [0.11, 0.15] 0.38 [0.33, 0.42] 0.45 [0.41, 0.48] 0.27</cell></row><row><cell cols="6">aLDA-200 0.34 [0.30, 0.38] 0.34 [0.32, 0.36] 0.60 [0.57, 0.63] 0.39 [0.36, 0.42] 0.42</cell></row><row><cell>NMF-50</cell><cell cols="5">0.06 [0.06, 0.06] 0.04 [0.03, 0.05] 0.34 [0.32, 0.37] 0.42 [0.39, 0.44] 0.22</cell></row><row><cell cols="6">NMF-100 0.15 [0.14, 0.17] 0.30 [0.27, 0.32] 0.65 [0.63, 0.67] 0.48 [0.47, 0.49] 0.40</cell></row><row><cell cols="6">NMF-200 0.50 [0.48, 0.52] 0.54 [0.52, 0.55] 0.73 [0.71, 0.75] 0.40 [0.38, 0.41] 0.54</cell></row><row><cell>PYP</cell><cell cols="5">0.65 [0.61, 0.68] 0.55 [0.52, 0.57] 0.79 [0.76, 0.81] 0.56 [0.52, 0.59] 0.64</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Biological dataset</cell><cell></cell></row><row><cell></cell><cell>Q1</cell><cell>Q2</cell><cell>Q3</cell><cell>Q4</cell><cell>All topics</cell></row><row><cell>LDA-50</cell><cell cols="5">0.00 [0.00, 0.00] 0.00 [0.00, 0.00] 0.01 [0.00, 0.03] 0.02 [0.01, 0.03] 0.01</cell></row><row><cell>LDA-100</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>0.01 [0.00, 0.02] 0.03 [0.03, 0.04] 0.11 [0.10, 0.12] 0.13 [0.09, 0.16] 0.07 LDA-200 0.04 [0.02, 0.06] 0.14 [0.12, 0.16] 0.19 [0.16, 0.22] 0.26 [0.22, 0.31] 0.16 aLDA-50 0.00 [0.00, 0.00] 0.00 [0.00, 0.01] 0.00 [0.00, 0.00] 0.03 [0.02, 0.05] 0.01 aLDA-1000.13 [0.11, 0.15] 0.21 [0.19, 0.23] 0.28 [0.25, 0.31] 0.31 [0.29, 0.33] 0.23 other topic model types should be performed.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 7 .</head><label>7</label><figDesc>Coverage, by models of varying types and sizes, of news topics divided into semantic categories according to two criteria: topic abstractness and correspondence of a topic to an issue. For each coverage score, a 95% bootstrap confidence interval is shown. .20 [0.18, 0.23] 0.32 [0.30, 0.35] 0.45 [0.42, 0.47] 0.15 [0.14, 0.17] 0.27 aLDA-200 0.30 [0.28, 0.32] 0.54 [0.52, 0.57] 0.56 [0.54, 0.59] 0.33 [0.30, 0.36] 0.42</figDesc><table><row><cell></cell><cell>Abstract</cell><cell>Concrete</cell><cell>Issue</cell><cell>Non-issue</cell><cell>All topics</cell></row><row><cell>LDA-50</cell><cell cols="5">0.13 [0.11, 0.15] 0.15 [0.14, 0.16] 0.22 [0.21, 0.24] 0.09 [0.08, 0.10] 0.14</cell></row><row><cell>LDA-100</cell><cell cols="5">0.26 [0.23, 0.29] 0.37 [0.35, 0.39] 0.48 [0.44, 0.52] 0.21 [0.20, 0.23] 0.31</cell></row><row><cell>LDA-200</cell><cell cols="5">0.32 [0.30, 0.34] 0.62 [0.59, 0.64] 0.61 [0.58, 0.64] 0.39 [0.37, 0.40] 0.47</cell></row><row><cell>aLDA-50</cell><cell cols="5">0.10 [0.09, 0.12] 0.13 [0.11, 0.14] 0.19 [0.17, 0.21] 0.07 [0.06, 0.08] 0.12</cell></row><row><cell cols="6">aLDA-100 0NMF-50 0.13 [0.11, 0.14] 0.32 [0.31, 0.33] 0.32 [0.31, 0.33] 0.17 [0.16, 0.17] 0.22</cell></row><row><cell cols="6">NMF-100 0.26 [0.25, 0.27] 0.54 [0.53, 0.55] 0.53 [0.52, 0.55] 0.32 [0.31, 0.33] 0.40</cell></row><row><cell cols="6">NMF-200 0.37 [0.35, 0.38] 0.71 [0.70, 0.72] 0.63 [0.61, 0.64] 0.49 [0.48, 0.50] 0.54</cell></row><row><cell>PYP</cell><cell cols="5">0.47 [0.44, 0.49] 0.80 [0.77, 0.83] 0.78 [0.75, 0.80] 0.55 [0.53, 0.57] 0.64</cell></row></table><note>an issue or a non-issue 2 topic. A topic was consid- ered abstract if it could be interpreted as an abstract concept, and it was considered concrete if it could be interpreted as either a person, a country, an organization, or an event. A topic was defined as corresponding to an issue if it was strongly related to an important social or political issue. Table 1 contains interpretable model topics representative of the reference topics.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 8 .</head><label>8</label><figDesc>Spearman rank correlations between coherence and coverage measures, calculated on 13.500 topics for each of the datasets. For each correlation coefficient, a 95% bootstrap confidence interval is shown.</figDesc><table><row><cell></cell><cell>News dataset</cell><cell></cell><cell cols="2">Biological dataset</cell></row><row><cell>Coherence</cell><cell>SupCov</cell><cell>AuCDC</cell><cell>SupCov</cell><cell>AuCDC</cell></row><row><cell>NPMI-wiki</cell><cell cols="4">-0.06 [-0.08, -0.05] 0.02 [0.00, 0.04] -0.05 [-0.07, -0.04] 0.18 [0.16, 0.19]</cell></row><row><cell cols="5">NPMI-corpus 0.29 [0.28, 0.31] 0.48 [0.46, 0.49] -0.05 [-0.07, -0.03] 0.10 [0.08, 0.12]</cell></row><row><cell>CP-wiki</cell><cell cols="4">-0.03 [-0.05, -0.01] 0.10 [0.08, 0.12] -0.16 [-0.18, -0.15] 0.00 [-0.02, 0.02]</cell></row><row><cell>CP-corpus</cell><cell cols="4">0.35 [0.33, 0.36] 0.50 [0.48, 0.51] -0.02 [-0.04, 0.00] 0.19 [0.17, 0.21]</cell></row><row><cell>CV-wiki</cell><cell cols="4">0.14 [0.13, 0.16] 0.22 [0.20, 0.24] -0.16 [-0.18, -0.14] -0.10 [-0.12, -0.08]</cell></row><row><cell>CV-corpus</cell><cell cols="3">0.23 [0.22, 0.25] 0.19 [0.18, 0.21] 0.27 [0.25, 0.28]</cell><cell>0.20 [0.19, 0.22]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 9 .</head><label>9</label><figDesc>Spearman correlations between the coverage and stability measures, as well as between each of the measures and the number of model topics. Correlations are calculated on 80 sets of 10 model instances. For each correlation coefficient, a 95% bootstrap confidence interval is shown. T is varied between the values of 100 and 500 in steps of 100, yielding five size variants. The final extended model set contains, for each of the two datasets, 80 sets of 10 model instances. Each instance set represents a specific model type and size, and contains instances built using different random seeds.</figDesc><table><row><cell></cell><cell cols="2">News dataset</cell><cell cols="2">Biological dataset</cell></row><row><cell></cell><cell>InstanceStabil</cell><cell>num. topics</cell><cell>InstanceStabil</cell><cell>num. topics</cell></row><row><cell>SupCov</cell><cell cols="4">-0.09 [-0.32, 0.14] 0.57 [0.34, 0.74] 0.10 [-0.17, 0.33] 0.74 [0.59, 0.85]</cell></row><row><cell>AuCDC</cell><cell cols="4">-0.26 [-0.50, -0.00] 0.79 [0.63, 0.89] 0.01 [-0.27, 0.25] 0.79 [0.66, 0.89]</cell></row><row><cell>InstanceStabil</cell><cell></cell><cell>-0.54 [-0.71, -0.33]</cell><cell></cell><cell>-0.53 [-0.71, -0.33]</cell></row><row><cell>number of topics</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 10 .</head><label>10</label><figDesc>Spearman correlation between the coverage-based stability measures and the InstanceStabil measure. For each correlation coefficient, a 95% bootstrap confidence interval is shown.</figDesc><table><row><cell></cell><cell>News dataset</cell><cell>Biological dataset</cell></row><row><cell>RefsetStabil</cell><cell cols="2">0.69 [0.51, 0.83] 0.69 [0.48, 0.84]</cell></row><row><cell cols="3">AuCDC-stabil 0.99 [0.98, 1.00] 1.00 [0.99, 1.00]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 12 .</head><label>12</label><figDesc>Running time (in seconds) of the supervised SupCov measure and the unsupervised AuCDC measure -time required for the processing of the entire set of 100 topic models, average time per model topic, and average time per model. The experiment was performed using a 2.4 GHz Intel i7 processor. dataset Biological dataset all models topic avg. model avg. all models topic avg. model avg. DAMIR KOREN?I? was born in Zagreb, Croatia, in 1983. He received the B.S./M.S. degree in mathematics in 2008 and the Ph.D. degree in computer science in 2019, both from the University of Zagreb. From 2010 to 2016 he was a Research Assistant at the Ruder Bo?kovi? Institute, and from 2018 to 2019 he was a Research Assistant at the Faculty of Electrical Engineering and Computing, University of Zagreb. Since 2019 he has been a Postdoctoral Researcher at the Department of Electronics, Ruder Bo?kovi? Institute in Zagreb, Croatia.</figDesc><table><row><cell cols="3">News SupCov 4707.43</cell><cell>0.349</cell><cell>47.07</cell><cell>1149.31</cell><cell>0.0851</cell><cell>11,49</cell></row><row><cell>AuCDC</cell><cell>39.10</cell><cell cols="2">0.0029</cell><cell>0.391</cell><cell>8.64</cell><cell>0.00064</cell><cell>0.0864</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">   VOLUME 4, 2016   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">VOLUME 4, 2016 Koren?i? et al.: A Topic Coverage Approach to Evaluation of Topic Models</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">   VOLUME 4, 2016   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">VOLUME 4, 2016 Koren?i? et al.: A Topic Coverage Approach to Evaluation of Topic Models</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">   VOLUME 4, 2016   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">VOLUME 4, 2016 Koren?i? et al.: A Topic Coverage Approach to Evaluation of Topic Models 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">   VOLUME 4, 2016   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We slightly abuse the language semantics and use the term "nonissue topic" to denote the topic that does not correspond to a news issue, not a topic of little or no importance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18">VOLUME 4, 2016 Koren?i? et al.: A Topic Coverage Approach to Evaluation of Topic Models</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20">VOLUME 4, 2016 Koren?i? et al.: A Topic Coverage Approach to Evaluation of Topic Models</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22">   VOLUME 4, 2016   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="24">VOLUME 4, 2016 Koren?i? et al.: A Topic Coverage Approach to Evaluation of Topic Models</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="26">VOLUME 4, 2016 Koren?i? et al.: A Topic Coverage Approach to Evaluation of Topic Models</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="28">VOLUME 4, 2016 Koren?i? et al.: A Topic Coverage Approach to Evaluation of Topic Models</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="30">   VOLUME 4, 2016   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">VOLUME 4, 2016   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">VOLUME 4, 2016 Koren?i? et al.: A Topic Coverage Approach to Evaluation of Topic Models</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="38">   VOLUME 4, 2016   </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We would like to thank Maria Brbi? for the patient help with the technical details related to the data from <ref type="bibr" target="#b19">[20]</ref>. We would also like to thank Mladen Karan for the help with the server used to conduct part of the experiments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Interpretation and trust: Designing model-driven visualizations for text analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="443" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lda-based document models for ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 29th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A topic model for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1024" to="1033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Computational Approach to Qualitative Analysis in Large Textual Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Evaluation methods for topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mimno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1105" to="1112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reading tea leaves: How humans interpret topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 22nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="288" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evaluating topic models with stability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>De Waal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th Annual Symposium of the Pattern Recognition Association of South Africa</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic evaluation of topic coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="100" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Topic model diagnostics: Assessing domain relevance via topical alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on machine learning (ICML-13)</title>
		<meeting>the 30th International Conference on machine learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="612" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploring the space of topic coherence measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>R?der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Both</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hinneburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth ACM international conference on Web search and data mining</title>
		<meeting>the eighth ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="399" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inferring concepts from topics: Towards procedures for validating topics as measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Montgomery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Society for Political Methodology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>PolMeth XXXVI</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stability of topic modeling via matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Namee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Greene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="159" to="169" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning the parts of objects by nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="issue">6755</biblScope>
			<biblScope unit="page">788</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Elevated threat levels and decreased expectations: How democracy handles terrorist threats</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bonilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Poetics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="650" to="669" />
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Quantitative analysis of large amounts of journalistic texts using topic modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jacobi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Atteveldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Welbers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Journalism</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="106" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">How to analyze political attention with minimal assumptions and costs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Colaresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Crespin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Political Science</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="209" to="228" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Bayesian Hierarchical Topic Model for Political Texts: Measuring Expressed Agendas in Senate Press Releases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mining the posterior cingulate: segregation between memory and pain components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">?</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balslev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="520" to="532" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The landscape of microbial phenotypic traits and associated genes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brbi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pi?korec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vidulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kri?ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>?muc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Supek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning topic models-going beyond svd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Topic model or topic twaddle? reevaluating semantic interpretability measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buntine</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2021.naacl-main.300" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="3824" to="3848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation: stability and applications to studies of user-generated content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koltcov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koltsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nikolenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ACM conference on Web science</title>
		<meeting>the 2014 ACM conference on Web science</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="161" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">TopicCheck: Interactive alignment for assessing topic model stability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tingley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="175" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Getting the agenda right: measuring media agenda using topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koren?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ristov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>?najder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Workshop on Topic Models: Post-Processing and Applications</title>
		<meeting>the 2015 Workshop on Topic Models: Post-Processing and Applications</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="61" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Probabilistic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="77" to="84" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conic scan-and-cover algorithms for nonparametric topic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yurochkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">L</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems, ser. NIPS&apos;17</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems, ser. NIPS&apos;17</meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3881" to="3890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Topic modelling meets deep neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buntine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00498</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="267" to="297" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National academy of Sciences</title>
		<meeting>the National academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploiting affinities between topic modeling and the sociological perspective on culture: Application to newspaper coverage of u.s. government arts funding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dimaggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Poetics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="570" to="606" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>topic Models and the Cultural Sciences</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Topic Modeling for Media and Communication Research: A Short Primer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puschmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Scheffler</surname></persName>
		</author>
		<ptr target="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2836478" />
		<imprint>
			<date type="published" when="2016-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Applying lda topic modeling in communication research: Toward a valid and reliable methodology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waldherr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Miltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niekler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Keinert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfetsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Reber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>H?ussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communication Methods and Measures</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="93" to="118" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rethinking LDA: Why Priors Matter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1973" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Peacock: Learning Long-Tail Topic Features for Industrial Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2015-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Experiments with non-parametric topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hierarchical Dirichlet Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">476</biblScope>
			<biblScope unit="page" from="1566" to="1581" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Probability</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="855" to="900" />
			<date type="published" when="1997-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A computational analysis of agenda setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Companion Publication of the 23rd International Conference on World Wide Web Companion</title>
		<meeting>the Companion Publication of the 23rd International Conference on World Wide Web Companion</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="323" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Utopian: User-driven topic modeling based on interactive nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unveiling the political agenda of the european parliament plenary: A topical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Cross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Science Conference</title>
		<meeting>the ACM Web Science Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An analysis of the coherence of descriptors in topic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>O&amp;apos;callaghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="5645" to="5657" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">How many topics? stability analysis for topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>O&amp;apos;callaghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery</title>
		<editor>Databases, T. Calders, F. Esposito, E. H?llermeier, and R. Meo</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="498" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Increasing topic coherence by aggregating topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Blair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Mulvenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Science, Engineering and Management</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A Survey of Predictive Modeling on Imbalanced Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Branco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="50" />
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data: open challenges and future directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krawczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="221" to="232" />
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Content Analysis: An Introduction to Its Methodology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Krippendorff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-04" />
			<publisher>SAGE Publications, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Machine Learning: A Probabilistic Perspective (Adaptive Computation and Machine Learning series)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-08" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1995-09" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Random Forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001-10" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Bhattacharyya and Expected Likelihood Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jebara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning Theory and Kernel Machines</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="57" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Cawley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L C</forename><surname>Talbot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2079" to="2107" />
			<date type="published" when="2010-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Auc: a statistically consistent and more discriminating measure than accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Joint Conference on Artificial intelligence</title>
		<meeting>the 18th International Joint Conference on Artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="519" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Comparing twitter and traditional media using topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-P</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on information retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="338" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Navigating the local modes of big data: The case of topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tingley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Social Science</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="51" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Introduction to Data Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-N</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005-05" />
			<publisher>Pearson</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Diversifying Restricted Boltzmann Machine for Document Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-08" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Document-based topic coherence measures for news media text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koren?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ristov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>?najder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="357" to="373" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Agenda setting and active audiences in online coverage of human trafficking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Papadouka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Evangelopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ignatow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information, Communication &amp; Society</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="655" to="672" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The agenda-setting function of mass media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Mccombs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Public Opinion Quarterly</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="176" to="187" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Policy diffusion: The issuedefinition stage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gilardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Shipan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wueest</surname></persName>
		</author>
		<ptr target="https://fabriziogilardi.org/resources/papers/diffusion-policy-perceptions.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Topic modeling on historical newspapers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-I</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Torget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LaTeCH &apos;11 Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Evaluating topic coherence using distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Aletras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013</title>
		<meeting>the 10th International Conference on Computational Semantics (IWCS 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="13" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A probabilistic theory of coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fitelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analysis</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">279</biblScope>
			<biblScope unit="page" from="194" to="199" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Handbook of latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">427</biblScope>
			<biblScope unit="page" from="424" to="440" />
		</imprint>
	</monogr>
	<note>Probabilistic topic models</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval research logistics quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Short text classification improved by learning multi-granularity topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Second International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1776" to="1781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Joint sentiment/topic model for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM conference on Information and knowledge management</title>
		<meeting>the 18th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="375" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American society for information science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">391</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization: A comprehensive review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1336" to="1353" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Mining Temporal Discriminant Frames via Joint Matrix Factorization: A Case Study of Illegal Immigration in the U.S. News Media,&quot; in Knowledge Science, Engineering and Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="260" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Bayesian checking for topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="227" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Optimizing semantic coherence in topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Talley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leenders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="262" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="530" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Evaluating topic coherence measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rosner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hinneburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>R?der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nettling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Both</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1403.6397" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">6397</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Topic modelling for qualitative studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Nikolenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koltcov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koltsova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Information Science</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="102" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Topic quality metrics based on distributed word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Nikolenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1029" to="1032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Measuring topic coherence through optimal word buckets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ramrakhiyani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pawar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hingmire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Palshikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="437" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Topic significance ranking of lda generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Alsumait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barbar?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gentle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domeniconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="67" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Improving topic evaluation using conceptual knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Velcin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Trausan-Matu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-A</forename><surname>Rizoiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1866" to="1871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">A semantic based approach for topic evaluation in information filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="66" to="977" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Probabalistic walks in semantic hierarchies as a topic model for WSD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">A joint model of text and aspect ratings for sentiment summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="308" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Exploring topic coherence over many models and many topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Andrzejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Buttler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="952" to="961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Computer-assisted content analysis: Topic models for exploring multiple subjective interpretations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tingley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Poursabzi-Sangdeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Findlater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems Workshop on Human-Propelled Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">A new evaluation framework for topic modeling algorithms based on synthetic corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gerlach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Diersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A N</forename><surname>Amaral</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1901.09848" />
		<imprint>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Topic identification and discovery on text and speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wintrode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Durme</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D15-1285" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="2377" to="2387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Topic modelling in social sciences: Case study of web of science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Pandur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dob?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kronegger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Active learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison Department of Computer Sciences</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Localized userdriven topic discovery via boosted ensemble of nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="503" to="531" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Ensemble topic modeling using weighted term co-associations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Greene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page">113709</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Topic modeling technique for text mining over biomedical text corpora through hybrid inverse documents frequency and fuzzy k-means clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Irtaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Nisar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gardezi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">80</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Labeled lda: A supervised topic model for credit attribution in multi-labeled corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 conference on empirical methods in natural language processing</title>
		<meeting>the 2009 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Sampling Table Configurations for the Hierarchical Poisson-Dirichlet Process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buntine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases. Springer Berlin Heidelberg</title>
		<imprint>
			<date type="published" when="2011-09" />
			<biblScope unit="page" from="296" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Adaptive Rejection Sampling for Gibbs Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Gilks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wild</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series C (Applied Statistics)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="337" to="348" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Projected gradient methods for nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2756" to="2779" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">SVD based initialization: A head start for nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Boutsidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gallopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1350" to="1362" />
			<date type="published" when="2008-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Api design for machine learning software: experiences from the scikit-learn project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Buitinck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Louppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grobler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.0238</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
